}
}
# Download every valid URL identified above:
download.urls = function(pdf.url, prescript){
iter <- 1
for(article.url in pdf.url){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
JournalEconLit.raw = GET("https://www.aeaweb.org/", path = "journals/jel/issues")
volume.urls        = get.volume.url(JournalEconLit.raw)
article.urls       = get.art.url(volume.urls)
get.art.url = function(vol.url){
for(vol.page in vol.url){
## e.g. Vol. 54 No. 2 June 2016
getID   = GET(vol.page)
getID   = htmlParse(getID)
IDs     = xpathSApply(getID, "//article/@id")
pdf.url = c(pdf.url, paste0("http://pubs.aeaweb.org/doi/pdfplus/", IDs))
}
return(pdf.url)
}
article.urls       = get.art.url(volume.urls)
article.urls[1:627] == article.urls[628:1254]
length(unique(article.urls))
download.urls(article.urls, prescript = "JournalEcLit_TEST_")
AmerEconRev.raw    = GET("https://www.aeaweb.org/", path = "journals/aer/issues")
volume.urls        = get.volume.url(AmerEconRev.raw)
article.urls       = get.art.url(volume.urls)
str(article.urls)
length(unique(volume.urls))
article.urls[100:150]
??koRpus
rm(list = ls())
library(httr)
library(rvest)
library(XML)
#### Functions:
# Get the URL to the complete volume overview:
get.volume.url = function(fromget){
fromget    = htmlParse(fromget)
links      =  xpathSApply(fromget, "//a/@href")
inds       = grepl(pattern = "/issues//*", x = links)
links      = links[inds]
return(paste0("https://www.aeaweb.org", links))
}
# Get the URL of every article in the list of volume links
get.art.url = function(vol.url){
pdf.url <- c()
for(vol.page in vol.url){
## e.g. Vol. 54 No. 2 June 2016
getID   = GET(vol.page)
getID   = htmlParse(getID)
IDs     = xpathSApply(getID, "//article/@id")
pdf.url = c(pdf.url, paste0("http://pubs.aeaweb.org/doi/pdfplus/", IDs))
}
return(pdf.url)
}
# Download every valid URL identified above:
download.urls = function(pdf.url, prescript){
iter <- 1
for(article.url in pdf.url){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
AmerEconRev.raw    = GET("https://www.aeaweb.org/", path = "journals/aer/issues")
volume.urls        = get.volume.url(AmerEconRev.raw)
article.urls       = get.art.url(volume.urls)
Sys.time()
start <- Sys.time()
download.urls(article.urls, prescript = "AmerEconRev")
warnings()
AmerEconJour_Macro = GET("https://www.aeaweb.org/", path = "journals/mac/issues")
volume.urls        = get.volume.url(AmerEconJour_Macro)
article.urls       = get.art.url(volume.urls)
tobeconverted <- list.files("Users/Ken/Q-Kolleg/PDF_2_TXT", pattern = "*.pdf")
tobeconverted <- list.files("/Users/Ken/Q-Kolleg/PDF_2_TXT", pattern = "*.pdf")
already.converted <- list.files("/Users/Ken/Q-Kolleg/StemTFIDF_Articles/", "*.txt")
already.converted <- substr(already.converted, 1, 8)
pdfcheck <- (substr(tobeconverted, 1, 8) %in% already.converted
pdfcheck <- (substr(tobeconverted, 1, 8) %in% already.converted
)
substr(tobeconverted, 1, 8)
(substr(tobeconverted, 1, 8) %in% already.converted)
pdfcheck <- (substr(tobeconverted, 1, 8) %in% already.converted)
pdfcheck <- !(substr(tobeconverted, 1, 8) %in% already.converted)
converthis <- tobeconverted[pdfcheck]
pdf2txt.path <- file.choose()
lapply(converthis, function(i){
system(paste('\"', pdf2txt.path, "\"", paste0(' "', i, '"'), sep = ""), wait = F)})
getwd()
mytxts = list.files(pattern = "*.txt")
mypdfs = list.files(pattern = "*.pdf")
succesful.convert = (substr(mypdfs, 0, nchar(mypdfs)-4) %in% substr(mytxts, 0, nchar(mytxts)-4))
converthis[1:6
]
pdf2move <- converthis
txt2move <- substr(converthis, nchar(converthis)-3, nchar(converthis))
txt2move <- substr(converthis, 1, nchar(converthis)-3)
txt2move <- substr(converthis, 1, nchar(converthis)-4)
txt2move <- paste0(txt2move, ".txt")
pdf2move
getwd()
pdf2move <- paste0(getwd(), "/", converthis)
pdf2move[1:5]
destination.txt <- paste0("Users/Ken/Q-Kolleg/StemTFIDF_Articles/", txt2move)
file.rename(txt2move, destination.txt)
warnings()
txt2move <- paste0(getwd(), "/", txt2move, ".txt")
file.rename(txt2move, destination.txt)
warnings()
txt2move <- substr(converthis, 1, nchar(converthis)-4)
txt2move <- paste0(getwd(), "/", txt2move, ".txt")
file.rename(txt2move, destination.txt)
warnings()
txt2move[1:5]
txt2move[1:5] %in% list.files()
getwd()
txt2move[1:5] %in% list.files(full.names = T)
list.files(full.names = T)
txt2move <- substr(converthis, 1, nchar(converthis)-4)
txt2move <- paste0("./", txt2move, ".txt")
file.rename(txt2move, destination.txt)
warnings()
txt2move[1:5] %in% list.files(full.name = T)
txt2move <- substr(converthis, 1, nchar(converthis)-4)
fullpath2move <- paste0("./", txt2move, ".txt")
destination.txt <- paste0("/Users/Ken/Q-Kolleg/StemTFIDF_Articles/", txt2move)
destination.txt <- paste0("/Users/Ken/Q-Kolleg/StemTFIDF_Articles/", txt2move, ".txt")
file.rename(fullpath2move, destination.txt)
rm(list = ls())
library(tm)
sparse_dtm = function(corpus, ...){
DocumentTermMatrix(corpus, control = list(...))
}
small_dtm = function(corpus, threshold = 0.99, ...){
removeSparseTerms(sparse_dtm(corpus, ...), sparse = threshold)
}
mytxts = list.files("StemTFIDF_Articles", pattern = "*.txt",
full.names = T)
getwd()
setwd("/Users/Ken/Q-Kolleg")
mytxts = list.files("StemTFIDF_Articles", pattern = "*.txt",
full.names = T)
abstractlist = lapply(mytxts, function(textdoc){
paste0(scan(textdoc, what = character(), fileEncoding = "latin1"),
collapse = " ")})
readthis <- function(text, header = TRUE, ...){
read.table(textConnection(text), header = header, ...)
}
session.size <- function(top = 10){
sort( sapply( ls( envir = globalenv()), function(x){
format(object.size(get(x)), units = "Mb")
}
), decreasing = T)[1:top]
}
session.size()
corpus = Corpus(VectorSource(abstractlist))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeWords, c(stopwords("english"),
"/f", "use", "can", "one", "will"))
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, stemDocument, "english")
TF = small_dtm(corpus, threshold = 0.995, weighting = weightTf)
save(TF, file = "TopicModelling_Articles/Articles_TFIDF2.Rdata")
Sys.getenv("HOME")
".Rprofile" %in% Sys.getenv("HOME")
".Rprofile" %in% list.files(Sys.getenv("HOME"))
list.files(Sys.getenv("HOME"))
?list.filles
?list.files
list.files(Sys.getenv("HOME"), all.files = T)
getwd()
setwd("PDF_2_TXT")
list.files(pattern = "*AmerEconRev*")
length(list.files(pattern = "*AmerEconRev*"))
library(httr)
library(rvest)
library(XML)
get.volume.url = function(fromget){
fromget    = htmlParse(fromget)
links      =  xpathSApply(fromget, "//a/@href")
inds       = grepl(pattern = "/issues//*", x = links)
links      = links[inds]
return(paste0("https://www.aeaweb.org", links))
}
get.art.url = function(vol.url){
pdf.url <- c()
for(vol.page in vol.url){
## e.g. Vol. 54 No. 2 June 2016
getID   = GET(vol.page)
getID   = htmlParse(getID)
IDs     = xpathSApply(getID, "//article/@id")
pdf.url = c(pdf.url, paste0("http://pubs.aeaweb.org/doi/pdfplus/", IDs))
}
return(pdf.url)
}
download.urls = function(pdf.url, prescript){
iter <- 1
for(article.url in pdf.url){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
AmerEconRev.raw    = GET("https://www.aeaweb.org/", path = "journals/aer/issues")
volume.urls        = get.volume.url(AmerEconRev.raw)
article.urls       = get.art.url(volume.urls)
download.urls = function(pdf.url, prescript){
iter <- 757
for(article.url in pdf.url){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
download.urls(article.urls, prescript = "AmerEconRev")
i
iter
length(list.files(pattern = "*AmerEconRev*"))
download.urls = function(pdf.url, prescript){
iter <- 1293
for(article.url in pdf.url){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
download.urls(article.urls, prescript = "AmerEconRev")
warnings()
download.urls = function(pdf.url, prescript){
iter <- 1293
for(article.url in pdf.url){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
download.urls(article.urls, prescript = "AmerEconRev")
download.urls = function(pdf.url, prescript){
iter <- 1551
start = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
for(article.url in pdf.url){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") < 60
if(keepthisloop){
stop()
}
keepthisloop
while(keepthisloop){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
}
download.urls(article.urls, prescript = "AmerEconRev")
start = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") < 60
if(keepthisloop){
stop()
}
keepthisloop = difftime(now, start, units = "min") > 60
if(keepthisloop){
stop()
}
download.urls = function(pdf.url, prescript){
iter <- 1551
start = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
for(article.url in pdf.url){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") > 60
if(keepthisloop){
stop()
}
keepthisloop
while(keepthisloop){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
}
AmerEconRev.raw    = GET("https://www.aeaweb.org/", path = "journals/aer/issues")
download.urls(article.urls, prescript = "AmerEconRev")
article.urls
download.urls = function(pdf.url, prescript){
iter <- 1551
start = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
for(article.url in pdf.url){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") > 60
try(if(keepthisloop){
stop()
})
keepthisloop
while(keepthisloop){
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
}
AmerEconRev.raw    = GET("https://www.aeaweb.org/", path = "journals/aer/issues")
download.urls(article.urls, prescript = "AmerEconRev")
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") > 60
try(if(keepthisloop){
stop()
})
for (i in 1:10)
{
for (j in 1:10)
{
for (k in 1:10)
{
cat(i," ",j," ",k,"\n")
if (k ==5) break
}
}
}
download.urls = function(pdf.url, prescript){
iter <- 1551
start = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
for(article.url in pdf.url){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") > 60
if(keepthisloop) break
tryCatch({download.file(article.url, paste0(prescript, "_", iter, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
download.urls(article.urls, prescript = "AmerEconRev")
getwd()
length(list.files(getwd(), pattern = "*AmerEconRev*"))
file.size(getwd())
sizes <- sapply(getwd(), file.size)
length(sizes)
sizes
amer.econ.docs <- list.files(getwd(), pattern = "*AmerEconRev*")
sizes <- sapply(amer.econ.docs, file.size)
sizes
length(sizes)
length(unique(sizes))
download.urls = function(pdf.url, prescript, start = 750){
iter <- 750
start = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
### Probably wrong since 750 (same articles saved in different name)
for(i in 1:length(pdf.url){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") > 60
if(keepthisloop) break
tryCatch({download.file(pdf.url[i+start], paste0(prescript, "_", iter+start, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter <- iter + 1
}
}
download.urls = function(pdf.url, prescript, start = 750){
iter = 1
start = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
### Probably wrong since 750 (same articles saved in different name)
for(i in 1:(length(pdf.url)-start)){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, start, units = "min") > 60
if(keepthisloop) break
tryCatch({download.file(pdf.url[i+start], paste0(prescript, "_", iter+start, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter = iter + 1
}
}
download.urls = function(pdf.url, prescript, start = 750){
iter = 1
start.time = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
### Probably wrong since 750 (same articles saved in different name)
for(i in 1:(length(pdf.url)-start)){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, starttime, units = "min") > 60
if(keepthisloop) break
tryCatch({download.file(pdf.url[i+start], paste0(prescript, "_", iter+start, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter = iter + 1
}
}
# Download every valid URL identified above:
# "pdf.url" is a character list with the URL of pdf-articles
# "prescript" is the prescript for the local document name to be saved to
# Download gets interrupted by ip-block after "maxtime" minutes passed
# After interruption, first pause and then continue download
#    at url number "start".
download.urls = function(pdf.url, prescript, start = 750, maxtime = 60){
iter = 1
start.time = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
for(i in 1:(length(pdf.url)-start)){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, starttime, units = "min") > maxtime
if(keepthisloop) break
tryCatch({download.file(pdf.url[i+start], paste0(prescript, "_", iter+start, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter = iter + 1
}
}
rm(list = ls())
library(httr)
library(rvest)
library(XML)
#### Functions:
# Get the URL to the complete volume overview:
get.volume.url = function(fromget){
fromget    = htmlParse(fromget)
links      =  xpathSApply(fromget, "//a/@href")
inds       = grepl(pattern = "/issues//*", x = links)
links      = links[inds]
return(paste0("https://www.aeaweb.org", links))
}
# Get the URL of every article in the list of volume links
get.art.url = function(vol.url){
pdf.url <- c()
for(vol.page in vol.url){
## e.g. Vol. 54 No. 2 June 2016
getID   = GET(vol.page)
getID   = htmlParse(getID)
IDs     = xpathSApply(getID, "//article/@id")
pdf.url = c(pdf.url, paste0("http://pubs.aeaweb.org/doi/pdfplus/", IDs))
}
return(pdf.url)
}
# Download every valid URL identified above:
# "pdf.url" is a character list with the URL of pdf-articles
# "prescript" is the prescript for the local document name to be saved to
# Download gets interrupted by ip-block after "maxtime" minutes passed
# After interruption, first pause and then continue download
#    at url number "start".
download.urls = function(pdf.url, prescript, start = 750, maxtime = 60){
iter = 1
start.time = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
for(i in 1:(length(pdf.url)-start)){
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
keepthisloop = difftime(now, starttime, units = "min") > maxtime
if(keepthisloop) break
tryCatch({download.file(pdf.url[i+start], paste0(prescript, "_", iter+start, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
iter = iter + 1
}
}
####### Download the articles:
setwd("PDF_2_TXT")
getwd()
AmerEconRev.raw    = GET("https://www.aeaweb.org/", path = "journals/aer/issues")
volume.urls        = get.volume.url(AmerEconRev.raw)
article.urls       = get.art.url(volume.urls)
download.urls(article.urls, prescript = "AmerEconRev",
start = 750, maxtime = 75)
download.urls = function(pdf.url, prescript, start = 750, maxtime = 60){
# Gives the starting time in minutes
start.time = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
for(i in 1:(length(pdf.url)-start)){
# Give the current time in minutes
now = strptime(format(Sys.time(), "%X"), "%H:%M:%S")
# Check whether difference between start.time and now is smaller than maxtime
# if so, then break the loop.
keepthisloop = difftime(now, start.time, units = "min") > maxtime
if(keepthisloop){
last.doc.nr = i + start
return(last.doc.nr)
break
}
tryCatch({download.file(pdf.url[i+start], paste0(prescript, "_", i+start, ".pdf"))},
error = function(e){cat("ERROR: ", conditionMessage(e), "\n")})
}
}
download.urls(article.urls, prescript = "AmerEconRev",
start = 750, maxtime = 75)
download.urls(article.urls, prescript = "AmerEconRev",
start = 1144, maxtime = 75)
download.urls(article.urls, prescript = "AmerEconRev",
start = 1148, maxtime = 45)
download.urls(article.urls, prescript = "AmerEconRev",
start = 1427, maxtime = 45)
warnings()
tobeconverted <- list.files("/Users/Ken/Q-Kolleg/PDF_2_TXT", pattern = "*.pdf")
already.converted <- list.files("/Users/Ken/Q-Kolleg/StemTFIDF_Articles/", "*.txt")
already.converted <- substr(already.converted, nchar(already.converted)-3, nchar(already.converted))
head(already.converted)
already.converted <- substr(already.converted, 1, nchar(already.converted)-3)
head(already.converted)
already.converted <- list.files("/Users/Ken/Q-Kolleg/StemTFIDF_Articles/", "*.txt")
already.converted <- substr(already.converted, 1, nchar(already.converted)-3)
head(already.converted)
already.converted <- list.files("/Users/Ken/Q-Kolleg/StemTFIDF_Articles/", "*.txt")
already.converted <- substr(already.converted, 1, nchar(already.converted)-4)
pdfcheck <- !(substr(tobeconverted, 1, nchar(tobeconverted)-4) %in% already.converted)
converthis <- tobeconverted[pdfcheck]
length(converthis)
pdf2txt.path <- file.choose()
getwd()
lapply(converthis, function(i){
system(paste('\"', pdf2txt.path, "\"", paste0(' "', i, '"'), sep = ""), wait = F)})
## Little trickery to get the correct .txt files moved:
pdf2move <- converthis
txt2move <- substr(converthis, 1, nchar(converthis)-4)
fullpath2move <- paste0("./", txt2move, ".txt")
pdf2move <- paste0(getwd(), "/", converthis)
destination.txt <- paste0("/Users/Ken/Q-Kolleg/StemTFIDF_Articles/", txt2move, ".txt")
file.rename(fullpath2move, destination.txt)
