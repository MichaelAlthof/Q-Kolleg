BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2009-028
Optimal Smoothing for a Computationally and
Statistically Efficient Single Index Estimator
Yingcun Xia* Wolfgang H‰rdle**
Oliver Linton***
* National University of Singapore, Singapore ** Humboldt-Universit‰t zu Berlin, Germany *** London School of Economics, United Kingdom
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Optimal Smoothing for a Computationally and Statistically Efficient Single Index Estimator 
Yingcun Xia
Department of Statistics and Applied Probability National University of Singapore
Wolfgang H®ardle
CASE - Center for Applied Statistics & Economics Institut fu®r Statistik und O®konometrie Wirtschaftswissenschaftliche Fakult®at Humboldt-Universit®at zu Berlin D - 10178 Berlin, Germany
Oliver Linton
Department of Economics, London School of Economics, Houghton Street,
London WC2A 2AE, United Kingdom.
May 7, 2009
Abstract
In semiparametric models it is a common approach to under-smooth the nonparametric functions in order that estimators of the finite dimensional parameters can achieve root-n consistency. The requirement of under-smoothing may result as we show from inefficient estimation methods or technical difficulties. Based on local linear kernel smoother, we propose an estimation method to estimate the single-index model without under-smoothing. Under some conditions, our estimator of the single-index is asymptotically normal and most efficient in the semi-parametric sense. Moreover, we derive higher expansions for our estimator and use them to define an optimal bandwidth for the purposes of index estimation. As a result we obtain a practically more relevant method and we show its superior performance in a variety of applications.
The first author is most grateful to Professor V. Spokoiny for helpful discussions and NUS FRG R-155-000-048-112 and the Alexander von Humboldt Foundation for financial support. The second author thanks the Deutsche Forschungsgemeinschaft SFB 649 "O® kouomisches Risiko" for financial support. The third author thanks the ESRC for financial support.
1

Key words and phrases: ADE; Asymptotics; Bandwidth; MAVE method; Semi-parametric efficiency. JEL classification: C00; C13; C14

1 Introduction

Single index models (SIMs) are widely used in the applied quantitative sciences. Although the context of applications for SIMs almost never prescribes the functional or distributional form of the involved statistical error, the SIM is commonly fitted with (low dimensional) likelihood principles. Both from a theoretical and practical point of view such fitting approach has been criticized and has led to semiparametric modelling. This approach involves high dimensional parameters (nonparametric functions) and a finite dimensional index parameter. Consider the following single-index model,

Y = g(0 X) + ,

(1)

where E(|X) = 0 almost surely, g is an unknown link function, and 0 is a single-index parameter with length one and first element positive for identification. In this model there is a single linear combination of covariates X that can capture most information about the relation between response variable Y and covariates X, thereby avoiding the "curse of dimensionality". Estimation of the single-index model is very attractive both in theory and in practice. In the last decade a series of papers has considered estimation of the parametric index and the nonparametric part with focus on root-n estimability and efficiency issues, see Carroll, Fan, Gijbels and Wand (1997) for an overview. There are numerous methods proposed or can be used for the estimation of the model. Amongst them, the most popular ones are the average derivative estimation (ADE) method investigated by H®ardle and Stoker (1989), the sliced inverse regression (SIR) method proposed by Li (1989); the semiparametric least squares (SLS) method of Ichimura (1993) and the simultaneous minimization method of H®ardle, Hall and Ichimura (1993).
The existing estimation methods are all subject to some or other of the following four critiques: (1) Heavy computational burden: see, for example, H®ardle, Hall and Ichimura (1993), Delecroix, H®ardle and Hristache (2003), Xia and Li (1999) and Xia et al. (1999). These methods include complicated optimization techniques (iteration between bandwidth choice and parameter estimation) for which no simple and effective algorithm is available up to now. (2) Strong restrictions on link functions or design of covariates X: Li (1991) required the covariate to have a symmetric distribution; H®ardle and Stoker (1989) and Hristache

2

et al. (2001) needed a non-symmetric structure for the link function, i.e., |Eg (0 X)| is bounded away from 0. If these conditions are violated, the corresponding methods are inconsistent. (3) Inefficiency: The ADE method of H®ardle and Stoker (1989) or the improved ADE method of Hristache et al. (2001) is not asymptotically efficient in the semi-parametric sense, Bickel et al. (1993). Nishiyama and Robinson (2000, 2005) considered the Edgeworth correction to the ADE methods. H®ardle and Tsybakov (1993) discussed the sensitivity of the ADE. Since this method involves high dimensional smoothing and derivative estimation, its higher order properties are poor. (4) Under-smoothing: Let hgopt be the optimal bandwidth in the sense of MISE for the estimation of link function g and let h be the bandwidth used for the estimation of 0. Most of the methods mentioned above require the bandwidth h to be much smaller than the bandwidth hogpt, i.e. h/hogpt  0 as n  , in order that estimators of 0 can achieve root-n consistency, see, H®ardle, and Stoker (1989) and Hristache et al. (2002), Robinson (1988), Hall (1989) and Carroll et al. (1997) among others. Due to technical complexities, there are few investigations about how to select the bandwidth h for the estimation of the single-index. Thus it could be the case that even if h = hgopt allows for root-n consistent estimation of , that hopt/hogpt  0 or hogpt/hopt  0, where hopt is the optimal bandwidth for estimation of . This would mean that using a single bandwidth hogpt would result in suboptimal performance for the estimator of . Higher order properties of other semiparametric procedures have been studied in Linton (1995) inter alia.
Because the estimation of 0 is based on the estimation of the link function g, we might expect that a good bandwidth for the link function should be a good bandwidth for the single-index, i.e., under-smoothing should be unnecessary. Unfortunately, most of the existing estimation methods involve for technical reason "under-smoothing" the link function in order to obtain a root-n consistent estimator of 0. See, for example, H®ardle and Stoker (1989), Hristache et al. (2001, 2002), Carroll et al. (1997) and Xia and Li (1999). H®ardle, Hall and Ichimura (1993) investigated this problem for the first time and proved that the optimal bandwidth for the estimation of the link function in the sense of MISE can be used for the estimation of the single-index to achieve root-n consistency. As mentioned above, for its computational complexity the method of H®ardle, Hall and Ichimura (1993) is hard to implement in practice.
This paper presents a method of joint estimation of the parametric and nonparametric parts. It avoids undersmoothing and the computational complexity of former procedures and achieves the semiparametric efficiency bound. It is based on the MAVE method of Xia et al (2002), which we outline in the next section. Using local linear approximation and global minimization, we give a very simple iterative algorithm. The
3

proposed method has the following advantages: (i) the algorithm only involves one-dimensional smoothing and is proved to converge at a geometric rate; (ii) with normal errors in the model, the estimator of 0 is asymptotically normal and efficient in the semiparametric sense; (iii) the optimal bandwidth for the estimation of the link function in the sense of MISE can be used to estimate 0 with root-n consistency; (iv) by a second order expansion, we further show that the optimal bandwidth for the estimation of the single-index 0, hopt, is of the same magnitude as hogpt.
Therefore, the commonly used "under-smoothing" approach is inefficient in the sense of second order approximation. Powell and Stoker (1996) investigated bandwidth selection for the ADE methods. We also propose an automatic bandwidth selection method for our estimator of . Xia (2006) has recently shown the first order asymptotic properties of this method. Our theoretical results are proven under weak moment conditions.
In section 3 we present our main results. We show the speed of convergence, give the asymptotic estimation and derive a smoothing parameter selection procedure. In the following section we investigate the proposed estimator in simulation and application. Technical details are deferred to the appendix.

2 The MAVE method

Suppose that {Xi, Yi : i = 1, 2, . . . , n} is a random sample from model (1). The basic idea of our estimation

method is to linearly approximate the smooth link function g and to estimate 0 by minimizing the overall

approximation errors. Xia et al (2002) proposed a procedure via the so called minimum average conditional

variance estimation (MAVE). The single index model (1) is a special case of what they considered, and we

can estimate it as follows. Assuming function g and parameter 0 are known, then the Taylor expansion of

g(0 Xi) at g(0 x) is

g(0 Xi)  a + d0 (Xi - x),

where a = g(0 x) and d = g (0 x). With fixed , the local estimator of the conditional variance is then

n

n2 (x|)

=

ma,idn{nf^ (x)}-1

[Yi
i=1

-

{a

+

d

(Xi - x)}]2Kh{

(Xi - x)},

where f^(x) = n-1

n i=1

Kh{

(Xi - x)}, where K is a univariate density function, h is the bandwidth and

Kh(u) = K(u/h)/h; see Fan et al (1996). The value n2(x|) can also be understood as the local departure

of Yi with Xi close to x from a local linear model with given . Obviously, the best approximation of 

4

should minimize the overall departure at all x = Xj, j = 1, ∑ ∑ ∑ , n. Thus, our estimator of 0 is to minimize

n
Qn() = n2(Xj|)
j=1

(2)

with respect to  : || = 1. This is the so-called minimum average conditional variance estimation (MAVE)

in Xia et al (2002). In practice it is necessary to include some trimming in covariate regions where density is low, so we weight n2(Xj|) by a sequence ^j, where ^j = n{f^(Xj)}, that is discussed further below.
The corresponding algorithm can be stated as follows. Suppose 1 is an initial estimate of 0. Set the number iteration  = 1 and bandwidth h1. We also set a final bandwidth h. Let Xij = Xi - Xj.

Step 1: With bandwidth h , calculate f^(Xj) = n-1

n i=1

Kh

(

Xij) and the solutions of aj and dj to the

inner problem in (2)

aj dj h

=

n1

Kh ( Xij )
i=1

 Xij/h

1  Xij/h

-1 n
Kh ( Xij )
i=1

1  Xij/h

Yi.

Step 2: Fix the weight Kh ( Xij), f(Xj), aj and dj. Calculate the solution of  to (2)

nn

={

Kh ( Xij)^j {d(Xj)}2XijXij f^( Xj)}-1

Kh ( Xij)^jd(Xj)Xij(yi-aj)/f^( Xj),

i,j=1

i,j=1

where ^j = n{f^(Xj)}. 
Step 3: Set  =  + 1,  := /|| and h := max{h, h / 2}, go to Step 1.

Repeat steps 1 and 2 until convergence.

The iteration can be stopped by the common rule. For example, if the calculated 's are stable at a

certain direction, we can stop the iteration. The final vector  := /|| is the MAVE estimator of 0, denoted by ^. Note that these steps are an explicit algorithm of the Xia et al (2002) method for the single-index

model with some version of what the called `refined kernel weighting' and boundary trimming. Similar to

the other direct estimation methods, the calculation above is easy to implement. See Horowitz and H®ardle

(1996) for more discussions. After  is estimated, the link function can be then estimated by the local linear smoother as g^(v), where

n
g^(v) = [n{s2(v)s0(v) - (s1(v))2}]-1 {s2(v) - s1(v)( Xi - v)/h }Kh ( Xi - v)Yi,
i=1

(3)

5

and sk (v) = n-1

n i=1

Kh

(

Xi - v){(

Xi - v)/h }k for k = 0, 1, 2.

Actually, g^^(v) is the final value of

aj in Step 1 with  Xj replaced by v.

In the algorithm, n(.) is a trimming function employed to handle the boundary points. There are many

choices for the estimator to achieve the root-n consistency; see e.g. H®ardle and Stocker (1989) and HHI

(1993). However, to achieve the efficiency bound, n(v) must tend to 1 for all v. In this paper, we take

n(v) as a bounded function with third order derivatives on R such that n(v) = 1 if v > 2c0n- ; n(v) = 0

if v  c0n- for some constants  > 0 and c0 > 0. As an example, we can take



 1,

n(v) = 

exp{(2c0

exp{(2c0n- -v)-1} n- -v)-1}+exp{(v-c0n-

)-1

}

,

0,

if v  2c0n- , if 2c0n- > v > c0n- , if v  c0n- .

(4)

The choice of  will be given below.

3 Main Results
We impose the following conditions to obtain the asymptotics of the estimators.
[(C1)] [Initial estimator] The initial estimator is in n = { : | - 0|  n-} for some 0 <   1/2.
[(C2)] [Design] The density function f(v) of  X and its derivatives up to 6th order are bounded on R for all   n, E|X|6 <  and E|Y |3 < . Furthermore, supvR,n |f(v) - f0(v)|  c| - 0| for some constant c > 0.
[(C3)] [Link function] The conditional mean g(v) = E(Y | X = v), E(X| X = v), E(XX | X = v) and their derivatives up to 6th order are bounded for all  : | - 0| <  where  > 0.
[(C4)] [Kernel function] K(v) is a symmetric density function with finite moments of all orders.
[(C5)] [Bandwidth and trimming parameter] Trimming parameter   1/20 and bandwidth h  n- for some  with 1/5 -    1/5 + for some > 0.
Assumption (C1) is feasible because such an initial estimate is obtainable using existing methods, such as H®ardle and Stoker (1989), Powell et al. (1989) and Horowitz and H®ardle (1996). Actually, H®ardle, Hall and Ichimura (1993) even assumed that the initial value is in a root-n neighborhood of 0, { : |-0|  C0n-1/2}. Assumption (C2) means that X may have discrete components providing that  X is continuous for  in a
6

small neighborhood of 0; see also Ichimura (1993). The moment requirement on X is not strong. H®ardle, Hall and Ichimura (1993) obtained their estimator in a bounded area of Rp, which is equivalent to assume that X is bounded; see also H®ardle and Stoker (1989). We impose slightly higher order moment requirement than second moment for Y to ensure the optimal bandwidth in (C5) can be used in applying Lemma 6.1 in section 6. The smoothness requirements on the link function in (C3) can be relaxed to the existence of a bounded second order derivative at the cost of more complicated proofs and smaller bandwidth. Assumption (C4) includes the Gaussian kernel and the quadratic kernel. Assumption (C5) includes the commonly used optimal bandwidth in both the estimation of the link function and the estimation of the index 0. Actually, imposing these constraints on the bandwidth is for ease of exposition in the proofs.
Let µ(x) = E(X| X =  x), (x) = µ(x)-x, w(x) = E(XX | X =  x), W0(x) = 0(x)0 (x). Let A+ denote the Moore-Penrose inverse of a symmetric matrix A. Recall that K is a symmetric density function. Thus, K(v)dv = 1 and vK(v)dv = 0. For ease of exposition, we further assume that µ2 =
v2K(v)dv = 1. Otherwise, we can redefine K(v) := µ21/2K(µ21/2v). We have the following asymptotic results for the estimators.

Theorem 3.1 (Speed of algorithm) Let  be the value calculated in Step 3 after  iterations. Suppose assumptions (C1)-(C5) hold. If h  0 and | - 0|/h2  0, we have

 +1

-

0

=

1 2

{(I

-

00

)

+

o(1)}(

-

0)

+

21 n Nn

+

O(n2 h4 )

almost surely, where Nn = [E{g (0 X)2W0(X)}]+n-1/2

n i=1

g

(0

Xi)0 (Xi)i

=

Op(n-1/2).

Theorem 3.1 indicates that the algorithm converges at a geometric rate, i.e. after each iteration, the estimation error reduces by half approximately. By Theorem 3.1 and the bandwidth requirement in the algorithm, we have

|+1 - 0|

=

{

1 2

+

o(1)}| +1

-

0|

+

O(n-1/2

+

n2 h4 ).

Starting with |1 -0| = Cn-, in order to achieve root-n consistency, say |k -0|  cn-1/2 i.e. 2-kCn-  cn-1/2, the number of iterations k can be calculated roughly by

k

=

{(

1 2

-

)

log

n

+

log(C/c)}/ log

2.

(5)

Based on Theorem 3.1, we immediately have the following limiting distribution.

7

Theorem 3.2 (Efficiency of estimator) Under the conditions (C1)-(C5), we have n(^ - 0) L N (0, 0),
where 0 = [E{g (0 X)2W0(X)}]+E{g (0 X)2W0(X)2}[E{g (0 X)2W0(X)}]+.
By choosing a similar trimming function, the estimators in H®ardle, Hall and Ichimura (1993) and Ichimura (1993) have the same asymptotic covariance matrix as Theorem 3.2. If we further assume that the conditional distribution of Y given X belongs to a canonical exponential family

fY |X (y|x) = exp{y(x) - B((x)) + C(y)}

for some known functions B, C and , then 0 is the lower information bound in the semiparametric sense (Bickel, Klaassen, Ritov and Wellner, 1993). See also the proofs in Carroll, Fan, Gijbels and Wand (1997) and H®ardle, Hall and Ichimura (1993). In other words, our estimator is the most efficient in the semiparametric sense.
For the estimation of the single-index model, it was generally believed that undersmoothing the link function must be employed in order to allow the estimator of the parameters to achieve root-n consistency. However, H®ardle, Hall and Ichimura (1993) established that undersmoothing the link function is not necessary. They derived an asymptotic expansion of the sum of squared residuals. We also derive an asymptotic expansion but of the estimator  itself. This allows us to measure the higher order cost of estimating the link function. We use the expansion to propose an automatic bandwidth selection procedure for the index. Let f0(.) be the density function of 0 X.

Theorem 3.3 (Higher Order Expansion) Under conditions (C1)-(C5) and i is independent of Xi, we

have almost surely

^ -

0

=

En

+

c1,n nh

+

c2,nh4

+

Hn

+

O{n2 n3},

where n = h2 + (nh/ log n)-1/2,

n
En = (Wn)+ n{f0 (Xj)}g (0 Xi)0 (0 Xi)i,
i=1

with Wn = n-1

n j=1

n{f0 (Xj)}(g

(0

Xi))20 (Xj )0 (Xj ),

Hn

=

O{n-1/2n

+

n-1h-1/2}

with

E{HnEn}

= o{(nh)-2 + h8} and

c1,n =

n
K2(v)v2dv2(nWn)-1 n{f(Xj)}{0 (Xj) + f0(Xj)0 (Xj)/f0 (Xj)},
j=1

8

c2,n

=

1 4

(

n
K(v)v4dv - 1)(nWn)-1 n{f(Xj)}g (0 Xj)g (0 Xj)0(Xj).
j=1

Because K(v) is a density function and we constrain that v2K(v) = 1, it follows that µ4 = K(v)v4dv > 1. In the expansion of ^ - 0, the first term En does not depend on h. The second and third terms are the
leading term among the remainders. The higher order properties of this estimator are better than those of

the AD method, see Nishiyama and Robinson (2000), and indeed do not reflect a curse of dimensionality.

To minimize the stochastic expansion, it is easy to see that the bandwidth should be proportional to n-1/5. Moreover, by Theorem 3.2 we consider the Mahalanobis distance

(^ - 0) 0+(^ - 0) = Tn + o{h8 + (nh)-2},

where

Tn

=

(En

+

c1,n nh

+ c2,nh4

+ Hn)

0+(En

+

c1,n nh

+

c2,nh4

+

Hn)

is the leading term. We have by Theorem 3.3 that

ETn

=

E(En

0+En)

+

(

c1 nh

+

c2h4)

+0 (

c1 nh

+

c2h4)

+

o{h8

+

(nh)-2},

where c1 =

K2(v)v2dv2W0+E{0(X) + f -1(X)f (X)0(X)}, W0 = E{(g (0 X))20(X)0(X)} and

c2

=

1 4

(

K(v)v4dv - 1)W0+E[g (0 X)g (0 X)0(X)].

Note that E(En 0+En) does not depend on h. By minimizing ETn with respective to h, the optimal

bandwidth should be

h =

(9r22 + 16r1)1/2 - 3r2 8

1/5
n-1/5,

where r1 = c1 +0 c1/(c2 +0 c2) and r2 = c1 +0 c2/c2 +0 c2. As a comparison, we consider the optimal bandwidth for the estimation of the link function g. By Lemma 5.1 and Theorem 3.2, if f0(v) > 0 we have

g^(v) = g(v) +

1 2

g

(v)2h2 +

1 nf0 (v)

n
Kh(0 Xi - v)i + OP (n-1/2 + h2n).
i=1

(6)

In other words, the link function can be estimated with the efficiency as if the index parameter vector is

known. A brief proof for (6) is given in section 5. It follows that

|g^(v) - g(v)|2 = Sn(v) + OP {(n-1/2 + h2n)n}.

9

where

the

leading

term

is

Sn(v)

=

[

1 2

g

(v)2 + {nf0 (v)}-1

n i=1

Kh(0

Xi - v)i]2.

Suppose

we

are

interested

in constant bandwidth in region [a, b] with weight w(v). Minimizing [a,b] ESn(v)w(v)dv with respect to h,

we have the optimal bandwidth for the estimation of the link function is

hg =

K2(v)dv [a,b] f-01(v)20 (v)w(v)dv [a,b] g (v)2w(v)dv

1/5
n-1/5.

It is noticeable that the optimal bandwidth for the estimation of the parameter vector 0 is of the same order as that for the estimation of the link function. In other words, under-smoothing may lose efficiency for the estimation of 0 in the higher order sense. These optimal bandwidth hopt and hgopt can be consistently estimated by plug-in methods; see Ruppert et al (1995).

Although the optimal bandwidth for the estimation of  is different from that for the link function,

its estimation such as the plug-in method may be very unstable because of the estimation of second order

derivatives. Moreover, its estimation needs another pilot parameter which is again hard to choose. In practice it is convenient to apply hgopt for hopt directly, and since hgopt and hopt have the same order, the loss of efficiency in doing so should be small. For the former, there are a number of estimation methods such as

CV and GCV methods. If CV methods is used, in each iteration with the latest estimator , the bandwidth

is selected by minimizing

n
h^g = argminn-1 {Yj - g^j( Xj)}2
h j=1

where g^j(v) is the delete-one-observation estimator of the link function, i.e. the estimator of g^(v) in (3)

using data {(Xi, Yi), i = j}. Another advantage for this approach is that we can also obtain the estimator

for the link function.

4 Numerical Results
In the following calculation, the Gaussian kernel function and the trimming function (4) with  = 1/20 and c0 = 0.01 are used. A MATLAB code rMAVE.m for the calculations below is available at
http://www.stat.nus.edu.sg/%7Estaxyc
In the first example, we check the behavior of bandwidths hg and h. We consider two sets of simulations to investigate the finite performance of our estimation method, and to compare the bandwidths for the

10

estimation of the link function g and the single-index 0. Our models are

model A: y = (0 X)2 + 0.2, model B: y = cos(0 X) + 0.2,

where 0 = (3, 2, 2, 1, 0, 0, -1, -2, -2, -3) /6, X  N10(0, I), and   N (0, 1) is independent of X. The

ADE method was used to choose the initial value of . With different sample size n and bandwidth h, we

estimate the model and calculate estimation errors

err = {1 - |0 ^|}1/2,

errg =

1 n

n j=1

n{f^^(^

Xj )}|g^^(^

Xj) - g(0 Xj)|,

where g^^(^ Xj) is defined in (3). With 200 replications, we calculate the mean errors mean(err) and
mean(errg). The results are shown in Figure 1. We have the following observations. (1) Notice that n1/2mean(err) tends to decrease as n increases,
which means the estimation error err enjoys a root-n consistency (and slightly faster for finite sample size).
(2) Notice that the U-shape curves of err has a wider bottom than those of errg. Thus, the estimation of 0 is more robust to the bandwidth than the estimation of g. (3) Let hopt = arg minh mean(err) and hogpt = arg minh mean(errg). Then hopt and hogpt represent the best bandwidths respectively for the estimation of the link function g and the single-index 0. Notice that hopt/hogpt tends to increase as n increases, which means the optimal bandwidth for the estimation of 0 tends to zero not faster than that

for the estimation of link function. Thus the under-smoothing bandwidth is not optimal.

Next, we compare our method with some of the existing estimation methods including ADE in H®ardle

and Stocker (1993), MAVE, the method in Hristache et al (2001), called HJS hereafter, the SIR and pHd

methods in Li (1991, 1992) and SLS in Ichimura (1993). For SLS, we use the algorithm in Friedman (1984)

in the calculation. The algorithm has best performance among those proposed for the minimization of SLS,

such as Weisberg and Welsh (1994) and Fan and Yao (2003). We consider the following model used in

Hristache et al (2001),

Y = (0 X)2 exp(a0 X) + ,

(7)

 where X = (x1, ∑ ∑ ∑ , x10) , 0 = (1, 2, 0, ..., 0) / 5, x1, ∑ ∑ ∑ , x10,  are independent and   N (0, 1). For the

covariates X: (xk + 1)/2  Beta(, 1) for k = 1, ∑ ∑ ∑ , p. Parameter a is introduced to control the shape of

function. If a = 0, the structure is symmetric; the bigger it is, the more monotonic the function is.

Following Hristache et al (2001), we use the absolute deviation

p j=1

|^j

-

j |

to

measure

the

estimation

errors. The calculation results for different  and  based on 250 replications are shown in Table 1. We have

11

Figure 1: The wide solid lines are the values of log{n1/2mean(err)} and the narrow lines are the values of log{n1/2mean(errg)} (re-scaled for easier visualisation). The dotted vertical lines correspond to the bandwidths h

and hg respectively.

0 model A, n=50 0 model A, n=100 0 model A, n=200 0 model A, n=400 0 model A, n=800

h/hg=1.34

h/hg=1.62

h/hg=2.33

h/hg=2.33

h/hg=2.36

-2 -2 -2 -2 -2

-4 -4 -4 -4 -4

-6 -6 -6 -6

0 hhg 0.5

1 0 hgh0.5

1 0 hgh0.5

1 0 hg h0.5

0 model B, n=100 h/hg=1.1
-2

0 model B, n=200 h/hg=1.26
-2

0 model B, n=400 h/hg=1.37
-2

-6

1 0 hgh 0.5

1

0 model B, n=800 h/hg=1.64
-2

-4 -4 -4 -4

-6 0 hgh 0.5

-6 1 0 hgh 0.5

-6 1 0 hgh 0.5

-6 1 0 hgh 0.5

1

Table 1. Average estimation errors

p j=1

|^j

-

j |

and their standard deviations (in square bracket) for model (7).

a=1

a=0

n   ADE HJS SIR/pHd SLS MAVE SIR/pHd SLS

200 0.1 1 0.6094 0.1397 0.6521 0.0645 0.0514 0.7500 0.6910

[0.1569] [0.0258] [0.0152] [0.1524] [1.2491]

200 0.2 1 0.6729 0.2773 0.6976 0.1070 0.0934 0.7833 0.8937

[0.1759] [0.0375] [0.0294] [0.1666] [1.3192]

400 0.1 0.75 0.7670 0.1447 0.3778 0.1151 0.0701 0.6037 0.0742

[0.0835] [0.0410] [0.0197] [0.1134] [0.0193]

400 0.1 1 0.4186 0.0822 0.4868 0.0384 0.0295 0.5820 0.5056

[0.1149] [0.0125] [0.0096] [0.1084] [1.0831]

400 0.1 1.5 0.2482 0.0412 0.5670 0.0208 0.0197 0.5760 0.0923

[0.1524] [0.0063] [0.0056] [0.1215] [0.0257]

400 0.2 1 0.4665 0.1659 0.5249 0.0654 0.0607 0.6084 0.7467

[0.1353] [0.0207] [0.0178] [0.1064] [1.2655]

400 0.4 1 0.5016 0.3287 0.6328 0.1262 0.1120 0.6994 0.9977

[0.1386] [0.0406] [0.0339] [0.1370] [1.2991]

 The values are adopted from Hristache et al (2001)

MAVE 0.0936
[0.0255]
0.1809
[0.0483]
0.0562
[0.0146]
0.0613
[0.0167]
0.0669
[0.0175]
0.1229
[0.0357]
0.2648
[0.1880]

12

the following observations from Table 1. Our methods has much better performance than ADE and the method of Hristache et al (2001). For each simulation, the better one of SIR and pHd is reported in Table 1, suggesting that these methods are not so competitive. Actually the main application of SIR and pHd is not in the estimation of single-index models. See Li (1991, 1992). For SLS, its performance depends much on the data and the model. If the model is easy to estimate (such as monotone and having big signal/noise ratio), it performance quite well. But overall SLS is still not so good as MAVE. The proposed method has the best performance in all the simulations we have done.

5 Proof of Theorems

Let f(v) be the density function of  X and n = {x : |x| < nc, f(x) > n-2 ,   n} where c > 1/3 and  > 0 is defined in (C5). Suppose An is a random matrix depending on x and . By An = O(an) (or An =O(an)) we mean that all elements in An are Oa.s.(an) (or oa.s.(an)) uniformly for   n and x  n. Let n = (nh/ log n)-1/2, n = h2 + n and  = | - 0|. For any vector V (v) of functions of v, we define (V (v)) = dV (v)/dv.

Suppose (Xi, Zi), i = 1, 2, . . . , n, are i.i.d. samples from (X, Z). Let Xix = Xi - x,

n
sk (x) = n-1 Kh( Xix){ Xix/h}k,
i=1

n
tk (x) = n-1 Kh( Xix){ Xix/h}kXi,
i=1

n
wk(x) = n-1 Kh( Xix){ Xix/h}kXiXi ,
i=1

n
ek(x) = n-1 Kh( Xix){ Xix/h}ki,
i=1

 k

=

sk (x) - Esk(x),

k

=

tk (x) - Etk (x),

Dn ,k (x)

=

s2(x)sk (x) - s1(x)sk+1(x),

En,k

=

s0(x)sk+1(x) -

s1(x)sk (x) for k = 1, 2, . . .. For any random variable Z and its random observations Zi, i = 1, ..., n, let

nn
Tn,k(Z|x) = s2(x)n-1 Kh(Xix)( Xix/h)kZi - s1(x)n-1 Kh(Xix)( Xix/h)k+1Zi,
i=1 i=1 nn
Sn,k(Z|x) = s0(x)n-1 Kh(Xix)( Xix/h)k+1Zi - s1(x)n-1 Kh(Xix)( Xix/h)kZi.
i=1 i=1

By the Taylor expansion of g(0 Xi) at 0 x, we have

g(0 Xi) = g(0 x) +

5

1 k!

g(k)(0

x){

Xix + (0 - )

Xix}k + O({

Xix + (0 - )

Xix}6)

k=1

= g(0 x) + A(x, Xi) + B(x, Xi)(0 - ) + O{( Xix)6 + 3(|Xi|6 + |x|6)},

(8)

13

where A(x, Xi) = 5=1(k!)-1g(k)(0 x)( Xix)k and

B(x, Xi) =

5

(k

1 -

1)!

g(k)(0

x)(

Xix)k-1Xix

+

1 2

g

(0 x)( - 0)

XixXix.

k=1

For ease of exposition, we simplify the notation and abbreviate g for g(0 x) and g , g , g for g (0 x), g (0 x), g (0 x) respectively. Without causing confusion, we write f( x) as f, f( Xj) as f(Xj) and Kh( Xij) as Kh(Xij). Similar notations are used for the other functions.

Lemma 5.1 (Link function) Let

n
n(x) = n-1 Kh(Xix)
i=1

1  Xix/h

1  Xix/h

and

a (x) d(x)h

n
= {nn(x)}-1 Kh(Xix)
i=1

Under assumptions (C2)≠(C5), we have

1  Xix/h

Yi.

a(x) = g(0 x) + An(x)h2 + Bn (x)(0 - ) + Vn(x) + O(h2n2 + 3)(1 + |x|6),

where

d(x)h = g (0 x)h + A~n (x)h2 + B~n (x)(0 - )h + V~n(x) + O(h2n2 + 3)(1 + |x|6),

An(x)

=

1 2

g

+

1 4

{(µ4

-

1)g

f-2 (f f

-

2(f )2 )

+

1 24

µ4g(4)}h2

+

H1,n(x),

A~n (x)

=

1 2

g

(µ4

-

1)f-1 f h

+

1 6

g(3)µ4h

+

1 2

g

f-1(

 3

-

1) + O(hn),

Bn (x) = g  + O(n + ), B~n (x) = g (0 x)f-1{f(x)} + O(n),

where

H1,n(x)

=

1 2

g

(0 x){f-1(

 2

-

0) + (2 - µ4)f-2fh

 1

-

f-2

f

h

 3

}

+

1 6

f-1g

h

 3

and

Vn (x)

=

f-1e0 -

f-2 f he1

+ µ4f-2f h2e0/2 + f-2(e0

 2

- e1

 1

)

-

µ4f-2f

h3e1

+ {f-2(f)2

- (µ4

+ 1)f-1f }{f-1h2e0

-

f-2 f h3 e1 }-f-1 (

 0

+

 1

){f-1

e0

-f-2f

e1

}+2f-2 f h

1f-1e0

and

V~n (x)

=

f-1e1+f-2f h2e1/2+f-2(

0e1 -

1e0) - f-2fhe0 + f-1

0[-(µ4 + 1)f-1f h2/2 - f-1(

 0

+

1) + f-2(f)2h2].

Lemma 5.2 (Summations) Let n (x)=n-1

n i=1

Kh (Xix )Xix i .

Under

conditions

(C1)-(C5),

we

have

n
An d=ef n-1 n{s0(xj )}g (0 Xj)n (Xj)/s0(Xj) = En + rn,0( - 0) + Qn + O(n2 n3),
j=1

14

where En =

Bn

d=ef

(nh)-1

n j=1

n{s0(Xj)}ek(Xj)n (Xj)/s0(Xj)

=

c~k,n nh

+

Rn

+

O(n2 n3),

n

Cn d=ef n-1

n (s0 (Xj ))

 k

(Xj

)n

(Xj

)/s0 (Xj

)

=

Mn

+ O(n2 n3),

j=1

n i=1

n {f (Xj )}g

(

Xi ) (

Xi)i, rn,0 =O (1),

En = O{(n/ log n)-1/2}, Qn = O{(n/ log n)-1/2n}, Rn = O{n-1/2n}, Mn = O{n-1/2n},

with E{EnQn} = o(h8 + (nh)-2), E{EnRn } = o(h8 + (nh)-2), E{EnMn} = o(h8 + (nh)-2), and c~k,n = vk+1K2(v)dvE[n(f(Xj))f-1(Xj)((Xj)f(Xj)) (Xj)] if k is odd, 0 otherwise.

Lemma 5.3 (Denominator) Let Dn = n-2

n i,j=1

n

(s0(Xj

))d2

(Xj

)Kh

(Xij

)Xij

Xij

/s0

(Xj

)

in

the

algo-

rithm. Suppose (, B) : p ◊ p is an orthogonal matrix. Then under (C1)-(C5), we have almost surely

(Dn )-1 =  d11h-2 - d12B h-1 - B(d12)  h-1 + Bd22B ,

where

d11 = (Gn )-1 +O(1),

d12 = Hnh + O(n),

d22

=

1 2

(B

WnB)-1 + O(n),

with Gn = n-1

n j=1

n(f

(Xj

))f-1(Xj

)(g

(0 Xj ))2

and

Hn

=

1 2

n-1

n j=1

n (f (Xj

))f-1 (Xj ){(f  )

(Xj )}

(Gn )-1(g (0 Xj))2B(B WnB)-1 and Wn = n-1

n j=1

n {f (Xj )}(g

(

Xi))2(Xj) (Xj).

Proof of Lemma 5.3 Let (, B) be an orthogonal matrix. It is easy to see that

nn
n-1 Kh(Xix) XixXix = s2(x)h2, n-1 Kh(Xix) XixXixB = {t1(x) - s1(x)x} Bh,
i=1 i=1

Thus

n
n-1 Kh(Xix)B XixXixB = B {w0(x) - t0(x)x - x(t0(x)) + xx s0(x)}B.
i=1

(Dn )-1 = (, B)

D11h2 B D12h

(D12) Bh B D22B

-1
(, B)

,

where

n
D11 = n-1 n(s0(Xj)){d(Xj)}2s2(Xj)/s0(Xj),
j=1
n
D12 = n-1 n(s0(Xj)){d(Xj)}2{t1(Xj) - s1(Xj)Xj} /s0(Xj),
j=1

15

n
D22 = n-1 n(s0(Xj))(d(Xj))2{w0(Xj) - t0(Xj)Xj - Xjt0(Xj) + XjXj s0(Xj)}/s0(Xj).
j=1
By the matrix inversion formula in blocks (Schott, 1997), we have the equation in Lemma 5.3 with
d11 = {D11 - (D12) BB (D22)-1BB D12}-1, d12 = d11(D12) B(B D22B)-1, d22 = {B D22B}-1 + d11{B D22B}-1B D12(D12) B{B D22B}-1. By Lemma 6.1, we have
D11 = Gn-1 +O(1), D12 = Hnh + O(n), D22 = 2Wn + O(n).

Thus, Lemma 5.3 follows.

n

Lemma 5.4 (Numerator) Let Nn = n-2

n(s0(Xj))Kh(Xij)Xij{Yi-a(Xj)-d(Xj)0 Xij}/s0(Xj).

i,j=1

Under assumptions (C1)≠(C5), we have almost surely

Nn

=

En

+

c~1,n nh

+

c~2,nh4

+

Rn

+

Bn (

-

0)

+

O{n2 (n3

+

3)},

where Rn = O{n-1(log n/h)1/2 + (log n/n)-1/2h2},  Rn = O{hn-1(log n/h)1/2 + (log n/n)-1/2h3} and E{RnE0} = O{(nh)-2 + h8}, Bn = Wn +O(1) with Wn defined in Lemma 5.3, c~1,n and E0 are defined in

Lemma 5.2 and

c~2,n =

1 4

(µ4

-

1)

n j=1

n {f (Xj )}g

(0

Xj )g

(0 Xj) (Xj).

Proof of Theorem 3.1 By assumption (C2), we have

n







P ( {Xi / n})  nP (Xi / n)  nP (|Xi| > nc) < nn-6cE|X|6 < 

n=1 i=1

n=1

n=1

n=1

for any c > 1/3. It follows from the Borel-Cantelli lemma that

n
P ( {Xi / n}) = 0.
n=1 i=1

(9)

Let ~n = {x : f( x) > 2n- }. Similarly, we have
n
P ( {Xi / ~ n}) = 0.
n=1 i=1

(10)

Thus, we can exchange summations over {Xj : j = 1, ∑ ∑ ∑ , n}, {Xj : Xj  n, j = 1, ∑ ∑ ∑ , n} and {Xj : Xj  ~n, j = 1, ∑ ∑ ∑ , n} in the sense of almost surely consistency. On the other hand, we have by (C2)

n-1 (1 + |Xj|6) = O(1).
|Xj |<nc

16

By the notation in Lemmas 5.3 and 5.4, after one iteration of Steps 1-3, the new  is

~ = 0 + (Dn )-1Nn.

(11)

Note that  En = 0,  c1,n = 0,  c2,n = 0,  Wn = 0, Wn(Wn)+ = I -  and /h2  0. We have

~ =0 + [ d11h-2{Rn + Bn ( - 0) + O{n2 (n3 + 3)}} - d12B h-1Nn]

- B(d12)  h-1[Rn + Bn ( - 0) + O{n2 (n3 + 3)}] + Bd22B Nn

=(1

+

an)0

+

{

1 2

(I

-

00

)

+

bn}(

-

0)

+

1 2

{Wn

}+

En

+

O(h4),

where an =O(1) and bn =O(1). By (25) below, we have s0(x) = f( x) + O(n). Thus by the smoothness of n(.) and (10), we have

n(s0(x)) = n(f( x)) + O(n n) = 1 + O(n n).

(12)

Since n(.) is bounded, we have E{n(f^( x)) - 1}2 =O(1). By (C3) and Lemma 6.1, we have

n

En = n-1

g (0 Xi)0 (Xj )i +O(n-1/2).

i=1

Note that Wn = W0 +O(). It is easy to check that |~| = 1 + an + bn + O(h4) = 1 +O(1). Thus

~/|~|

=

0

+

{

1 2

(I

-

00

)

+O(1)}(

-

0)

+

1 2

n-1W0+

n

g (0 Xi)0 (Xi)i +O(h3 + n-1/2).

i=1

Let (k) be the value of  after k iteration. Because hk+1 = max{hk/ch, h}. Therefore,

|k+1 - 0|/h2k+1  0,

for all k > 1. We have

(k+1)

=

0

+

{

1 2

(I

-

00

)

+O(1)}((k)

-

0)

+

1 2

n-1W0+

n

g (0 Xi)0 (Xi)i +O(hk3 + n-1/2).

i=1

Recursing the above equation, we have

(k+1)

=

0

+

{

1 2k

(I

-

00

)

+O(1)

k

1 2

}((1)

-

0)

+

{

k

1 2

}n-1W0+

n
g (0 Xi)0 (Xi)i

=1 =1 i=1

k
+O(
=1

1 2

h3k-

+

n-1/2).

17

Thus as the number of iterations k  , Theorem 3.1 follows immediately from the above equation and the central limit theorem.
Proof of Theorem 3.3 Based on Theorem 3.2, we can assume  = (log n/n)1/2. Note that  {En + c1,n(nh)-1 + c2,nh4} = 0. We consider the product of each term in (Dn )-1 with Nn. We have
 d11h-2Nn =  d11h-2[Rn + Bn ( - 0) + O{n2 (n3 + 3)}] = an0 + an( - 0), d12B h-1Nn = bn 0 + bn( - 0), B(d12)  h-1

(Dn )-1Nn = {Sn ( - 0) + HnEn + O(n2 n4)} = 0{Sn ( - 0) + HnE0 + O(n2 n4)} + cn( - 0),

where Sn = O(1) and cn = O(n/h). It is easy to see that cn =O (1) providing that | - 0|/h2  0. By

Lemma 5.3 and 5.4, we have

~ =0{1

+

Sn

(

-

0)

+

HnE0

+

O(n2 n4)}

+

1 2

Wn

{E0

+

c1,n nh

+

c2,nh4

+

Rn

+

Qn }

+

{

1 2

(I

-



) + cn}( - 0) + O{n2 (n3 + h log n/n)}.

It is easy to see that |~| = 1 + Sn ( - 0) + HnE0 + O(n2 n4). Thus

~/|~|

=

0

+

1 2

Wn

{E0

+

c1,n nh

+

c2,nh4

+

Rn

+

E0 Hn

E0 }

+

{

1 2

(I

-



) + cn}( - 0) + O{n2 (n3 + h log n/n)},

where cn =O (1). Similar to the proof of Theorem 3.1, we complete the proof with c1,n = Wn-1c1,n and c2,n = Wn-1c2,n.

6 Proofs of the Lemmas

In this section, we first give some results about the uniform consistency. Based on these results, the Lemmas are proved.

Lemma 6.1 Suppose Gn,i() is a martingale with respect to Fi = {Gn, (),  i} with   X and X is a

compact region in a multidimensional space such that (I) |Gn,i()| < i, where i are IID and sup E12r < 

for some r > 2; (II) EG2n,k() < ans() with inf s() positive, and (III) |Gn,i() - Gn,i(~)| < n1| - ~|Mi,

where Mi, i = 1, 2, ... are IID with EM12 < . If and an = cn- with 0   < 1 - 2/r, then for any 1 > 0

we have

n

sup n-1s-1/2() Gn,i() = O{(n-1an log n)1/2}

||n1

i=1

18

almost surely. Suppose for any fixed n and k, Gn,i,k() is a martingale with respect to Fi,k = {Gn, ,k(),  i} such that (I) |Gn,i,k()|  i, (II) EG2n,i,k() < an and (III) |Gn,i,k() - Gn,i,k(~)| < n2| - ~|Mi, where i, an and Mi are defined above. If E|k|2r <  and E{k|Gn,i,j(), i < j, j = 1, ..., k - 1} = 0, then

n

sup n-2



k=2

k-1
Gn,i,k() k = O{(an log n)1/2/n}
i=1

almost surely.

Proof of Lemma 6.1 We give the details for the second part of the Lemma. The first part is easier

and can be proved similarly. Let n() be the expression between the absolute symbols in the equation.

By (III) and the strong low of large numbers, it is easy to see that there are n1 = n3 balls centered at

 : B = { : | - | < n-4} with 4 > 2 + 2, such that

n1 =1

B



.

By

the

strong

law

of

large

numbers,

we have

nn

max
1n1

sup
B

|n()

-

n()|



n2

max
1n1

sup
B

|

-

|n-2

k=1

|k|

i=1

Mi

=

O{(an

log

n)1/2/n}

almost surely. Let n,k() =

k-1 i=1

Gn,i,k (

).

Next,

we

show

that

there

is

a

constant

c1

such

that

pn d=ef P



=1

n=

{ max
1<kn

max
1<n1

|n,k ( )|

>

c1(nan

log

n)1/2}

= 0.

(13)

Let Tn = {nan log(n)}1/2, GIn,i,k() = Gn,i,k()I(|Gn,i,k()|  Tn) and GOn,i,k() = Gn,i,k() - GIn,i,k().

Write

k-1 k-1
n,k() = {GIn,i,k() - EGnI ,i,k()} + {GnO,i,k() - EGnO,i,k()}.
i=1 i=1

(14)

Note that E|GOn,i,k()|  Tn-r+1E|1|r = E|1|r{nan log(n)}-(r-1)/2. If an = cn- with 0   < 1 - 2/r and

k  n, we have

k-1
| EGOn,i,k()|  E|1|r(k - 1){nan log(n)}-(r-1)/2  CE|1|r{nan log(n)}1/2.
i=1

Note that

nn

n

|GOn,i,k()|  |i|I(|i| > Tn)  Tn-r+1 |i|rI(|i| > Tn)

i=1 i=1

i=1

For fixed T , by the strong law of large numbers, we have

n
n-1 |i|rI(|i| > T )  E{|1|rI(|1| > T )}
i=1

(15)

19

almost surely. The right hand side above is dominated by E{|1|r} and  0 as T  . Note that Tn

increase to  with n. For large n such that Tn > T , we have
nn
n-1 |i|rI(|i| > Tn)  n-1 |i|rI(|i| > T )  0
i=1 i=1

almost surely as T  . It follows

n
|GnO,i,k()| = o(nTn-r+1) = o{(nan log n)1/2}
i=1

(16)

almost surely. Thus by (15) and (16), if c1 > CE|1|r we have

pn d=ef P P
= 0.



k-1

=1

n=

{ max
1<kn

max
1<n1

|

{GOn,i,k ( )
i=1

-

E GnO,i,k ( )}|

>

c1(nan

log

n)1/2}

 n

{ |i|(|i|  Tn) > c1(nan log n)1/2}

=1 n= i=1



k-1

+P

=1

n=

{ max
1<kn

max
1<n1

|

i=1

E GOn,i,k ( )|

>

c1(nan

log

n)1/2}

(17)

By condition (II), if k  n we have

k-1

max
1n1

Var

{GIn,i,k ( )
i=1

-

E GIn,i,k ( )}



c2nan

d=ef

N1,

(18)

where c2 is a constant. By the condition on an and the definition of GnI ,i,k(), we have constants c3 and c4 such that

max
1n

|{GIn,i,k ( )

-

E GIn,i,k ( )}|



c3Tn

= c3{nan/ log n}1/2{an-r logr+1 n/nr-2}1/(2(r-1))

 c4{nan/ log n}1/2 d=ef N2.

(19)

Let N3 = c5{nan log n}1/2 with c25 > 2(3 + 3)(c2 + c4c5). By the Bernstein's inequality (cf. DE LA Pen~a, 1999), we have from (18) and (19) that for any k  n,

k-1
P (| {GIn,i,k() - EGIn,i,k()}| > N3)
i=1



2 exp

-N32 2(N1 + N2N3)

 2 exp{-c52 log n/(2c2 + 2c4c5)}

 c6n-3-3.

20

Let c1 > max{c5, c1}. We have


P
n=1

k-1

max
1<kn

max
1<n1

|

[GIn,i,k ( )
i=1

-

EGnI ,i,k()]|

>

c1(nan

log

n)1/2

 n n1

k-1

 P | [GnI ,i,k() - EGIn,i,k()]| > c1(nan log n)1/2

n=1 k=2 =1

i=1


 c6n-3-3n1+3 < .

n=1

By (14), (17) and (20) and the Borel-Cantelli lemma, we have

(20)

pn  P



k-1

=1 n=

max
1<kn

max
1<n1

|

[GIn,i,k ( )
i=1

-

E GIn,i,k ( )]|

>

c1(nan

log

n)1/2

Therefore (13) follows.

+ pn = 0.

Let In,k() = n,k()I{|n,k()|  c1(nan log n)1/2} and U () = k=2 nI ,k()k. Write

n
n() = Un() + On,k()k,
k=2

where On,k() = n,k() - In,k(). It is easy to see from (13) that for the second part on the right hand

side above,

n

max
1<n1

|

k=2

nO,k ( )k |

=

O{n(an

log

n)1/2}

(21)

almost surely, since for any constant c > 0,

n



n=1

P

{ max
1<n1

|

k=2

On,k ( )k |

>

cn(an

log

n)1/2}



n=1

P

( max
1<n1

max
1<kn

|nO,k ( )|

>

0)





n=1

P

{ max
1<n1

max
1<kn

|n,k ( )|

>

c1(nan

log

n)1/2}

< .

Now consider the first term. Let Tn1/2/ log n,

U I () = In,k(){k(|k|  Tn) - E[k(|k|  Tn)]}
k=2
and U O() = U () - U I (). Similar to the proof of (15) and (16), we have almost surely

| nO,k()E{k(|k| > Tn)}| = O{n(an log n)1/2},
k=2
| On,k()k(|k| > Tn)| = O{n(an log n)1/2}.
k=2

(22) (23)

21

Note that

|nI ,k(){k(|k|  Tn) - E[k(|k|  Tn)]}| < 2c1(nan log n)1/2Tn = 2c1n(an/ log n)1/2 d=ef N4

and by (II), Var{U I ()} = c22an d=ef N5, where c2 is a constant. Let N6 = c3n(an log n)1/2 with c32 > 2(3 + 3)(2c1c3 + c2). By the Berenstein's inequality, we have

P

(|UnI ()|



N6)



2

exp{-

N62 2(N6N4 +

N5)

}



2n-3-3.

Therefore

nn

n=1

P

{ max
1n1

|UnI ()|



N6}

<

n=1

n1P

{|UnI ()|



N6}

<

.

By the Borel-Cantelli lemma, we have

max
1n1

|UnI ()|

=

O(N6)

almost surely. Lemma 6.1 follows from (21), (22), (23) and (24).

(24)

Proof of Lemma 5.1 Write sk (x) = k (x) + Esk(x). By Taylor expansion, we have

3

sk(x) =

µk+ f()(x)h +

 k

(x)

+

O(h4).

 =0

(25)

Because

V

ar{

 k

(x)}

=

O{(nh)-1},

it

follows

from

Lemma

6.1

that

 k

(x)

=

O(n).

It

is

easy

to

check

that

Dn ,0 (x)

=

f2

+

1 2

(µ4

+ 1)ff h2

- (f)2h2

+ f(

 0

+

 2

)

-

2f

h

 1

+ O(n2).

Dn,2(x) = f2 + µ4(ff

- (f)2)h2 + 2f

 2

-

f h

 3

-

µ4 f h

 1

+

O(n2 ).

Dn,3(x) = f

 3

+

O(hn

),

Dn,4(x) = µ4f2 + O(n),

Dn,5(x) = O(h).

Tn,0(X|x) = f2(x) + O(n), Sn,0(X|x) = O(h), Tn,k(X|x) = O(1), Sn,k(X|x) = O(1), for k  1,

Tn,0(| Xix|6|x) = O(h6), Sn,0(| Xix|6|x) = O(h6), Tn,0(XX |x) = O(1), Sn,0(XX |x) = O(h),

En,2(x)

=

(µ4

-

1)f f h

+

f(

 3

-

1) + O(hn),

En,3(x) = µ4f2 + O(n),

En,4(x) = O(h).

Note that

a(x) = Tn,0(Y |x)/Dn,0(x), d(x)h = Sn,0(Y |x)/Dn,0(x).

22

and

An(x)

=

5 k=2

1 k!

g(k)(0

x)

Dn ,k Dn ,0

(x) (x)

hk-2

,

Bn (x)

=

4 k=0

1 k!

g(k+1)(0

x)

Tn,k

(X

|x) - Dn,k Dn ,0 (x)

(x)x

hk

,

Cn(x, ) =

1 2

g

(0 x){Tn,0(XX

|x) - Tn,0(X|x)x

- xTn,0(X

|x) + xx Dn,0(x)}{Dn,0(x)}-1,

A~n (x)

=

4 k=2

1 k!

g(k)(0

x)

En ,k (x) Dn ,0 (x)

hk-2

,

B~n (x)

=

4 k=1

k k!

g(k)(0

x)

Sn,k

(X|x) - En,k Dn ,0 (x)

(x)x

hk ,

C~n(x, ) =

1 2

g

(0 x){Sn,0(XX

|x) - Sn,0(X|x)x

- xSn,0(X

|x) + xx En,0(x)}{Dn,0(x)}-1.

Lemma 5.1 follows from simple calculations based on the above equations.

Proof of Lemma 5.2 It follows from Lemma 6.1 that n (x) = O(n)(1 + |x|) and s0 = f + ~0 where

~0 =

 k

+ (Esk

- f)

=

O(n).

Because

|n(.)| < n2 ,

we

have

n(s0(Xj)) = n(f(Xj)) + n(f(Xj))~0(Xj) + O(n2 n2).

(26)

Thus

An = E~n + Qn,1 + O(n2 n3),

where E~n = n-2

n i=1

n j=1

n (f (Xj ))f-1 (Xj )g

(

Xj )Kh(Xij )Xij i, and Qn ,1 = n-1

n i=1

Gn,i

with

n
Gn,i = n-1

1 2

f

(Xj

){n (f (Xj ))

-

n (f (Xj ))f-1 (Xj

)}h2

+

{1

-

n (f (Xj )f-1 (Xj )}~0 (Xj

)

j=1

◊ f-1(Xj)g ( Xj)Kh(Xij)Xiji.

Simple calculations lead to EE~n = 0, E(E~n)2 = O(n-1), E(Gn,i) = 0 and E(Gn,i)2 = O{h4 + (nh)-1}. By the first part of Lemma 6.1, we have

E~n = O{(log n/n)1/2}, Qn,1 = O{h2(log n/n)1/2 + n-1(log n/h)1/2}.

By Taylor expansion, g (0 x) = g ( x) + g (v)(0 - ) x, where v is a value between  x and 0 x. Write

E~n = En + Qn,2 + rn,0( - 0),

where Qn ,2 = n-2

n i=1

n j=1

{n

(f

(Xj

))f-1

(Xj

)g

(

Xj)Kh(Xij)Xij - n(f(Xi))g (

Xi)(Xi)}i and

rn,0 = O(n/h). By Lemma 6.1 and that V ar(Qn,2) = O{h4 + (nh)-1}, we have

Qn,2 = O{(n/ log n)-1/2n}.

23

Let Qn = Qn,1 + Qn,2. It is easy to check that E{Qn En} = o(h8 + (nh)-2). Therefore, the first part of
Lemma 5.2 follows.
Similarly, we have from (26) that
n
Bn = (nh)-1 {n(f(Xj)) + n(f(Xj))~0(Xj)}ek(Xj)n (Xj)/f(Xj) + O(n2 n4/h).
j=1
Let R~n be the first term on the right hand side above. Then
nn
R~n = n-3 {n(f(Xj)) + n(f(Xj))~0(Xj)} Kh2( Xij)( Xij/h)kXij2i /f(Xj)
j=1 i=1 nn
+ n-3 {n(f(Xj)) + n(f(Xj))~0(Xj)} Kh( Xij)( Xij/h)kKh( X j)X ji /f(Xj)
j=1 i=
d=ef R~n ,1 + R~n ,2 + R~n ,3 + R~n ,4.

If  is independent of X, then

2
E(x) d=ef E{Kh2( Xix)( Xij/h)kXix2i } = h-1

1 !

µ~k+

{f (x) (x)}(

)h

2

+

O(h2),

=0

where µ~k = Thus

K2(v)vkdv. By Lemma 6.1, we have
n
n-1 Kh2( Xix)( Xix/h)kXixi2 - E(x) = O(h-1n).
i=1

nn
Rn ,0 d=ef (n2h)-1 n(f(Xj)) n-1 Kh2( Xij)( Xij/h)kXiji2 - E(Xj) = O{(nh2)-1n}.
j=1 i=1

It is easy to check that E{EnRn ,0} = 0. Write
n
(n2h)-1 n(f(Xj))E(Xj) = (nh)-1E{n(f(Xj))E(Xj)} + Rn ,1,
j=1

where E{Rn ,1En} = 0 and
n
Rn ,1 = (n2h)-1 [n(f(Xj))E(Xj) - E{n(f(Xj))E(Xj)}] = O{(nh2)-1(n/ log n)-1/2}.
j=1

Note that E{n(f(X))(X)} = 0. We have

(nh)-1 E {n (f (Xj ))E (Xj )}

=

c~k,n nh

+ Rn ,2.

(27)
(28) (29)

24

where Rn ,2 = O(n-1) and E{Rn ,2E0} = 0. By (27)-(29) and the fact that (n/ log n)-1/2 = o(n), we have

R~n ,1

=

c~k nh

+

Rn ,1

+

Rn ,2.

(30)

Similarly

R~n ,2 = O{(nh)-1n}.

Let Gn ,i, = n-1

n j=1

n(f

(Xj

))Kh(

Xij )(

Xij /h)k Kh (

X j)X j/f(Xj). Write R~n ,3 as

R~n ,3 = n-2

n

1 2

(Gn,i,

+ Gn, ,i)i

= n-2

n

i= =1

1 2

(Gn,i,

+ Gn , ,i)i

.

i<

By the second part of Lemma 6.1, we have

(31)

R~n ,3 = O{n-1/2n}.

(32)

Similarly, we have

R~n ,4 = O{n-1/2n}.

(33)

Thus the second part of Lemma 5.2 follows from (30) and (31). The third part of Lemma 5.2 can be proved similarly as the proof of the second part. Proof of Lemma 5.4 By (8), Lemma 5.1 and 0 =  + (0 - ), simple calculations lead to

Yi - a(x) - d(x)0 Xix = i + {A~(x, Xi) - An (x)h2} + {B~(x, Xi) - Bn (x)} (0 - ) - Vn(x) + O{h2n2 + 3},

where A~(x, Xi) = A(x, Xi) - d(x) Xix and B~(x, Xi) = B(x, Xi) - d(x)Xix. It follows from the Taylor expansion that

n

Cn,k(x) d=ef n-1

Kh (Xix )(

i=1

Xix/h)kXix =

5

1 !

µk+

(f µ)(

)h

=0

+ ~k + O(h6),

where ~k = n-1

n i=1

{Kh (Xix )(

Xix/h)kXix - EKh(Xix)(

Xix /h)k Xix }

=

k

-x

 k

.

We

have

n-1

n i=1

Kh (Xix )Xix A~ (x,

Xi)

=

{g

(0

x)

-

d (x)}Cn,1 (x)h

+

5 k=2

1 k!

g(k)(0

x)Cn ,k (x)hk

=

-

1 2

g

(µ4

-

1)f f-1 ( f )

h4

+

1 2

g

( f )

(

 3

-

1)h2

+

V~n {( f )

h

+

1 6

µ4

( f

)

h3 + ~1}

+

1 2

g

h2{f +

1 2

µ4(f



)

h2} +

1 24

g (4) µ4 f 

h4

+

1 2

g

h2~2 +

1 6

g

h3~3 + O(h2n2).

25

Thus

n-1

n

Kh (Xix )Xix {A~ (x,

Xi)

-

An(x)h2}

=

1 4

(µ4

-

1)g

f 

+ Bn,1( - 0) + H2,n + O(h2n2),

i=1

where

Bn ,1

=

{( f )

h

+

1 6

µ4

(

f

)

h3 + ~1}Bn (x)

with Bn (x) defined in Lemma 5.1, and

H2,n

=

1 2

g

(

 3

-

1 )h2 ( f )

+ f-1e1(f)

h+

1 2

f-2f

(f

 )

h3e1

+ f-2(f )

h(

0 e1

-

1e0)

- f -2f

(f )

h2e0

+ f -1(f )

h

0{-

1 2

(µ4

+ 1)f -1f h2

- f -1(

 0

+

1) + f -2(f )2h2}

+

1 6

µ4f

-1

e1 (f

)

h3 + f -1e1~1 - f -2f

he0 ~1

+

1 2

g

h2~2

+

1 6

g

h3~3

-

1 2

g

h2~0

-

1 2

g

(0

x){f-1(

 2

-

0) + (2

- µ4)f-2fh

 1

- f-2fh

 3

}

f

h2

-

1 6

g

3  h3 .

(34)

By the expansions of d(x) in Lemma 5.1, n(s0(x)) in (26), and (34), we have
nn
n-2 n(s0(Xj))d(Xj) Kh(Xix)Xij{A~(Xj, Xi) - An(Xj)h2}/s0(Xj)
j=1 i=1
= c~2,nh4 + (Bn,2) ( - 0) + R~n ,1 + O{n2 (h2n2 + 2h + 3)},

where Bn,2 = n-1

n j=1

n (s0 (Xj ))d (Xj )B1,n (Xj )/s0 (Xj )

and

R~n ,1

=

n-1

n j=1

n(sn (Xj

))d

(Xj

)H2,n(Xj

)

/s0(Xj). Again by the expansion of d and that ~1 = O(n), we have Bn,2 = O(h + n). It is easy to check

that H2,n = O(hn + n2). We have

R~n,1 =n-1

n

[n(f(Xj)) + n(f(Xj)){f(Xj) +

1 2

f

(Xj

)h2

+

0 (Xj )}]{g

(0

Xj )

+

1 6

g

(0 Xj)h2

j=1

+ V~n(Xj)/h}H2,n(Xj)f-1(Xj){1 -

1 2

f-1(Xj

)f

(Xj

)h2

- f-1(Xj)

0(Xj)} + O(n2 n3)

d=efRn,1 + O(n2 n3).

Next, we need to consider the terms in Rn,1 one by one. Write

n

Rn ,1,1 d=ef n-1

n(f(Xj))f-1(Xj)(f(Xj)(Xj)) e1h

j=1

n
=hn-2

n
Kh (Xij )n (f (Xj ))f-1 (Xj )(f (Xj ) (Xj ))

i=1 j=1

i.

Note that E{n(f(X))f-1(X)(f(X)(X)) | X} = 0. We have by Lemma 6.1

Rn ,1,1 = O{hn-1(h-1 log n)-1/2}

26

and

nn

E{E0Rn ,1,1} = hn-3E

Kh(Xij)n(f(Xj))f-1(Xj)(f(Xj)(Xj)) n(f(Xi))g ( Xi)(Xi)i2

i=1 j=1

n

= hn-3E

Kh(0)n(f(Xj))f-1(Xj)(f(Xj)(Xj)) n(f(Xj))g ( Xj)(Xj)2j

j=1

= O(n-2).

Applying similar approach to all the terms in Rn ,1, we have Rn ,1 = O{n-1(log n/h)1/2 + (log n/n)1/2h2} and E{EnRn ,1} = o{(nh)-2 + h8}.

(35)

By Lemmas 5.1 and 6.1, we have

nn
Bn,3 d=ef n-2 (s0(Xj))d(Xj) Kh(Xij)Xij{B~(Xj, Xi) - Bn (Xj)} /s0(Xj) = Wn + O{(n + )/h}.
j=1 i=1

By Lemma 5.2, we have

n-2

n

(s0 (Xj ))d (Xj )

n

Kh(Xij )Xij i/s0(Xj )

=

E0

+

c~1,n nh

+

Bn ,4 (0

-

)

+

Rn ,2

+

O(n2 n3),

j=1 i=1

where c~1,n is defined in the lemma, and
n
B4,n = n-1 {n(f(Xj)) + n(f(Xj)) 0(Xj)}n (Xj)(B~n (Xj)) /h
j=1

and

Rn ,2 = n-1

n

[

1 6

n

(f (Xj

))g

(0

Xj )h2

+

n (f (Xj ))

 0

(Xj

){g

(0

Xj )

+

V~n(Xj)/h}]n (Xj).

j=1

Noting that n = O(n), we have B4,n = O(n/h). Similarly, we have
nn
n-2 n(s0(Xj))d(Xj)Vn(Xj ) Kh(Xij)Xij/s0(Xj) = Rn ,3 + O(n2 n3),
j=1 i=1

where

Rn ,3 = n-1

n

n (s0 (Xj ))d (Xj )Vn (Xj )[ (Xj )

+

1 2

f-1

{(f



)

- f-1f (Xj)}h2 + 0(Xj) -

0 (Xj )].

j=1

By the same arguments leading to (35), we have

Rn ,2 = O{n-1(log n/h)1/2 + (log n/n)1/2h2} and E{EnRn ,2} = o{(nh)-2 + h8}, Rn ,3 = O{n-1(log n/h)1/2 + (log n/n)1/2h2} and E{EnRn ,3} = o{(nh)-2 + h8}.

(36) (37)

Lemma 5.4 follows from the above equations with Rn = Rn ,1 + Rn ,2 + Rn ,3 and Bn = Bn,2 + Bn,3 + Bn,4 = Wn + O{n2 (n + )/h}.

27

References
[1] Bickel, P., Klaassen, A. J., Ritov, Y. and Wellner, J. A. (1993) Efficient and Adaptive Inference in Semiparametric Models, Baltimore: Johns Hopkins University Press.
[2] Carroll, R.J., Fan. J. Gijbels, I. and Wand, M.P. (1997) Generalized partially linear single-index models. J. Am. Statist. Ass., 92, 477-489.
[3] Delecroix, M., Hristache, M. and Patilea, V. (2004) On semiparametric M-estimation in single-index regression. J. Statist. Plann. and Infer. (to appear).
[4] Fan, J. and Gijbels, I. (1996) Local Polynomial Modeling and Its Applications. Chapman & Hall, London. [5] Fan, J. and Yao, Q. (2003) Nonlinear Time Series : nonparametric and parametric methods. New York
: Springer Verlag. [6] Friedman, J. H. (1984) SMART User's Guide. Laboratory for Computational Statistics, Stanford Uni-
versity Technical Report No. 1. [7] H®ardle, W., Hall, P. and Ichimura, H. (1993) Optimal smoothing in single-index models. Ann. Statist.,
21, 157-178. [8] Ha®rdle, W. and Stoker, T. M. (1989) Investigating smooth multiple regression by method of average
derivatives. J. Amer. Stat. Ass. 84 986-995. [9] Ha®rdle, W. and A.B. Tsybakov (1993). How sensitive are average derivatives? Journal of Econometrics
58 31-48. [10] Horowitz, J.L. & H®ardle, W. (1996) Direct semiparametric estimation of single-index models with
discrete covariates. J Amer. Stat. Assoc., 91 1632-1640. [11] Hristache, M., Juditsky, A. and Spokoiny, V. (2001) Direct estimation of the single-index coefficients
in single-index models. Ann. Statist. [12] Ichimura, H. (1993) Semiparametric least squares (SLS) and weighted SLS estimation of single-index
models. J. Econometrics 58 71-120.
28

[13] Ichimura, H. and Lee, L. (1991) Semiparametric least squares estimation of multiple index models: Single equation estimation. Nonparametric and Semiparametric Methods in Econometrics and Statistics, edited by Barnett, W., Powell, J. and Tauchen, G.. Cambridge University Press.
[14] Li, K. C. (1991) Sliced inverse regression for dimension reduction (with discussion). Amer. Statist. Ass., 86, 316-342.
[15] Linton, O. (1995) Second order approximation in the partially linear regression model. Econometrica, 63, 1079-1112.
[16] Nishiyama, Y., and P. M. Robinson (2000). Edgeworth expansions for semiparametric average derivatives. Econometrica 68, 931-980.
[17] Nishiyama, Y., and P. M. Robinson (2005). The Bootstrap and the Edgeworth Correction for semiparametric average derivatives. Econometrica 73, 903-948.
[18] Penrose, R. (1955) A generalized inverse for matrices, Proc. Cambridge Philos. Soc. 51, 406-413.
[19] Powell, J.L., J.H. Stock, and T.M. Stoker (1989). Semiparametric estimation of index coefficients. Econometrica 57, 1403-1430.
[20] Powell, J.L. and T.M. Stoker (1996). Optimal bandwidth choice for density weighted averages. Journal of Econometrics 755, 291-316.
[21] Ruppert, D., Sheather, J., and Wand, P. M. (1995) An effective bandwidth selector for local least squares regression. J. Am. Statist. Ass., 90, 1257-1270.
[22] Schott, J.R. (1997) Matrix Analysis for Statistics. John Wiley & Sons. New York.
[23] Weisberg, S. and Welsh, A. H. (1994) Estimating the missing link functions, Ann. of Statist. 22, 16741700.
[24] Xia, Y., Tong, H., Li, W. K. and Zhu, L. (2002) An adaptive estimation of dimension reduction space (with discussions). J. Roy. Statist. Soc. B., 64, 363-410.
[25] Xia, Y. (2006). Asymptotic distributions for two estimators of the single-index model. Econometric Theory (to appear)
29

[26] Xia, Y. and Li, W. K. (1999) On single-index coefficient regression models. Journal of the American Statistical Association, 94, 1275-1285.
[27] Yin, X. & Cook, R. D. (2005). Direction estimation in single-index regressions. Biometrika, 92, 371-384.
30

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Implied Market Price of Weather Risk" by Wolfgang H‰rdle and Brenda LÛpez Cabrera, January 2009.
002 "On the Systemic Nature of Weather Risk" by Guenther Filler, Martin Odening, Ostap Okhrin and Wei Xu, January 2009.
003 "Localized Realized Volatility Modelling" by Ying Chen, Wolfgang Karl H‰rdle and Uta Pigorsch, January 2009.
004 "New recipes for estimating default intensities" by Alexander Baranovski, Carsten von Lieres and AndrÈ Wilch, January 2009.
005 "Panel Cointegration Testing in the Presence of a Time Trend" by Bernd Droge and Deniz Dilan Karaman ÷rsal, January 2009.
006 "Regulatory Risk under Optimal Incentive Regulation" by Roland Strausz, January 2009.
007 "Combination of multivariate volatility forecasts" by Alessandra Amendola and Giuseppe Storti, January 2009.
008 "Mortality modeling: Lee-Carter and the macroeconomy" by Katja Hanewald, January 2009.
009 "Stochastic Population Forecast for Germany and its Consequence for the German Pension System" by Wolfgang H‰rdle and Alena Mysickova, February 2009.
010 "A Microeconomic Explanation of the EPK Paradox" by Wolfgang H‰rdle, Volker Kr‰tschmer and Rouslan Moro, February 2009.
011 "Defending Against Speculative Attacks" by Tijmen DaniÎls, Henk Jager and Franc Klaassen, February 2009.
012 "On the Existence of the Moments of the Asymptotic Trace Statistic" by Deniz Dilan Karaman ÷rsal and Bernd Droge, February 2009.
013 "CDO Pricing with Copulae" by Barbara Choros, Wolfgang H‰rdle and Ostap Okhrin, March 2009.
014 "Properties of Hierarchical Archimedean Copulas" by Ostap Okhrin, Yarema Okhrin and Wolfgang Schmid, March 2009.
015 "Stochastic Mortality, Macroeconomic Risks, and Life Insurer Solvency" by Katja Hanewald, Thomas Post and Helmut Gr¸ndl, March 2009.
016 "Men, Women, and the Ballot Woman Suffrage in the United States" by Sebastian Braun and Michael Kvasnicka, March 2009.
017 "The Importance of Two-Sided Heterogeneity for the Cyclicality of Labour Market Dynamics" by Ronald Bachmann and Peggy David, March 2009.
018 "Transparency through Financial Claims with Fingerprints ≠ A Free Market Mechanism for Preventing Mortgage Securitization Induced Financial Crises" by Helmut Gr¸ndl and Thomas Post, March 2009.
019 "A Joint Analysis of the KOSPI 200 Option and ODAX Option Markets Dynamics" by Ji Cao, Wolfgang H‰rdle and Julius Mungo, March 2009.
020 "Putting Up a Good Fight: The GalÌ-Monacelli Model versus `The Six Major Puzzles in International Macroeconomics'", by Stefan Ried, April 2009.
021 "Spectral estimation of the fractional order of a LÈvy process" by Denis Belomestny, April 2009.
022 "Individual Welfare Gains from Deferred Life-Annuities under Stochastic Lee-Carter Mortality" by Thomas Post, April 2009.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
023 "Pricing Bermudan options using regression: optimal rates of convergence for lower estimates" by Denis Belomestny, April 2009.
024 "Incorporating the Dynamics of Leverage into Default Prediction" by Gunter Lˆffler and Alina Maurer, April 2009.
025 "Measuring the effects of geographical distance on stock market correlation" by Stefanie Eckel, Gunter Lˆffler, Alina Maurer and Volker Schmidt, April 2009.
026 "Regression methods for stochastic control problems and their convergence analysis" by Denis Belomestny, Anastasia Kolodko and John Schoenmakers, May 2009.
027 "Unionisation Structures, Productivity, and Firm Performance" by Sebastian Braun, May 2009.
028 "Optimal Smoothing for a Computationally and Statistically Efficient Single Index Estimator" by Yingcun Xia, Wolfgang H‰rdle and Oliver Linton, May 2009.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

