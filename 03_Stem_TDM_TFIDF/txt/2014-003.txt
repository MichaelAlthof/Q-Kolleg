BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2014-003
An Extended Single Index Model with
Missing Response at Random
Qihua Wang* Tao Zhang** Wolfgang Karl Härdle***
* Shenzhen University, China; Chinese Academy of Sciences, China ** Guangxi University of Technology, China
*** Humboldt-Universität zu Berlin, Germany and C.A.S.E Center for Applied Statistics and Economics, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

An Extended Single Index Model with Missing Response at
Random
Qihua Wang1,3, Tao Zhang2 and Wolfgang Karl H¨ardle4,5
1Institute of Statistical Science, Shenzhen University, Shenzhen 518060, China 2College of Science, Guangxi University of Technology, Liuzhou, China
3 Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences Beijing 100190, China
4 School of Business, Singapore Management University, 50 Stamford Road, Singapore 178899 and
5 C.A.S.E Center for Applied Statistics and Economics, Humboldt-Universit¨at zu Berlin Berlin 10099, Germany
Abstract An extended single-index model is considered when responses are missing at random. A three-step estimation procedure is developed to define an estimator for the single index parameter vector by a joint estimating equation. The proposed estimator is shown to be asymptotically normal. An iterative scheme for computing this estimator is proposed. This algorithm only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high model dimensionality. Some simulation study is conducted to investigate the finite sample performances of the proposed estimators. Key words: Missing data; Estimating equations; Single-index models; Asymptotic normality. AMS(2000) subject classifications. 62J99; 62E20
1

1 Introduction
The single-index model has been paid considerable attention recently because it is useful in several areas of science such as econometrics, biostatistics, finance and so on. The singleindex model (SIM), which is investigated extensively, is of the following form

Y = g( X) + ,

(1.1)

where Y is the univariate response and X is a d-dimensional covariable vector,  is an unknown index parameter vector of interest, the function g(·) is an unknown link function, and E(|X) = 0. The SIM provides dimension reduction in the sense that, if one can estimate the index  efficiently, the univariate index  X serves as a covariable to estimate the nonparametric link g(·). Much effort has been devoted to estimating the index  efficiently. Hall (1989), Zhu and Fang (1992) considered a projection pursuit framework. H¨ardle et al. (1993) employed the kernel smoothing method to study the model (1.1), and gave an empirical rule for bandwidth selection. Ichimura (1993) studied the properties of a semiparametric least-squares estimator in a general single-index model. Ichimura (1987) showed that the parameter vector  can be estimated root-n consistently. H¨ardle et al.(1993) and Hristache et al.(2001) obtained a n consistent estimator of the index vector  using the average derivative method. The technology of sliced inverse regression can also be used to achieve n consistent estimator, see Li (1991) and Zhu (1996).
Let (Yi, Xi) denote the observed values with Yi being the response variable and Xi being the vector of d explanatory variables. In this paper we consider an extended single index model (ESIM) which specifies the relationship of the mean and variance of Yi as follows

E(Yi|Xi) = µ{g( Xi)}, Var(Yi|Xi) = 2V {g( Xi)},

(1.2)

where µ(·) is a known monotonic function, V (·) is a known covariance function, g(·) is an unknown univariate link function and  is an unknown index vector which belongs to the parameter space  = { = (1, · · · , d) :  = 1, 1 > 0,   Rd}. Cui, H¨ardle and Zhu (2011) developed a method of estimating function (EFM) to study the ESIM. They investigated the efficiency and computation of the estimates for the ESIM, and obtained the

2

asymptotic properties of the EFM. However, the existing work is for the case where data are observed fully.
In practice, some responses may be missing, by design (as in two-stage studies) or by circumstance. For example, the response Y s may be very expensive to measure and only part of Y s are available. Another example is that the Y s represent the responses to a set of questions and some sampled individuals refuse to supply the desired information. Actually, missingness of responses is very common in opinion polls, market research surveys, mail enquiries, social-economic investigations, medical studies and other scientific experiments. Missing data issues have been investigated extensively. See, e.g., Rosenbaum and Rubin (1983), Robins et al. (1994), Robins et al. (1995), Wang et al. (2002), Wang et al. (2004) and among others. To the best of our knowledge, the literature is reduced to just a few recent papers for the single-index models (1.1) with µ{g( Xi)} = g( Xi) and V {g( Xi) = 1 for missing data. For this special case, Wang et al. (2010) derived semi-parametric nonlinear least squares estimators by incorporating missing mechanism into the least-squares loss function proposed by H¨ardle et al.(1993) and minimizing the loss function with respect to the bandwidth and the parameters simultaneously. They obtained the central limit theorem(CLT), the law of the iterated logarithm(LIL) for the estimator of , and the optimal convergence rate for the estimator of g(·). However, the computational burden of solving the minimization problem is very high when the dimension of explanatory variable vector is large.
In this paper, we extend the EFM due to Cui, H¨ardle and Zhu (2011) to the missing response case for estimating both  and g(·) in model (1.2). That is, we consider the case where some Y -values may be missing and X is observed completely. The data we observe are
{(Yi, i, Xi)}ni=1
where i = 0 if Yi is missing, otherwise i = 1. Throughout this paper, it is assumed that Y is missing at random (MAR). The MAR assumption implies that  and Y are conditionally independent given X. That is, P( = 1|Y, X) = P( = 1|X). MAR is a common assumption for statistical analysis with missing data and is reasonable in many practical situations, see Little and Rubin (2002).
In this paper, we develop a three-steps estimating approach for estimating both  and
3

g(·) by extending the EFM due to Cui, H¨ardle and Zhu (2011) to the missing response problem. Unlike the two-step estimating approach of Cui, H¨ardle and Zhu (2011), the threesteps estimating approach can define an estimator of g(·). For the estimating approach, the estimating function system only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high dimensionality. Firstly, unlike the method proposed by Wang et al.(2010) for the special case of the ESIM where the minimization is difficult to implement when d is large, our method is easy to implement. Secondly, unlike the method proposed by Wang et al.(2010) where the methodology can only be applied to the case of homogeneous errors, our method can apply to the case of heterogeneous errors. Hence, the proposed methodology based on model (1.2) has more wide application and much more flexible framework. Cui, H¨ardle and Zhu (2011) define the estimator of  only when data are observed fully. However, we define the estimators of both  and g(·) and investigate their asymptotic properties with data missing.
This paper is organized as follows. In Section 2, we describe the estimating procedures. In Section 3, we establish the asymptotic theory for the proposed procedure. Some simulation studies are provided in Section 4. In Section 5, we analyze a real data set to illustrate the proposed procedures and all proofs are included in Section 6.

2 Three-Step Estimation
We develop the following three-step approach to define the estimators of  and g(·), respectively. Step 1: We use the nonparametric fusion-refinement (FR) approach to get the initial estimate of , denoted by ~ with ~ = 1, see Ding and Wang (2011). Step 2: Define the estimator of g(·) and g (·).
Note that under MAR, we have

µ{g(t)} = E[Y | X = t]/E[| X = t].

We then may obtain an initial estimator of µ{g(t)}

µ{g~(t)} = (

n j=1

j

Yj

Hhn

(t

-

~

Xj ))/(

n j=1

j

Hhn

(t

-

~

Xj )),

4

where H(·) is a kernel function with support on (-1, 1), hn is a bandwidth sequence and Hhn(·) = H(·/hn).
Denote by 0 and 1 the values of g(·) and g (·) evaluating at  x, respectively. The local linear approximation for g( X) in a neighborhood of  x is g0( X) = 0+1( X- x). The estimators G( x) d=ef (g( x), g ( x)) are obtained by solving the kernel estimating
equations:

n j=1

Kbn

(~

Xj - 

x)µ {g0(~

Xj)}V -1{g0(~

Xj )}

×[jYj + (1 - j)µ{g~(~ Xj)} - µ{g0(~ Xj)}] = 0,

n j=1

(~

Xj - 

x)Kbn (~

Xj - 

x)µ {g0(~

Xj)}V -1{g0(~

Xj )}

×[jYj + (1 - j)µ{g~(~ Xj)} - µ{g0(~ Xj)}] = 0

(2.1)

where Kbn(·) is the symmetric kernel density function satisfying Kbn(·) = K(·/bn) and bn is

a bandwidth, with respect to 0 and 1, yielding G( x) = (g( x), g ( x)) = (0, 1).

Step 3: Obtain the estimator of . Similar to Cui et al (2011), by eliminating 1, the

parameter space  can be rearranged to the form  = {(1 -

d r=2

r2)1/2, 2

···

, d)

:

d r=2

r2

<

1}.

We turn to the estimation of   . First, we estimate (1) = (2, · · · , d), which can

be obtained by solving the following equation

nj=1[µ{g( Xj)}/(1)]V -1{g( Xj)}[jYj + (1 - j)µ{g~( Xj)} - µ{g( Xj)}] = 0. (2.2)
The solution is defined as ~(1) and hence we obtain ~ by the transformation. Repeat Steps 2 and 3 until convergence and hence we can obtain the estimate of (1) and , (1) and 
say, respectively.

3 Asymptotic theory
To establish asymptotic theory, we firstly give some notations. Let q1(z, y) = µ (z)V -1(z){y- µ(z)}, l(z) = {µ (z)}lV -1(z), (X) = P( = 1|X). Let
j = tjK(t)dt and j = tjK2(t)dt, j = 1, 2, ...
5

  

and S =  0 0

0 , S =  0 2 1

1 2

.

Denote by 0 = (10, (1)0

)

the true values of

 = (1, (1)

)

.

Denote

J

=

 (1)

be the Jacobian matrix of size d × (d - 1) with



J =  -(1) /

1-

 (1)

2
.

Id-1

Denote C = (1 - )J E{X| X} + J (X - E{X| X})g {( X)}. Let

A = E[2{g( X)}C C], B = E[2{g( X)}2C C].
We are ready to present the asymptotic results of the proposed estimators. The proofs of the theorem are provided in Section 6. Theorem 3.1 Suppose that conditions (a) - (f ) hold in Section 6, if nb4n  0, nhn4  0, nh2n/ log(1/hn)   and nb2nhn2  0, then
n((1) - (1)0) L Nd-1(0, ),

where  = A-1BA-1|(1)=(1)0 . Remark 3.1 When  = 1, the asymptotic co-variance matrix reduces to that of Cui, H¨ardle and Zhu (2011).
To define a consistent estimator of the asymptotic variance, a natural way is first to define estimators of h(t) = E{X| X} using the local linear estimate as
nn
h(t) = bi(t)Xi/ bi(t).
i=1 i=1
where bi(t) = Kbn( Xi -t){Sn,2(t)-( Xi -t)Sn,1(t)} and Sn,k(t) = Kbn( Xi -t)( Xi - t)k, k = 1, 2. Let Ci = (1 - i)J h( Xi) + J (Xi - h( Xi))g {( Xi)}. Then the asymptotic variance  can be estimated as

 = [n-1

n i=1

2{g(

Xi)}CiCi ]-1

×{n-1

n i=1

iq12[g(

Xi), Yi]CiCi }[n-1

n i=1

2{g(

Xi)}CiCi ]-1.

6

Remark 3.2 If µ{g( X)} = g( X), 2V {g( X)} = 2, then the matrix  in Theorem 3.1 reduces to
A-1BA-1 = E[{(1 - )J E(X| X) + J (X - E(X | X))[g ( X)]} ×{(1 - )J E(X| X) + J (X - E(X | X))[g ( X)]} 2].

The asymptotic normality of  = (1, (1) ) follows from Theorem 3.1 with a simple application of the multivariate delta-method, since 1 = 1 - (1) . Corollary 3.2 Under the conditions of Theorem 3.1, we have

 n(

-

0)

L

Nd-1(0,

0 ),

where 0 = J J |=0. Using the plug in method, the asymptotic variance 0 can be estimated by JJ , where
J is J with  replaced by . Theorem 3.3 Suppose that conditions of Theorem 3.1 hold, we have

 nbn(g(

X) - g(0

X)

-

µ(2){g( 2

x)} e1S-1U b2n) L N(0, 1),

where U = (µ2, µ3), e1 = (1, 0) and 1 = (x)2{g(

2 x)}f

x(

x) e1S-1SS-1.

Let Zi = (1, b

)Xi-t
bn

.

The asymptotic variance 1 can be estimated by

1 =

e1[n-1

n i=1

iq2[µ{g(

Xi)}, Yi]ZiZi

Kbn (

Xi - t)]-1

×n-1

n i=1

iq12[µ{g(

Xi)}, Yi]ZiZi

Kb2n (

Xi - t)}

×[n-1

n i=1

iq2[µ{g(

Xi)}, Yi]ZiZi

Kbn (

Xi - t)]-1

Remark 3.2. The choice of bandwidth is a very important topic in nonparametric regression estimation. For the semiparametric problem considered here, the n1/2-rate asymptotic normality of the proposed estimators of the global parameter vectors  implies that a proper choice of the bandwidths depends only on the second order term of the mean square errors of these estimators. Therefore the selection of bandwidths might be not so critical if one is only interested in estimation of . However, the estimators of g(·) depend the choice of the bandwidth heavily. The popular cross-validation method such as cross-validation, gen-

7

eralized cross-validation (GCV) and the rule of thumb can be used to select the optimal bandwidth for the estimator of g(·). Here, we recommend using GCV to determine the optimal bandwidth.

4 Simulation studies
We conducted some Monte Carlo simulation studies to evaluate the performance of the proposed estimators for finite samples.
In our simulation, kernel functions H(·) and K(·) were taken as Gaussian kernel. As pointed out in Remark 3.2, the selection of bandwidths is not so critical if one is only interested in estimation of the parametric part. In the following simulation study, the bandwidths were directly taken to be hn = n-2/5 and bn = n-1/3 which satisfy the conditions in the above theorems.
Example 1. To compare the proposed method with Wang et al (2010), we first consider the following simple single-index model

Y = (X )2 + ,

(4.1)

where X is generated from Nd(2, I) for d = 50,   N(0, 0.2), the true parameter is  = 
(2/ 5, 1/ 5, 0, · · · , 0). Take the missing mechanism:

logit{P( = 1|Y, X)} =  X + c0,

(4.2)

 where logit(a) = log{a/(1 - a)},  = ( 2/4, · · · , 2/4, 0, c1) / 1 + c12, c0 is a constant to control missing proportion and c1 is a constant to control the distance between  and . The number of replications is 500. The size of the sample was taken to be n=100, 200 and 400, respectively.
The proposed estimator  is compared to wang of Wang, et al (2010) and the complete case (CC) estimator (denoted by cc), ignoring the missing data. We compute the average absolute bias (AB) which is defined by

AB

=

1 500

500

i=1

1 d

d
|ni ,s - s|

,

s=1

8

where ni ,s is the sth component of ni and ni is one of , wang and cc at the ith run. We also compute the square root of the trace of the standard covariance matrix (SRTSC) which is defined by

SRTSC =

1 499

500

{

1 d

(ni

-

¯)(ni

-

¯)

},

i=1

where

¯

=

1 500

500 i=1

ni

.

The

results

of

AB

and

SRTSC

for

,

 wang ,

 cc

with

about

25%

and

50% missing proportions are reported in Table 1.

insert Table 1 about here

Several observations can be made from Tables 1. Firstly, we can see that AB and SRTSC of all estimators decrease as the sample size increases as expected. Secondly, we also see that  clearer outperforms wang and cc in terms of AB and SRTSC. This shows that the proposed method for the simple single index model improves the method due to Wang,et al (2010) although the proposed methods are suggested for the extended single index model. AB and SRTSC increase with the missing rate increasing for all the estimators.
Example 2. In this study, we consider the following the extended single index model

E(Y |X) = exp{g( X)}, g( X) = sin(X )

Var(Y |X) = 2,

 = 0.2.

(4.3)

 The true parameter is  = (2/ 5, 1/ 5, 0, · · · , 0), X is generated from Nd(2, I) for d = 50,   N(0, 0.04) and the missing mechanism follows the model (4.2). We calculated AB and SRTSC for  and cc where µ(·) = exp(·) in (1.2). At the same time, AB and SRTSC for wang are also computed where we treated model (4.3) as a simply single index model. For each sample size of n=100, 200 and 400, 500 replications were calculated. The simulation results are summarized in Tables 2.

insert Table 2 about here
In this setting, we also compare the AB and SRTSC of , cc and wang. From Table 2, the similar observations to Example 1 can be found except that  have more obviously

9

advantage than cc. This shows that the proposed method is more attractive than cc for the extended single index model.
Example 3. To illustrate the adaptivity of our algorithm to heterogeneous errors, we consider model (4.1),

E(Y |X) = {g( X)}2,

g( X) = X 



Var(Y

|X )

=

2

exp{

5 7

g(

X )},

2 = 1.

(4.4)

 where the true parameter is  = (2/ 5, 1/ 5, 0, · · · , 0), X is generated from Nd(2, I) for d = 50 and the missing mechanism follows the model (4.2). We calculated AB and SRTSC

for  and cc. For each sample size of n=100, 200 and 400, 500 replications were calculated.

The simulation results are also summarized in Tables 3.

For the heteroscedastic setting, wang cannot be calculated and hence we compare  with

cc only. From Table 3, the similar observations to Example 1 can be found except that 

have more obviously advantage than cc.

insert Table 3 about here

5 Real data analysis
ACTG 175 data have been studied by some authors (see, e.g., Hammer et al., 1996; Davidian et al., 2005; Ding and Wang 2011; Hu et al., 2010). In an HIV clinical trial, 2139 HIV positive patients were involved. The patients were randomized into four arms to receive monotherapy (ZDV) or combined therapy (ADV+didanosine, ZDV+zalcitabine, and didanosine). We apply the proposed methods to this data set. The response Y = I( the CD4 count at 96 ± 5 weeks  300). The predictors X are six baseline characteristics: age, weight, CD4 counts at baseline and 20 ± 5 weeks, CD8 counts at baseline and 20 ± 5 weeks. Let T denote the received therapy, i.e., T = 1 if receiving combined therapy, and T = 0 otherwise. Among the 746 patients, there were 473 patients with observations in Y , including 105 patients receiving monotherapy and 368 patients receiving other therapies, and due to death and dropout there were 273 patients with missing observations in Y , including 74 patients with T = 0 and 199 patients with T = 1. All the patients had predictors X observed.

10

The single-index model will be used to model the relationship between the CD4 count at 96 ± 5 weeks and the relevant 6 predictors X = (X1, · · · , X6) :
P( the CD4 count at 96 ± 5 weeks  300|X) = exp{g( X)}/[1 + exp{g( X)}], (5.1)
where  = (1, · · · , 6) . We first focused on the subset of data labeled by T = 0. we can obtain the estimator  by the proposed method. The estimator  is (0.1289, 0.9195, 0.0161, 0.3546, -0.0677) . For the subset of data labeled by T = 1, we can also obtain  = (0.1927, -0.9792, -0.0058, -0.0079, 0.0582, 0.0244) .
As one can see from two estimates, 'weight' has the larger positive influence when patients receive combined therapy. On the contrary, there is a negative influence when patients receive monotherapy for proposing method. 'Age' has the positive influence in the two setting, this is true because resistance become more and more weak with increasing age.
We also plot the scatter plot of the estimated single index g( X) against  X in the setting of T = 0 and T = 1, respectively. The scatter plot suggests a curvature relationship between the response and covariates. The pattern is displayed in Fig 1 and Fig 2, respectively.
insert Figure 1 about here
insert Figure 2 about here
It is seen that there is a nonlinear trend. Therefore, using the model (5.1) in the regression is perhaps more appropriate than using the internally linear model
P( the CD4 count at 96 ± 5 weeks  300|X) = exp( X)/{1 + exp(beta X)}. (5.2)
6 Technical Assumptions and Proofs
6.1 Technical Assumptions
In order to prove the asymptotic normality of the estimators, we first introduce some regularity conditions. (a) µ(·), V (·) and g(·) have two bounded and continuous derivatives. V (·) is uniformly bounded and bounded away from 0.
11

(b) Assume that q(z, y)/z < 0 for z  R and y in the range of the response variable.

(c) Define the block partition of matrix  as follows:

  =  11
21

 12  22

where 11 is a positive constant, 12 is a (d - 1)-dimensional row vector, 21 is a (d - 1)dimensional column vector and 22 is a (d - 1) × (d - 1) nonnegative definite matrix. The largest eigenvalues of 22 is bounded away from infinity. (d) The density function of X has a continuous second derivative on its support A. The

density function f X( X) of random variable  X is bounded away from 0 on T and satisfies the Lipschitz condition of order 1 on T, where T = { X : X  T } and T is the compact support set of X.

(e) The kernel K(·) is a bounded and symmetric density function with a bounded derivative,

and satisfies

+ -

|t|2 K (t)dt

<

,

H(·) is a bounded kernel function of order 2 with bounded support. (f) (·) > 0 and µ(·) = 0.

6.2 Proofs of Theorems
In order to prove the asymptotic normality of the estimators, we first introduce several lemmas. Lemma 1. Let (x1; y1), · · · , (xn; yn) be i.i.d random vectors, where the yis are univariate random variables. Assume that E |y|r <  and supx |y|rp(x, y) < , where p denotes the joint density of (x, y). Let K be a bounded positive function with bounded support, satisfying the Lipschitz condition. Then

supx |n-1

ni=1{Kh(xi

-

x)yi

-

E[Kh(xi

-

x)yi]}|

=

Op{(

-

log nh

h

)1/2}

provided that n2-1h   for some  < 1 - r-1. 12

Lemma 1 is a direct result of Mack and Silverman (1982), which is also cited by many papers on kernel method. In what follows, we give an important lemma which derives the asymptotic structure of g which will be used to get the asymptotic property of parameters. Lemma 2. Suppose that conditions of Theorem 3.1 hold, G( x) and G( x) are defined in Step 2 of Section 2, then

 nbn(H2{G(

x) - G(

x) - (x)}) L N(0, (x)2{g(

2 x)}f

x(

x) S-1SS-1)

where H2 = diag(1, bn) and

(x)

=

µ(2){g( 2

x)} S-1U b2n.

Proof Let Yi = iYi + (1 - i)µ(g~{ Xi)} and Yi = iYi + (1 - i)µ(g{ Xi)}. Note that solution G( x) of the estimating equation defined in (2.1) can be obtained by maximizing the quasi-likelihood:

(0, 1) d=ef

n i=1

Q[µ{0

+

1(

Xi - 

x)}, Yi]Kbn(

Xi - 

x)

with respect to (0, 1), where Q(µ, y) =

µ y-s y V {µ-1(s)}

and

µ-1(·)

is

the

inverse

function

of

µ(·).

Let

ql(z, y)

=

l zl

Q[µ(z

),

y],

l

=

1, 2, 3,

then

q1[z, y]

=

{y - µ(z)}1(z)

and

q2[z, y]

=

{y - µ(z)}l(z) - 2(z), where l(z) = µ(l){z}V -1{z}.

Denote



=

 nbnH2(g(

x)-g(

x), g (

x)-g (

x)),



=

 nbnH2(0-g(

x), 1-

g (

x)) and ¯i(

x) = 0 + 1(

Xi - 

x) and Xi = (1, 

Xi- bn

x ).

Then  is the so-

lution of the following normalized function

() = bn

ni=1{Q[µ{¯i(

x)

+

1 nbn



Xi}, Yi] - Q[µ{¯i(

x)}, Yi]}Kbn(

Xi - 

x).

13

By Taylor expansion, we have

()

=

Vn



+

1 2

Bn(1

+ Op(1)),

(6.1)

where

Vn =

bn n

n i=1

q1[¯i(

x), Y ]XiKbn(

Xi - 

x),

Bn

=

1 n

n i=1

q2[¯i(

x), Yi]XiXi

Kbn (

Xi - 

x)

According to the definition of q2[x, y], we have

It can be observed

q2(¯i, Yi) - q2(¯i, Yi) = 1(¯i)(Yi - Yi) = (1 - i)1(¯i)(µ{g~( Xi)} - µ{g( Xi)}).

(6.2)

µ{g~( Xi)} - µ{g( Xi)}

={

n j=1

j

Hhn

(

Xi - 

Xj )}-1 {

n j=1

j

Yj

Hhn

(

Xi - 

Xj)} - µ{g(

Xi)}

=[

n j=1

j

Yj

Hhn

(

Xi - 

Xj) -

n j=1

j

µ{g(

Xi)}Hhn (

Xi - 

Xj )]

×{

n j=1

j

Hhn

(

Xi - 

Xj )}-1

=

n j=1

j [Yj

-

µ{g(

Xj )}Hhn (

Xi - 

Xj ){

n j=1

j

Hhn

(

Xi - 

Xj )}-1

-

n j=1

j

[µ{g(

Xj)} - µ{g(

Xi)}]Hhn (

Xi - 

Xj ){

n j=1

j

Hhn

(

Xi - 

Xj )}-1

=

n j=1

j

q1

[µ{g

(

Xj)}, Yj]{1{g(

Xj )}-1 Hhn (

Xi - 

Xj )}

×{nhn(Xi)f Xi( Xi)}-1 + Op(1).

(6.3)

14

By (6.2) and (6.3), we have

Bn

=

1 n

n i=1

q2{¯i(

x), Yi}XiXi

Kbn (

Xi - 

x) + Op(1)

= E[q2[¯1( x), Y1]X1X1 Kbn( X1 -  x)] + Op(1)

= -2{g( x)}f x( x)S + Op(1).

For Vn, we have

(6.4)

Vn =

bn n

n i=1

q1[¯i

(

x), Yi]XiKbn(

Xi - 

x)

+

bn n

n i=1

[q1

(¯i(

x), Yi) - q1(¯i(

x), Yi)]XiKbn(

Xi - 

x)

d=ef Vn1 + Vn2.

For Vn2, by Taylor expansion, we have

(6.5)

Vn2 =

bn n

ni=1(1 - i)1{g(¯i( x))}[µ{g~( Xi)} - µ{g( Xi)}]XiKbn( Xi -  x)

 +Op( nbn|µ{g~(

Xi)} - µ{g(

Xi)}|2)

= bn n (1-i)1{g(¯i( x))}
n i=1 nhnf Xi ( Xi)(Xi)

×

n j=1

j

q1[µ{g( 1{g(

Xj )},Yj Xj )}

]

XjKhn

(

Xj - 

Xi)Kbn (

Xi - 

x) + Op(1)

=

bn n

n i=1

iq1[µ{g(

Xi

)},

Yi]

(1-(Xi)1{g(¯i( (Xi)1{g( Xi

x))} )}

XiKbn

(

Xi - 

x) + Op(1)

 Tn + Op(1).

(6.6)

By (6.1), (6.4), (6.5) and (6.6), we have

() = (Vn1 + Tn)  - B/2 + Op(1). According to quadratic approximation lemma, we obtain

(6.7)

 = B-1(Vn1 + Tn) + Op(1).

(6.8)

15

It is easy to show that

E(Vn1 + Tn)

=

bn n

E[q1{¯1(

x), Y1}X1Kbn(

X1 - 

x)]

+

bn n

E[1q1[µ{g(

X1

)},

Y1

]

(1-(X1)1{g(¯1( (X11{g( x)}

x))}

X1Kbn

(

X1 - 

x)]

=

bn µ(2){g( n2

x)}b2n 2{g(

x)}f

x(

x)U {1 + O(1)}.

and

Var(Vn1 + Tn)

= bn Var[q1{¯1( x), Y1}X1Kbn( X1 -  x)]

+bn Var[1q1[µ{g(

X1

)},

Y1

]

(1-(X1)1{g(¯1 (X11{g(

( x)}

x))}

X1Kbn

(

= 22{g(

x)}f (x)

x(

x) S{1 + O(1)}.

X1 - 

x)]

(6.9)

Since Vn1 + Vn2 is a sum of i.i.d. random vectors and Liapounov's condition is satisfied, thus proof is completed. Lemma 3. Suppose that conditions of Theorem 3.1 hold, then

where C is defined in Section 3.

gb( x) (1)

p

C,

16

Proof The first equation of (2.1) can be decomposed as

0 = n-1

n j=1

Kbn

(

Xj - t)µ {0 + 1(

Xj - 

x)}V -1{0 + 1(

Xj - 

x)}

×[jYj + (1 - j)µ{g~( Xj)} - µ{0 + 1( Xj -  x)}]

= n-1

n j=1

Kbn

(

Xj - t)µ {0 + 1(

Xj - 

x)}V -1{0 + 1(

Xj - 

x)}

×[jYj + (1 - j)µ{g( Xj)} - µ{0 + 1( Xj -  x)}] + op(1)

= n-1

n j=1

j

Kbn

(

Xj - t)µ {0 + 1(

Xj - 

x)}V -1{0 + 1(

Xj - 

x)}

×[Yj - µ{0 + 1( Xj -  x)}]

+n-1 jn=1(1 - j)Kbn( Xj - t)µ {0 + 1( Xj -  x)}V -1{0 + 1( Xj -  x)}

×[µ{g( Xj)} - µ{0 + 1( Xj -  x)}] + op(1)

d=ef D1 + D2 + op(1).

(6.10)

For D1, taking derivatives with respect to (1), we have

= nD1
(1)

-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)µ {0 + 1(

Xj - 

x)}

×V -1{0 + 1( Xj -  x)}[Yj - µ{0 + 1( Xj -  x)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)µ {0 + 1(

Xj - 

x)}

×[ b0
(1)

+

b1 



(1)

(

Xj - 

x) + 1J

(Xj - x)]

×V -1{0 + 1( Xj -  x)}[Yj - µ{0 + 1( Xj -  x)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)µ {0 + 1(

Xj - 

x)}(V -1) {0 + 1(

Xj - 

x)}

×[ b0
(1)

+

b1 



(1)

(

Xj - 

x) + 1J

(Xj - x)]

×[Yj - µ{0 + 1( Xj -  x)}]

-n-1

n j=1

j

Kbn

(

Xj - 

x)[µ {0 + 1(

Xj - 

x)}]2

×[ b0
(1)

+

b1 



(1)

(

Xj - 

x) + 1J

(Xj - x)]

×V -1{0 + 1( Xj -  x)}

d=ef F1 + F2 + F3 - F4.

17

We decompose F1 as follows:

F1 = n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)µ {g(0

Xj)}V -1{g(0

Xj)}[Yj - µ{g(0

Xj )}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)[µ {0 + 1(

Xj - 

x)} - µ {g(0

Xj )}]

×V -1{g(0 Xj)}[Yj - µ{g(0 Xj)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)µ {g(0

Xj )}

×V -1{g(0 Xj)}[µ{0 + 1( Xj -  x) - µ{g(0 Xj)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)[µ {0 + 1(

Xj - 

x)} - µ {g(0

Xj )}]

×V -1{g(0 Xj)}[µ{0 + 1( Xj -  x) - µ{g(0 Xj)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)µ {g(0

Xj )}

×[V -1{0 + 1( Xj -  x)} - V -1{g(0 Xj)}][Yj - µ{g(0 Xj)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)[µ {0 + 1(

Xj - 

x)} - µ {g(0

Xj )}]

×[V -1{0 + 1( Xj -  x)} - V -1{g(0 Xj)}][Yj - µ{g(0 Xj)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)µ {g(0

Xj )}

×[V -1{0 + 1( Xj -  x)} - V -1{g(0 Xj)}]

×[µ{0 + 1( Xj -  x) - µ{g(0 Xj)}]

+n-1

n j=1

j

Kbn

(

Xj - 

x)J

(Xj - x)[µ {0 + 1(

Xj - 

x)} - µ {g(0

Xj )}]

×[V -1{0 + 1( Xj -  x)} - V -1{g(0 Xj)}]

×[µ{0 + 1( Xj -  x) - µ{g(0 Xj)}]

d=ef

8 i=1

F1i.

Noting that ukK(1)(u)du = 0 when k is an even number and using the arguments similar

to the proof of Theorem 5.2 in Ichimura (1993), we have F1i = op(1) for k = 1, · · · , 8. Similarly, we can show that F2 = Op(1) and F3 = Op(1) under Conditions (a), (d) and

18

(e). Further, we also can show

n-1

n j=1

j

Kbn

(

Xj - 

x)[µ {0 + 1(

Xj - 

x)}]2

×

b1 (1)

(

Xj - 

x)V -1{0 + 1(

Xj - 

x)}

= Op(1).

According to Lemma 1, we obtain

n-1

n j=1

j

Kbn

(

Xj - 

x)[µ {0 + 1(

Xj - 

x)}]2V -1{0 + 1(

Xj - 

x)}

= E{[µ {g( X)}]2V -1{g( X)}| X = u}f (u){1 + Op(1)}.

and

n-1

n j=1

j

Kbn

(

Xj - 

x)[µ {0 + 1(

Xj - 

x)}]2J

XjV -1{0 + 1(

Xj - 

x)}

= E{[µ {g( X)}]2V -1{g( X)}J X| X = u}f (u){1 + Op(1)}.

Then, we have

D1 =

- E{[µ {g(

X)}]2V -1{g(

X )}|

X

=

u}f

(u)

b0 (1)

- E{[µ {g( X)}]2V -1{g( X)}J X| X = u}f (u)1

+ E{[µ {g( X)}]2V -1{g( X)}J x| X = u}f (u)1 + Op(1).

(6.11)

19

For D2, similarly taking derivatives with respect to (1), we have

= nD2
(1)

-1

jn=1(1 - j)Kbn( Xj -  x)J (Xj - x)µ {0 + 1( Xj -  x)}

×V -1{0 + 1( Xj -  x)}[µ{g( Xj)} - µ{0 + 1( Xj -  x)}]

+n-1

n j=1

(1

-

j

)Kbn

(

Xj - 

x)µ {0 + 1(

Xj - 

x)}

×[ b0
(1)

+

b1 



(1)

(

Xj - 

x) + 1J

(Xj - x)]

×V -1{0 + 1( Xj -  x)}[µ{g( Xj)} - µ{0 + 1( Xj -  x)}]

+n-1

n j=1

(1

-

j

)Kbn

(

Xj - 

x)µ {0 + 1(

Xj - 

x)}(V -1)

×{0 + 1(

Xj - 

x)}[

b0 (1)

+

b1 



(1)

(

Xj - 

x) + 1J

(Xj - x)]

×[µ{g( Xj)} - µ{0 + 1( Xj -  x)}]

-n-1 jn=1(1 - j)Kbn( Xj -  x)[µ {0 + 1( Xj -  x)}]2

×[ b0
(1)

+

b1 



(1)

(

Xj - 

x) + 1J

(Xj - x)]V -1{0 + 1(

Xj - 

x)}

+n-1

n j=1

(1

-

j

)Kbn

(

Xj - 

x)µ {0 + 1(

Xj - 

x)}

×µ {g( Xj)}J XjV -1{0 + 1( Xj -  x)}

d=ef R1 + R2 + R3 - R4 + R5.

Using the arguments similar to F1, we can obtain Rj = Op(1) for j = 1, 2, 3. Again, according to Lemma 1, we also obtain

R4 =

E{(1 - )[µ {g(

X)}]2V -1{g(

X )}|

X

=

u}f

(u)

b0 (1)

+ E{(1 - )[µ {g( X)}]2V -1{g( X)}J X| X = u}f (u)1

- E{(1 - )[µ {g( X)}]2V -1{g( X)}J x| X = u}f (u)1 + op(1).

and

R5 = E{(1 - )[µ {g( X)}]2V -1{g( X)}J X| X = u}f (u) + Op(1).

20

Then, we obtain

D2 =

- E{(1 - )[µ {g(

X)}]2V -1{g(

X )}|

X

=

u}f

(u)

b0 (1)

- E{(1 - )[µ {g( X)}]2V -1{g( X)}J X| X = u}f (u)1

+ E{(1 - )[µ {g( X)}]2V -1{g( X)}J x| X = u}f (u)1

+ E{(1 - )[µ {g( X)}]2V -1{g( X)}J X| X = u}f (u) + Op(1).

(6.12)

Combining the (6.10), (6.11) and (6.12), we obtain

b0 (1)

=

gb( x) (1)

p C.

The proof is completed. Proof of Theorem 3.1 By Taylor expansion, we have

[µ{g( Xj)} - µ{g( Xj)}]

= µ {g( Xj)}{g( Xj) - g( Xj)} + Op(1)

= µ {g( Xj)}{g( Xj) - g( Xj) + g( Xj) - g( Xj)} + Op(1)

= µ {g(

Xj

)}



gb( Xj (1)

)

(

(1)

-

(1))

+µ {g( Xj)}{g( Xj) - g( Xj)} + Op(1).

Let

Zj = [µ {g(

Xj

)}

gb( Xj (1)

)

]

V -1{g(

Xj)}[µ {g(

Xj

)}



gb( Xj (1)

)

],

Qj = [µ {g(

Xj

)}



gb( Xj (1)

)

]

V -1{g(

Xj)}µ {g(

Xj )}{g(

Xj) - g(

Xj )}.

21

Then, it can be observed

n-1

n j=1

Zj n( (1)

-

(1))

= n-1/2

nj=1[µ {g(

Xj

)}



gb( Xj (1)

)

]

V -1{g(

Xj )}

×[µ{g( Xj)} - µ{g( Xj)}] - n-1/2

n j=1

Qj

+

Op(1).

= n-1/2

nj=1{[µ {g(

Xj

)}



gb( Xj (1)

)

]

V -1{g(

Xj )}{j [µ{g( (1)

Xj)} - µ{g((1)

Xj )}]

+(1 - j)[µ{g((1) Xj)} - µ{g((1) Xj)}]} - n-1/2

n j=1

Qj

+

Op(1).

(6.13)

By Lemma 2 and some tedious calculations, we have

n-1/2

jn=1{[µ {g(

Xj)} - µ {g(

Xj

)}]



gb( Xj (1)

)

]

V -1{g(

Xj )}

×{j[Yj - µ{g( Xj)}] + (1 - j)[µ{g~( Xj)} - µ{g( Xj)}]} = Op(1).

(6.14)

By (6.13) and (6.14), we have

n((1) - (1)) = Z-1{n-1/2

n j=1

µ

{g(

Xj )}

gb( Xj ) (1)

V

-1{g(

Xj }

×{j[Yi - µ{g( Xj)}] + (1 - j)[µ{g~( Xj)} - µ{g( Xj)}]}

-n-1/2

n j=1

Qj

+

Op(1).

(6.15)

By Lemma 1 and Lemma 2, we can obtain

n-1/2

n j=1

Qj

=

Op(1).

By condition (a), (6.15) and (6.16), we obtain

n((1) - (1)) = Z-1n-1/2

n j=1

µ

{g(

Xj

)}

gb( Xj (1)

)

V

-1{g(

Xj )}

×{j[Yi - µ{g( Xj)}]} + Op(1).

Theorem 3.1 follows directly form Lemma 3. The proof is completed.

(6.16) (6.17)

22

Proof of Theorem 3.3 By Theorem 3.1, we know that  is a root-n consistent estimator

of 0. Then, using the arguments similar to the proof of Proposition 1 (iii) in Cui et al

(2011), we have that

 nbn(g(

X) - g(0

X)) = Op(1).

According to Lemma 2, we know

 nbn{g(

0

X) - g(0

X)

-

µ(2){g( 2

x)} e1S-1U bn2 }

L

N(0,

2{g( (x)f x

x)} ( x)

S

-1S



S

-1

),

Therefore, we have

 nbn{g(

X) - g(0

X)

-

µ(2){g( 2

x)} e1S-1U b2n}

 = nbn{g(

X) - g(0

X )}

 + nbn[g(

0

X) - µ{g(0

X)

-

µ(2){g( 2

x)} e1S-1U bn2 }]

L

N(0,

2{g( (x)f x

x)} ( x)

S

-1S



S

-1

).

The proof is completed.

Acknowledgements. Wang's research was supported by the National Science Fund for Distinguished Young Scholars in China (10725106), the National Natural Science Foundation of China (General program 11171331 and Key program 11331011), a grant from the Key Lab of Random Complex Structure and Data Science, CAS and the Natural Science Foundation of SZU. H¨ardle's research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

23

References
Carroll, R. J., Ruppert, D., Welsh, A. H., (1998). Local estimating equations. J. Amer. Statist. Assoc., 93:214-227.
Cui, X., H¨ardle, W., Zhu, L. X. (2011). The EFM approach for single-index models. Ann. Statist., 39:1658­1688.
Ding, X. B., Wang, Q. H. (2011). Fusion-Refinement procedure for dimension reduction with missing response at random J. Amer. Statist. Assoc., 106:1193­1207.
Fan, J., Gijbels, I. (1996). Local polynomial modeling and its applications. London: Chapman and Hall.
Gelman, A., Carlin, J. B., Sterm, H. S., Rubin, D. B.(1995). Bayesian Data Analysis. London: Chapman and Hall
Hall, P. (1989). On projection pursuit regression. Ann. Statist., 17:573­588.
H¨ardle, W., Hall, P., Ichimura, H. (1993). Optimal smoothing in single-index models Ann. Statist., 21:157­178.
H¨ardle, W., Tsybakov, A. B.(1993). How sensitive are average derivatives. Journal of Econometrics, 58:31­48.
Hristache, M., Juditsky, A., Spokoiny, V. (2001). Direct estimation of the index coefficient in a single-index model. Ann. Statist., 29:595­623.
Hu, Z. H., Follmann, D. A., and Qin, J. (2010). Semiparametric dimension reduction estimation for mean response with missing data. Biometrika, 97:305­319.
Ichimura, H. (1993). Semiparametric least squares (SLS) and weighted SLS estimation of single-index models. Journal of Econometrics, 58:72­120.
Ichimura, H. (1987). Estimation of single index models. Ph.D. Dissertation, Dept. Economics, MIT.
Mack, Y., Silverman, B. (1982). Weak and strong uniform consistency of kernel regression estimates. Probability Theory and Related Fields, 61:405­415.
24

Robins, J. M., Rotnitzky, A., Zhao L. P. (1994). Estimation of regression coefficients when some regressors are not always observed. J. Amer. Statist. Assoc., 89:846­866.
Robins, J. M., Rotnitzky, A., Zhao L. P. (1995). Analysis of semiparametric regression models for repeated outcomes in the presence of missing data. J. Amer. Statist. Assoc., 90:106­121.
Rosenbaum, P. R., Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70:41­55.
Wang, Q. H., Rao, J. N. K. (2002). Empirical likelhood-based inference under imputation for mssing response data. The Annals of Statist., 30:896­924.
Wang, Q. H., Linton,O., H¨ardle, W. (2004). Semiparametric regression analysis with missing response at random. J. Amer. Statist. Assoc., 99:334­345.
Li, K. C. (1991). Sliced inverse regression for dimension reduction. J. Amer. Statist. Assoc., 86:316­342.
Wang, Y. H., Shen, J. S., He, S. Y., Wang, Q. H. (2010). Estimation of single index model with missing response at random. J. Stat. Plan. Inference, 140:1671­1690.
Zhu, L. X., Fang, K. T. (1992). On projection pursuit approximation for nonparametric regression. In P.S. Sen, I.A. Salama (Eds.), Proceedings of Order Statistics and Nonparametrics: Theory and Applications, 455­469.
Zhu, L. X., Fang, K. T. (1996). Asymptotics for kernel estimate of sliced inverse regression. Ann. Statist., 14:1053­1068.
25

Table 1: AB and SRTSC of , cc and wang with different missing proportion and different sample sizes.

np

100

p = 0.25 p = 0.50

200

p = 0.25 p = 0.50

400

p = 0.25 p = 0.50

100

p = 0.25 p = 0.50

200

p = 0.25 p = 0.50

400

p = 0.25 p = 0.50

AB
 0.0247 0.0476 0.0104 0.0203 0.0063 0.0096
SRTSC 0.0305 0.0572 0.0133 0.0253 0.0086 0.0125

 cc 0.0967 0.1256 0.0405 0.0737 0.0062 0.0247
0.0932 0.0808 0.0784 0.0934 0.0271 0.0623

 wang 0.1014 0.1277 0.0413 0.0209 0.0114 0.0102
0.1092 0.1090 0.0873 0.0775 0.0494 0.0439

26

Table 2: AB and SRTSC of , cc and wang with different missing proportion and different sample sizes.

np

100

p = 0.25 p = 0.50

200

p = 0.25 p = 0.50

400

p = 0.25 p = 0.50

100

p = 0.25 p = 0.50

200

p = 0.25 p = 0.50

400

p = 0.25 p = 0.50

AB
 0.0347 0.0685 0.0106 0.0232 0.0064 0.0095
SRTSC 0.0442 0.0775 0.0138 0.0305 0.0090 0.0125

 cc 0.1273 0.1366 0.0784 0.1144 0.0203 0.0535
0.0813 0.0722 0.0904 0.0861 0.0203 0.0831

 wang 0.1044 0.1262 0.0184 0.0730 0.0059 0.0092
0.1267 0.1371 0.0581 0.1070 0.0296 0.0438

27

Table 3: AB and SRTSC of  and cc with different missing proportion and different sample sizes.

AB

np

100

p = 0.25 p = 0.50

200

p = 0.25 p = 0.50

400

p = 0.25 p = 0.50

 0.0500 0.0718 0.0325 0.0443 0.0227 0.0294

SRTSC

100

p = 0.25 p = 0.50

0.0567 0.0779

200

p = 0.25 p = 0.50

0.0382 0.0504

400

p = 0.25 p = 0.50

0.0263 0.0340

 cc 0.1214 0.1335 0.0829 0.1106 0.0360 0.0630
0.0873 0.0759 0.0916 0.0913 0.0556 0.0824

28

0.57 0.565 0.56 0.555 0.55 0.545 0.54 0.535 0.53 0.525 0.52
-6 -5 -4 -3 -2 -1 0 1 2 3
Figure 1: the scatter plot of the estimated single index g( X) against  X in the setting of T = 0.
29

1 0.9 0.8 0.7 0.6 0.5 0.4
-4 -3 -2 -1 0 1 2 3 4 5
Figure 2: the scatter plot of the estimated single index g( X) against  X in the setting of T = 1.
30

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Principal Component Analysis in an Asymmetric Norm" by Ngoc Mai Tran, Maria Osipenko and Wolfgang Karl Härdle, January 2014.
002 "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data" by Lijie Gu, Li Wang, Wolfgang Karl Härdle and Lijian Yang, January 2014.
003 "An Extended Single Index Model with Missing Response at Random" by Qihua Wang, Tao Zhang, Wolfgang Karl Härdle, January 2014.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

