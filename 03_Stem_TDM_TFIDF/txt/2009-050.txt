BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2009-050
Generalized single-index models:
The EFM approach
Xia Cui* Wolfgang Karl Härdle**
Lixing Zhu***
* Sun Yat-Sen University Guangzhou, China ** Humboldt-Universität zu Berlin, Germany *** Hong Kong Baptist University, Hong Kong
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Generalized single-index models: The EFM approach 
Xia Cui (cuixia@mail.sysu.edu.cn )
School of Mathematics & Computational Science Sun Yat-Sen University, Guangzhou, China
Wolfgang Karl H¨ardle (haerdle@wiwi.hu-berlin.de)
CASE-Center for Applied Statistics & Economics Institut fu¨r Statistik und O¨konometrie Wirtschaftswissenschaftliche Fakulta¨t Humboldt-Universit¨at zu Berlin D-10178 Berlin, Germany
National Central University, Taipei, Taiwan
Lixing Zhu (lzhu@hkbu.edu.hk)
Department of Mathematics, Hong Kong Baptist University, Hong Kong School of Finance and Statistics, East China Normal University, Shanghai, China
The authors made equal contributions to this paper. Correspondence should be addressed to Lixing Zhu. The second author thanks the Deutsche Forschungsgemeinschaft SFB 649 "O¨ konomisches Risiko" for financial support. The third author thanks the "HKBU 2030/07P" for financial support.
1

Abstract. Generalized single-index models are natural extensions of linear models and circumvent the so-called curse of dimensionality. They are becoming increasingly popular in many scientific fields including biostatistics, medicine, economics and financial econometrics. Estimating and testing the model index coefficients  is one of the most important objectives in the statistical analysis. However, the commonly used assumption on the index coefficients,  = 1, represents a non-regular problem: the true index is on the boundary of the unit ball. In this paper we introduce the EFM approach, a method of estimating functions, to study the generalized single-index model. The procedure is to first relax the equality constraint to one with (d - 1) components of  lying in an open unit ball, and then to construct the associated (d - 1) estimating functions by projecting the score function to the linear space spanned by the residuals with the unknown link being estimated by kernel estimating functions. The root-n consistency and asymptotic normality for the estimator obtained from solving the resulting estimating equations is achieved, and a Wilk's type theorem for testing the index is demonstrated. A noticeable result we obtain is that our estimator for  has smaller or equal limiting variance than the estimator of Carroll et al. (1997). A fixed point iterative scheme for computing this estimator is proposed. This algorithm only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high model dimensionality. Numerical studies based on simulation and on applications suggest that this new estimating system is quite powerful and easy to implement.
Key words and phrases: Generalized single-index model, index coefficients, estimating equations, asymptotic properties, iteration.
noindentJEL-Codes: C02, C13, C14, C21
1. Introduction
Single-index models combine flexibility of modelling with interpretability of (linear) coefficients. They circumvent the curse of dimensionality and are becoming increasingly popular in many scientific fields. The reduction of dimension is achieved by assuming the link function to be a univariate function applied to the projection of
2

explanatory covariate vector on to some direction. In this paper we consider an extension of single index models where, instead of a distributional assumption, assumptions of only the mean function and variance function of the response are made. Let (Yi, Xi), i = 1, · · · , n, denote the observed values with Yi being the response variable and Xi as the vector of d explanatory variables. The relationship of the mean and variance of Yi are specified as follows:

E(Yi|Xi) = µ{g( Xi)}, Var(Yi|Xi) = 2V {g( Xi)},

(1.1)

where µ is a known monotonic function, V is a known covariance function, g is an unknown univariate link function and  is an unknown index vector which belongs to the parameter space  = { = (1, · · · , d) :  = 1, 1 > 0,   Rd}. Here we assume the parameter space is  rather than the entire Rd in order to ensure that  in the representation (1.1) can be uniquely defined. This is a commonly used assumption on the index parameter (see Carroll et al., 1997; Zhu and Xue, 2006). Model (1.1) is flexible enough to cover a variety of situations. If µ is the identity function and V is equal to constant 1, (1.1) reduces to a single-index model (H¨ardle, Hall and Ichimura, 1993). Model (1.1) is an extension of the generalized linear model (McCullagh and Nelder, 1989) and the single index model. When the conditional distribution of Y is logistic, then µ{g( X)} = exp{g( X)}/[1 + exp{g( X)}] and V {g( X)} = exp{g( X)}/[1 + exp{g( X)}]2.

For single-index models: µ{g( X)} = g( X) and V {g( X)} = 1, various

strategies for estimating  have been proposed in the last decades. Two most popular

methods are the average derivative method (ADE) introduced in Powell et al. (1989)

and H¨ardle and Stoker (1989), and the simultaneous minimization method of H¨ardle,

Hall and Ichimura (1993). Next we will review these two methods in short. The

ADE method is based on that E(Y |X = x)/x = g ( x) which implies that the

gradient of the regression function is proportional to the index parameter . Then a

natural estimator for  is ^ = n-1

n
G(Xi)/ n-1

n
G(Xi)

with G(x) denoting

i=1 i=1
E(Y |X = x)/x and · being the Euclidean norm. An advantage of the ADE

approach is that it allows estimating  directly. However, the high-dimensional kernel

smoothing used for computing G(x) suffers from the "curse of dimensionality" if

3

the model dimension d is large. Hristache, Juditski and Spokoiny (2001) improved the ADE approach by lowering the dimension of the kernel gradually. The method of H¨ardle, Hall and Ichimura (1993) is carried out by minimizing a least squares criterion based on nonparametric estimation of the link g with respect to  and bandwidth h. However, the minimization is difficult to implement since it depends on an optimization problem in a high-dimensional space. Xia et al. (2002) proposed to minimize average conditional variance (MAVE). Because the kernel used for computing  is a function of Xi - Xj , MAVE meets the problem of data sparseness. All the above estimators are consistent under some regular conditions. Asymptotic efficiency comparisons of the above methods have been discussed in Xia (2006) resulting in the MAVE estimator of  having the same limiting variance as the estimators of H¨ardle, Hall and Ichimura (1993), and claiming that alternative versions of the ADE method having larger variance.
The main challenges of estimation in the semiparametric model (1.1) are that, the support of the infinite dimensional nuisance parameter g(·) depends on the finite dimensional parameter , and the parameter  is on the boundary of a unit ball. For estimating  the former challenge forces us to deal with the infinite dimensional nuisance parameter g. The latter one represents a non-regular problem. The classic assumptions about asymptotic properties of the estimates for  are not valid. In addition, as a model proposed for dimension reduction, the dimension d may be very high and one often meets the problem of computation. To attack the above problems, in this paper we will develop an estimating function method (EFM) and then introduce a computational algorithm to solve the equations based on a fixed point iterative scheme. We first choose an identifiable parametrization which transforms the boundary of a unit ball in Rd to the interior of a unit ball in Rd-1. By eliminating 1, the parameter space
dd
 can be rearranged to a form {((1 - r2)1/2, 2, · · · , d) : r2 < 1}. Then the
r=2 r=2
derivatives of a function with respect to (2, · · · , d) are readily obtained by chain rule and the classic assumptions on the asymptotic normality hold after transformation. The estimating functions (equations) for  can be constructed by replacing g( X) with g^( X). The estimate g^ for the nuisance parameter g is obtained using kernel estimating functions and the smoothing parameter h is selected using K-fold crossvalidation. For the problem of testing the index, we establish a quasi-likelihood ratio
4

based on the proposed estimating functions and show that the test statistics asymptotically follow a 2-distribution whose degree of freedom does not depend on nuisance parameters, under the null hypothesis. Then a Wilks' type theorem for testing the index is demonstrated.
The proposed EFM technique is essentially a unified method of handling different types of data situations including categorical response variable and discrete explanatory covariate vector. The main results of this research are as follows:
(a) Efficiency. A surprising result we obtain is that our EFM estimator for  has smaller or equal limiting variance than the estimator of Carroll et al. (1997).
(b) Computation. The estimating function system only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high model dimensionality. Unlike the quasi-likelihood inference (Carroll et al., 1997) where the maximization is difficult to implement when d is large, the reparametrization and the explicit formulation of the estimating functions faciliate an efficient computation algorithm. Here we use a fixed point iterative scheme to compute the resultant estimator. The simulation results show that the algorithm adapts to higher model dimension and richer data situations than the MAVE method of (Xia et al., 2002).
The paper is organized as follows. In Section 2, we state the generalized singleindex model, discuss estimation of g using kernel estimating functions and of  using profile estimating functions, and investigate the problem of testing the index using quasi-likelihood ratio. In Section 3 we provide a computation algorithm for solving the estimating functions and illustrate the method with simulation and practical studies. The proofs are deferred to the Appendix.
2. Estimating function method (EFM) and its large sample properties
In this section, which is concerned with inference based on the estimating function method, the model of interest is determined through specification of mean and variance functions, up to an unknown vector  and an unknown function g. Except for Gaussian
5

data, model (1.1) needs not be a full semiparametric likelihood specification. Note that the parameter space  = { = (1, · · · , d) :  = 1, 1 > 0,   Rd} means that  is on the boundary of a unit ball and it represents therefore a non-regular problem. So we first choose an identifiable parametrization which transforms the boundary of a unit ball in Rd to the interior of a unit ball in Rd-1. By eliminating 1, the parameter
dd
space  can be rearranged to a form {((1 - r2)1/2, 2, · · · , d) : r2 < 1}.
r=2 r=2
Then the derivatives of a function with respect to (1) = (2, · · · , d) are readily obtained by chain rule and the classic assumptions on the asymptotic normality hold after transformation. This reparametrization is the key to analyzing the asymptotic properties of the estimates for  and to faciliating an efficient computation algorithm. We will investigate the estimation for g and  and propose a quasi-likelihood method to test the statistical significance of certain variables in the parametric component.

2.1. The kernel estimating functions for the nonparametric part g

If  is known, then we estimate g(·) and g (·) using the local linear estimating

functions. Let h denote the bandwidth parameter, and let K(·) denote the symmetric

kernel density function satisfying Kh(·) = h-1K(·/h). The estimation method involves local linear approximation. Denote by 0 and 1 the values of g and g evaluating at t, respectively. The local linear approximation for g( x) in a neighborhood of t is

g~( x) = 0 + 1( x - t). The estimators g^(t) and g^ (t) are obtained by solving the

kernel estimating functions with respect to 0, 1:

 

n
Kh( Xj - t) µ {g~( Xj)} V -1{g~( Xj)} [Yj - µ{g~( Xj)}] = 0,

j=1



n
( Xj - t)Kh( Xj - t) µ {g~( Xj)} V -1{g~( Xj)} [Yj - µ{g~( Xj)}] = 0,

j=1

(2.1)

Having estimated 0, 1 at t as ^0, ^1, the local linear estimators of g(t) and g (t) are g^(t) = ^0 and g^ (t) = ^1 respectively.

The key to obtain the asymptotic normality of the estimates for  lies in the asymptotic properties of the estimated nonparametric part. The following theorem will provide some useful results. The following notation will be used. Let X = {X1, · · · , Xn},

6

l(z)

=

{µ (z)}lV -1(z)

and

J

=

 (1)

the

Jacobian

matrix

of

size

d × (d - 1)

with



J =  -(1) / 1 - (1) 2  , Id-1

(1) = (2, · · · , d) .

The moments of K and K2 are denoted respectively by, j = 0, 1, · · ·

j = tjK(t)dt and j = tjK2(t)dt.

Proposition 1. Under regularity conditions (a), (b), (d) and (e) given in the Appendix, we have

(i) With h  0, n   such that h  0 and nh  ,   , the asymptotic conditional bias and variance of g^ is given by

E {g^( x) - g( x)}2 X

=

1 2

2h2

g

(

x)

2

+ 02/[nhf X( x)2{g( x)}]

+ OP (h4 + n-1h-1).

(2.2)

(ii) With h  0, n   such that h  0 and nh3  , for the estimates of the derivative g , it holds that

E {g^ ( x) - g ( x)}2 X

=

1 6

42-1

h2

g

(

x)

+

1 2

(42-1

-

2)h2g

(

x)

× [ 2{g( x)}/2{g( x)}
2
+ f X( x)/f X( x)]

+ 22-22/[nh3f X( x)2{g( x)}]

+ OP (h4 + n-1h-3).

(2.3)

(iii) With h  0, n   such that h  0 and nh3  , we have that

E

g^( x) (1)

-

g

(

x)J

{x - E(x|

x)}

2X

= OP (h4 + n-1h-3),

(2.4)

The proof of this Proposition appears in the Appendix. Results (i) and (ii) in Proposition 1 is routine and in principal similar to Carroll, Ruppert and Welsh (1998). In the situation where 2V = 2 and the function µ is identity, results (i) and (ii) coincides with that given by Fan and Gijbels (1996). From result (iii), it is seen that
7

g^( x)/(1) converges in probability to g ( x)J {x - E(x| x)}, rather than

g ( x)J x as if g were known. That is, lim {g^( x)/(1)} = { lim g^( x)}/(1),

n

n

which means that the convergence in probability and the derivation of the sequence

g^n( x) (as a function of n) can not commute. This is primarily caused by the fact that the support of the infinite dimensional nuisance parameter g(·) depends on the

finite dimensional projection parameter . In contrast, a semiparametric model where

the support of the nuisance parameter is independent of the finite dimensional param-

eter is a partially linear regression model having form Y = X  + (T ) + . It is

easy to check that the limit of ^(T )/ is equal to E(X|T ), which is the derivative of

lim ^(T ) = E(Y |T ) - E(X |T ) with respect to . Result (iii) ensures that the pro-
n
posed estimator does not require undersmoothing of g(·) to obtain a root-n consistent

estimator for  and it is also of its own interest in inference theory for semiparametric

models.

2.2. The asymptotic distribution for the estimates of the parametric part 

We will now proceed to the estimation of   . We need to estimate the (d - 1)-

dimensional vector (1), the estimator of which will be defined via:

n
[µ{g^( Xi)}/(1)]V -1{g^( Xi)}[Yi - µ{g^( Xi)}] = 0.
i=1

(2.5)

This is the direct analogue of the "ideal" estimating equation for known g, in that

it is calculated by replacing g(t) with g^(t). An asymptotically equivalent and easily

computed version of this equation is:

G^ () d=ef n J g^ ( Xi){Xi - h^ ( Xi)}1{g^( Xi)}[Yi - µ{g^( Xi)}],
i=1

(2.6)

with J =

 (1)

the Jacobian mentioned above, g^ and g^

are defined by (2.1), and h^ (t)

the local linear estimate for h(t) = E(X| X = t) = (h1(t), · · · , hd(t)) ,

n
h^ (t) = bi(t)Xi
i=1

n
bi(t),
i=1

n
where bi(t) = Kh( Xi - t){Sn,2(t) - ( Xi - t)Sn,1(t)} and Sn,k = Kh( Xi -
i=1
t)( Xi - t)k, k = 1, 2. We use (2.6) to estimate (1) in the generalized single-index

8

model, and then use the fact that 1 = 1 - (1) 2 to obtain ^1. The use of (2.6) constitutes in our view a new approach to estimating generalized single index models, since (2.6) involves smooth pilot estimation of g, g and h we call it the Estimation Function Method (EFM) for .

Remark 1. It can be seen from the proof in the Appendix that the population version of G^ () is

n
G() = J g ( Xi){Xi - h( Xi)}1{g( Xi)}[Yi - µ{g( Xi)}],
i=1

(2.7)

which is obtained by replacing g^, g^ , h^ with g, g , h in (2.6). One important property of

(2.7) is that the second Bartlett identity holds, for any  :

E{G()G

()}

=

-E{

G() (1)

}.

This property makes the semiparametric efficiency of the EFM (2.6) possible.

We will focus now on the asymptotic normality of the estimator ^(1) derived from
(2.6). The reason is that the asymptotic consistency has been achieved for many
existing estimators (H¨ardle, Hall and Ichimura, 1993; H¨ardle and Stoker, 1989) in the simple single-index model. Let 0 = (10, (1)0 ) denote the true parameter and B+ denote the Moore-Penrose inverse of any given matrix B. We assume in the following context that ^(1) is in a root-n neighborhood of the true parameter (1)0, ^(1)  {(1) : (1) - (1)0  Cn-1/2} with C is some constant. We have the following asymptotic result for the estimator ^(1).

Theorem 1. Let ^(1) denote the solution of the estimating function (2.6). With

h  0, n   such that nh6  0 and nh4   and regularity conditions given in

the Appendix, we have:

n(^(1) - (1)0) -L Nd-1(0, (1)0 ),

(2.8)

where (1)0 = {J

J}+|(1)=(1)0 ,

J

=

 (1)

and

 = E {XX - E(X| X)E(X | X)}2{g( X)}{g ( X)}2/2 .

9

Remark 2. Note that   = 0, so the nonnegative matrix  degenerates in the direction of . If the mean function µ is the identity function and the variance function is equal to a scale constant, that is, µ{g( X)} = g( X), 2V {g( X)} = 2, the matrix  in Theorem 1 reduces to be
 = E {XX - E(X| X)E(X | X)}{g ( X)}2/2 .

Technically speaking, Theorem 1 shows that an undersmoothing approach is unnec-

essary and that root-n consistency can be achieved. The asymptotic covariance (1)0 in general can be estimated by replacing terms in its expression by estimates of those terms. The asymptotic normality of ^ = (^1, ^(1) ) will follow from Theorem 1 with a simple application of the multivariate delta-method, since ^1 = 1 - ^(1) 2. Ac-
cording to the results of Carroll et al. (1997), the asymptotic variance of their estimator

is +. Define the block partition of matrix  as follows: 
 =  11 12  , 21 22

(2.9)

where 11 is a positive constant, 12 is a (d - 1)-dimensional row vector, 21 is a (d - 1)-dimensional column vector and 22 is a (d - 1) × (d - 1) nonnegative definite matrix.

Corollary 1. Under the conditions of Theorem 1, we have n(^ - 0) -L Np(0, 0),

(2.10)

with 0 = J{J J}+J

. Further,
=0

0



+

,
=0

and the stick less-than sign holds when det(22) = 0. That is, in this case the EFM is

more efficient than that of Carroll et al. (1997).

2.3. Profile quasi-likelihood ratio test

In applications, it is important to test the statistical significance of added predictors in a regression model. Here we establish a quasi-likelihood ratio statistic to test

10

the significance of certain variables in the linear index. The null hypothesis that the model is correct is tested against a full model alternative. Fan and Jiang (2007) gave a recent review about generalized likelihood ratio tests. Bootstrap tests for nonparametric regression, generalized partially linear models and single index models can be found in H¨ardle and Mammen (1993), H¨ardle, Mammen and Mu¨ller (1998) and H¨ardle, Mammen and Proenca (2001). Consider the testing problem:

r rd

H0 : g(·) = g( kXk)  H1 : g(·) = g( kXk +

kXk).

k=1

k=1

k=r+1

(2.11)

We mainly focus on testing k = 0, k = r+1, · · · , d, though the following test procedure can be easily extended to a general linear testing B~ = 0 where B is a known matrix

with full row rank and ~ = (r+1, · · · , d) . The profile quasi-likelihood ratio test is

defined by

Tn = 2{sup Q^() - sup Q^()},



,=0

(2.12)

where Q^() =

n
Q[µ{g^(

Xi)}, Yi], Q[µ, y] =

y µ

V

{µ-1(s)}(s-y)ds

and

µ-1(·)

is

the

i=1

inverse function of µ(·). The following Wilks' type theorem shows that the distribution

of Tn is asymptotically chi-squared and independent of nuisance parameters.

Theorem 2. Under the assumptions of Theorem 1, if k = 0, k = r + 1, · · · , d, then

Tn -L 2(d - r).

(2.13)

3. Numerical Studies
3.1. Computation of the estimates
Solving the joint estimating equations (2.1) and (2.6) poses some interesting challenges, since the functions g^( X) and g^ ( X) depend on  implicitly. Treating  X as a new predictor (with given ), equation (2.1) gives us g^, g^ as in Fan, Heckman and Wand (1995). We therefore focus on (2.6), as estimating equations. It cannot be solved explicitly, and hence one needs to find solutions using numerical methods. The Newton-Raphson algorithm is one of the popular and successful methods for finding
11

roots. However, the computational speed of this algorithm crucially depends on the initial value. We propose therefore a fixed point iterative algorithm that is adapted to higher dimension and unsensitive to the initial value. It is worth noting that this algorithm can be implemented in the case that d > n, because the resultant procedure only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high dimensionality.

Rewrite the estimating functions as G^ () = J F^ () with F^ () = (F^1(), · · · , F^d()) and

n
F^s() = {Xsi - h^s( Xi)}µ {g^( Xi)}g^ ( Xi)V -1{g^( Xi)}[Yi - µ{g^( Xi)}].
i=1

Setting G^ () = 0 and after some simple calculations, we can get that   1 = |F^1()|/ F^() , s = 1  s2 = F^s2()/ F^() 2, s  2

(3.1)

Based on this, the fixed point iterative algorithm is summarized as:
Step 0. Choose initial values for , denoted by old. Step 1. Solving the estimating equation (2.1) with respect to , yields g^(oldXi) and g^ (oldXi), 1  i  n. Step 2. Update old with old = new/ new by solving the equation (2.6) in the fixed point iteration

 =  + F^ ( ),new

M F^1(old)/ F^(old) +M old

|F^1(old)|/ F^(old) 2 F^1(old)/ F^(old) +M

old

where M is a constant satisfying F^1()/ F^() + M = 0 for any  (M can be chosen to be 2).

Step

3.

Repeat

Steps

1

and

2

until

max
1sd

|new,s

-

old,s|



tol

is

met

with

tol

being

a

prescribed tolerance.

The final vector new/ new is the estimator of 0. Similar to other direct estimation methods (Horowitz and H¨ardle, 1996), the preceding calculation is easy to
 implement. Empirically the initial value for , (1, 1, · · · , 1) / d can be used in the calculations. The Epanechnikov kernel function K(t) = 3/4(1 - t2)I(|t|  1) is used.

12

The bandwidth involved in Step 1 can be chosen to be optimal for estimation of g^(t) and g^ (t) based on the observations {oldXi, Yi}. So the standard bandwidth selection methods, such as K-fold cross validation, generalized cross validation (GCV) and the rule of thumb, can be adopted. In this step, we recommend K-fold cross validation to determine the optimal bandwidth using the quasi-likelihood as a criterion function. The K-fold cross validation is not too computationally intensive while making K not take too large values (for example, K = 5 or 10). Here we recommend that the smoothing parameters be chosen graphically using partial residual plots. The simplest way to do this is to try a number of smoothing parameters that smooth the data and pick the one that seems most reasonable.

3.2. Simulation results

Example 1 (continuous response). We report a simulation study to investigate the finite-sample performance of the proposed estimator and compare it with the rMAVE (refined MAVE, for details see Xia et al. (2002)) estimator. We consider the following model similar to that used in Xia (2006):

E(Y | X) = g( X), g( X) = ( X)2 exp( X); V ar(Y | X) = 2,  = 0.1.

(3.2)

 Let the true parameter  = (2, 1, 0, · · · , 0) / 5. Two sets of designs for X are consid-

ered: Design (A) and Design (B). In Design (A), (Xs + 1)/2  Beta(, 1), 1  s  d and in Design (B) (X1 + 1)/2  Beta(, 1) and P (Xs = ±0.5) = 0.5, s = 2, 3, 4, · · · , d. The data generated in Design (A) are not elliptically symmetric. All the components

of Design (B) are discrete except for the first component X1. Y generates from normal distribution. This simulation data set consists of 400 observations with 250 replica-

tions. The results are shown in Table 1. Both MAVE and EFM estimates are close

to the true parameter vector for d = 10. However, the average estimation errors from

MAVE estimates for d = 50 are about 10 times larger as those of the EFM estimates.

This indicates that the fixed point algorithm is more adaptive to high dimension.

Example 2 (binary response). This simulation design assumes an underlying generalized

13

single index model for binary responses with

P (Y = 1|X) = µ{g( X)} = exp{g( X)}/[1 + exp{g( X)}], g( X) = exp(5 X - 2)/{1 + exp(5 X - 3)} - 1.5.

(3.3)

 The underlying regression coefficients are assumed to be  = (2, 1, 0, · · · , 0) / 5. We

consider two sets of designs: Design (C) and Design (D). In Design (C), X1 and X2 follow the uniform distribution U (-2, 2). In Design (D), X1 is also assumed to be uniformly distributed in interval (-2, 2) and (X2 + 1)/2  Beta(1, 1). The similar designs for generalized partially linear single index models are assumed in Kane, Holt

and Allen (2004). Here a sample size of 700 is used for the case d = 10 and 3000 is used

for d = 50. Different sample sizes from Example 1 are used due to varying complexity

of the two examples. For this example, 250 replications are simulated and the results

are displayed in Table 2. In this set of simulations, the average estimation errors from

rMAVE estimates are about three times as large as EFM estimates under both Design

(C) and Design (D) for d = 10 or d = 50. The values in the row marked by d = 50

look a little bigger. However, it is reasonable because of the number of summands in

the average estimate error for d = 50 is five times as large as that for d = 10. Again it

appears that the EFM procedure achieves more precise estimators.

Table 1: Average estimation errors d |^s - s| for
s=1
model (3.2)

Design (A)

Design (B)

d  rMAVE EFM rMAVE EFM

10 0.75 0.0559* 10 1.5 0.0323* 50 0.75 6.2019 50 1.5 3.1792

0.0809 0.0431 0.9119 0.2313

0.0522* 0.0417* 6.1265 3.9514

* The values are adopted from Xia (2006).

0.0715 0.0523 0.5066 0.2769

14

Table 2: Average estimation errors d |^s - s| for
s=1
model (3.3)

Design (C) d rMAVE EFM 10 1.2600 0.4785 50 4.8857 1.2009

Design (D)

rMAVE EFM

1.8096 5.8280

0.7415 1.9908

Table 3: Average estimation errors d |^s - s|
s=1
for model (3.4)

d = 10 d = 50 d = 100 d = 120

rMAVE 0.6841 6.7224 --

--

EFM 0.0955 0.9363 4.4518 5.9280

-- means that the values can not be calculated by rMAVE because of high dimension.

15

power power power power

Design (A), d=10, =1.5 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.02

0.04



Design (A), d=50, =1.5 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.02

0.04



Design (A), d=10, =0.75 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.05

0.1



Design (A), d=50, =0.75 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0 0.1 0.2


Figure 1. Simulation results for Design (A) in Example 1. The left graphs depict the case

 = 1.5 with  is the first parameter in Beta(, 1). The right graphs are for  = 0.75.

power power power power

Design (B), d=10, =1.5 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.02

0.04



Design (B), d=50, =1.5 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.02

0.04



Design (B), d=10, =0.75 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.05

0.1



Design (B), d=50, =0.75 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0 0.1 0.2


Figure 2. Simulation results for Design (B) in Example 1. The left graphs depict the case  = 1.5 with  is the first parameter in Beta(, 1). The right graphs are for  = 0.75.

power power power power

Design (C), d=10 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0 0.2 0.4


Design (C), d=50 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0 0.2 0.4


Design (D), d=10 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0 0.5 1


1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

Design (D), d=50
0.2 0.4 0.6


Figure 3. Simulation results for Example 2. The left graphs depict the case of Design (C)

with parameter dimension being 10 and 50. The right graphs are for Design (D).

Example 3 (A simple model). To illustrate the adaptivity of our algorithm to high dimension,

16

we consider the following simple single index model Y = ( X)2 + .

(3.4)

The true parameter is  = (2, 1, 0, · · · , 0) , X is generated from Nd(0, I) and   N (0, 0.22). The results given in Table 3 are based on a sample size of n = 100 and 250 replicates. An important result from this simulation is that the proposed EFM procedure can provide a reasonable estimate even when the dimension of parameter is larger than the number of observations.

Performance of profile quasi-likelihood ratio test. To illustrate how the profile quasi-likelihood ratio performs for linear hypothesis problems, we simulate the same data as above, except that we allow some components of the index to follow the null hypothesis:

H0 : 4 = 5 = · · · = d = 0.
We examine the power of the test under a sequence of the alternative hypotheses indexed by parameter  as follows:
H1 : 4 = , s = 0 for s  5. When  = 0, the alternative hypothesis becomes the null hypothesis.

We examine the profile quasi-likelihood ratio test under a sequence of alternative models, progressively deviating from the null hypothesis, namely, as  increases. The power functions are calculated at the significance level: 0.05, using the asymptotic distribution. We calculate test statistics from 250 simulations by employing the fixed point algorithm and find the percentage of test statistics greater than or equal to the associated quantile of the asymptotic distribution. The pictures in Figure 1, 2 and 3 illustrate the power function curves for two models under the given significance levels. The power curves increase rapidly with , which shows the profile quasi-likelihood ratio test is powerful. When  is close to 0, the test sizes are all approximately the significance levels.

3.3. A real data example

Income, to some extent, is considered as an index of a successful life. It is generally believed that demographic information, such as education level, relationship in the household, marital status, the fertility rate and gender, among others, have effects on amounts of income. For example, Murray (1997) illustrated that adults with higher intelligence have higher income.

17

Kohavi (1996) predicted income using a Bayesian classifier offered by a machine learning algorithm. Madalozzo (2008) examined income differentials between married women and those who remain single or cohabitation by using multivariate linear regression. Here we will use the generalized single index model to explore the relationship between income and some of its possible determinants.
We use the "Adult" database, which was extracted from the census bureau database and is publicly available form website: http://archive.ics.uci.edu/ml/datasets/Adult. It was originally used to model income exceeds over USD 50000/year based on census data. After excluding a few missing data, the data set in our study includes 30162 subjects. The selected explanatory variables are: X1 =age (integer): number of years of age and greater than or equal to 17. X2 =work-class (categorical): 1 = Federal-gov, 2 = Local-gov, 3 = Never-worked, 4 =
Private, 5 = Self-emp-inc (self-employed, incorporated), 6 = Self-emp-not-inc (selfemployed, not incorporated), 7 = State-gov, 8 = Without-pay. X3 =fnlwgt (continuous): The final sampling weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US. X4 =education (ordinal): 1=Preschool (less than 1st Grade), 2=1st-4th, 3=5th-6th, 4=7th8th, 5=9th, 6=10th, 7=11th, 8=12th (12th Grade no Diploma), 9=HS-grad (high school Grad-Diploma or Equiv), 10=Some-college (some college but no degree), 11=Assocvoc (associate degree-occupational/vocationl), 12=Assoc-acdm (associate degree-academic program), 13=Bachelors, 14=Masters, 15=Prof-school (professional school), 16=Doctorate. X5 =education-num (continuous): Number of years of education. X6 =marital-status (categorical): 1 = Divorced, 2 = Married-AF-spouse (married, armed forces spouse present), 3 = Married-civ-spouse (married, civilian spouse present ), 4 = Married-spouse-absent (married, spouse absent (exc. separated)), 5 = Never-married, 6 = Separated, 7 = Widowed. X7 =occupation (categorical): 1 = Adm-clerical (administrative support and clerical), 2 = Armed-Forces, 3 = Craft-repair, 4 = Exec-managerial (executive-managerial), 5 = Farming-fishing, 6 = Handlers-cleaners, 7 = Machine-op-inspct (machine operator inspection), 8 = Other-service, 9 = Priv-house-serv (private household services), 10= Prof-specialty (professional specialty), 11= Protective-serv, 12= Sales, 13= Techsupport, 14= Transport-moving.
18

X8 =relationship (categorical): 1 = Husband, 2 = Not-in-family, 3 = Other-relative, 4 = Own-child, 5 = Unmarried, 6 = Wife.
X9 =race (categorical): 1 = Amer-Indian-Eskimo, 2 = Asian-Pac-Islander, 3 = Black, 4 = Other, 5 = White .
X10 =sex (categorical): 1=Male, 0=Female. X11 =capital-gain (continuous): a profit that results from investments into a capital asset.

X12 =capital-loss (continuous): a loss that results from investments into a capital asset. X13 =hours-per-week (continuous): usual number of hours worked per week. X14 =native-country (categorical): 1=United-States, 0=others.

The generalized single index model will be used to model the relationship between income and the relevant 14 predictors X = (X1, · · · , X14) :

P ("income" > 50000|X) = exp{g( X)}/[1 + exp{g( X)}],

(3.5)

where Y = I("income" > 50000) and  = (1, · · · , 14) and s represents the effect of the sth predictor. Formally, we are testing

H0 : 7 = 0  H1 : 7 = 0.

(3.6)

After a preliminary data check, we find that the explanatory variables X3 = "fnlwgt", X11 = "capital-gain" and X12 = "capital-loss" are very skewed to the left and the later two often take zero value. So before fitting (3.5) we first transformed these three variables. A natural choice is to use log("fnlwgt"), log(1 + "capital-gain") and log(1 + "capital-loss"). The fixed point iterative algorithm is employed to compute the estimate for . To illustrate further the practical implications of this approach, we compare our results to those obtained by using a logistic regression. The coefficients of the two models are given in Table 4. The relative magnitudes as well as the sign of the estimated coefficients are good proxies of the relevance of each of the explanatory variables. In both models, the variables have the same expected sign. The generalized single index model provides more reasonable results: X5 = "education-num" has its strongest positive effect on income, those who got a bachelor's degree or higher seem to have much higher income than those with lower education level.
In contrast, results derived from a logistic regression show that "sex" is the largest positive contributor, an unconvincing conclusion. It appears that men consistently earn more than
19

women on average -- and this is true for lawyers, doctors, etc. Men at most achieved about twice income of their female counterparts in each education level, however, the median weekly earnings of full time workers with doctorate degree is about five times that of those with lowest education (pre-schools). So "education" has a larger impact on income than "sex".
Some other interesting conclusions could be obtained by looking at the output. "Age" accounts for the experience effect and has a positive effect. Persons who worked without pay in a family business, unpaid childcare and others earn a lower income than persons who worked for wages or for themselves. The "fnlwgt" attribute has a positive relation to income. Males are likely to make much more money (about 1.4 times) than females. The expected sign for marital status is negative, given that the household production theory affirms that division of work is efficient when each member of a family dedicates their time to the more productive job. Men usually receive relatively better compensation for their time in the labor market than in home production. Thus, the expectation is that married women dedicate more time to home tasks and less to the labor market, and this would imply a different probability of working given the marital status choice.
Also "race" influences the income and Asian or Pacific Islanders seem to make more money than other races. And also, one's income significantly increases as working hours increases. Both "capital-gain" and "capital-loss" have positive effects, so we think that people make more money who can use more money to invest. The presence of young children has a negative influence on the income. The coefficient for "native-country" and "occupation" indicators are not significant. Hence the conclusion based on the generalized single index model is consistent with what we expect. To help with interpretation of the model, plots of  X versus predicted response probability and g( X) are generated, respectively, and can be found on the right column in Figure 4.
We now employ the quasi-likelihood ratio test to the test problem (3.6). The QLR test statistic is 172.4157 with one degree of freedom, resulting in a P-value of < 10-5. Hence this result provides strong evidence that occupation has a significant influence on high income.
20

Table 4: Fitted coefficients for model (3.5)

Variables
Constant Age Workclass log(Fnlwgt) Education Education-Num Marital-Status Occupation Relationship Race Sex log(1 + Capital-Gain) log(1 + Capital-Loss) Hours-per-Week Native-Country

^ of GSIM
-- 0.0835 (.0004) -0.0664 (.0037) 0.0476 (.0055) 0.0379 (.0009) 0.6517 (.0018) -0.3163 (.0020) 0.0137 (.0009) -0.3511 (.0027) 0.0953 (.0036) 0.1792 (.0059) 0.4569 (.0025) 0.2290 (.0031) 0.0734 (.0004) 0.0091 (.0113)

^ of LR
-9.1710 (.3789) 0.0371 (.0014) -0.0867 (.0132) 0.1104 (.0258) 0.0161 (.0053) 0.3414 (.0074) -0.2163 (.0123) 0.0002 (.0040) -0.1193 (.0014) 0.1035 (.0223) 0.9135 (.0507) 0.2054 (.0060) 0.1573 (.0087) 0.0297 (.0015) 0.0069 (.0656)

21

response probability g^(^ X)

Fitting based on generalzied single index model 1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 5

10 15
^ X

20

25

The fitted curve for the unknown link function 4

2

0

-2

-4

-6

-8 5

10 15
^ X

20

25

Figure 4. Adult data: The left graph is a plot of predicted response probability based on

the generalized single index model. The right graph is the fitted curve for the unknown link

function g(·).

References
Carroll, R. J., Fan, J., Gijbels, I. and Wand, M. P. (1997). Generalized partially linear single-index models. J. Am. Statist. Ass., 92 447­489.
Carroll, R. J., Ruppert, D. and Welsh, A. H. (1998). Local estimating equations. J. Am. Statist. Ass., 93 214­227.
Fan, J. and Gijbels, I. (1996). Local polynomial modeling and its applications. London: Chapman and Hall.
Fan, J., Heckman, N. E. and Wand, M. P. (1995). Local polynomial kernel regression for generalized linear models and quasi-likelihood functions. J. Am. Statist. Ass., 90 141­150.
Fan, J. and Jiang, J. (2007). Nonparametric inference with generalized likelihood ratio test. Test, 16 409­478.
Ha¨rdle, W., Hall, P. and Ichimura, H. (1993). Optimal smoothing in single-index models. Ann. Statist., 21 157­178.
Ha¨rdle, W. and Mammen, E. (1993). Testing parametric versus nonparametric regression. Ann. Statist., 21 1926­1947.
Ha¨rdle, W., Mammen, E. and Mu¨ller, M. (1998). Testing parametric versus semiparametric modelling in generalized linear models. J. Am. Statist. Ass., 93 1461­1474.

22

Ha¨rdle, W., Mammen, E. and Proenca, I. (2001). A bootstrap test for single index models. Statistics, 35 427­452.
Ha¨rdle, W. and Stoker, T. M. (1989). Investigating smooth multiple regression by method of average derivatives. J. Am. Statist. Ass., 84 986­995.
Horowitz, J. L. and Ha¨rdle, W. (1996). Direct semiparametric estimation of a singleindex model with discrete covariates. J. Am. Statist. Ass., 91 1632­1640.
Hristache, M., Juditski, A. and Spokoiny, V. (2001). Direct estimation of the index coefficients in a single-index model. Ann. Statist., 29 595­623.
Kane, M., Holt, J. and Allen, B. (2004). Results concerning the generalized partially linear single-index model. J. Statist. Computn Simuln., 72 897­912.
Kohavi, R. (1996). Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid. In In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. CA: AAAI Press, Menlo Park, 202­207.
Madalozzo, R. C. (2008). An analysis of income differentials by marital status. Estudos Econo^icos, 38 267­292.
McCullagh, P. and Nelder, J. A. (1989). Generalzied Linear Models (2nd ed.). London: Champman and Hall.
Murray, C. (1997). IQ and economic success. The Public Interest.
Powell, J. L., Stock, J. H. and Stoker, T. M. (1989). Semiparametric estimation of index coefficients. Econometrica, 57 1403­1430.
Xia, Y. (2006). Asymptotic distributions for two estimators of the single-index model. Economet Theory, 22 1112­1137.
Xia, Y., Tong, H., Li, W. K. and Zhu, L. (2002). An adaptive estimation of dimension reduction space (with discussions). J. R. Statist. Soc. B., 64 363­410.
Zhu, L. X. and Xue, L. G. (2006). Empirical likelihood confidence regions in a partially linear single-index model. J. R. Statist. Soc. B., 68 549­570.
23

4. Appendix

In this section the proof of Theorem 1 will be given. We first introduce some regularity conditions. Regularity Conditions (a) µ(·), V (·), g(·), h(·) = E(X| X = ·) have two bounded and continuous derivatives.
(b) Let q(z, y) = µ (z)V -1(z){y - µ(z)}. Assume that q(z, y)/z < 0 for z  R and y in the range of the response variable.

(c) The largest eigenvalues of 22 is bounded away from infinity.
(d) The density function f X( x) of random variable  X is bounded away from 0 on T and satisfies the Lipschitz condition of order 1 on T, where T = { x : x  T } and T is a compact support set of X.

(e) The kernel K is a bounded and symmetric density function with a bounded derivative,

and satisfies


t2K(t)dt = 0 and
-


|t|jK(t)dt < , j = 1, 2, · · ·
-

Condition (a) is some mild smoothness conditions on the involved functions of the model. We impose Condition (b) to guarantee that the solutions of equation (2.1), g^(t) and g^ (t), lie in a compact set. Condition (c) implies that the second moment of estimating equation (2.7), tr(J J), is bounded. Then the CLT can be applied to G(). Condition (d) means that X may have discrete components and the density function of  X is positive, which ensures that the denominators involved in the nonparametric estimators, with high probability, bounded away from 0. Condition (e) is a commonly used smoothness condition, including the Gaussian kernel and the quadratic kernel. All of the conditions can be relaxed at the expense of longer proofs.

Throughout the Appendix, Zn = OP (an) denotes that an-1Zn is bounded in probability and the derivation for the order of Zn is based on the fact that Zn = OP { E(Zn2)}. Therefore, it allows to apply the Cauchy-Schwartz inequality to the quantity having stochastic order an.

Proof of Proposition 1.

(i). Conditions (a), (b), (d) and (e) are essentially equivalent conditions given by Carroll, Ruppert and Welsh (1998), and as a consequence the derivation of bias and variance for

24

g^( x) and g^ ( x), is similar to that of Carroll, Ruppert and Welsh (1998). The asymptotic expansion for g^( x) and g^ ( x) is helpful in the following proofs. The asymptotic expansion is:

g^( x) - g( x) =

n-1

n

Kh(

Xj - 

x)2{0 + 1(

Xj - 

-1
x)}

j=1

n
×n-1 Kh( Xj -  x)q{0 + 1( Xj -  x), Yj}

j=1

+OP (h8 + n-1h + n-2h-2)1/2,

(A.1)

and

hg^ ( x) - hg ( x)

=

n-1

n
{(

Xj - 

x)/h}2Kh(

Xj - 

x)2{0 + 1(

Xj - 

-1
x)}

j=1

n

×n-1 {( Xj -  x)/h}Kh( Xj -  x)q{0 + 1( Xj -  x), Yj}

j=1

- n-1

n
{(

Xj - 

x)/h}2Kh(

Xj - 

x)2{0 + 1(

Xj - 

-1
x)}

j=1

n

× n-1 {( Xj -  x)/h}Kh( Xj -  x)2{0 + 1( Xj -  x)}

j=1

× n-1

n

Kh(

Xj - 

x)2[0 + 1(

Xj - 

-1
x)]

j=1

n

×n-1 Kh( Xj -  x)[µ{g( Xj)} - µ{0 + 1( Xj -  x)}]

j=1

+OP (h8 + n-1h + n-2h-2)1/2,

(A.2)

where 0 = g( x), 1 = g ( x), q(z, y) = µ (z)V -1(z){y - µ(z)}, l(z) = {µ (z)}lV -1(z)

and OP (1) denoting a random quantity is bounded in probability.

(ii). The first equation of (2.1) is
n
0 = Kh( Xj -  x) µ {^0 + ^1( Xj -  x)} V -1{^0 + ^1( Xj -  x)}
j=1
×[Yj - µ{^0 + ^1( Xj -  x)}].

Taking derivatives with respect to (1) on both sides, direct observations lead to

^0 (1)

=

{B(

x)}-1{A1(

x) + A2(

x) + A3(

x)},

25

where

n

B( x) = - Kh( Xj -  x)qz{^0 + ^1( Xj -  x), Yj},

j=1

n

A1( x) = Kh( Xj -  x)J (Xj - x)qz{^0 + ^1( Xj -  x), Yj}^1,

j=1

n

A2(

x) = Kh(
j=1

Xj - 

x)qz{^0 + ^1(

Xj - 

x), Yj}(

Xj - 

x)

^1 (1)

,

n

A3( x) = h-1Kh( Xj -  x)J (Xj - x)q{^0 + ^1( Xj -  x), Yj},

j=1

with Kh(·) = h-1K (·/h). Note that ^0/(1) = g^( x)/(1), then we have

g^( x) (1)

=

{B(

x)}-1A1(

x) + {B(

x)}-1A2(

x) + {B(

x)}-1A3(

x).

(A.3)

In the following, we will prove that

E {B( x)}-1A1( x) - g ( x)J {x - h( x)} 2 = OP (h4 + n-1h-3),

(A.4)

the second term in (A.3) is of order OP (h4 + n-1h), and the third term is of order OP (h4 + n-1h-3). The combination of (A.3) and these three results can directly lead to Result (ii) of
Proposition 1. The proof is summarized in three steps.

Step 1. The analysis of term {B( x)}-1A1( x).

First we analyze {B( x)}-1A1( x), which can be decomposed as follows,

{B( x)}-1A1( x) =

g^ ( x)J {x - h^ ( x)}
n
-g^ ( x)J {B( x)}-1 Kh( Xj -  x)Xj
j=1
×qz{^0 + ^1( Xj -  x), Yj} - h^ ( x)

(A.5)

By applying the result about g^ ( x) in (i) of Proposition 1 and the classical asymptotic theory of local linear regression estimate for h^ ( x) (see Fan et al., 1996), it can be shown
that

E g^ ( x)J {x - h^ ( x)} - g ( x)J {x - h( x)} 2 = OP (h4 + n-1h-3). (A.6)

Next we show that the second moment of the second term in (A.5) is of order OP (h4+n-1h-1).

Define

n
B1( x) = Kh( Xj -  x)2{0 + 1( Xj -  x)}.
j=1

(A.7)

26

By conditions (a), (d) and (e), we obtain several useful results, for k = 0, 1, 2,

n
E {B1( x)}-1 Kh( Xj -  x)Xj( Xj -  x)k
j=1 2
×qz {0 + 1( Xj -  x), Yj}
= OP {cI(k = 0) + h4I(k = 0) + (nh)-1h2k},
n
E {B1( x)}-1 Kh( Xj -  x)Xj( Xj -  x)k
j=1 2
×1{0 + 1( Xj -  x)}[Yj - µ{0 + 1( Xj -  x)}]
= OP {h4I(k = 0) + h8I(k = 0) + (nh)-1h2k},

(A.8)

and that

E {B( x)/n}-1 - {B1( x)/n}-1 2 = OP (h4 + n-1h-1).

(A.9)

It is easy to check that (A.8) also holds for the case where qz is replaced with qz, the case 1 is replaced with 1 or 1 and the case when multiplier Xj (not Xj in linear index  Xj) is deleted.

Applying a Taylor expansion to qz{^0 + ^1( Xj -  x), Yj} at 0, 1, then the second term in (A.5) can be rewritten as

R1 + R2 + R3 + R4 + others,

(A.10)

where
n
R1( x) = g ( x)J {B1( x)}-1 Kh( Xj -  x)Xj
j=1
×qz{0 + 1( Xj -  x), Yj} - h( x) , R2( x) = -g ( x)J h^ ( x) - h( x) ,
n
R3( x) = -g ( x)J {B1( x)}-1 Kh( Xj -  x)Xj
j=1
×[2{0 + 1( Xj -  x)} + 2{0 + 1( Xj -  x)}](^0 - 0),
n
R4( x) = g ( x)J [{B1( x)}-1 - {B( x)}-1] Kh( Xj -  x)Xj
j=1
×2{0 + 1( Xj -  x)},
and "others = OP (h4 + n-1h-1) according to (A.8), (A.9) and Result (i) of Proposition 1. So the second term in (A.5) is dominated by R1 + R2 + R3 + R4, and the conditional second moment of every term is of order OP (h4 + n-1h-1). Combining this and the result of (A.6), the proof for this step completes.

27

This step yields the following result

{B( x)}-1A1( x) =

g^ ( x)J {x - h^ ( x)}
-[R1( x) + R2( x) + R3( x) + R4( x)] +OP (h4 + n-1h-1),

where R1( x), R2( x), R3( x), R4( x) are introduced in (A.10).

Step 2. The analysis of term {B( x)}-1A2( x).

(A.11)

This term can be rewritten as,

{B( x)}-1A2( x) = {B1( x)}-1A2( x) + [{B( x)}-1 - {B1( x)}-1]A2( x).

(A.12)

We first deal with {B1( x)}-1A2( x). By a Taylor expansion at 0, 1 and using (A.8) and result (i) of Proposition 1, we have that

{B1( x)}-1A2( x)

=

R5(

x)

^1 (1)

+{B1(

n
x)}-1 Kh(
j=1

Xj - 

x)(

Xj - 

x)

^1 (1)

×qz {0 + 1( Xj -  x), Yj}{(^0 - 0) + (^1 - 1)( Xj -  x)}

+OP (h4 + n-1h-1)

=

R5(

x)

^1 (1)

+

OP (h4

+

n-1h-1),

(A.13)

where

n
R5( x) = -{B1( x)}-1 Kh( Xj -  x)2{0 + 1( Xj -  x)}
j=1
×( Xj -  x).

From (A.13) and (A.8), we conclude that

E {B1( x)}-1A2( x) 2 = OP (h4 + n-1h).

In addition, we have with Cauchy-Schwarz:

E [{B( x)}-1 - {B1( x)}-1]A2( x) 2



E1/2 [{B(

x)}-1 - {B1(

x)}-1]{B1(

x)}

4
E

1/2

{B1(

x)}-1A2(

x)

4

= OP (h8 + n-1h3).

28

Then it follows that

E

{B(

x)}-1A2(

x)

2


=

2E {B1( x)}-1A2( x) 2 +2E [{B( x)}-1 - {B1( x)}-1]A2( x) 2
OP (h4 + n-1h) + OP (h8 + n-1h3) = OP (h4 + n-1h). (A.14)

This step gives that

{B(

x)}-1A2(

x) = R5(

x)

^1 (1)

+ OP (h4

+ n-1h-1),

(A.15)

with R5( x) given in (A.13).

Step 3. The analysis of term {B( x)}-1A3( x).

We now proceed to show that this term is of order OP (h4 + n-1h-3) and write

{B( x)}-1A3( x) = {B1( x)}-1A3( x) + [{B( x)}-1 - {B1( x)}-1]A3( x).

(A.16)

Noting that tkK (t)dt = 0 when k is an even number and following similar derivations as used for {B1( x)}-1A2( x), we have that

{B1( x)}-1A3( x)

= R6( x)
n
+{B1( x)}-1 h-1Kh( Xj -  x)J (Xj - x)qz{0 + 1( Xj -  x), Yj}
j=1

×(^0 - 0)
n
+{B1( x)}-1 h-1Kh( Xj -  x)J (Xj - x)qz{0 + 1( Xj -  x), Yj}
j=1

×{(^1 - 1)( Xj -  x)}

+OP (h8 + n-1h + n-2h-4)1/2

= R6( x) + R7( x) + OP (h8 + n-1h + n-2h-4)1/2,

(A.17)

where

n
R6( x) = {B1( x)}-1 h-1Kh( Xj -  x)J (Xj - x)
j=1
×q{0 + 1( Xj -  x), Yj}
n
R7( x) = -{B1( x)}-1 h-1Kh( Xj -  x)J (Xj - x)
j=1
×2{0 + 1( Xj -  x)}(^0 - 0).

29

The last equality in (A.17) is achieved by plugging in the asymptotic expansion for ^0 - 0 provided in (A.1) and by merging the terms of order OP (h8 + n-1h + n-2h-4)1/2 or of smaller order than this one. Similar to the treat on (A.8), we obtain that E R6( x) 2 = OP (h4 + n-1h-3) and E R7( x) 2 = OP (h4 + n-1h-1), therefore, E {B1( x)}-1A3( x) 2 = OP (h4 + n-1h-3). An application of Cauchy-Schwartz inequality indicates that the second
term in (A.16) is ignorable compared to the first term. Therefore

E {B( x)}-1A3( x) 2 = OP (h4 + n-1h-3).

(A.18)

This step indicates that {B( x)}-1A3( x) = R6( x) + R7( x) + OP (h8 + n-1h + n-2h-4)1/2,
with R6( x) and R7( x) are defined in (A.17).

(A.19)

Combining the above three steps, we obtain Result (ii) of Proposition 1.

Remark 3. By mimicking the forgoing analysis for g^( x)/(1), we have that

E g^ ( x)/(1) - g ( x)J {x - h( x) - h ( x)} - S1( x) - S2( x) 2

= OP (h4 + n-1h-3),

where

S1(

x) =

n-1

n
{(

Xj - 

x)/h}2Kh(

Xj - 

x)2{0 + 1(

Xj - 

-1
x)}

j=1

n

×n-1 h-2J (Xj - x)Kh( Xj -  x)q{0 + 1( Xj -  x), Yj},

j=1

S2(

x) =

n-1

n
{(

Xj - 

x)/h}2Kh(

Xj - 

x)2[0 + 1(

Xj - 

-1
x)]

j=1

n

×n-1 h-2{( Xj -  x)/h}J (Xj - x)Kh( Xj -  x)

j=1

×1{0 + 1( Xj -  x)}[Yj - µ{0 + 1( Xj -  x)}].

Using this, we can conclude the result of step 2 as

{B( x)}-1A2( x) = R5( x)g ( x)J {x - h( x)}

(A.20)

+R5( x){S1( x) + S2( x)} + OP (h8 + n-1h)1/2.

In a summary, it follows from the three steps and (A.20) that

g^( x)/(1) = g^ ( x)J {x - h^ ( x)}

-{R1( x) + R2( x) + R3( x) + R4( x)} +R5( x)g ( x)J {x - h( x)} +R5( x){S1( x) + S2( x)} +R6( x) + R7( x) + OP (h8 + n-1h + n-2h-4)1/2,

(A.21)

30

where Rk, k  4 are given in step 1, R5 in step 2 and Rk, k  6 in step 3. Proof of (2.6) and (2.7).

Proof of (2.6). We only need to prove that

G^()

-

n i=1

µ{g^( Xi)} (1)

V

-1{g^(

Xi)}[Yi - µ{g^(

 Xi)}] = OP ( n),

and

G^() (1)

+

n i=1



µ{g^( Xi (1)

)}

V

-1{g^(

Xi)}[



µ{g^( Xi)} (1)

]

= OP (n).

(A.22) (A.23)

Using the notations introduced in (A.3), the term in the left hand side of (A.22) can be decomposed into the following three terms:

G^() -

n
[µ{g^(

Xi)}/(1)]V -1{g^(

Xi)}[Yi - µ{g^(

Xi)}]

i=1

= C1 + C2 + C3,

where
n
C1 = µ {g^( Xi)}V -1{g^( Xi)}[Yi - µ{g^( Xi)}]g^ ( Xi)
i=1 n
×J {B( Xi)}-1 Kh( Xj -  Xi)Xj
j=1
×qz{^0 + ^1( Xj -  Xi), Yj} - h^ ( Xi)
n
C2 = - µ {g^( Xi)}V -1{g^( Xi)}[Yi - µ{g^( Xi)}]{B( Xi)}-1A2( Xi)
i=n1
C3 = - µ {g^( Xi)}V -1{g^( Xi)}[Yi - µ{g^( Xi)}]{B( Xi)}-1A3( Xi).
i=1
Next we will prove that Ck = OP (n), k = 1, 2, 3. Under the assumptions on h in Theorem
1, by Result (ii) of Proposition 1, results given in (A.11), (A.15) and (A.19) and that for any
function (·) defined on X = {Xi, 1  i  n}
n µ {g( Xi)}V -1{g( Xi)}[Yi - µ{g( Xi}](X ) = OP (n),
i=n1
µ {g( Xi)}V -1{g( Xi)}[Yi - µ{g( Xi)}] i=1 ×OP (h8 + n-1h + n-2h-4)1/2 = OP (n),

31

we observe that it is sufficient to show that Ck = OP (n), k = 1, 2, 3, where
n
C1 = µ {g( Xi)}V -1{g( Xi)}[Yi - µ{g( Xi)}]
i=1
×{R1( Xi) + R3( Xi) + R4( Xi)},
n
C2 = - µ {g( Xi)}V -1{g( Xi)}[Yi - µ{g( Xi)}]
i=1
×R5( Xi){S1( Xi) + S2( Xi)},
n
C3 = - µ {g( Xi)}V -1{g( Xi)}[Yi - µ{g( Xi)}]
i=1
×{R6( Xi) + R7( Xi)}.

(A.24)

with

n
R1( x) = {B1( x)}-1 Kh( Xj -  x)Xj1{0 + 1( Xj -  x)}
j=1

×[Yj - µ{0 + 1( Xj -  x)}].

We first show that C1 = OP (n), where

C1 =

n
µ {g( Xi)}V -1{g( Xi)}[Yi - µ{g( Xi)}]R1( Xi)
i=1 n
+ µ {g( Xi)}V -1{g( Xi)}[Yi - µ[g( Xi)]]R3( Xi)
i=n1
+ µ {g( Xi)}V -1{g( Xi)}[Yi - µ{g( Xi)}]R4( Xi)
i=1

The second moment of the first term in the above expression is

E

n

µ {g(

Xi)}V -1{g(

Xi)}[Yi - µ{g(

Xi)}]R1(

Xi)

2
= 1/h,

(A.25)

i=1

and the first term is of order OP (n) if nh  . As a consequence of Cauchy-Schwartz

inequality, the second term satisfies

n

µ {g(

Xi)}V -1{g(

Xi)}[Yi - µ{g(

Xi)}]R3(

2
Xi)

ni=1

 µ 2{g( Xi)}V -2{g( Xi)}[Yi - µ{g( Xi)}]2g 2( Xi)

i=1 n

× {B1( Xi)}-1 Kh( Xj -  x)Xj

j=1

2

×[2{0 + 1( Xj -  x)} + 2{0 + 1( Xj -  x)}]

n
× (^0( Xi) - 0( Xi))2
i=1

= nOP (h2 + n-1h-1)OP (h4 + n-1h-1).

(A.26)

So the second term is of order OP (n) when nh6  0 and nh2  . The structure of

R4, R7 is similar to that of R3 and R5S1, R5S2, R6 similar to R1 respectively, hence we can

32

use the similar arguments of (A.26) and (A.25) to analyze the remaining terms. Under the bandwidth conditions nh6  0 and nh4  , we find that Ck = OP (n), k = 1, 2, 3. The proof for (A.22) completes.

(A.23) is a direct consequence of Proposition 1. As a byproduct, we can establish that

G^()/(1) +

n
{g (

Xi)}2[µ {g(

Xi)}]2V -1{g(

Xi)}

i=1

×J {Xi - h( Xi)}{Xi - h( Xi)} J = oP (n).

(A.27)

The proof for (2.7). Equation (A.27) implies that G^()/(1) - G()/(1) = OP (n), so we only need concentrate on showing that G^() - G() = OP (n). After some tedious calculations, we find that it is equivalent to show that the following two dominated terms D1, D2 satisfy that Dk = OP (n), k = 1, 2
n
D1 = J {g^ ( Xi) - g ( Xi)}{Xi - h( Xi)}1{g( Xi)}[Yi - µ{g( Xi)}],
i=n1
D2 = J g ( Xi){Xi - h( Xi)}1{g( Xi)}[µ{g( Xi)} - µ{g^( Xi)}].
i=1
Plugging (A.2) and (A.1) into above expression, it is straightforward to obtain the desired result.

We have completed the proof for (2.6) and (2.7).

Proof of Theorem 1. Recall the notation J,  and G() introduced in Section 2. By (2.

7), we have shown that

n(^(1) - (1)0) = 1n {J J}+G() + OP (1).

(A.28)

Theorem 1 follows directly form the above asymptotic expansion and the fact that E{G()G ()} = nJ J.

Proof of Corollary 1.

The asymptotic covariance of ^ can be obtained by adjusting the asymptotic covariance of ^(1) via the multivariate delta method, and is of form J(J J)+J . Next we will compare

this asymptotic covariance with that (denoted by +) given in Carroll et al. (1997). Write

 as

  =  11
21

 12  22

33

where 22 is a (d - 1) × (d - 1) matrix. We will next investigate two cases respectively: det(22) = 0 and det(22) = 0. Let  = -(1)/ 1 - (1) 2 = -(1)/1.

Consider the case that det(22) = 0. Because rank() = d-1, det(1122-2112) = 0.

Note that 22 is nondegenerate, it can be easily shown that 11 = 12-22121. Combining

this with the following fact,







J J

=

 Id-1

 11 12    

21 22

Id-1





= 22 + (21/ 11 + 11)(12/ 11 + 11 ) - 2112/11,

we can get that J J is nondegenerate. In this situation, its inverse (JJ)+ is just the ordinary inverse (JJ)-1. Then J(J J)+J = J(J J)-1/2 (J J)-1/2J , a full-

rank decomposition. Then

J(J J)+J

+
=

J(J J)-1/2

× (J J)-1/2J J(J J)-1J J(J J)-1/2 -1

× (J J)-1/2J

= J(J J)-1J J(J J)-1J

= .

This means that J(J J)+J = +.

When det(22) = 0, we can obtain that 
+ =  1/11 + 122+2.121/121 -+22.121/11

 -12+22.1/11  ,
2+2.1

with 22.1 = 22 - 2112/11. Write J(J J)+J as



  (J J)+  (J J)+  .

(J J)+

(J J)+





Note that J J = 22.1 + (21/ 11 + 11)(12/ 11 + 11 ), so J J  22.1.

Combining this with rank(22) = d - 2, we have that (J J)+  +22.1. It is easy to check

that  22.1 = 0, so   span(22.1) and  2+2.1 = 0, and then  (J J)+ = 0. In

this situation, J(J J)+J  + and the stick less-than sign holds since J J = 22.1

and 1/11 > 0.

Proof of Theorem 2.

34

Under H0, we have  = [e B] ( 1 - (1) 2, (1) )

dimensional vector,

 B= 0
Ir-1

 0 0

with e = (1, 0, · · · , 0)

is a r-

is a r×(d-1) matrix and (1) = (2, · · · , r) is a (r-1)×1 vector. Let  = ( 1 - (1) 2, (1) ) . So under H0 the estimator is also the local maximizer ^ of the problem

Q^([e B] ^ ) = sup Q^([e B] ).
(1) <1

Expanding Q^(B

^ ) at ^(1)

by a Taylor's expansion and noting that

Q^() (1)

 (1) =^(1)

= 0, then

Q^(^) - Q^(B ^ ) = T1 + T2 + OP (1), where

T1

=

-

1 2

^(1) - B

^

T2

=

1 6

^(1) - B

^

2Q^() (1)(1)

 (1) =^(1)

^(1) - B

^

,



(^(1) - B

^ )

2Q^()   (1)   (1)

|(1)=^(1) (^(1)

-

B

^ )

(1)

.

Assuming the conditions in Theorem 1 and under the null hypothesis H0, it is easy to show

that

 n(B

^ - B

) = 1n B

B(J

J)+G() + OP (1).

Combining this with (A.28), under the null hypothesis H0, n(^(1) - B ^ (1))

= 1n (J J)1/2+{Id-1 - (J J)1/2B B(J J)1/2+}(J J)1/2+G() + oP (1). (A.29)

Since

1n G()

=

OP (1),

2Q^()   (1)   (1)

= -nJ
(1)

J + OP (n) and matrix J

J has eigenval-

ues uniformly bounded away from 0 and infinity, we have ^(1) - B ^ (1) = OP (n-1/2) and

then |T2| = OP (1). Combining this and (A.29), we have

Q^(^) - Q^(B ^ ) =

n 2

(^(1)

-

B

^ (1))

J

J(^(1) - B

^ (1))

=

n 2

G

()(J

J)1/2+P(J

J)1/2+G(),

with P = Id-1 - (J J)1/2B B(J J)1/2+. Here P is idempotent having rank d - r, so

it can be written as P = S S where S ia a (d - r) × (d - 1) matrix satisfying SS = Id-r.

Consequently,

2{Q^(^) - Q^(B ^ )} =

 nS(J

J)1/2+G()

 nS(J

J)1/2+G()

-L 2(d - r).

35

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Implied Market Price of Weather Risk" by Wolfgang Härdle and Brenda López Cabrera, January 2009.
002 "On the Systemic Nature of Weather Risk" by Guenther Filler, Martin Odening, Ostap Okhrin and Wei Xu, January 2009.
003 "Localized Realized Volatility Modelling" by Ying Chen, Wolfgang Karl Härdle and Uta Pigorsch, January 2009.
004 "New recipes for estimating default intensities" by Alexander Baranovski, Carsten von Lieres and André Wilch, January 2009.
005 "Panel Cointegration Testing in the Presence of a Time Trend" by Bernd Droge and Deniz Dilan Karaman Örsal, January 2009.
006 "Regulatory Risk under Optimal Incentive Regulation" by Roland Strausz, January 2009.
007 "Combination of multivariate volatility forecasts" by Alessandra Amendola and Giuseppe Storti, January 2009.
008 "Mortality modeling: Lee-Carter and the macroeconomy" by Katja Hanewald, January 2009.
009 "Stochastic Population Forecast for Germany and its Consequence for the German Pension System" by Wolfgang Härdle and Alena Mysickova, February 2009.
010 "A Microeconomic Explanation of the EPK Paradox" by Wolfgang Härdle, Volker Krätschmer and Rouslan Moro, February 2009.
011 "Defending Against Speculative Attacks" by Tijmen Daniëls, Henk Jager and Franc Klaassen, February 2009.
012 "On the Existence of the Moments of the Asymptotic Trace Statistic" by Deniz Dilan Karaman Örsal and Bernd Droge, February 2009.
013 "CDO Pricing with Copulae" by Barbara Choros, Wolfgang Härdle and Ostap Okhrin, March 2009.
014 "Properties of Hierarchical Archimedean Copulas" by Ostap Okhrin, Yarema Okhrin and Wolfgang Schmid, March 2009.
015 "Stochastic Mortality, Macroeconomic Risks, and Life Insurer Solvency" by Katja Hanewald, Thomas Post and Helmut Gründl, March 2009.
016 "Men, Women, and the Ballot Woman Suffrage in the United States" by Sebastian Braun and Michael Kvasnicka, March 2009.
017 "The Importance of Two-Sided Heterogeneity for the Cyclicality of Labour Market Dynamics" by Ronald Bachmann and Peggy David, March 2009.
018 "Transparency through Financial Claims with Fingerprints ­ A Free Market Mechanism for Preventing Mortgage Securitization Induced Financial Crises" by Helmut Gründl and Thomas Post, March 2009.
019 "A Joint Analysis of the KOSPI 200 Option and ODAX Option Markets Dynamics" by Ji Cao, Wolfgang Härdle and Julius Mungo, March 2009.
020 "Putting Up a Good Fight: The Galí-Monacelli Model versus `The Six Major Puzzles in International Macroeconomics'", by Stefan Ried, April 2009.
021 "Spectral estimation of the fractional order of a Lévy process" by Denis Belomestny, April 2009.
022 "Individual Welfare Gains from Deferred Life-Annuities under Stochastic Lee-Carter Mortality" by Thomas Post, April 2009.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
023 "Pricing Bermudan options using regression: optimal rates of convergence for lower estimates" by Denis Belomestny, April 2009.
024 "Incorporating the Dynamics of Leverage into Default Prediction" by Gunter Löffler and Alina Maurer, April 2009.
025 "Measuring the effects of geographical distance on stock market correlation" by Stefanie Eckel, Gunter Löffler, Alina Maurer and Volker Schmidt, April 2009.
026 "Regression methods for stochastic control problems and their convergence analysis" by Denis Belomestny, Anastasia Kolodko and John Schoenmakers, May 2009.
027 "Unionisation Structures, Productivity, and Firm Performance" by Sebastian Braun, May 2009.
028 "Optimal Smoothing for a Computationally and Statistically Efficient Single Index Estimator" by Yingcun Xia, Wolfgang Härdle and Oliver Linton, May 2009.
029 "Controllability and Persistence of Money Market Rates along the Yield Curve: Evidence from the Euro Area" by Ulrike Busch and Dieter Nautz, May 2009.
030 "Non-constant Hazard Function and Inflation Dynamics" by Fang Yao, May 2009.
031 "De copulis non est disputandum - Copulae: An Overview" by Wolfgang Härdle and Ostap Okhrin, May 2009.
032 "Weather-based estimation of wildfire risk" by Joanne Ho and Martin Odening, June 2009.
033 "TFP Growth in Old and New Europe" by Michael C. Burda and Battista Severgnini, June 2009.
034 "How does entry regulation influence entry into self-employment and occupational mobility?" by Susanne Prantl and Alexandra Spitz-Oener, June 2009.
035 "Trade-Off Between Consumption Growth and Inequality: Theory and Evidence for Germany" by Runli Xie, June 2009.
036 "Inflation and Growth: New Evidence From a Dynamic Panel Threshold Analysis" by Stephanie Kremer, Alexander Bick and Dieter Nautz, July 2009.
037 "The Impact of the European Monetary Union on Inflation Persistence in the Euro Area" by Barbara Meller and Dieter Nautz, July 2009.
038 "CDO and HAC" by Barbara Choro, Wolfgang Härdle and Ostap Okhrin, July 2009.
039 "Regulation and Investment in Network Industries: Evidence from European Telecoms" by Michal Grajek and Lars-Hendrik Röller, July 2009.
040 "The Political Economy of Regulatory Risk" by Roland Strausz, August 2009.
041 "Shape invariant modelling pricing kernels and risk aversion" by Maria Grith, Wolfgang Härdle and Juhyun Park, August 2009.
042 "The Cost of Tractability and the Calvo Pricing Assumption" by Fang Yao, September 2009.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
043 "Evidence on Unemployment, Market Work and Household Production" by Michael C. Burda and Daniel S. Hamermesh, September 2009.
044 "Modelling and Forecasting Liquidity Supply Using Semiparametric Factor Dynamics" by Wolfgang Karl Härdle, Nikolaus Hautsch and Andrija Mihoci, September 2009.
045 "Quantifizierbarkeit von Risiken auf Finanzmärkten" by Wolfgang Karl Härdle and Christian Wolfgang Friedrich Kirchner, October 2009.
046 "Pricing of Asian temperature risk" by Fred Benth, Wolfgang Karl Härdle and Brenda López Cabrera, October 2009.
047 "MM-Stat ­ MultiMedia-Statistik: Statistische Datenanalyse ­ webbasiert, interaktiv und multimedial" by Sigbert Klinke, Dina Kuhlee, Christian Theel, Cornelia Wagner and Christian Westermeier, October 2009.
048 "Migration of the Highly Skilled: Can Europe catch up with the US?" by Lydia Mechtenberg and Roland Strausz, October 2009.
049 "A blocking and regularization approach to high dimensional realized covariance estimation" by Nikolaus Hautsch, Lada M. Kyj and Roel C.A. Oomen, October 2009.
050 "Generalized single-index models: The EFM approach" by Xia Cui, Wolfgang Karl Härdle and Lixing Zhu, October 2009.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

