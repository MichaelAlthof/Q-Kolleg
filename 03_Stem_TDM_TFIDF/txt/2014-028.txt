BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2014-028
Confidence Corridors for Multivariate Generalized
Quantile Regression
Shih-Kang Chao* Katharina Proksch**
Holger Dette** Wolfgang Härdle*
* Humboldt-Universität zu Berlin, Germany ** Ruhr-Universität Bochum, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Confidence Corridors for Multivariate Generalized Quantile Regression

Shih-Kang Chao

Katharina Proksch Wolfgang H¨ardle§
May 16, 2014

Holger Dette

Abstract
We focus on the construction of confidence corridors for multivariate nonparametric generalized quantile regression functions. This construction is based on asymptotic results for the maximal deviation between a suitable nonparametric estimator and the true function of interest which follow after a series of approximation steps including a Bahadur representation, a new strong approximation theorem and exponential tail inequalities for Gaussian random fields.
As a byproduct we also obtain confidence corridors for the regression function in the classical mean regression. In order to deal with the problem of slowly decreasing error in coverage probability of the asymptotic confidence corridors, which results in meager coverage for small sample sizes, a simple bootstrap procedure is designed based on the leading term of the Bahadur representation. The finite sample properties of both procedures are investigated by means of a simulation study and it is demonstrated that the bootstrap procedure considerably outperforms the asymptotic bands in terms of coverage accuracy. Finally, the bootstrap confidence corridors are used to study the efficacy of the National Supported Work Demonstration, which is a randomized employment enhancement program launched in the 1970s. This article has supplementary materials online.
Keywords: Bootstrap; Expectile regression; Goodness-of-fit tests; Quantile treatment effect; Smoothing and nonparametric regression. JEL: C2, C12, C14
Financial support from the Deutsche Forschungsgemeinschaft (DFG) via SFB 649 "Economic Risk" (Teilprojekt B1), SFB 823 "Statistical modeling of nonlinear dynamic processes" (Teilprojekt C1, C4) and Einstein Foundation Berlin via the Berlin Doctoral Program in Economics and Management Science (BDPEMS) are gratefully acknowledged.
Ladislaus von Bortkiewicz Chair of Statistics, C.A.S.E. - Center for applied Statistics and Economics, Humboldt-Universit¨at zu Berlin, Unter den Linden 6, 10099 Berlin, Germany. email: shihkang.chao@cms.hu-berlin.de; haerdle@wiwi.hu-berlin.de.
Ruhr-Universit¨at Bochum, Fakulta¨t fu¨r Mathematik, 44780 Bochum, Germany. email: katharina.proksch@rub.de; holger.dette@rub.de.
§Lee Kong Chian School of Business, Singapore Management University, 50 Stamford Road, Singapore 178899, Singapore.

1

1. Introduction
Mean regression analysis is a widely used tool in statistical inference for curves. It focuses on the center of the conditional distribution, given d-dimensional covariates with d  1. In a variety of applications though the interest is more in tail events, or even tail event curves such as the conditional quantile function. Applications with a specific demand in tail event curve analysis include finance, climate analysis, labor economics and systemic risk management.
Tail event curves have one thing in common: they describe the likeliness of extreme events conditional on the covariate X. A traditional way of defining such a tail event curve is by translating "likeliness" with "probability" leading to conditional quantile curves. Extreme events may alternatively be defined through conditional moment behaviour leading to more general tail descriptions as studied by Newey and Powell (1987) and Jones (1994). We employ this more general definition of generalized quantile regression (GQR), which includes, for instance, expectile curves and study statistical inference of GQR curves through confidence corridors.
In applications parametric forms are frequently used because of practical numerical reasons. Efficient algorithms are available for estimating the corresponding curves. However, the "monocular view" of parametric inference has turned out to be too restrictive. This observation prompts the necessity of checking the functional form of GQR curves. Such a check may be based on testing different kinds of variation between a hypothesized (parametric) model and a smooth alternative GQR. Such an approach though involves either an explicit estimate of the bias or a pre-smoothing of the "null model". In this paper we pursue the Kolmogorov-Smirnov type of approach, that is, employing the maximal deviation between the null and the smooth GQR curve as a test statistic. Such a model check has the advantage that it may be displayed graphically as a confidence corridor (CC; also called "simultaneous confidence band" or "uniform confidence band/region") but has been considered so far only for univariate covariates. The basic technique for constructing CC of this type is extreme value theory for the sup-norm of an appropriately centered nonparametric estimate of the quantile curve.
For a one-dimensional predictor confidence corridors were developed under various settings. Classical one-dimensional results are confidence bands constructed for histogram estimators by Smirnov (1950) or more general one-dimensional kernel density estimators by Bickel and Rosenblatt (1973). The results were extended to a univariate nonparametric mean regression setting by Johnston (1982), followed by H¨ardle (1989) who derived CCs for one-dimensional kernel M-estimators. Claeskens and Van Keilegom (2003) proposed uniform confidence bands and a bootstrap procedure for regression curves and their derivatives.
In recent years, the growth of the literature body shows no sign of decelerating. In the same spirit of H¨ardle (1989), H¨ardle and Song (2010) and Guo and Ha¨rdle (2012) constructed uniform confidence bands for local constant quantile and expectile curves. Fan and Liu (2013) proposed an integrated approach for building simultaneous confidence band that covers semiparametric models. Gin´e and Nickl (2010) investigated adaptive density estimation based on linear wavelet and kernel density estimators and Lounici and Nickl (2011) extended the framework of Bissantz et al. (2007) to adaptive deconvolution density estimation. Bootstrap procedures are proposed as a remedy for the poor coverage performance of asymptotic confidence corridors. For example, the bootstrap for the density estimator is proposed in Hall (1991) and Mojirsheibani (2012), and for local constant quantile estimators in Song et al. (2012).
However, only recently progress has been achieved in the construction of confidence bands
2

for regression estimates with a multivariate predictor. Hall and Horowitz (2013) derived an expansion for the bootstrap bias and established a somewhat different way to construct confidence bands without the use of extreme value theory. Their bands are uniform with respect to a fixed but unspecified portion (smaller than one) of points in a possibly multidimensional set in contrast to the classical approach where uniformity is achieved on the complete set considered. Proksch et al. (2014) proposed multivariate confidence bands for convolution type inverse regression models with fixed design.
To the best of our knowledge results of the classical Smirnov-Bickel-Rosenblatt type are not available for multivariate GQR or even mean regression with random design.
In this work we go beyond the earlier studies in three aspects. First, we extend the applicability of the CC to d-dimensional covariates with d > 1. Second, we present a more general approach covering not only quantile or mean curves but also GQR curves that are defined via a minimum contrast principle. Third, we propose a bootstrap procedure and we show numerically its improvement in the coverage accuracy as compared to the asymptotic approach.
Our asymptotic results, which describe the maximal absolute deviation of generalized quantile estimators, can not only be used to derive a goodness-of-fit test in quantile and expectile regression, but they are also applicable in testing the quantile treatment effect and stochastic dominance. We apply the new method to test the quantile treatment effect of the National Supported Work Demonstration program, which is a randomized employment enhancement program launched in the 1970s. The data associated with the participants of the program have been widely applied for treatment effect research since the pioneering study of LaLonde (1986). More recently, Delgado and Escanciano (2013) found that the program is beneficial for individuals of over 21 years of age. In our study, we find that the treatment tends to do better at raising the upper bounds of the earnings growth than raising the lower bounds. In other words, the program tends to increase the potential for high earnings growth but does not reduce the risk of negative earnings growth. The finding is particularly evident for those individuals who are older and spent more years at school. We should note that the tests based on the unconditional distribution cannot unveil the heterogeneity in the earnings growth quantiles in treatment effects.
The remaining part of this paper is organized as follows. In Section 2 we present our model, describe the estimators and state our asymptotic results. Section 3 is devoted to the bootstrap and we discuss its theoretical and practical aspects. The finite sample properties of both methods are investigated by means of a simulation study in Section 4 and the application of the new method is illustrated in a data example in Section 5. The assumptions for our asymptotic theory are listed and discussed after the references. All detailed proofs are available in the supplement material.
2. Asymptotic confidence corridors
In Section 2.1 we present the prerequisites such as the precise definition of the model and a suitable estimate. The result on constructing confidence corridors (CCs) based on the distribution of the maximal absolute deviation are given in Section 2.2. In Section 2.3 we describe how to estimate the scaling factors, which appear in the limit theorems, using residual based estimators. Section 3.1 introduce a new bootstrap method for constructing CCs, while Section 3.2 is devoted to specific issues related to bootstrap CCs for quantile regression. Assumptions are listed and discussed after the references.
3

2.1. Prerequisites

Let (X1, Y1), ..., (Xn, Yn) be a sequence of independent identically distributed random vectors in Rd+1 and consider the nonparametric regression model

Yi = 0(Xi) + i, i = 1, ..., n,

(1)

where  is an aspect of Y conditional on X such as the  -quantile, the  -expectile or the mean regression curve. The function (x) can be estimated by:

^(x)

=

arg min
R

1 n

n i=1

Kh(x -

X i )(Yi

-

),

(2)

where Kh(u) = h-dK (u/h) for some kernel function K : Rd  R, and a loss-function  : R  R. In this paper we are concerned with the construction of uniform confidence corridors for quantile as well as expectile regression curves when the predictor is multivariate,
that is, we focus on the loss functions

 (u) = 1(u < 0) -  |u|k,
for k = 1 and 2 associated with quantile and expectile regression. We derive the asymptotic distribution of the properly scaled maximal deviation supxD |^n(x) - (x)| for both cases, where D  Rd is a compact subset. We use strong approximations of the empirical process, concentration inequalities for general Gaussian random fields and results from extreme value theory. To be precise, we show that

P (2 log n)1/2 sup rn(x) ^n(x) - 0(x) / K 2 - dn < a  exp - 2 exp(-a) , (3)
xD
as n  , where r(x) is a scaling factor which depends on x, n and the loss function under consideration.

2.2. Asymptotic results

In this section we present our main theoretical results on the distribution of the uniform maximal deviation of the quantile and expectile estimator. The proofs of the theorems at their full lengths are deferred to the appendix. Here we only give a brief sketch of proof of Theorem 2.1 which is the limit theorem for the case of quantile regression.
THEOREM 2.1. Let ^n(x) and 0(x) be the local constant quantile estimator and the true quantile function, respectively and suppose that assumptions (A1)-(A6) in Section A.1 hold. Let further vol(D) = 1 and

dn = (2d log n)1/2 +

2d(log n) -1/2

1 2

(d

-

1)

log

log

n

+

log

(2)-1/2H2(2d)(d-1)/2

,

where H2 =

2 K

2 2

-d/2 det()1/2,

=

ij 1i,jd =

dK(u) K(u)
ui uj u

,
1i,jd

r(x) =

nhdfX (x)  (1 -  )

fY

|X

0(x)|x

,

Then the limit theorem (3) holds.

4

Sketch of proof. A major technical difficulty is imposed by the fact that the loss-function  is not smooth which means that standard arguments such as those based on Taylor's theorem
do not apply. As a consequence the use of a different, extended methodology becomes necessary. In this context Kong et al. (2010) derived a uniform Bahadur representation for an M-regression function in a multivariate setting (see appendix). It holds uniformly for x  D, where D is a compact subset of Rd:

^n(x) - 0(x)

=

1 nSn,0,0(x)

n i=1

Kh(x - Xi)

Yi - 0(x)

+O

log n

3 4

nhd

,

a.s.

(4)

Here Sn,0,0(x) = K(u)g(x + hu)fX(x + hu)du,  (u) = 1(u < 0) -  is the piecewise derivative of the loss-function  and

g(x) =

 t

E[

(Y

- t)|X

= x]

.
t=0 (x)

Notice that the error term of the Bahadur expansion does not depend on the design X

3
and it converges to 0 with rate log n/nhd 4 which is much faster than the convergence rate

(nhd

)-

1 2

of

the

stochastic

term.

Rearranging (4), we obtain

Sn,0,0(x){^n(x)

-

0(x)}

=

1 n

n

Kh(x - Xi)

Yi - 0(x)

+O

i=1

log n

3 4

nhd

.

(5)

Now we express the leading term on the right hand side of (5) by means of the centered empirical process

where Fn(y, x) = n-1 theorem,

Zn(y, u) = n1/2{Fn(y, u) - F (y, u)},

(6)

n i=1

1(Yi



y, Xi1



x1, ..., Xid



xd).

This yields, by Fubini's

Sn,0,0(x){^n(x) - 0(x)} - b(x) = n-1/2

Kh(x - u) y - 0(x) dZn(y, u) + O

log n

3 4

nhd

,

(7)

where

b(x) = -Ex

1 n

n
Kh(x - Xi) Yi - 0(x)

i=1

denotes the bias which is of order O(hs) by Assumption (A3) in the Appendix. The variance

of the first term of the right hand side of (7) can be estimated via a change of variables and

Assumption (A5), which gives

(nhd)-2nE K2 (x - Xi)/h 2 Yi - 0(x)

= (nhd)-2nhd

K2(v)2 y - 0(x) fY |X (y|x - hv)fX (x - hv)dydv

= (nhd)-1

K2(v)2 y - 0(x) fY |X(y|x)fX (x)dydv + O (nhd-1)-1

= (nhd)-1fX (x)2(x)

K

2 2

+

O

(nhd)-1h

,

5

where 2(x) = E[2 Y - 0(x) |X = x]. The standardized version of (5) can therefore be approximated by



nhd fX (x)(x)

K

Sn,0,0(x){^n(x) - 0(x)}
2

=1 hdfX (x)(x) K 2

K

x-u h

  Yi - 0(x) dZn(y, u) + O nhdhs + O

log n

3 4

nhd

.

(8)

The dominating term is defined by

Yn(x) d=ef

1 hdfX (x)(x)

K

x-u h

 y - 0(x) dZn(y, u).

(9)

Involving strong Gaussian approximation and Bernstein-type concentration inequalities, this process can be approximated by a stationary Gaussian field:

Y5,n(x)

=

1 hd

K

x-u h

dW u ,

(10)

where W denotes a Brownian sheet. The supremum of this process is asymptotically Gumbel distributed, which follows, e.g., by Theorem 2 of Rosenblatt (1976). Since the kernel is symmetric and of order s, we can estimate the term

Sn,0,0 = fY |X(0(x)|x)fX (x) + O(hs).

if (A5) holds. On the other hand, 2(x) =  (1 -  ) in quantile regression. Therefore, the statements of the theorem hold.

Corollary 2.2 (CC for multivariate quantile regression). Under the assumptions of Theorem 2.1, an approximate (1 - ) × 100% confidence corridor is given by

^n(t) ± (nhd)-1/2  (1 -  ) K 2/f^X(t) 1/2f^|X 0|t -1 dn + c()(2d log n)-1/2 ,

where   (0, 1) and c() = log 2 - log log(1 - ) and f^X(t), f^|X 0|t are consistent estimates for fX (t), f|X 0|t with convergence rate faster than Op (log n)-1/2 .

The expectile confidence corridor can be constructed in an analogous manner as the quantile confidence corridor. The two cases differ in the form and hence the properties of the loss function. Therefore we find for expectile regression:

Sn,0,0(x) = -2 FY |X 0(x |x)(2 - 1) -  fX(x) + O(hs).

Through similar approximation steps as the quantile regression, we derive the following theorem.

THEOREM 2.3. Let ^n(x) be the the local constant expectile estimator and 0(x) the true expectile function. If Assumptions (A1), (A3)-(A6) and (EA2) of Section A.1 hold with a

constant b1 satisfying

n-1/6h-d/2-3d/(b1-2) = O(n- ),  > 0.

6

Then the limit theorem (3) holds with a scaling factor
r(x) = nhdfX (x)-1(x) 2  - FY |X(0(x)|x)(2 - 1) ,
the same constants H2 and dn as defined in Theorem 2.1, where 2(x) = E[2(Y -0(x))|X = x] and  (u) = 2(1(u  0) -  )|u| is the derivative of the expectile loss-function  (u) =  - 1(u < 0) |u|2.
The proof of this result is deferred to the appendix. The next corollary shows the CC for expectiles.
Corollary 2.4 (CC for multivariate expectile regression). Under the same assumptions of Theorem 2.3, an approximate (1 - ) × 100% confidence corridor is given by
^n(t) ± (nhd)-1/2 ^2(t) K 2/f^X (t) 1/2 - 2 F^|X 0|t (2 - 1) -  -1 dn + c()(2d log n)-1/2 ,
where   (0, 1) c() = log 2-log log(1-) and f^X(t), ^2(t) and F^|X(0|x) are consistent estimates for fX (t), 2(t) and F|X(0|x) with convergence rate faster than Op (log n)-1/2 .
A further immediate consequence of Theorem 2.3 is a similar limit theorem in the context of local least squares estimation of the regression curve in classical mean regression.
Corollary 2.5 (CC for multivariate mean regression). Consider the loss function (u) = u2 corresponding to (u) = 2u. Under the assumptions of Theorem 2.3, with the same constants H2 and dn, (3) holds for the local constant estimator ^ and the regression function (x) = E[Y | X = x] with scaling factor r(x) = nhdfX(x)-1(x) and 2(x) =Var[Y | X = x].
For the appropriate bandwidth choice, it is enough to take h = O(n-1/(2s+d)-), given s > d and  > 0 to make our asymptotic theories hold, where s is the order of H¨older continuity of the function 0. In the simulation study we use the rule-of-thumb bandwidth with adjustments proposed by Yu and Jones (1998) for nonparametric quantile regression, and for expectile regression we use the rule-of-thumb bandwidth for the conditional distribution smoother of Y given X, chosen with the np package in R. In the application, we use the cross-validated bandwidth for conditional distribution smoother of Y given X, chosen with the np package in R. This package is based on the paper of Li et al. (2013).
2.3. Estimating the scaling factors
The performance of the confidence bands is greatly influenced by the scaling factors f^|X(v|x), F|X(v|x) and ^(x)2. The purpose of this subsection is thus to propose a way to estimate these factors and investigate their asymptotic properties.
Since we consider the additive error model (1), the conditional distribution function FY |X(0(x)|x) and the conditional density fY |X(0(x)|x) can be replaced by F|X(0|x) and f|X(0|x), respectively, where F|X and f|X are the conditional distribution and density functions of . Similarly, we have
2(x) = E  Y - 0(x) 2 X = x = E  ()2 X = x
where  may depend on X due to heterogeneity. It should be noted that the kernel estimators for f|X(0|x) and fY |X(0(x)|x) are asymptotically equivalent, but show different finite sample behavior. We explore this issue further in the following section.
7

Introducing the residuals ^i = Yi - ^n(Xi) we propose to estimate F|X, f|X and 2(x) by

n
F^|X(v|x) = n-1 G
i=1

v - ^i h0

Lh¯ (x - Xi)/f^X (x),

n
f^|X(v|x) = n-1 gh0 (v - ^i) Lh¯(x - Xi)/f^X(x),

i=1

n
^2(x) = n-1 2(^i)Lh¯(x - Xi)/f^X (x),

i=1

(11) (12) (13)

where f^X (x) = n-1 tribution function and

n i=1

Lh¯ (x

-

X i),

G

g is its derivative.

is a The

continuously differentiable cumulative dissame bandwidth h¯ is applied to the three

estimators, but the choice of h¯ will make the convergence rate of (13) sub-optimal. More

details on the choice of h¯ will be given later. Nevertheless, the rate of convergence of (13) is

of polynomial order in n. The theory developed in this subsection can be generalized to the

case of different bandwidth for different direction without much difficulty.

The estimators (11) and (12) belong to the family of residual-based estimators. The

consistency of residual-based density estimators for errors in a regression model are explored

in the literature in various settings. It is possible to obtain an expression for the residual

based kernel density estimator as the sum of the estimator with the true residuals, the partial

sum of the true residuals and a term for the bias of the nonparametrically estimated function,

as shown in Muhsal and Neumeyer (2010), among others. The residual based conditional

kernel density case is less considered in the literature. Kiwitt and Neumeyer (2012) consider

the residual based kernel estimator for conditional distribution function conditioning on a

one-dimensional variable.

Below we give consistency results for the estimators defined in (11), (12) and (13). The

proof can be found in the appendix.

Lemma 2.6. Under conditions (A1), (A3)-(A5), (B1)-(B3) in Section A.1, we have 1) supvI supxD F^|X(v|x) - F|X(v|x) = Op an , 2) supvI supxD f^|X(v|x) - f|X(v|x) = Op an , 3) supxD ^2(x) - 2(x) = Op bn ,
where an = O hs0 + hs + h¯s + (nh¯d)-1/2 log n + (nhd)-1/2 log n = O(n-), and bn = O hs + h¯s + (nh¯d)-1/2 log n + (nhd)-1/2 log n = O(n-1) for some constants , 1 > 0.

The factor of log n shown in the convergence rate is the price which we pay for the supnorm deviation. Since these estimators uniformly converge in a polynomial rate in n, the asymptotic distributions in Theorem 2.1 and 2.3 do not change if we plug these estimators into the formulae.
The choice of h0 and ¯h should minimize the convergence rate of the residual based estimators. Hence, observing that the terms related to h0 and h¯ are similar to those in usual (d + 1)-dimensional density estimators, it is reasonable to choose h0  h¯  n-1/(5+d), given that L, g are second order kernels. We choose the rule-of-thumb bandwidths for conditional densities with the R package np in our simulation and application studies.

8

3. Bootstrap confidence corridors

3.1. Asymptotic theory

In the case of the suitably normed maximum of independent standard normal variables, it is shown in Hall (1979) that the speed of convergence in limit theorems of the form (3) is of order 1/ log n, that is, the coverage error of the asymptotic CC decays only logarithmically. This leads to unsatisfactory finite sample performance of the asymptotic methods, especially for small sample sizes. However, Hall (1991) suggests that the use of a bootstrap method, based on a proper way of resampling, can increase the speed of shrinking of coverage error to a polynomial rate of n. In this section we therefore propose a specific bootstrap technique and construct a confidence corridor for the objects to be analysed.
Given the residuals ^i = Yi - ^n(Xi), the bootstrap observations (Xi, i) are sampled from

f^,X (v,

x)

=

1 n

n

gh0 (^i - v) Lh¯(x - Xi),

i=1

(14)

where g and L are a kernel functions with bandwidths h0, h¯ satisfying assumptions (B1)(B3). In particular, in our simulation study, we choose L to be a product Gaussian kernel. In the following discussion P and E stand for the probability and expectation conditional
on the data (Xi, Yi), i = 1, ..., n. We introduce the notation

An(x)

=

1 n

n

Kh(x - Xi) (i ),

i=1

and define the so-called "one-step estimator" (x) from the bootstrap sample by

^(x) - ^n(x) = S^n-,10,0(x) {An(x) - E[An (x)]} ,

(15)

where

S^n,0,0(x) =

f^|X 0|x f^X (x),

quantile case;

2  - F^|X 0|x (2 - 1) f^X (x), expectile case.

(16)

note that E[^(x) - ^n(x)] = 0, so ^(x) is unbiased for ^n(x) under E. As a remark, we note that undersmoothing is applied in our procedure for two reasons: first, the theory we developed so far is based on undersmoothing; secondly, it is suggested in Hall (1992) that undersmoothing is more effective than oversmoothing given that the goal is to achieve coverage accuracy.
Note that the bootstrap estimate (15) is motivated by the smoothed bootstrap procedure proposed in Claeskens and Van Keilegom (2003). In constrast to these authors we make use of the leading term of the Bahadur representation. Mammen et al. (2013) also use the leading term of a Bahadur representation proposed in Guerre and Sabbah (2012) to construct bootstrap samples. Song et al. (2012) propose a bootstrap for quantile regression based on oversmoothing, which has the drawback that it requires iterative estimation, and oversmoothing is in general less effective in terms of coverage accuracy.

9

For the following discussion define

Yn(x) =

1 hdf^X (x)(x)

K

x-u h

 v dZn(v, u)

as the bootstrap analogue of the process (9), where

(17)

Zn(y, u) = n1/2 Fn(v, u) - F^(v, u) , (x) = E  (i)2|x

(18)

and

Fn(v, u)

=

1 n

n

1 {i  v, X1  u1, ..., Xd  ud} .

i=1

The process Yn serves as an approximation of a standardized version of ^n - ^n, and similar to the previous sections the process Yn is approximated by a stationary Gaussian field Yn,5 under P with probability one, that is,

Y5,n(x)

=

1 hd

K

x-u h

dW (u).

Finally, supxD Y5,n(x) is asymptotically Gumbel distributed conditional on samples.

THEOREM 3.1. Suppose that assumptions (A1)-(A6), (C1) in Section A.1 hold, and vol(D) = 1, let

r(x) =

f^X

nhd (x)2

(x)

S^n,0,0(x),

where S^n,0,0(x) is defined in (16) and 2(x) is defined in (18). Then

P (2d log n)1/2 sup r(x)|^(x) - ^n(x)| / K 2 - dn < a  exp - 2 exp(-a) , a.s.
xD
(19)

as n   for the local constant quantile regression estimate. If (A1)-(A6) and (EC1) hold with a constant b  4 satisfying

n h-

1 6

+

4 b2

-

1 b

-

d 2

-

6d b

= O(n-),

 > 0,

then (19) also holds for expectile regression with corresponding 2(x).
The proof can be found in the appendix. The following lemma suggests that we can replace (x) in the limiting theorem by ^(x).
Lemma 3.2. If assumptions (B1)-(B3), and (EC1) in Section A.1 are satisfied with b > 2(2s + d + 1)/(2s + 3), then

2(x) - ^2(x) = Op (log n)-1/2 , a.s. The following corollary is a consequence of Theorem 3.1.

10

Corollary 3.3. Under the same conditions as stated in Theorem 3.1, the (asymptotic) bootstrap confidence set of level 1 -  is given by


  : sup
 xD

S^n,0,0(x)

^n(x) - (x)

f^X (x)^2(x)

    , 

(20)

where  satisfies 

lim P sup

n

xD

S^n,0,0(x) ^(x) - ^n(x) f^X (x)^2(x)

   = 1 - ,

a.s.

(21)

where S^n,0,0 is defined in (16).
Note that it does not create much difference to standardize the ^n(x) - 0(x) in (19) with f^X and ^2(x) constructed from original samples or f^X and ^2(x) from the bootstrap samples. The simulation results of Claeskens and Van Keilegom (2003) show that the two ways of standardization give similar coverage probabilities for confidence corridors of kernel ML estimators.

3.2. Implementation

In this section, we discuss issues related to the implementation of the bootstrap for
quantile regression.
The one-step estimator for quantile regression defined in (15) depends sensitively on the estimator of S^n,0,0(x). Unlike the expectile case, the function (·) in quantile case is bounded, and as the result the bootstrapped density based on (20) is very easily influenced by the factor S^n,0,0(x); in particular, f^|X(0|x). As pointed out by Feng et al. (2011), the residual of quantile regression tends to be less dispersed than the model error; thus f^|X(0|x) tends to over-estimate the true f|X(0|x) for each x.
The way of getting around this problem is based on the following observation: An additive
error model implies the equality fY |X v + 0(x)|x = f|X v|x but this property does not hold for the kernel estimators

n
f^|X(0|x) = n-1 gh0 (^i) Lh¯(x - Xi)/f^X (x)
i=1 n
f^Y |X (^n(x)|x) = n-1 gh1 Yi - ^n(x) Lh~ (x - Xi)/f^X (x),
i=1

(22) (23)

of the conditional density functions. In general f^|X(0|x) = f^Y |X(^n(x)|x) in x although both estimates are asymptotically equivalent. In applications the two estimators can differ
substantially due to the bandwidth selection because for data-driven bandwidths we usually
have h0 = h1. For example, if acommon method for bandwidth selection such as a rule-ofthumb is used, h1 will tend to be larger than h0 since the sample variance of Yi tends to be larger than that of ^i. Given that the same kernels are applied, it happens often that f^Y |X(^n(x)|x) > fY |X(0(x)|x), even if ^n(x) is usually very close to 0(x). To correct such abnormality, we are motivated to set h1 = h0 which is the rule-of-thumb bandwidth of f^|x(v|x) in (23). As the result, it leads to a more rough estimate for f^Y |X(^n(x)|x).

11

In order to exploit the roughness of f^Y |X(^n(x)|x) while making the CC as narrow as possible, we develop a trick depending on

f^Y |X ^n(x)|x f^|X (0|x)

=

h0 h1

n i=1

gh1

Yi - ^n(x) /h1 Lh~(x - Xi)

n i=1

gh0

(^i/h0)

Lh¯ (x

-

X

i)

.

(24)

As n  , (24) converges to 1. If we impose h0 = h1, as the multiple h0/h1 vanishes, (24) captures the deviation of the two estimators without the difference of the bandwidth in the
way. In particular, the bandwidth h0 = h1 is selected with the rule-of-thumb bandwidth for f^|X(y|x). This makes f^|X(y|x) larger and thus leads to a narrower CC, as will be more clear below.
We propose the alternative bootstrap confidence corridor for quantile estimator:

 : sup f^X(x)f^Y |X ^n(x)|x ^n(x) - (x)   ,
xD

where  satisfies

P

sup
xD

f^X

(x)-1/2

f^Y

|X ^n(x)|x f^|X (0|x)

An (x) - EAn (x)

 

= 1 - .

(25)

Note that the probability on the left-hand side of (25) can again be approximated by a Gumbel distribution function asymptotically, which follows by Theorem 3.1.

4. A simulation study
In this section we investigate the methods described in the previous sections by means of a simulation study. We construct confidence corridors for quantiles and expectiles for different levels  and use the quartic (product) kernel. For the confidence based on asymptotic distribution theory, we use the rule of thumb bandwidth chosen from the R package np, and then rescale it as described in Yu and Jones (1998), finally multiply it by n-0.05 for undersmoothing. The sample sizes are given by n = 100, 300 and 500, so the undersmoothing multiples are 0.794, 0.752 and 0.733 respectively. In the quantile regression bootstrap CC, the bandwidth h1 used for estimating f^Y |X(y|x) is chosen to be the rule-of-thumb bandwidth of f^|X(0|x) and multiplied by a multiple 1.5. This would give slightly wider CCs.
The data are generated from the normal regression model
Yi = f (X1,i, X2,i) + (X1,i, X2,i)i, i = 1, . . . , n
where the independent variables (X1, X2) follow a joint uniform distribution taking values on [0, 1]2, Cov(X1, X2) = 0.2876, f (X1, X2) = sin(2X1) + X2, and i are independent standard Gaussian random variables. For both quantile and expectile, we look at three quantiles of the distribution, namely  = 0.2, 0.5, 0.8.
In the homogeneous model, we take (X1, X2) = 0, for 0 = 0.2, 0.5, 0.7. In the heterogeneous model, we take (X1, X2) = 0 + 0.8X1(1 - X1)X2(1 - X2). 2000 simulation runs are carried out to estimate the coverage probability.
The upper part of Table 1 shows the coverage probability of the asymptotic CC for nonparametric quantile regression functions. It can be immediately seen that the asymptotic

12

Method n

Homogeneous  = 0.5  = 0.2  = 0.8
0 = 0.2

Heterogeneous  = 0.5  = 0.2  = 0.8

100 .000(0.366) .109(0.720) .104(0.718) .000(0.403) .120(0.739) .122(0.744) 300 .000(0.304) .130(0.518) .133(0.519) .002(0.349) .136(0.535) .153(0.537) 500 .000(0.262) .117(0.437) .142(0.437) .008(0.296) .156(0.450) .138(0.450)
0 = 0.5

Asympt.

100 .070(0.890) 300 .276(0.735) 500 .364(0.636)

.269(1.155) .281(1.155) .369(0.837) .361(0.835) .392(0.711) .412(0.712)
0 = 0.7

.078(0.932) .325(0.782) .381(0.669)

.300(1.193) .380(0.876) .418(0.743)

.302(1.192) .394(0.877) .417(0.742)

100 .160(1.260) .381(1.522) .373(1.519) .155(1.295) .364(1.561) .373(1.566) 300 .438(1.026) .450(1.109) .448(1.110) .481(1.073) .457(1.155) .472(1.152) 500 .533(0.888) .470(0.950) .480(0.949) .564(0.924) .490(0.984) .502(0.986)

0 = 0.2

100 .325(0.676) .784(0.954) .783(0.954) .409(0.717) .779(0.983) .778(0.985) 300 .442(0.457) .896(0.609) .894(0.610) .580(0.504) .929(0.650) .922(0.649) 500 .743(0.411) .922(0.502) .921(0.502) .839(0.451) .950(0.535) .952(0.536)
0 = 0.5

Bootst.

100 .929(1.341) .804(1.591) .818(1.589) .938(1.387) .799(1.645) .773(1.640) 300 .950(0.920) .918(1.093) .923(1.091) .958(0.973) .919(1.155) .923(1.153) 500 .988(0.861) .968(0.943) .962(0.942) .990(0.902) .962(0.986) .969(0.987)
0 = 0.7

100 .976(1.811) .817(2.112) .808(2.116) .981(1.866) .826(2.178) .809(2.176) 300 .986(1.253) .919(1.478) .934(1.474) .983(1.308) .930(1.537) .920(1.535) 500 .996(1.181) .973(1.280) .968(1.278) .997(1.225) .969(1.325) .962(1.325)

Table 1: Nonparametric quantile model coverage probabilities. The nominal coverage is 95%. The number in the parentheses is the volume of the confidence corridor. The asymptotic method corresponds to the asymptotic quantile regression CC and bootstrap method corresponds to quantile regression bootstrap CC.
CC performs very poorly, especially when n is small. A comparison of the results with those of one-dimensional asymptotic simultaneous confidence bands derived in the paper of Claeskens and Van Keilegom (2003) or Fan and Liu (2013), shows that the accuracy in the two-dimensional case is much worse. Much to our surprise, the asymptotic CC performs better in the case of  = 0.2, 0.8 than in the case of  = 0.5. On the other hand, it is perhaps not so amazing to see that asymptotic CCs behave similarly under both homogeneous and heterogeneous models. As a final remark about the asymptotic CC we mention that it is highly sensitive with respect to 0. Increasing values of 0 yields larger CC, and this may lead to greater coverage probability.
The lower part of Table 1 shows that the bootstrap CCs for nonparametric quantile regression functions yield a remarkable improvement in comparison to the asymptotic CC. For the bootstrap CC the coverage probabilities are in general close to the nominal coverage of 95%. The bootstrap CCs are usually wider, and getting narrower when n increases. Such phenomenon can also be found in the simulation study of Claeskens and Van Keilegom (2003). Bootstrap CCs are less sensitive than asymptotic CCs with respect to the choice 0, which is also considered as an advantage. Finally, we note that the performance of bootstrap CCs does not depend on which variance specification is used too.

13

Method n

Homogeneous  = 0.5  = 0.2  = 0.8
0 = 0.2

Heterogeneous  = 0.5  = 0.2  = 0.8

100 .000(0.428) .000(0.333) .000(0.333) .000(0.463) .000(0.362) .000(0.361) 300 .049(0.341) .000(0.273) .000(0.273) .079(0.389) .001(0.316) .002(0.316) 500 .168(0.297) .000(0.243) .000(0.243) .238(0.336) .003(0.278) .002(0.278)
0 = 0.5

Asympt.

100 .007(0.953) 300 .341(0.814) 500 .647(0.721)

.000(0.776) .000(0.781) .019(0.708) .017(0.709) .067(0.645) .065(0.647)
0 = 0.7

.007(0.997) .355(0.862) .654(0.759)

.000(0.818) .017(0.755) .061(0.684)

.000(0.818) .018(0.754) .068(0.684)

100 .012(1.324) .000(1.107) .000(1.107) .010(1.367) .000(1.145) .000(1.145) 300 .445(1.134) .021(1.013) .013(1.016) .445(1.182) .017(1.062) .016(1.060) 500 .730(1.006) .062(0.928) .078(0.929) .728(1.045) .068(0.966) .066(0.968)

0 = 0.2

100 .686(2.191) .781(2.608) .787(2.546) .706(2.513) .810(2.986) .801(2.943) 300 .762(0.584) .860(0.716) .876(0.722) .788(0.654) .877(0.807) .887(0.805) 500 .771(0.430) .870(0.533) .875(0.531) .825(0.516) .907(0.609) .904(0.615)
0 = 0.2

Bootst.

100 .886(5.666) .906(6.425) .915(6.722) .899(5.882) .927(6.667) .913(6.571) 300 .956(1.508) .958(1.847) .967(1.913) .965(1.512) .962(1.866) .969(1.877) 500 .968(1.063) .972(1.322) .972(1.332) .972(1.115) .971(1.397) .974(1.391)
0 = 0.2

100 .913(7.629) .922(8.846) .935(8.643) .929(8.039) .935(9.057) .932(9.152) 300 .969(2.095) .969(2.589) .971(2.612) .974(2.061) .972(2.566) .979(2.604) 500 .978(1.525) .976(1.881) .967(1.937) .981(1.654) .978(1.979) .974(2.089)

Table 2: Nonparametric expectile model coverage probability. The nominal coverage is 95%. The number in the parentheses is the volume of the confidence corridor. The asymptotic method corresponds to the asymptotic expectile regression CC and bootstrap method corresponds to expectile regression bootstrap CC.
The upper part of Table 2 shows the coverage probabiltiy of the CC for nonparametric expectile regression functions. The results are similar to the case of quantile regression. The asymptotic CCs do not give accurate coverage probabilities, and in some cases like  = 0.2 and 0 = 0.2, not a single simulation in the 2000 iterations yields a case where surface is completely covered by the asymptotic CC.
The lower part of Table 2 shows that bootstrap CCs for expectile regression give more accurate approximates to the nominal coverage than the asymptotic CCs. One can see in the parenthesis that the volumes of the bootstrap CCs are significantly larger than those of the asymptotic CCs, especially for small n.
5. Application: a treatment effect study
The classical application of the proposed method consists in testing the hypothetical functional form of the regression function. Nevertheless, the proposed method can also be applied to test for a quantile treatment effect (see Koenker; 2005) or to test for conditional stochastic dominance (CSD) as investigated in Delgado and Escanciano (2013). In this section we shall apply the new method to test these hypotheses for data collected from a real

14

government intervention.
The estimation of the quantile treatment effect (QTE) recovers the heterogeneous im-
pact of intervention on various points of the response distribution. To define QTE, given vector-valued exogenous variables X  X where X  Rd, suppose Y0 and Y1 are response variables associated with the control group and treatment group, and let F0|X and F1|X be the conditional distribution for Y0 and Y1, the QTE at level  is defined by

 (x) d=ef Q1|X ( |x) - Q0|X ( |x), x  X ,

(26)

where Q0|X (y|x) and Q1|X (y|x) are the conditional quantile of Y0 given X and Y1 given X respectively. This definition corresponds to the idea of horizontal distance between the treatment and control distribution functions appearing in Doksum (1974) and Lehmann (1975).
A related concept in measuring the efficiency of a treatment is the so called "conditional stochastic dominance". Y1 conditionally stochastically dominates Y0 if

F1|X (y|x)  F0|X (y|x) a.s. for all (y, x)  (Y, X ),

(27)

where Y, X are domains of Y and X. For example, if Y0 and Y1 stand for the income of two groups of people G0 and G1, (27) means that the distribution of Y1 lies on the right of that of Y0, which is equivalent to saying that at a given 0 <  < 1, the  -quantile of Y1 is greater than that of Y0. Hence, we could replace the testing problem (27) by

Q1|X( |x)  Q0|X ( |x) for all 0 <  < 1 and x  X .

(28)

Comparing (28) and (26), one would find that (28) is just a uniform version of the test  (x)  0 over 0 <  < 1.
The method that we introduced in this paper is suitable for testing a hypothesis like  (x) = 0 where  (x) is defined in (26). One can construct CCs for Q1|X( |x) and Q0|X( |x) respectively, and then check if there is overlap between the two confidence regions. One can also extend this idea to test (28) by building CCs for several selected levels  .
We use our method to test the effectiveness of the National Supported Work (NSW) demonstration program, which was a randomized, temporary employment program initiated in 1975 with the goal to provide work experience for individuals who face economic and social problems prior to entering the program. The data have been widely applied to examine techniques which estimate the treatment effect in a nonexperimental setting. In a pioneer study, LaLonde (1986) compares the treatment effect estimated from the experimental NSW data with that implied by nonexperimental techniques. Dehejia and Wahba (1999) analyse a subset of Lalonde's data and propose a new estimation procedure for nonexperimental treatment effect giving more accurate estimates than Lalonde's estimates. The paper that is most related to our study is Delgado and Escanciano (2013). These authors propose a test for hypothesis (27) and apply it to Lalonde's data, in which they choose "age" as the only conditional covariate and the response variable being the increment of earnings from 1975 to 1978. They cannot reject the null hypothesis of nonnegative treatment effect on the earnings growth.
The previous literature, however, has not addressed an important question. We shall depict this question by two pictures. In Figure 1, it is obvious that Y1 stochastically dominates Y0 in both pictures, but significant differences can be seen between them. For the left one, the 0.1 quantile improves more dramatically than the 0.9 quantile, as the distance between A and A is greater than that between B and B. In usual words, the gain of the 90% lower

15

tau 0.1 0.9
tau 0.1 0.9

F0 F1

F0 F1

A B A' B' Earnings growth

A A'

B

Earnings growth

B'

Figure 1: The illustrations for the two possible types of stochastic dominance.

bound of the earnings growth is more than that of the 90% upper bound of the earnings growth after the treatment. "90% lower bound of the earnings growth" means the probability that the earnings growth is above the bound is 90%. This suggests that the treatment induces greater reduction in downside risk but less increase in the upside potential in the earnings growth. For the right picture the interpretation is just the opposite.
To see which type of stochastic dominance the NSW demonstration program belongs to, we apply the same data as Delgado and Escanciano (2013) for testing the hypothesis of positive quantile treatment effect for several quantile levels  . The data consist of 297 treatment group observations and 423 control group observations. The response variable Y0 (Y1) denotes the difference in earnings of control (treatment) group between 1978 (year of postintervention) and 1975 (year of preintervention). We first apply common statistical procedures to describe the distribution of these two variables. Figure 2 shows the unconditional densities and distribution function. The cross-validated bandwidth for f^0(y) is 2.273 and 2.935 for f^1(y). The left figure of Figure 2 shows the unconditional densities of the income difference for treatment group and control group. The density of the treatment group has heavier tails while the density of the control group is more concentrated around zero. The right figure shows that the two unconditional distribution functions are very close on the left of the 50% percentile, and slight deviation appears when the two distributions are getting closer to 1. Table 3 shows that, though the differences are small, but the quantiles of the unconditional cdf of treatment group are mildly greater than that of the control group for each chosen  . The two-sample Kolmogorov-Smirnov and Cram´er-von Mises tests, however, yield results shown in the Table 4 which cannot reject the null hypothesis that the empirical cdfs for the two groups are the same with confidence levels 1% or 5%.

 (%)

10 20 30 50 70 80 90

Treatment -4.38 -1.55 0.00 1.40 5.48 8.50 11.15

Control -4.91 -1.73 -0.17 0.74 4.44 7.16 10.56

Table 3: The unconditional sample quantiles of treatment and control groups.

16

Density
0.00 0.02 0.04 0.06 0.08
Fn(y)
0.0 0.2 0.4 0.6 0.8 1.0

-40 -20

0

20 40 60

Earnings in 78-75 (in thousand dollars)

-40 -20

0

20 40 60

Earnings in 78-75 (in thousand dollars)

Figure 2: Unconditional empirical density function (left) and distribution function (right) of the difference of earnings from 1975 to 1978. The dashed line is associated with the control group and the solid line is associated with the treatment group.

Type of test

Statistics p-value

Kolmogorov-Smirnov 0.0686 0.3835

Cram´er-von Mises

0.2236 0.7739

Table 4: The two sample empirical cdf tests results for treatment and control groups.
Next we apply our test on quantile regression to evaluate the treatment effect. In order to compare with Delgado and Escanciano (2013), we first focus on the case of a one-dimensional covariate. The first covariate X1i is the age. The second covariate X2i is the number of years of schooling. The sample values of schooling years lie in the range of [3, 16] and age lies between [17, 55]. In order to avoid boundary effect and sparsity of the samples, we look at the ranges [7,13] for schooling years and [19,31] for age. We apply the bootstrap CC method for quantiles  = 0.1, 0.2, 0.3, 0.5, 0.7, 0.8 and 0.9. We apply the quartic kernel. The cross-validated bandwidths are chosen in the same way as for conditional densities with the R package np. The resulting bandwidths are (2.2691,2.5016) for the treatment group and (2.7204, 5.9408) for the control group. In particular, for smoothing the data of the treatment group, for  = 0.1 and 0.9, we enlarge the cross-validated bandwidths by a constant of 1.7; for  = 0.2, 0.3, 0.7, 0.8, the cross-validated bandwidths are enlarged by constant factor 1.3. These inflated bandwidths are used to handle violent roughness in extreme quantile levels. The bootstrap CCs are computed with 10,000 repetitions. The level of the test is  = 5%.
The results of the two quantile regressions with one-dimensional covariate, and their CCs for various quantile levels are presented in Figure 3 and 4. We observe that for all chosen quantile levels the quantile estimates associated to the treatment group lie above that of the control group when age is over certain levels, and particularly for  = 10%, 50%, 80% and 90%, the quantile estimates for treatment group exceeds the upper CCs for the quantile estimates of the control group. On the other hand, at  = 10%, the quantile estimates for the control group drop below the CC for treatment group for age greater than 27. Hence, the results here show a tendency that both the downside risk reduction and the upside

17

potential enhancement of earnings growth are achieved, as the older individuals benefit the most from the treatment. Note that we observe a heterogeneous treatment effect in age and the weakly dominance of the conditional quantiles of the treatment group to that of the control group, i.e., (28) holds for the chosen quantile levels, which are in line with the findings of Delgado and Escanciano (2013).

tau=10%

tau=20%

tau=30%

Earnings in 78-75 (in thousand dollars) -6 -4 -2 0 2 4

Earnings in 78-75 (in thousand dollars) -10 -5 0 5

Earnings in 78-75 (in thousand dollars) -15 -10 -5 0

20 22 24 26 28 30
Age
tau=50%

20 22 24 26 28 30
Age
tau=70%

20 22 24 26 28 30
Age
tau=80%

10 15 20

Earnings in 78-75 (in thousand dollars)

10 15

Earnings in 78-75 (in thousand dollars)

5 10

Earnings in 78-75 (in thousand dollars)

5

5

-5 0

-5 0

0

20 22 24 26 28 30
Age

20 22 24 26 28 30
Age
tau=90%

20 22 24 26 28 30
Age

5 10 15 20 25 30 35

Earnings in 78-75 (in thousand dollars)

0

20 22 24 26 28 30
Age
Figure 3: Nonparametric quantile regression estimates and CCs for the changes in earnings between 1975-1978 as a function of age. The solid dark lines correspond to the conditional quantile of the treatment group and the solid light lines sandwich its CC, and the dashed dark lines correspond to the conditional quantiles of the control group and the solid light lines sandwich its CC.
We now turn to Figure 4, where the covariate is the years of schooling. The treatment effect is not significant for conditional quantiles at levels  = 10%, 20% and 30%. This suggests that the treatment does little to reduce the downside risk of the earnings growth for individuals with various degree of education. Nonetheless, we constantly observe that the regression curves of the treatment group rise above that of the control group after a certain level of the years of schooling for quantiles level  = 50%, 70%, 80% and 90%. Notice
18

that for  = 50% and 80% the regression curves associated to the treatment group reach the upper boundary of the CC of the control group. This suggests that the treatment effect tends to raise the upside potential of the earnings growth, in particular for those individuals who spent more years in the school. It is worth noting that we also see a heterogeneous treatment effect in schooling years, although the heterogeneity in education is less strong than the heterogeneity in age.

tau=10%

tau=20%

tau=30%

Earnings in 78-75 (in thousand dollars) -4 -2 0 2 4

Earnings in 78-75 (in thousand dollars) -4 -2 0 2 4

Earnings in 78-75 (in thousand dollars) -10 -5 0 5

7 8 9 10 11 12 13
Schooling in years
tau=50%

7 8 9 10 11 12 13
Schooling in years
tau=70%

7 8 9 10 11 12 13
Schooling in years
tau=80%

10 12 14

Earnings in 78-75 (in thousand dollars)

10 12

8

6

Earnings in 78-75 (in thousand dollars)

Earnings in 78-75 (in thousand dollars) -2 0 2 4 6

8

4

6

4

2

2

0

7 8 9 10 11 12 13
Schooling in years

7 8 9 10 11 12 13
Schooling in years
tau=90%

7 8 9 10 11 12 13
Schooling in years

10 15 20

Earnings in 78-75 (in thousand dollars)

5

0

7 8 9 10 11 12 13
Schooling in years
Figure 4: Nonparametric quantile regression estimates and CCs for the changes in earnings between 1975-1978 as a function of years of schooling. The solid dark lines correspond to the conditional quantile of the treatment group and the solid light lines sandwich its CC, and the dashed dark lines correspond to the conditional quantiles of the control group and the solid light lines sandwich its CC.
The previous regression analyses separately conditioning on covariates age and schooling years only give a limited view on the performance of the program, we now proceed to the analysis conditioning on the two covariates jointly (X1i, X2i). The estimation settings are similar to the case of univariate covariate. Figure 5 shows the quantile regression CCs. From a first glance of the pictures, the  -quantile CC of the treatment group and that of the
19

control group overlap extensively for all  . We could not find sufficient evidence to reject the null hypothesis that the conditional distribution of treatment group and control group are equivalent.

20

0

Earnings in 78-75 -20

30

13 12 11
10 Schooling in years 9

8

28 26 -40 24 Age 22
20

7

(a)  = 10%

0

30

13 12

Earnings in 78-75 -20

28 26

11
10
9 Schooling in years

8

24 Age
22
-40 20

(b)  = 20%

5

13 12 11 10
Schooling in years 9

0 Earnings in 78-75
-5

30 28 26

24 Age

-10 22

8 7

20

(c)  = 30%

20 20

13 12 11 10
9
Schooling in years

8

10 20

0 -10 -20 7

30 13 28 12
26 11

24 10

22 Age

9 Schooling in years

20

8

10

0 -10 -20 7

30 28 26 24 22 Age
20

13 12
11

10

9

Schooling in years

8

10

0 -10 -20 7

30 28 26 24 22 Age
20

(d)  = 50%

(e)  = 70%

(f)  = 80%

50

13 12 11 10
Schooling in years9

8

0
-50 7

30 28 26 24 22 Age
20

(g)  = 90%

Figure 5: The CCs for the treatment group and the control group. The net surface corresponds to the control group quantile CC and the solid surface corresponds to the treatment group quantile CC.

The second observation obtained from comparing subfigures in Figure 6, we find that the treatment has larger impact in raising the upper bound of the earnings growth than improving the lower bound. For lower quantile levels  = 10%, 20% and 30% the solid surfaces uniformly lie inside the CC of the control group, while for  = 50%, 70%, 80% and 90%, we see several positive exceedances over the upper boundary of the CC of the control group. Hence, the program tends to do better at raising the upper bound of the earnings

20

growth but does worse at improving the lower bound of the earnings growth. In other words, the program tends to increase the potential for high earnings growth but does little in reducing the risk of negative earnings growth.

0

13 12 11
10 Schooling in years
9

Earnings in 78-75 -20

-40

8 7

20

22

30 28 26
24 Age

(a)  = 10%

10

Earnings0 in 78-75

Schooling in years

13 12 11
10

9

8

Age

-10 -20

30 28 26 24
22

20 7

(b)  = 20%

5

13 12 11 10
Schooling in years9

0 Earnings in 78-75
30 -5 28
26
24 Age
-10 22
8 20 7

(c)  = 30%

15

15 10

Earnin5gs in 78-75

10 5

0 30
A2g8e

0

-5 26

13 Schooling in years 12 11 10 9

8

24
22 -10 20

7

13 Sc1h2ooling in years 11
10
9

8

-5 Age28 30 26
24 -10 22
20 7

(d)  = 50%

(e)  = 70%

20

15

10

13 1S2cho1o1ling1in0 years 9

Earnings in 78-75 5 30 28 0 2A6ge
24 -5 22
20 8
7

(f)  = 80%

40

30

20 Earnings in 78-75

10 30 28

26Age

0 24

13 12Sch1o1oling1in0 years9

22 20

8

7

(g)  = 90%

Figure 6: The conditional quantiles (solid surfaces) for the treatment group and the CCs (net surfaces) for the control group.

Our last conclusion comes from inspecting the shape of the surfaces: conditioning on different levels of years of schooling (age), the treatment effect is heterogeneous in age (years of schooling). The most interesting cases happens when conditioning on high age and high years of schooling. Indeed, when considering the cases of  = 80% and 90%, when conditioning on the years of schooling at 12 (corresponding to finishing the high school), the earnings increment of the treatment group rises above the upper boundary of the CC of the control group. This suggests that the individuals who are older and have more years of schooling

21

tend to benefit more from the treatment.
Supplementary Materials
Section A contains the detailed proofs of Theorems 2.1, 2.3, 3.1 and Lemmas 2.6 and 3.2, as well as intermediate results. Section B contains some results obtained by other authors, which we use in our study. We incorporate them here for completeness.
References
Bickel, P. J. and Rosenblatt, M. (1973). On some global measures of the deviations of density function estimates, The Annals of Statistics 1(6): 1071­1095.
Bickel, P. J. and Wichura, M. J. (1971). Convergence criteria for multivariate stochastic processes and some applications, The Annals of Mathematical Statistics 42(5): 1656­1670.
Bissantz, N., Du¨mbgen, L., Holzmann, H. and Munk, A. (2007). Nonparametric confidence bands in deconvolution density estimation, Journal of the Royal Statistical Society: Series B 69(3): 483­506.
Claeskens, G. and Van Keilegom, I. (2003). Bootstrap confidence bands for regression curves and their derivatives, The Annals of Statistics 31(6): 1852­1884.
Dedecker, J., Merlev´ede, F. and Rio, E. (2014). Strong approximation of the empirical distribution function for absolutely regular sequences in Rd, Electronic Journal of Probability 19(9): 1­56.
Dehejia, R. H. and Wahba, S. (1999). Causal effects in nonexperimental studies: Reevaluating the evaluation of training programs, Journal of the American Statistical Association 94(448): 1053­1062.
Delgado, M. A. and Escanciano, J. C. (2013). Conditional stochastic dominance testing, Journal of Business & Economic Statistics 31(1): 16­28.
Doksum, K. (1974). Empirical probability plots and statistical inference for nonlinear models in the two-sample case, The Annals of Statistics 2(2): 267­277.
Fan, Y. and Liu, R. (2013). A direct approach to inference in nonparametric and semiparametric quantile regression models, Preprint.
Feng, X., He, X. and Hu, J. (2011). Wild bootstrap for quantile regression, Biometrika 98(4): 995­999.
Gin´e, E. and Nickl, R. (2010). Confidence bands in density estimation, The Annals of Statistics 38(2): 1122­1170.
Guerre, E. and Sabbah, C. (2012). Uniform bias study and Bahadur representation for local polynomial estimators of the conditional quantile function, Econometric Theory 28(1): 87­ 129.
22

Guo, M. and H¨ardle, W. (2012). Simultaneous confidence bands for expectile functions, AStA Advances in Statistical Analysis 96(4): 517­541.
Hall, P. (1979). On the rate of convergence of normal extremes, Journal of Applied Probability 16(2): 433­439.
Hall, P. (1991). On convergence rates of suprema, Probability Theory and Related Fields 89(4): 447­455.
Hall, P. (1992). Effect of bias estimation on coverage accuracy of bootstrap confidence intervals for a probability density, The Annals of Statistics 20(2): 675­694.
Hall, P. and Horowitz, J. (2013). A simple bootstrap method for constructing nonparametric confidence bands for functions, The Annals of Statistics 41(4): 1892­1921.
Hansen, B. E. (2008). Uniform convergence rates for kernel estimation with dependent data, Econometric Theory 24(3): 726­748.
H¨ardle, W. (1989). Asymptotic maximal deviation of M-smoothers, Journal of Multivariate Analysis 29(2): 163­179.
H¨ardle, W. and Song, S. (2010). Confidence bands in quantile regression, Econometric Theory 26(4): 1180­1200.
Johnston, G. J. (1982). Probabilities of maximal deviations for nonparametric regression function estimates, Journal of Multivariate Analysis 12(3): 402­414.
Jones, M. C. (1994). Expectiles and M-quantiles are quantiles, Statistics & Probability Letters 20(2): 149­153.
Kiwitt, S. and Neumeyer, N. (2012). Estimating the conditional error distribution in nonparametric regression, Scandinavian Journal of Statistics 39(2): 259­281.
Koenker, R. (2005). Quantile Regression, Econometric Society Monographs, Cambridge University Press, New York.
Kong, E., Linton, O. and Xia, Y. (2010). Uniform Bahadur representation for local polynomial estimates of M-regression and its application to the additive model, Econometric Theory 26(5): 1529­1564.
LaLonde, R. J. (1986). Evaluating the econometric evaluations of training programs with experimental data, The American Economic Review 76(4): 604­620.
Lehmann, E. L. (1975). Nonparametrics: Statistical Models Based on Ranks, Springer, San Francisco, CA.
Li, Q., Lin, J. and Racine, J. S. (2013). Optimal bandwidth selection for nonparametric conditional distribution and quantile functions, Journal of Business & Economic Statistics 31(1): 57­65.
Li, Q. and Racine, J. S. (2007). Nonparametric Econometrics: Theory and Practice, Prince-
23

ton university press, New Jersey.
Lounici, K. and Nickl, R. (2011). Global uniform risk bounds for wavelet deconvolution estimators, The Annals of Statistics 39(1): 201­231.
Mammen, E., Van Keilegom, I. and Yu, K. (2013). Expansion for moments of regression quantiles with applications to nonparametric testing, ArXiv e-prints .
Meerschaert, M. M., Wang, W. and Xiao, Y. (2013). Fernique-type inequalities and moduli of continuity for anisotropic Gaussian random fields, Transations of the American Mathematical Society 365(2): 1081­1107.
Mojirsheibani, M. (2012). A weighted bootstrap approximation of the maximal deviation of kernel density estimates over general compact sets, Journal of Multivariate Analysis 112: 230­241.
Muhsal, B. and Neumeyer, N. (2010). A note on residual-based empirical likelihood kernel density estimator, Electronic Journal of Statistics 4: 1386­1401.
Newey, W. K. and Powell, J. L. (1987). Asymmetric least squares estimation and testing, Econometrica 55(4): 819­847.
Owen, A. B. (2005). Multidimensional variation for quasi-Monte Carlo, Vol. 2 of Ser. Biostat., World Sci. Publi., Hackensack, NJ., pp. 49­74.
Proksch, K., Bissantz, N. and Dette, H. (2014). Confidence bands for multivariate and time dependent inverse regression models, Bernoulli . To appear.
Rosenblatt, M. (1976). On the maximal deviation of k-dimensional density estimates, The Annals of Probability 4(6): 1009­1015.
Smirnov, N. V. (1950). On the construction of confidence regions for the density of distribution of random variables, Doklady Akad. Nauk SSSR 74: 189­191.
Song, S., Ritov, Y. and H¨ardle, W. (2012). Partial linear quantile regression and bootstrap confidence bands, Journal of Multivariate Analysis 107: 244­262.
Yu, K. and Jones, M. C. (1998). Local linear quantile regression, Journal of the American Statistical Association 93(441): 228­237.
Appendices
A. Technical Proofs
A.1. Assumptions and notations
(A1) K is of order s - 1 (see (A3)) has bounded support [-A, A]d, is continuously differentiable up to order d with bounded derivatives, i.e. K  L1(Rd) exists and is
24

continuous for all multi-indices   {0, 1}d
(A2) Let an be an increasing sequence, an   as n  , and the marginal density fY be such that

(log n)h-3d

fY (y)dy = O(1)

|y|>an

and (log n)h-d
as n   hold.

fY |X(y|x)dy = O(1), for all x  D
|y|>an

(29)

(A3) The function 0(x) is continuously differentiable and is in Ho¨lder class with order s > d.

(A4) fX(x) is bounded, continuously differentiable and its gradient is uniformly bounded. Moreover, infxD fX (x) > 0.

(A5) The joint probability density function f (y, u) is bounded, positive and continuously differentiable up to sth order (needed for Rosenblatt transform). The conditional density fY |X(y|x) exists and is boudned and continuouly differentiable with respect to x. Moreover, infxD fY |X 0(x)|x > 0.
(A6) h satisfies nhdhslog n  0 (undersmoothing), and nh3d(log n)-2  .

(EA2) supxD vb1f|X(v|x)dv < , for some b1 > 0.
(B1) L  L1(Rd) is a Lipschitz, bounded, symmetric kernel. G is Lipschitz continuous cdf with G(x), 1 - G(x)  Ce-x for C > 0, and g  L1(R) is the derivative of G and is also a density, which is Lipschitz continuous, bounded, symmetric and five times continuously differentiable kernel.
(B2) F|X(v|x) is in s + 1 order H¨older class with respect to v and continuous in x, s > max{2, d}. fX(x) is in second order H¨older class with respect to x and v. E[2(i)|x] is second order continuously differentiable with respect to x  D.
(B3) nh0h¯d  , h0, h¯ = O(n-), where  > 0.
(C1) There exist an increasing sequence cn, cn   as n   such that

as n  .

(log n)3(nh6d)-1

f(v)dv = O(1),

|v|>cn/2

(30)

(EC1) supxD |v|bf|X(v|x)dv < , for some b > 0.

Remark A.1. Assumption (A6) makes h small. This would undermine the rate of the KMT approximation used in Lemma (A.5). As a result, we have to impose a strong enough smoothness condition s  d. On the other hand, a smoother 0 slows down the rate of h  0, which gives benefit to the rate of the Gaussian strong approximation.

25

Define the approximating processes

Yn(x) d=ef

1 hdfX (x)(x)

K

x-u h

 (y - 0(x))dZn(y, u).

(31)

Y0,n(x) =

1 hd fX (x)n2 (x)

K
n

x-u h

 (y - 0(x))dZn(y, u),

(32)

where n = {y : |y|  an} and n2(x) = E[2(Y - 0(x))1(Yi  an)|X = x].

Y1,n(x) =

1 hd fX (x)n2 (x)

K
n

x-u h

 (y - 0(x))dBn T (y, u)

(33)

where Bn T (y, u) = Wn T (y, u) - F (y, u)Wn(1, ..., 1) and T (y, u) is the Rosenblatt transformation

T (y, u) = FX1|Y (u1|y), FX2|Y (u2|u1, y), ..., FXd|Xd-1,...,X1,Y (ud|ud-1, ..., u1, y), FY (y) .

Y2,n(x) =

1 hd fX (x)n2 (x)

K
n

x-u h

 (y - 0(x))dWn T (y, u)

(34)

Y3,n(x) =

1 hd fX (x)n2 (x)

K
n

x-u h

 (y - 0(u))dWn T (y, u)

Y4,n(x) =

1 hdfX (x)n2(x)

n2(u)fX (u)K

x-u h

dW u .

(35) (36)

Y5,n(x)

=

1 hd

K

x-u h

dW u .

In these approximating processes, the function

(37)

 (u) =

1(u  0) - ,

Quantile;

2(1(u  0) -  )|u|, Expectile.

In the proofs, we suppress the subscript " ". Next we introduce some notations which are used repeatedly in the following proofs.
Definition A.2 (Neighboring Block in D  Rd, Bickel and Wichura (1971) p.1658). A block B  D is a subset of D of the form B = i(si, ti] with s and t in D; the pth-face of B is i=p(si, ti]. Disjoint blocks B and C are p-neighbors if they abut and have the same pth face; they are neighbors if they are p-neighbors for some p  1.

To illustrate the idea of neighboring block, take d = 3 for example, the blocks (s, t] × (a, b] × (c, d] and (t, u] × (a, b] × (c, d] are 1-neighbors for s  t  u.

26

Definition A.3 (Bickel and Wichura (1971) p.1658). Let X : Rd  R. The increment of X on the block B, denoted X(B), is defined by

X(B) =

(-1)d-||X s +   (t - s) ,

{0,1}d

(38)

where "" denotes the componentwise product; that is, for any vectors u, v  Rd, u  v = (u1v1, u2v2, ..., udvd).

Below we give some examples of the increment of a multivariate function X on a block:

· d = 1: B = (s, t], X(B) = X(t) - X(s);

· d = 2: B = (s1, t1] × (s2, t2]. X(B) = X(t1, t2) - X(t1, s2) + X(s1, s2) - X(s1, t2).

A.2. Proof of Theorem 2.1

Lemma A.4.

Yn(x) - Y0,n(x) = Op (log n)-1/2 ,

where · denotes the sup norm with respect to x  D.

PROOF. By the triangle inequality we have

Yn - Yn,0  Yn - Y^n,0 + Y^n,0 - Yn,0 d=ef E1 + E2,
where Y^n,0 = 2(x)/n(x)Yn,0(x) and the terms E1 and E2 are defined in an obvious manner. We now show that Ej = Op (log n)-1/2 , j = 1, 2. Note that

|Y^n,0(x) - Yn,0(x)| = (x)/n(x) - 1 Yn,0(x) . 
It is shown later that Yn,0 = Op log n , hence it remains to prove that

sup (x)/n(x) - 1 = O (log n)-1 .
xD

(39)

To this end let ~n2 = E[2{Yi - 0(x)}1(|Yi| > an)|X = x]. Since n2(x)   (1 -  ) > 0 for n  , by (29), and 2(·)  max{ 2, (1 -  )2}, |(log n)2~n2(x)/n2(x)|  (log n)hdO(1)  0. Therefore,

(log n) sup
xD

2(x) n2 (x)

-

1

= (log n) sup
xD

~n2(x) + n2(x) n2 (x)

-

1

 sup
xD

(log n)2~n2(x) n2 (x)

 0,

as n  , hence E2 = Op (log n)-1/2 . We now use Lemma B.2 in order to show that E1 too is negligible.

(log n)1/2E1 = (log n)1/2 sup |Yn(x) - Y^n,0(x)|
xD

= (log n)1/2 sup
xD

1 hdfX (x)2(x)

= sup
xD

fX

1 (x)2(x)

Vn(x)

,

K
{|y|>an }

x-u h

{y - 0(x)}dZn(y, u)

27

where and

n
Vn(x) = Wn,i(x),
i=1

Wn,i(x) = (log n)1/2(nhd)-1/2

(Yi - 0(x))1(|Yi| > an)K

x - Xi h

-E (Yi - 0(x))1(|Yi| > an)K

x - Xi h

.

Note that fX(x)2(x) = fX(x) (1 -  ) > 0 for x  D by Assumption (A4).

E[Wn,i(x)2]  (log n)(nhd)-1E

2(Yi - 0(x))1(|Yi| > an)K2

x - Xi h

Thus, from (29),

 (log n)(nhd)-1C,K

fY (y)dy.

{|y|>an }

n

E Wn,i(x) 2  (log n)h-dC,K

fY (y)dy = h2dOp(1)  0,

i=1 {|y|>an}

as n  . From Markov's inequality, |Vn(x)| p 0 for each fixed x  D. We now show the tightness of Vn(x) for x  D in order to obtain the uniform convergence.
To simplify the expression, define

g(x) d=ef {y - 0(x)}K

x-u h

.

Take arbitrary neighboring blocks B, C  D (see Definition A.2) and suppose B = id=1(si, ti],

E[Vn(B)2]1/2  (log n)1/2h-d/2 E 1(Yi > an)

(-1)d-||g s +   (t - s)

{0,1}d

+ E 1(Yi < -an)

(-1)d-||g s +   (t - s) 2 1/2

{0,1}d

2

d=ef (log n)1/2h-d/2(I1 + I2)1/2,

where I1 and I2 are defined in an obvious manner. When n is large, an is large as well and the integral is restricted to the set {Yi > an}. Taking into account that  is uniformly bounded on the compact set D by Assumption (A4) we deduce that (Yi - 0(x)) =  for sufficiently large n on the event {Yi > an : i = 1, ..., n}. Hence, I1 can be estimated as

I1   2

1(y > an)

(-1)d-||K

s +   (t - s) - u /h

2
f (y, u)dydu.

{0,1}d

Note that
(-1)d-||K
{0,1}d

s +   (t - s) - u /h

=

(1,...,1)K
B

v-u h

dv  h-dCKµ(B),

28

where the constant CK satisfies supuD |K(u)|  CK and µ(·) is the Lebesgue measure. As consequence it follows that

I1   2

1(y > an) CKµ(B) 2f (y, u)dydu =  2 h-dCKµ(B) 2

fY (y)dy.

{y>an }

Similarly, I2  (1 -  )2 CKh-dµ(B) 2 {y<-an} fY (y)dy. Hence,

E[Vn(B)2]1/2  (log n)1/2h-3d/2CKµ(B)  2

fY (y)dy + (1 -  )2

fY (y)dy

{y>an }

{y<-an }

1/2

 (log n)1/2h-3d/2CK max(, 1 -  )

fY (y)dy µ(B).

{|y|>an }

1/2

Analogously we obtain the estimate

E[Vn(C)2]1/2  (log n)1/2h-3d/2CK max(, 1 -  ) which finally yields

1/2

fY (y)dy
{|y|>an }

µ(C ),

E[|Vn(B)||Vn(C)|]  E[|Vn(B)|2]1/2E[|Vn(C)|2]1/2

 (log n)h-3dCK2  max(, 1 -  )2

fY (y)dy µ(C)µ(B).

{|y|>an}

By Assumption (A2) it follows (log n)h-3d {|y|>an} fY (y)dy is bounded. Thus, applying Lemma B.2 with 1 = 2 = 1 = 2 = 1 yields the desired result.

Lemma A.5. Y0,n - Y1,n = Op n-1/6h-d/2(log n)+(2d+4)/3 , a.s. for any  > 0.

PROOF. We adopt the notation that if   {0, 1}d+1, then we write  = (1, 2) where 1  {0, 1} and 2  {0, 1}d. In the computation below, we focus on Bx = dj=1 xj -Ah, xj +Ah instead of Rd since K has compact support. Recall definition A.2 of an increment of a function X over a block B. Integration by parts yields

Y0,n(x)

1

=

hdfX (x)n2 (x)

Zn(y, u) d (y - 0(x))K((x - u)/h)
Bx n

+

Zn ·1, ·2  ·1 -0(x) K

x - ·2 h

n × Bx

+
{0,1}d+1 -{0,1}

Zn(·1, ·2) d1  ·1 -0(x) 2K (x - ·2)/h
(n ×Bx )

(40) n × Bx 1-

where 1 = (1, ..., 1)  {0, 1}d+1 and 0 = (0, ..., 0)  {0, 1}d+1. n × Bx is a d + 1 dimensional cube. ·1 corresponds to the one-dimensional variable y and ·2 corresponds to the two-dimensional variable u. The second term in (40) can be evaluated with the formula
(38). n × Bx 1- can be viewed as the projection of n × Bx on to the space spanned by those axes whose numbers correspond to positions of ones of the multi-index 1 - . This
leaves us with an ||-fold integral.

29

Moreover, d (y - 0(x))K((x - u)/h) = d(y - 0(x))12K (x - u)/h , where 12 = (1, ..., 1)  {0, 1}d and d(y - 0(x)) = 0(x)(y) denotes the Dirac measure at 0(x).
By integration by parts applied to Y1,n and an application of Theorem 3.2 in Dedecker et al.
(2014) we obtain for every  > 0,

hd/2n1/6(log n)--(2d+4)/3|Y0,n - Y1,n|

 O(1)

1

fX (x)n2(x)

dK((x - u)/h)
Bx

+



·1 -0(x) K

x - ·2 h

n × Bx

+ 2 K (x - ·2)/h (Bx)12-2
1=1,2{0,1}d-{1} (Bx)2

+ 2K (x - ·2)/h | ·1 -0(x) | n × (Bx)12-2
1=0,2{0,1}d-{0} (Bx)2

a.s. (41)

By (A1), K is of bounded variation in the sense of Hardy and Krause (Owen (2005) definition 2), and this leads to the desired result that (41) is bounded.
Lemma A.6. Y1,n - Y2,n = Op hd/2 .
PROOF. Since Bn T (y, u) = Wn T (y, u) -F (y, u)W (1, ..., 1), where T (y, u) is the Rosenblatt transformation and the Jacobian of T (y, u) is f (y, u),by a change of variables and the first order approximation to f (y, x - hv):

|Y1,n(x) - Y2,n(x)| 1
hdfX (x)n2(x) 1
hdfX (x)n2(x)  hd/2 K (v) dv
 hd/2 K (v) dv

K
n

x-u h

(y - 0(x))f (y, u)dydu |W (1, ..., 1)|

K (v) (y - 0(x))f (y, x - hv)hddydv |W (1, ..., 1)|
n

1 fX (x)n2(x)

|(y - 0(x))|f (y, x)dy + O(h) |W (1, ..., 1)|
n

1 max{, 1 -  } + O(h) |W (1, ..., 1)|, fX (x)n2(x)

note that |W (1, ..., 1)| = Op(1).

Lemma A.7. Y2,n - Y3,n = Op h1/2- for an arbitrarily small 0 <  < 1/2.

Remark A.8. We note that the rate of h1/2- is not sharp rate but sufficiently fast for our purpose.

PROOF. Define
Vn(x) d=ef Y2,n(x) - Y3,n(x) =1 hd fX (x)n2 (x)

{(y - 0(x)) - (y - 0(u))}K
n

x-u h

dW T (y, u) . (42)

30

Vn = Op h1/2- if

lim P sup V(x) > h- = 0, for all n  N.



xD

h

Since (y - 0(x)) - (y - 0(u)) = sign(0(u) - 0(x))1 [0(x)  0(u), 0(x)  0(u)] , thus
(y - 0(x)) - (y - 0(u)) 2 = 1 [0(x)  0(u), 0(x)  0(u)] .

By assumption the conditional distribution function FY |X and the function 0 are both continuously differentiable and change of variables and an application of the multivariate mean value theorem gives

E Vn(x) 2 h

=

1 hd+1fX (x)n2 (x)

{(y - 0(x)) - (y - 0(u))}2K2
n

x-u h

f (y, u)dydu



1 hd+1fX (x)n2(x)

FY |X(0(x)|u) - FY |X (0(u)|u) K2

x-u h

fX (u)du

=

1 hfX (x)n2(x)

K 2 (z )

 FY |X  0  |hz|fX (x)dz + O(h)

||=1



1 n2 (x)


||=1

FY |X  0

|z|K2(z)dz + O(h),

where  lies on the line connecting x and u. Note that n2(x)  min{ 2, (1 -  )2}. It follows from the continuous differentiability of FY |X and 0 that (FY |X  0) is bounded.

2 d=ef sup E Vn(x) 2  C + O(h), xh

(43)

Now we compute d(s, t) defined in Lemma B.3. Again from n2(x)  min{ 2, (1 -  )2} and (A4),

E Vn(t)- Vn(s) 2 h



C

1 hd+1

n

(y - 0(t)) - (y - 0(u)) K

t-u h

- (y - 0(s)) - (y - 0(u)) K

s-u h

2
f (y, u)dydu

=

C

1 hd+1

n

(y - 0(t)) - (y - 0(u))

K

t-u h

-K

- ((y - 0(s)) - (y - 0(u))) - ((y - 0(t)) - (y - 0(u))) K

s-u h
s-u h

2
f (y, u)dydu,

31

which implies

E Vn(t)- Vn(s) 2 h



2C hd+1

(y - 0(t)) - (y - 0(s)) 2K2
n

s-u h

f (y, u)dydu

+

2C hd+1

(y - 0(t)) - (y - 0(u)) 2 K
n

t-u h

-K

s-u h

d=ef I1 + I2.

2
f (y, u)dydu

Furthermore,

I1



2C hd+1

FY |X(0(t)|u) - FY |X(0(s)|u) K2

s-u h

fX (u)du



2C D hd+1

s-t



K2

s-u h

fX (u)du



2C  D h

s-t

,

where s - t  = supj |sj - tj|. A change of variables and the fact that K is bounded yield

I2



2C hd+1

FY |X(0(t)|u) - FY |X(0(u)|u)

K

t-u h

-K

s-u h



4C h

s-t  h

K (z) - K

z

+

s

- h

t

dz

2
fX (u)du

 4C

s-t  h2

|K (z)| dz +
[-A,A]d

[-A,A]d-

s-t h

K

z

+

s

- h

t

dz

= 4C

s-t h2



 Thus, for the function  defined in Lemma B.3 we obtain the estimate ()  C( /h) and

thus

Q(m)



(2

+

 2)

C h

 1

 m2-y2 dy





C

m h

,

where C > 0. Observe that the graph of the inverse of a univariate, injective function Q(m)

is its reflection about the line y = x, so the inverse of an upper bound for Q would be a lower

bound for Q-1. Given the upper bound above, we can therefore bound Q-1 from below by

Q-1(a)  (C)-2h2a2.

We have Q-1 1/(h-)  (C)-2-1h2+2. Applying Lemma B.3 yields

P

sup Vn(x)

xD

hn

> hn-

 Cdhn-2d(1+) exp -C2hn-2  0,

as    for all n  N.

Lemma A.9. Y3,n =L Y4,n.

32

PROOF. Since both processes are Gaussian with mean zero, we only need to check the equality of the covariance functions of the two processes at any given time points s, t  D. Ignoring the normalizing factors in the front, the covariance of Y3,n function is:

r3(s, t) = = =

2 y - 0(u) K
n

s-u h

K

t-u h

f (y, u)dydu

E 2 Yi - 0(u) 1(|Yi|  an)|u K

s-u h

K

t-u h

n2(u)fX (u)K

s-u h

K

t-u h

du = r4(s, t)

fX (u)du

which is, up to a factor, the covariance function of Y4,n. Lemma A.10. Y4,n - Y5,n = Op(h1-), for 0 <  < 1. PROOF. We will proceed as in Lemma A.7 and apply Lemma B.3. Set

Y~ (x) d=ef Y4,n-Y5,n =

1 hdfX (x)n2(x)

n2(u)fX(u) -

n2(x)fX (x) K

x-u h

dW (u).

Notice that where

n2(u) =  (1 -  ) -

2 y - 0(u) fY |X(y|u)dy,

{|y|>an }

2 y - 0(u) fY |X(y|u)dy 

fY |X(y|u)dy.

{|y|>an }

{|y|>an}

(29) suggests that

fY |X(y|u)dy = O(hd(log n)-1).
{|y|>an }

Hence, we have n2(u)  C + En, where En = O(hd(log n)-1), and C =  (1 -  ).

E

Y~ (t) h

2

=

1 hd+2fX (t)n2(t)

n2(u)fX(u) -

n2(t)fX (t) 2 K2

t-u h

du

=

1 hd+2fX (t)n2(t)

n2(u) fX (u) - fX (t)

+ fX (x)

n2(u) -

n2 (t)

2 K2

t-u h

du

 2Ch-d-2 max{ 2, (1 -  )2}

fX (u) -

fX (t)

2
K

2

t-u h

du

+C

n2(u) -

n2 (t)

2
K

2

t-u h

du

,

Since

2
n2(u) - n2(t) =

n2(u) - n2(t) n2(u) + n2(t)

2
 CEn2 = O(h2d(log n)-2);

33

moreover, fX(x) is continuously differentiable on D by assumption (A4). Along with |z|2K(z) < , we may bound

sup E
tD

Y~ (t) h

2

 C + O(h2d-2(log n)-2).

On the other hand,

Y~ (t) - Y~ (s) 2 Eh

 Ch-d-2

n2(u)fX (u) -

n2(t)fX(t) K

t-u h

-

n2(u)fX(u) -

n2(s)fX (s) K

s-u h

2
du

= Ch-d-2

n2(u)fX(u) -

n2(t)fX (t) K

t-u h

-K

s-u h

+

n2(t)fX(t) -

n2(s)fX (s) K

s-u h

2
du

 2Ch-d-2

n2(u)fX (u) -

2
n2(t)fX(t) K

t-u h

-K

s-u h

2
du

+ 2Ch-d-2

n2(t)fX(t) -

n2(s)fX (s)

2
K

2

s-u h

d=ef I1 + I2.

du

From
2
n2(t)fX (t) - n2(s)fX(s) =

n2(t)fX (t) - n2(s)fX (s) n2(t)fX (t) + n2(s)fX (s)

2
C

t-s

2,

we obtain

I2 = C

t-s h2

2
.

By change of variables and a similar argument as to bound I2 in the proof of Lemma A.7, it

follows

I1  C

s-t h3

.

Hence, under the condition that s - t  < 1 and h  0, we conclude that

E

Y~ (t) - Y~ (s) 2 h

C

s-t h3

.

(44)

With Q(m)

the C

same h-3/2

notations as in m. Therefore,

Lemma

B.3,

(44)

implies

()



C h-3/2 ,

which

gives

Q-1(a)  Ch3a2,

(45)

and

Q-1 (h-)-1  Ch3-2h2.

(46)

34

Lemma B.3 asserts that

P

sup
xD

Y~ (x) h

> h-

as    and h  0.

 Ch-(3+2)d2d exp -h-22  0,

Finally, an application of Theorem 2 of Rosenblatt (1976) to Y5,n(x) concludes the proof of Theorem 2.1.

A.3. Proof of Theorem 2.3

Now let  (u) = | - 1(u < 0)|u2, be the loss function associated to quantile regression. Then  (u) = -2  - 1(u < 0) |u| and

g(x) =

 t

E[(Y

- t)|X = x]

= -2

t=0 (x)

FY |X

0(x)|x

(2 - 1) - 

.

It is obvious that g(x) > 0 for 0 <  < 1, and consequently

Sn,0,0(x) = -2 FY |X 0(x |x)(2 - 1) -  fX(x) + O(hs).

Lemma A.11. Yn - Y0,n = Op (log n)-1/2 .
PROOF. We have Yn - Y0,n  Yn - Y^n,0 + Y^n,0 - Y0,n , where Y^n,0 is defined as in Lemma A.4, with an  (h-3d log n)1/(b1-2). With such a choice we have

h-3d log n sup

y2fY |X (y|x)dy = O(1)

xD |y|>an

(47)

which implies h-3d log n |y|>an y2fY (y)dy = O(1). It follows that Yn -Y^n,0 via similar arguments as in Lemma A.4.

Since

E Wn2,i(x)  (log n)(nhd)-1C

y2fY (y)dy,

|y|>an

we conclude by Markov's inequality that |Vn(x)|  0 for each x  D. As to the tightness, we have

= O (log n)-1/2

I1
 4 2

1(y > an)

(-1)d-|| y - 0(s +   (t - s)) K

s +   (t - s) - u h

{0,1}d

f (y, u)dydu

 8 2 h-dCµ(B) 2

y2fY (y)dy + h-dCµ(B) 2

fY (y)dy

y>an

y>an

 8 2 h-dCµ(B) 2

y2fY (y)dy.

y>an

Hence,

E V (B)2 1/2  (log n)1/2h-3d/2C

1/2
y2fY (y)dy µ(B).
y>an

The desired result follows by similar arguments as those used to prove Lemma A.4.

35

Lemma A.12. If n-1/6h-d/2-3d/(b1-2) = O(n-),  > 0,

Y0,n - Y1,n = Op n-1/6h-d/2(log n)+(2d+4)/3an ,

for any  > 0. PROOF. With similar arguments as in Lemma A.5,

hd/2n1/6(log n)--(2d+4)/3a-n 1|Y0,n - Y1,n|

 O(1)

a-n 1 fX (x)n2 (x)

( - 1)(0(x) + an) +  (an - 0(x))

+  (an - 0(x)) + ( - 1)(an - 0(x))

K

x - ·2 h

Bx

dK((x - u)/h)
Bx

+ ( - 1)(0(x) + an) +  (an - 0(x))

2 K (x - ·2)/h (Bx)12-2

1=1,2{0,1}d-{12} (Bx)2

+  (an - 0(x)) + ( - 1)(an - 0(x))
1 =0,2 {0,1}d -{02 }

2 K (x - ·2)/h
(Bx )2

(Bx )12 -2

, a.s. (48)

by the assumption on the kernel K, (48) is almost surely bounded bounded.

hd/2n1/6(log n)--(2d+4)/3 = O(1)

by the choice of an given inLemma A.11.
Lemma A.13. Y1,n - Y2,n = Op hd/2 .
PROOF. Since Bn T (y, u) = Wn T (y, u) - F (y, u)Wn(1, ..., 1), we obtain by a change of variables and a first order approximation to f (y, x - hv):

Y1,n - Y2,n  2hd/2 K (v) dv
 2hd/2 K (v) dv

1 fX (x)n2(x)

(y - 0(x)) f (y, x)dy + O(h)
n

|W (1, ..., 1)|

1 fX (x)n2(x)

max{, 1 -  }

E

|Yi|

x

+ 0(x)

+ O(h)

|W (1, ..., 1)|.

Note that |W (1, ..., 1)| = Op(1), Yi has a finite second moment by assumption and 0 is uniformly bounded on D.

Lemma A.14. Y2,n - Y3,n = Op h1- , where 0 <  < 1.

PROOF. Note that the derivative of expectile loss function is 2 1(u  0) -  u , which
is Lipschitz continuous with Lipschitz constant 2 max{, 1 -  }. Define V (x) as in Lemma A.7.

E

V (x) h

2

=

1 hd+2fX (x)n2 (x)

{(y - 0(x)) - (y - 0(u))}2K2
n

x-u h

f (y, u)dydu



C0 max{, 1 -  }2 hd+2fX (x)n2 (x)

FY |X (an|u) - FY |X (-an|u) |x - u|2 K2

x-u h

fX (u)du



C2 h2fX (x)n2 (x)

K 2(z )|hz |2 fX (x)dz

+

O(h)



2C 2 n2 (x)

K

2 2

+

O(h),

36

E

V (t) - V (s) h

2



2C hd+2

(y - 0(t)) - (y - 0(s)) 2K2
n

s-u h

f (y, u)dydu+

2C hd+2

(y - 0(t)) - (y - 0(u)) 2 K
n

t-u h

-K

s-u h

2
f (y, u)dydu d=ef I1 + I2,

where

I1



C hd+2

t - s 2K2



C hd+2

s-t

2 

K2

s-u h
s-u h

fX (u)du

fX(u)du  C

s-t h2

2
 + O(1).

By a change of variables and a similar argument as used to bound I2 in Lemma A.7, we

obtain

I2  C

s-t h3

.

for s - t < 1. Following the lines of proof of Lemma A.7 or Lemma A.10 completes the

proof of the lemma.

Lemma A.15. Y3,n =d Y4,n

PROOF. The proof resembles the proof for Lemma A.9 and is omitted for brevity.

Lemma A.16. Y4,n - Y5,n = Op h1- , where 0 <  < 1.

PROOF. The proof resembles the proof for Lemma A.10 by using (47). The details are omitted for brevity.

A.4. Proof of Lemma 2.6

We first show assertion 1.). (11). Let F~|X(v|x) be defined as

n
F~|X(v|x) = n-1 G
i=1

v - i h0

Lh¯ (x - Xi)/f^X (x).

(49)

Since supxD |f^X(x) - fX (x)| = Op(h¯s + (nh¯d)-1/2 log n), linearisation yields

F~|X (v|x)

=

M~ (v, x) fX (x)

+

Rn,

where Rn = Op(h¯2 + (nh¯d)-1/2 log n) uniformly over x  D by assumption (B2), where

M~ (v, x) = F~|X(v|x)f^X (x) = n-1

n i=1

G

v-i h0

Lh¯ (x - Xi).

By Theorem 6.2.

(i) of

Li and Racine (2007), E M~ (v, x) - F,X(v, x) is of order O(h02 + dh¯2). It remains to show

that

sup sup M~ (v, x) - E M~ (v, x) = Op (nh¯d)-1/2 log n .
vI xD

(50)

37

By Theorem 6.2. (ii) of Li and Racine (2007), Var M~ (v, x) = O (nh¯d)-1 . By virtue of a standard n-net discretization argument and the Bernstein inequality we obtain (50).
Next we show that F^|X(v|x) - F~|X(v|x) = Op(h2 + (nhd)-1/2 log n). We have

F^|X(v|x) - F~|X(v|x)

=

1 nf^X (x)

n i=1

=

1 nf^X (x)

n i=1

G

v - i h0

-G

v - ^i h0

Lh¯(x - Xi)

h0-1g

v - i h0

(i - ^i)

Lh¯ (x - Xi) + R1,n,

where R1,n is of negligible order by (B1) under the claim in Section 3.3 of Muhsal and Neumeyer (2010). i - ^i = ^n(Xi) - 0(Xi), which is stochastically bounded with hs + (nhd)-1/2 log n, for arbitrary  > 0. Moreover, observe that

1 n

n

h-0 1g

i=1

v - i h0

Lh¯(x - Xi)

is a kernel density estimator which has standard bias and variance and which is is stochastically bounded. Hence, in order to shrink P F^|X(v|x) - F~|X(v|x) > n- , splitting
the probability of under the event ^n(Xi) - 0(Xi) > hs + (nhd)-1/2 log n and its complement, where n- = h20 + hs + h¯2 + (nh0h¯d)-1/2 log n + (nhd)-1/2 log n, we get the desired result.
Next we show assertion 2.). Let f~|X(v|x) be defined as

n
f~|X(v|x) = n-1 gh0 (v - i) Lh¯(x - Xi)/f^X(x).
i=1

(51)

By standard theory for kernel density estimation, we have

f~|X (v|x)

=

m~ (v, x) fX (x)

+

Rn,

where Rn = Op(h¯s + (nh¯d)-1/2 log n) uniformly over x  D by assumption (B2), where

m~ (v, x) = f~|X(v|x)f^X (x) = n-1

n i=1

gh0

(i

- v) Lh¯(x - Xi).

It

follows

from

the

standard

theory of density estimation that

m~ (v, x) - f,X(v, x) = Op(h20 + h¯2 + (nh0h¯d)-1/2 log n).

(52)

A Taylor expansion yields

f~|X(v|x) - f^|X(v|x)

=

1 nf^X (x)

n i=1

{gh0

(v

- i) - gh0

(v

- ^i)} Lh¯(x - Xi)

=

1 nf^X (x)

n i=1

h-0 2g

v - i h0

^n(Xi) - 0(Xi)

Lh¯ (x - Xi) + R2,n

it follows from Muhsal and Neumeyer (2010) that R2,n is negligible under condition (B1).

Again

1 n

n

h-0 2g

i=1

v - i h0

Lh¯ (x - Xi)

38

is a kernel estimator for the derivative of the conditional density function and is thus stochastically bounded. Applying the stochastic bound for ^n(Xi) - 0(Xi) and similar probability separating argument for proving 1.), assertion 2.) follows.
For the third estimator (13), define

n
~2(x) = n-1 2(i)Lh¯(x - Xi)/f^X (x).
i=1
Using a weak uniform consistency result for kernel regression, see, for instance, Hansen (2008), ~2(x)-2(x) = Op h¯2 +(nh¯d)-1/2 log n . Below we separately discuss the quantile and expectile case.
In the quantile case, (u) = 1(u < 0) -  , then

n

^2(x) - ~2(x) = n-1

2(^i) - 2(i) Lh¯(x - Xi)

i=1

n

= (1 - 2 )n-1

1(^i < 0) - 1(i < 0) Lh¯(x - Xi).

i=1

Note that 1(^i < 0) - 1(i < 0) = 1(0(Xi) < Yi < ^n(Xi)) - 1(^n(Xi) < Yi < 0(Xi)). Applying the fact that supxD |^n(x) - 0(x)| stochastically bounded, we first restrict our focus on the event ^n(Xi)-0(Xi) < hs+(nhd)-1/2 log n. If  = 1/2, then 2(^i)-2(i) = 0 and we are done. Given  = 1/2,

(1 - 2 )-1E ^2(x) - ~2(x) = E 1(^i < 0) - 1(i < 0) Lh¯(x - Xi)

= 2 F (^n(u)|u) - F (0(u)|u) Lh¯(x - u)fX(u)du

= 2 f ((u)|u)(^n(u) - 0(u))Lh¯(x - u)fX (u)du,

where (u) lies between ^n(u) and 0(u). By condition (B2), f (v|x) is uniformly bounded, we deduce that E ^2(x)-~2(x) = O(hs +(nhd)-1/2 log n). Observe that (1(^i < 0)-1(i <
0))2 = 1 [^n(Xi)  0(Xi), ^n(Xi)  0(Xi)] . It follows from similar computations

E ^2(x) - ~2(x) 2 = O hs + (nhd)-1/2 log n .

Again observe that 1 [^n(Xi)0(Xi), ^n(Xi)0(Xi)] is independent of the variable x, a discretization argument and the Bernstein inequality yield the result that n1· ^2(x)-~2(x)
is stochastically bounded.
For the expectile case, (u) = 2 1(u < 0) -  |u|. Since

2(i) - 2(^i) = 4 1(i < 0) -  2|i|2 - 4 1(^i < 0) -  2|^i|2 = 4 1(^i < 0) -  2 |i|2 - |^i|2 + 4 1(i < 0) - 1(^i < 0) 2|i|2,

Thus,

n

^2(x) - ~2(x) = 4n-1

1(^i < 0) -  |i|2 - |^i|2 Lh¯(x - Xi)

i=1

n

+ 4(1 - 2 )n-1

1(i < 0) - 1(^i < 0) |i|2Lh¯ (x - Xi)

i=1

d=ef 4R3,n(x) + 4(1 - 2 )R4,n(x).

39

Again, it is sufficient to focus on the set {|^n(Xi) - 0(Xi)| < n-0}, where n-0  hs + (nhd)-1/2 log n. For R3,n(x), notice that

|i|2 - |^i|2 = 0(Xi) - ^n(Xi) 0(Xi) + ^n(Xi) - 2Yi = R5,n(u) 20(u) + R5,n(u) - 2Yi ,

where supxD |R5,n(x)| = O(n-0), so

ER3,n(x) = E 1(^i < 0) -  0(Xi) - ^n(Xi) 0(Xi) + ^n(Xi) - 2Yi Lh¯(x - Xi)

= (1 -  )2 - 2

R5,n(u)(20(u) + R5,n(u) - 2y)Lh¯(x - u)fY |X(y|u)fX (u)dydu
y<^n (u)
R5,n(u)(20(u) + R5,n(u) - 2y)Lh¯(x - u)fY |X (y|u)fX (u)dydu.
y>^n(u)

Hence, |ER3,n(x)| < Cn-0 for some constant C.

Var R3,n(x)  n-1 max{(1 -  )2,  2}2  C(nh¯d)-1n-20 .

R52,n(u)(20(u) + R5,n(u) - 2y)2L2h¯ (x - u)fY |X (y|u)fX (u)dydu

One can apply discretization and the Bernstein inequality to show that supxD R3,n(x) = Op(n-0 log n).
For R4,n(x), again suppose without loss of generality that {|^n(Xi) - 0(Xi)| < n-0}, where n-0  hs + (nhd)-1/2 log n,

|E[R4,n(x)]|  2

|y - 0(u)|2fY |X(y|u)dy |Lh¯(x - u)|fX (u)du = O(n-20).
|y-0 (u)|<R6,n (u)

An application of Markov's inequality yields the desired result.

A.5. Proof of Theorem 3.1

Proof of Lemma 3.2. We will discuss the case of quantile and expectile regression separately. Consider first (u) = 1(u < 0) -  .

n
2(x) - ^2(x) = n-1
i=1

2(v)gh0(v - ^i) - 2(^i) Lh¯ (x - Xi)/f^X (x).

(53)

40

By a change of variables,

2(v)gh0(v - ^i)dv - 2(^i)  2(^i + wh0) - 2(^i) g(w)dw

 2 max{, 1 -  } |(^i + wh0) - (^i)| g(w)dw

-^i/h0



= C 1(^i > log(n) · h0)

g(w)dw + 1(^i < - log(n) · h0)

g(w)dw

-

-^i/h0

+ 1(|^i|  log(n) · h0) g(w)dw

R

- log(n)



 C 1(^i > log(n) · h0)
-

g(w)dw + 1(^i < - log(n) · h0)

g(w)dw

log(n)

+ 1(|^i|  log(n) · h0)

 C

- log(n)



g(w)dw +

g(w)dw + 1(|^i|  log(n) · h0) .

- log(n)

Hence, the sup norm of (53) is bounded by I1 + I2 + supx |I3(x)|, where I1 d=ef C G(- log n), I2 d=ef C 1 - G(log n) and

n
I3(x) d=ef n-1 1(|^i|  h0 log n) Lh¯(x - Xi) /|f^X(x)|,
i=1

since f^X (x) = n-1

n i=1

Lh¯

(x

-

X

i).

I1

and

I2

decay

polynomially

in

n

by

assumption

(A1).

Note that for any  > 0,

n
P sup (1 - E) 1(|^i|  h0 log n)|Lh¯(x - Xi)| > n(log n)-1 
x i=1

n
P sup (1 - E) 1(|^i|  h0 log n)|Lh¯(x - Xi)| > n(log n)-1, ^n(x) - 0(x)  En log n
x i=1

+ P ^n(x) - 0(x) > En log n ,

(54)

where En = hs + (nhd)-1/2 log n. The uniform convergence of ^n(x) to 0(x) yields that


P
n=1

^n(x) - 0(x) > En log n < .

(55)

For the first probability, it is easy to see that it is bounded by the sum

An + Bn d=ef P

sup
x

n
(1 - E) 1(|i|  h0 log n + En log n)|Lh¯ (x - Xi)
i=1

>

1 2

n(log

n)-1

n

+ 1 sup E

1(h0 log n - En log n < |i|  h0 log n + En log n)|Lh¯ (x - Xi)

x i=1

>

1 2

n(log

n)-1

.

41

After an explicit computation of the expectation, one concludes that Bn is equal to zero for any  > 0 if n is sufficiently large. Now we need to bound An. Note that for any fixed x, we can estimate the variance by

n
Var 1(|i|  h0 log n + En log n)|Lh¯(x - Xi)|  CLnh0h¯-d log n,
i=1

applying a concentration inequality, one gets for any  > 0,

n

P (1 - E) 1(|i|  h0 log n + En log n)|Lh¯(x - Xi)| > n(log n)-1

i=1

 2 exp

-

1 4

CL nh0 h¯ -d

n2(log log n +

n)-42 CLnh¯-d(log

n)-2

,

which decreases exponentially in n since nh¯d   polynomially in n by assumption (B3).
By a discretization argument, one can show that An is also summable (the grid size grows polynomially in n). Hence, we conclude that the probability (54) is summable. The stochastic part of the numerator of I3(x) is therefore of Op((log n)-1) a.s. by an application of the Borel-Cantelli lemma.
The mean of the numerator of I3(x) can be estimated by the law of iterative expectation:

EE =E

n
n-1 1(|^i|  h0 log n) Lh¯ (x - Xi) X, ^n(x) - 0(x)
i=1
^n(x)-0(x)+h0 log n
f e|X, ^n(x) - 0(x) de|Lh¯(x - Xi)|
^n(x)-0(x)-h0 log n

 2h0 log nC = O((log n)-1),

since the density f e|X, ^n(x) - 0(x) is bounded and L  L1(Rd). Finally, applying a linearization argument we obtain that I3(x) = Op((log n)-1) = O((log n)-1/2) a.s.
In the case of expectile regression, we need to consider (u) = 2(1(u < 0) -  )|u|,
which is Lipschitz continuous (see Lemma A.14). Note that |^i|  |i| + En, where En = O(hs + (nhd)-1/2 log n) a.s. by the Bahadur representation of n, a discretization argument and an application of the Bernstein inequality. Hence,

n

n-1 2(v)gh0(v - ^i) - 2(^i)dv Lh¯(x - Xi)

i=1

n

 n-1C

h0(2|^i| + h0|w|)|w|g(w)dw|Lh¯(x - Xi)|

i=1

n

= C h02 |w|2g(w)dw + C,g2h0n-1 |^i||Lh¯(x - Xi)|

i=1

nn

 C h20 |w|2g(w)dw + 2C,gh0Enn-1 |Lh¯(x - Xi)| + 2C,gh0n-1 |i||Lh¯(x - Xi)|.

i=1 i=1

The first term converges almost surely to 0, faster than (log n)-1, based on assumption (B3). The second term and the third term can be handled by similar argument for showing the uniform almost sure convergence of the Nadaraya-Watson estimator, see Hansen (2008) for more details.

42

Our strategy is to follow the sequence of approximation steps that are similar to Section A.2 and A.3. Define

Y0,n(x) =

1 hd f^X (x)n2,(x)

K
n

x-u h

 (v)dZn(v, u),

(56)

where n2,(x) = E  (i )21(|i | < bn)|x , and n = {v : |v|  bn}.

Y1,n(x) =

1 hd f^X (x)n2,(x)

K
n

x-u h

 (v)dBn T^(v, u) ,

(57)

where Bn T^(v, u) = Wn T^(v, u) - F^(v, u)Wn(1, ..., 1), W  is a Brownian motion defined conditional on the sample, and T^(v, u) is the Rosenblatt transformation:

T^(v, u) = F^X1|(u1|v), F^X2|(u2|u1, v), ..., F^Xd|Xd-1,...,X1,(ud|ud-1, ..., u1, v), F^(v) , given F^X1|(u1|v), F^X2|(u2|u1, v), ..., F^Xd|Xd-1,...,X1,(ud|ud-1, ..., u1, v), F^(v) are associated cdfs obtained from integrating f^,X(v, u).

Y2,n(x) =

1 hd f^X (x)n2,(x)

K
n

x-u h

 (v)dWn T^(v, u) ,

(58)

Y4,n(x) =

1 hd f^X (x)n2,(x)

f^X (u)n2,(u)K

x-u h

dWn u ,

(59)

Y5,n(x)

=

1 hd

K

x-u h

dWn u .

(60)

From (56) to (57) the proof resembles Lemma A.4 for quantile regression and A.11 for expectile regression. For the bootstrap version of these proofs to hold, it is sufficient to verify the conditions

(log n)h-3d

f^(v)dv = O(1), a.s.

|v|>cn

for quantile regression and

(61)

(log n)h-3d

v2f^(v)dv = O(1), a.s.

|v|>cn

(62)

for expectile regression, where f^(v) = (nh0)-1

n i=1

g((v

-

^i)/h0).

The rest follows from

similar arguments in Lemma A.4 and A.11.

We will only consider the kernel g with compact support; in particular, with support

[-1, 1]. Via standard arguments one could generalize the result here immediately to, e.g.,

the Gaussian kernel.

43

Let n = (log n)-1h3d. Let En = hs + (nhd)-1/2 log n.

f^(v)dv
|v|>cn

=

1 nh0

n i=1

g
|v|>cn

^i - v h0

dv



1 nh0

Cg

n i=1

1(|^i - v|  h0)dv
|v|>cn



1 nh0

Cg

n i=1

1(|v| - |^i|  h0)1(^i - h0  v  ^i + h0)dv
|v|>cn



1 nh0

Cg

n i=1

1(cn

-

h0



|^i|)

1(^i - h0  v  ^i + h0)dv
|v|>cn



2 n

Cg

n

1(cn - h0  |^i|)

i=1

(63)

where Cg is a constant depending on g. For any  > 0 and a constant  > 0 small such that Enn  0 as n  , consider

n
P (1 - E)n-1 1(cn - h0  |^i|) > 2n 
i=1 n
P (1 - E)n-1 1(cn - h0  |^i|) > 2n, ^n(x) - 0(x)
i=1
d=ef P1,n + P2,n.

 Enn

+P

^n(x) - 0(x) > Enn

P2,n is summable by similar argument in the proof of Lemma 3.2. Without loss of generality, we assume cn is large enough so that h0 + Enn < cn/2 since h0, Enn  0. Thus,

Let Sn =

P1,n  P

n
(1 - E)n-1 1(cn/2  |i|) > 2n .
i=1

n i=1

1(cn/2



|i|).

From

(30)

in

assumption

(C2),

Var(Sn) = n

f(v)dv = O(n2(log n)-3h6d) = O(n2(log n)-1n2).

|v|>cn/2

This yields



P (|Sn| > 2nn)  2 exp

n=1

n=1

-

4

4n22n2 Var(Sn) + 8nn


= 2 exp
n=1

-

1

+

2 log n 2 log2 n/(nh3d

)

< ,

(64)

given that  > 1, since nh3d(log n)-2   by assumption (A7). It follows by the BorelCantelli lemma that the stochastic part of (63) is of Op(n). For the expectation, we note that

1(cn - h0  |^i|)  1(cn - h0  |i| + ^n(x) - 0(x) )

 1(cn - h0 - Enn  |i|)1( ^n(x) - 0(x)  Enn)

+ 1(cn - h0  |i| + ^n(x) - 0(x) )1( ^n(x) - 0(x) > Enn)

 1(cn/2  |i|) + 1( ^n(x) - 0(x) > Enn).

(65)

44

Therefore,

n
E n-1 1(cn - h0  |^i|)  E [1(cn/2  |i|)] + P
i=1

^n(x) - 0(x) > Enn

= f(v)dv + O(e-nµ1 ) = O((log n)-3nh6d),
|v|>cn/2

for some µ1 > 0. Next we show (62). The sequence cn will be chosen appropriately later,

v>cn

v2f^(v)dv



1 nh0

Cg

n i=1

1(cn

- h0



|^i|)

v21(|v|  h0 + |^i|)dv
|v|>cn



1 nh0

Cg

n i=1

1(cn

- h0



|^i|)(2h0^2i

+ 2h30)



2 n

Cg

n

^2i 1(cn

-

h0



|^i|)

+

2h20 n

Cg

n

1(cn - h0  |^i|)

i=1 i=1

T1,n



4 n

Cg

n i=1

2i 1(cn

-

h0



|^i|)

+

4 n

Cg

n i=1

^n(Xi) - 0(Xi) 21(cn - h0  ^i) +T1,n

= T3,n + T2,n + T1,n.

T2,n
(66)

Choosing cn  (n4/b-1(log n)1+8/bn-2)1/(b-2). Note cn > ((log n)3(nh6d)-1)1/b, and therefore (30) holds naturally in this case, by assumption (EC1),

f(v)dv 
|v|>cn

|v|>cn

|v|b |cn|b

f(v)dv

=

O(cbn)

=

O(n4/b-1(log n)1+8/bn-2).

It can be shown via similar arguments for showing (61) that Ti,n = Op((log n)-1h3d) a.s. for i = 1, 2.
To bound T3,n, given b from (EC1), we choose Mn = n1/b(log n)2/b and obtain

P {|(1 - E)T3,n| > 2n}  P(|(1 - E)Sn | > 2nn, i < Mn, i) + nP(|i|  Mn) + P ^n(x) - 0(x) > Enn

d=ef U1,n + U2,n + U3,n,

where Sn = Cg

n i=1

i21(cn/2



|i|),

the

term

U2,n

is

of

order

O(Mn-b)

by

(EC1)

and

hence

summable. U3,n is summable by a similar argument as used in the proof of (61). Restricting

Sn to the set in=1{|i| < Mn}, we find



Var(Sn )  Mn4nCg2

f(v)dv  Cg,bMn4ncn-b = O(n2(log n)-1n2).

cn/2

This yields



U1,n  2 exp

n=1

n=1

-

4

4n22n2 Var(Sn ) + 8nn


= 2 exp
n=1

-

1

+

2 log n 2 log2 n/(nh3d

)

< , (67)

45

given that  > 1 and assumption (EA2). It follows by the Borel-Cantelli lemma that (1 - E)T3,n = O(n) a.s. It left to control the expectation. By computation in (65),

1(cn - h0  |^i|)  1(cn - h0  1(cn/2  |i|) + 1( ^n(x) - 0(x) > Enn).

Thus, by law of iterative expectation,

E[T3,n]  E i1(cn/2  |i|) + E = O(c2n-b) + O(e-nµ2 ).

n
n-1 iP
i=1

^n(x) - 0(x) > Enn

It follows immediately by the order of cn that E[T3,n] = O(n). In order to show the almost sure uniform convergence of Y4,n(x) to Y5,n(x) we need to
verify that for quantile regression

h-d log n sup

f^|X(v|x)dv = O(1), a.s.

xD |v|>cn

and for expectile regression

(68)

h-d log n sup

v2f^|X(v|x)dv = O(1). a.s.

xD |v|>cn

(69)

The first condition can be shown in the same way as showing (61), and the second one is
similar to (62) given b  4. A discretization argument is needed in both cases, but the grid
size only grows in polynomial rate in n. The proofs are omitted for brevity.
Using analogous arguments as in Lemma A.4 for quantile regression and A.11 for expectile regression with (61) and (62), it can be shown that Yn(x) converges uniformly in probability to Y0,n(x). The almost sure uniform convergence in probability of Y0,n(x) to Y5,n(x) follows by similar arguments in Lemma A.5, A.6, A.9 and A.10 for quantile regression and Lemma A.12, A.13, A.15 and A.16 for expectile regression, except that fX(x), n2(x), F (y, x) are replaced by f^X(x), 2,n(x), F^(v, x) respectively, and that the approximation shown in Lemma A.7 and A.14 is not needed here. Finally, the proof of Theorem 3.1 is completed by an application of the extreme value theorem of Rosenblatt (1976) to Y5,n(x).

B. Supporting Lemmas

Lemma B.1 (Kong et al. (2010)). Under (A1),(A3)-(A5), for some s  0, and D is an compact subset of Rd. Then

sup Hn ^(x) - (x) - n(x) = O
xD

log n nhd

(s)

.

(70)

where

n(x)

=

-

1 nhd

SK-,1g,f Hn-1

n
Kh(Xi - xi)(i)
i=1

(1, Xi - x);

(71) (72)

46

 is the piecewise derivative of , and

(s) = min

2

2 +

s,

6 8

+ +

2s 4s

.

(73)

Note that under the i.i.d. case, the constant s, which controls the weak dependence, is 0.

Lemma B.2 (Bickel and Wichura (1971): Tightness of processes on a multidimensional cube). If {Xn}n=1 is a sequence in D[0, 1]d, P(X  [0, 1]d) = 1. For neighboring blocks B, C in [0, 1]d (see Definition A.2) constants 1 + 2 > 1, 1 + 2 > 0, {Xn}n=1 is tight if

E[|Xn(B)|1|Xn(C)|2]  µ(B)1µ(C)2,

(74)

where µ(·) is a finite nonnegative measure on [0, 1]d (for example, Lebesgue measure), where the increment of Xn on the block B is defined by

Xn(B) =

(-1)d-||Xn s +   (t - s) .

{0,1}d

Lemma B.3 (Meerschaert, M. M., Wang, W. and Xiao, Y. (2013)). Suppose that Y = {Y (t), t  Rd} is a centered Gaussian random field with values in R, and denote

d(s, t) d=ef dY (s, t) = E|Y (t) - Y (s)|2 1/2, s, t  Rd.

Let D be a compact set contained in a cube with length r in Rd and let 2 = suptD E[Y (t)2]. For any m > 0,  > 0, define
() = sup d(s, t)
s,tD, s-t 

and

 Q(m) = (2 + 2)


 (m2-y2 )dy .

1

Then for all a > 0 which satisfy a  (1 + 4d log 2)1/2( + a-1),

P

sup |Y (t)| > a
tS

 22d+2

r Q-1(1/a)

+

1

d  + a-1 exp a

-

2(

a2 + a-1)2

,

where Q-1(a) = sup{m : Q(m)  a}.

(75)

47

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018

"Principal Component Analysis in an Asymmetric Norm" by Ngoc Mai Tran, Maria Osipenko and Wolfgang Karl Härdle, January 2014. "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data" by Lijie Gu, Li Wang, Wolfgang Karl Härdle and Lijian Yang, January 2014. "An Extended Single Index Model with Missing Response at Random" by Qihua Wang, Tao Zhang, Wolfgang Karl Härdle, January 2014. "Structural Vector Autoregressive Analysis in a Data Rich Environment: A Survey" by Helmut Lütkepohl, January 2014. "Functional stable limit theorems for efficient spectral covolatility estimators" by Randolf Altmeyer and Markus Bibinger, January 2014. "A consistent two-factor model for pricing temperature derivatives" by Andreas Groll, Brenda López-Cabrera and Thilo Meyer-Brandis, January 2014. "Confidence Bands for Impulse Responses: Bonferroni versus Wald" by Helmut Lütkepohl, Anna Staszewska-Bystrova and Peter Winker, January 2014. "Simultaneous Confidence Corridors and Variable Selection for Generalized Additive Models" by Shuzhuan Zheng, Rong Liu, Lijian Yang and Wolfgang Karl Härdle, January 2014. "Structural Vector Autoregressions: Checking Identifying Long-run Restrictions via Heteroskedasticity" by Helmut Lütkepohl and Anton Velinov, January 2014. "Efficient Iterative Maximum Likelihood Estimation of HighParameterized Time Series Models" by Nikolaus Hautsch, Ostap Okhrin and Alexander Ristig, January 2014. "Fiscal Devaluation in a Monetary Union" by Philipp Engler, Giovanni Ganelli, Juha Tervala and Simon Voigts, January 2014. "Nonparametric Estimates for Conditional Quantiles of Time Series" by Jürgen Franke, Peter Mwita and Weining Wang, January 2014. "Product Market Deregulation and Employment Outcomes: Evidence from the German Retail Sector" by Charlotte Senftleben-König, January 2014. "Estimation procedures for exchangeable Marshall copulas with hydrological application" by Fabrizio Durante and Ostap Okhrin, January 2014. "Ladislaus von Bortkiewicz - statistician, economist, and a European intellectual" by Wolfgang Karl Härdle and Annette B. Vogt, February 2014. "An Application of Principal Component Analysis on Multivariate TimeStationary Spatio-Temporal Data" by Stephan Stahlschmidt, Wolfgang Karl Härdle and Helmut Thome, February 2014. "The composition of government spending and the multiplier at the Zero Lower Bound" by Julien Albertini, Arthur Poirier and Jordan RoulleauPasdeloup, February 2014. "Interacting Product and Labor Market Regulation and the Impact of Immigration on Native Wages" by Susanne Prantl and Alexandra SpitzOener, February 2014.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasrecahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
019 "Unemployment benefits extensions at the zero lower bound on nominal interest rate" by Julien Albertini and Arthur Poirier, February 2014.
020 "Modelling spatio-temporal variability of temperature" by Xiaofeng Cao, Ostap Okhrin, Martin Odening and Matthias Ritter, February 2014.
021 "Do Maternal Health Problems Influence Child's Worrying Status? Evidence from British Cohort Study" by Xianhua Dai, Wolfgang Karl Härdle and Keming Yu, February 2014.
022 "Nonparametric Test for a Constant Beta over a Fixed Time Interval" by Markus Reiß, Viktor Todorov and George Tauchen, February 2014.
023 "Inflation Expectations Spillovers between the United States and Euro Area" by Aleksei Netsunajev and Lars Winkelmann, March 2014.
024 "Peer Effects and Students' Self-Control" by Berno Buechel, Lydia Mechtenberg and Julia Petersen, April 2014.
025 "Is there a demand for multi-year crop insurance?" by Maria Osipenko, Zhiwei Shen and Martin Odening, April 2014.
026 "Credit Risk Calibration based on CDS Spreads" by Shih-Kang Chao, Wolfgang Karl Härdle and Hien Pham-Thu, May 2014.
027 "Stale Forward Guidance" by Gunda-Alexandra Detmers and Dieter Nautz, May 2014.
028 "Confidence Corridors for Multivariate Generalized Quantile Regression" by Shih-Kang Chao, Katharina Proksch, Holger Dette and Wolfgang Härdle, May 2014.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

