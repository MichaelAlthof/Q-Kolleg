BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2005-030
The Shannon Information of Filtrations and the Additional Logarithmic Utility of Insiders
Stefan Ankirchner* Peter Imkeller*
Steffen Dereich**
* Department of Mathematics, Humboldt-Universit‰t zu Berlin, Germany
** Department of Mathematics, Technische Universit‰t Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

The Shannon information of filtrations and the additional logarithmic utility of insiders 

Stefan Ankirchner Institut fu®r Mathematik Humboldt-Universit®at zu Berlin
Unter den Linden 6 10099 Berlin Germany

Steffen Dereich Fachbereich Mathematik Technische Universita®t Berlin Strasse des 17. Juni 136
10623 Berlin Germany

Peter Imkeller Institut fu®r Mathematik Humboldt-Universita®t zu Berlin
Unter den Linden 6 10099 Berlin Germany

May 27, 2005

Abstract The background for the general mathematical link between utility and information theory investigated in this paper is a simple financial market model with two kinds of small traders: less informed traders and insiders, whose extra information is represented by an enlargement of the other agents' filtration. The expected logarithmic utility increment, i.e. the difference of the insider's and the less informed trader's expected logarithmic utility is described in terms of the information drift, i.e. the drift one has to eliminate in order to perceive the price dynamics as a martingale from the insider's perspective. On the one hand, we describe the information drift in a very general setting by natural quantities expressing the probabilistic better informed view of the world. This on the other hand allows us to identify the additional utility by entropy related quantities known from information theory. In particular, in a complete market in which the insider has some fixed additional information during the entire trading interval, its utility increment can be represented by the Shannon information of his extra knowledge. For general markets, and in some particular examples, we provide estimates of maximal utility by information inequalities.
2000 AMS subject classifications: primary 60H30, 94A17 ; secondary 91B16, 60G44. Key words and phrases: enlargement of filtration; logarithmic utility; utility maximization; heterogeneous information; insider model; Shannon information; information difference; entropy; differential entropy.
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk"
1

Introduction
A simple mathematical model of two agents on a financial markets taking their portfolio decisions on the basis of different information horizons has attracted much attention in recent years. Both agents are small, and unable to influence the price dynamics of the risky assets constituting the market. One agent just acts on the basis of the evolution of the market, the other one, the insider, possesses some additional knowledge at every instant of the continuous trading interval. This basic fact is modelled by associating two different filtrations with each agent, from which they make their portfolio decisions: the less informed agent, at time t, just has the -field Ft, corresponding to the natural evolution of the market up to this time, at his disposal for deciding about future investments, while the insider is able to make better decisions, taking his knowledge from a bigger -field Gt  Ft. We give a short selection of some among many more papers dealing with this model, just indicating the most important mathematical techniques used for its investigation. Methods are focused on martingale and stochastic control theory, and techniques of enlargement of filtrations (see Yor , Jeulin , Jacod in [JY85]), starting with the conceptual paper by Duffie, Huang [DH86], mostly in the initial enlargement setting, i.e. the insider gets some fixed extra information at the beginning of the trading interval. The model is successively studied on stochastic bases with increasing complexity: e.g. Karatzas, Pikovsky [PK96] on Wiener space, Grorud, Pontier [GP98] allow Poissonian noise, Biagini and Oksendal [BO03] employ anticipative calculus techniques. In the same setting, Amendinger, Becherer and Schweizer [ABS03] calculate the value of insider information from the perspective of specific utilities. Baudoin [Bau01] introduces the concept of weak additional information consisting in the knowledge of the law of some random element. Campi [Cam03] considers hedging techniques for insiders in the incomplete market setting. It is clear that the expected utility the insider is able to gain from final wealth in this simple model will be bigger than the uninformed traders' utility, for every utility function. And in fact many of the quoted papers deal with the calculation of a better informed agent's additional utility.
In Amendinger et al. [AIS98], in the setting of initial enlargements and logarithmic utility, a crucial and natural link between the additional expected logarithmic utility and information theoretic concepts was made. The insider's logarithmic utility advantage is identified with the Shannon entropy of the additional information. In the same setting, Gasbarra, Valkeila [GV03] extended this link by interpreting the logarithmic utility increment by the Kullback-Leibler information of the insider's additional knowledge from the perspective of Bayesian modelling. In the environment of this utility-information paradigm the papers [Imk96], [IPW01], [Imk02], [Imk03], Corcuera et al. [CIKHN03], and Ankirchner et al. [AI05] describe additional utility, treat arbitrage questions and their interpretation in information theoretic terms in increasingly complex models of the same base structure, including some simple examples of progressive enlargements. It is clear that utility concepts different from the logarithmic one correspond on the information theoretic side to the generalized entropy concepts of f -divergences.
In this paper we shall continue the investigation of mathematical questions related to the link between utility and information theory in the most general setting of enlargements of filtrations: besides assuming eventually that the base space be standard, to ensure the
2

existence of regular conditional probabilities, we shall let the filtration of the better informed agent just contain the one of the natural evolution of knowledge. To concentrate on one kind of entropy in this general setting, we shall consider logarithmic utility throughout. In this framework, Ankirchner et al. [AI05] calculate the maximal expected utility of an agent from the intrinsic point of view of his (general) filtration, and relate the finiteness of expected utility via the (NFLVR) condition to the characterization of semimartingales by the theorem of Dellacherie-Meyer-Mokobodski. The compensator in the Doob-Meyer decomposition of underlying asset price processes with respect to the agent's filtration is determined by the information drift process. In this paper we shall give a general analysis of the nature of this process, and relate it to measuring the difference of the information residing in the two filtrations, independently of the particular price dynamics. The basic observation we start with in Section 2 identifies the information drift process with Radon-Nikodym densities of the stochastic kernel in an integral representation of the conditional probability process and the conditional probability process itself. This observation allows for an identification of the additional utility by the information difference of the two filtrations in terms of Shannon entropy notions in Section 5, again independent of particular price dynamics of the financial market.
The paper is organized as follows. In the preparatory Section 1, we recall the main results about the connection between finite utility filtrations, properties of the price dynamics from the perspective of different agents, and properties of the information drift from Ankirchner et al. [AI05]. In Section 2 (Theorems 2.6 and 2.10) properties of the conditional probability processes with respect to the agents' filtrations and the information drift process are investigated in depth, and lead to the identification of the information drift by subjective conditional probability quantities. The description of the additional utility in terms of entropy notions is more easily obtained, if the additional information in the bigger filtration comes in discrete bits along a sequence of partitions of the trading interval, leading to stepwise "initial enlargements" which ultimately converge to the big filtration as the mesh of the partitions shrinks to 0. This is done in Section 5 (Theorem 5.8), after being prepared in Sections 3 and 4 by a general investigation of the convergence properties of information drifts going along with the convergence of such discretized enlargements to the big filtration. In the final Section 6, general facts known from Shannon information theory (see Ihara [Iha93]) are applied to estimate the expected maximal logarithmic utility of a better informed agent via the identification theorem of Section 5, in several particular cases. Entropy maximizing properties of Gaussian random variables play an important role.
1 Preliminaries
In this preparatory section we define the financial market model and recall some basic facts about expected utility maximization. Our favorite utility function will be the logarithmic one, for which we will then compare the maximal expected utilities of agents on the market who act on the background of asymmetric information. Recalling a result from [AI05], we will describe the utility increment of a better informed agent by the respective information drift of the agents' filtrations.
3

Let (, F , P ) be a probability space with a filtration (Ft)0tT , where T > 0 is a fixed time horizon. We consider a financial market with one non-risky asset of interest rate normalized to 0, and one risky asset with price St at time t  [0, T ]. We assume that S is a continuous (Ft)-semimartingale with values in R and write A for the set of all S-integrable and (Ft)-predictable processes such that 0 = 0. If   A, then we denote by ( ∑ S) the usual stochastic integral process. For all x > 0 we interpret

x + ( ∑ S)t, 0  t  T,

as the wealth process of a trader possessing an initial wealth x and choosing the investment strategy  on the basis of his knowledge horizon corresponding to the filtration (Ft). Throughout this paper we will suppose the preferences of the agents to be described by the logarithmic utility function. Furthermore we suppose that the traders' total wealth has always to be strictly positive, i.e. for all t  [0, T ]

x + ( ∑ S)t > 0 a.s.

(1)

Strategies  satisfying equation (1) will be called x-superadmissible. The agents want to maximize their expected logarithmic utility from their wealth at time T . So we are interested in the exact value of

u(x) = sup{E log(x + ( ∑ S)T ) :   A x - superadmissible}.

Sometimes we will write uF (x), in order to stress the underlying filtration. The expected

logarithmic utility of the agent can be calculated easily, if one has a semimartingale decom-

position of the form

t
St = Mt + s d M, M s,
0

(2)

where  is a predictable process. Such a decomposition is given for a large class of semi-

martingales. For example, if S satisfies the property (NFLVR), then it may be decomposed

as in equation (2) (see [DS95]). As is shown in a forthcoming PhD thesis [Ank05], finiteness

of u(x) implies already such a decomposition to exist. Hence a decomposition as in (2) may

be given even in cases where arbitrage exists. We state Theorem 2.9 of [AI05].

Proposition 1.1. Suppose S can be decomposed into S = M +  ∑ M, M . Then for any x > 0 the following equation holds

1 u(x) = log x + E
2

T
s2 d M, M s.
0

This proposition motivates the following definition.

(3)

Definition 1.2. A filtration (Gt) is called finite utility filtration for S, if S is a (Gt)-semi-

martingale with decomposition dS = dM +  ∑ d M, M , where  is (Gt)-predictable and

belongs to L2(M ), i.e. E

T 0

2

d

M, M

< . We write

F = {(Ht)  (Ft) (Ht) is a finite utility filtration for S}.

4

We now compare two traders who take their portfolio decisions not on the basis of the same

filtration, but on the basis of different information flows represented by the filtrations (Gt)

and (Ht) respectively. Suppose that both filtrations (Gt) and (Ht) are finite utility filtrations.

We denote by

S = M +  ∑ M, M

(4)

the semimartingale decomposition with respect to (Gt) and by

S = N +  ∑ N, N

(5)

the decomposition with respect to (Ht). Obviously, M, M = S, S = N, N

and therefore the utility difference is equal to

1

uH(x)

-

uG (x)

=

E 2

T
(2 - 2) d M, M .
0

Furthermore, the equations (4) and (5) imply

M = N - ( - ) ∑ M, M a.s.

(6)

If Gt  Ht for all t  0, equation (6) can be interpreted as the semimartingale decomposition of M with respect to (Ht). In this case one can show that the utility difference depends only on the process µ =  - . We therefore use the following notion.
Definition 1.3. Let (Gt) be a finite utility filtration and S = M + ∑ M, M the Doob-Meyer decomposition of S with respect to (Gt). Suppose that (Ht) is a filtration such that Gt  Ht for all t  [0, T ]. The (Ht)-adapted measurable process µ satisfying
∑
M - µt d M, M t is a (Ht) - local martingale
0
is called information drift (see [Imk03]) of (Ht) with respect to (Gt).

The following proposition relates the information drift to the expected logarithmic utility increment.

Proposition 1.4. Let (Gt) and (Ht) be two finite utility filtrations such that Gt  Ht for all t  [0, T ]. If µ is the information drift of (Ht) w.r.t. (Gt), then we have

uH(x)

-

uG (x)

=

1 E
2

T
µ2 d M, M .
0

Proof. See Theorem 2.13 in [AI05].
So far we only required the information drift to be measurable and adapted. Due to the continuity of S we have the following.

Proposition 1.5. The information drift, provided it exists, may be chosen to be predictable.

5

Proof. Suppose µ is a measurable and (Gt)-adapted process such that

∑
M - µt d M, M t
0

is a (Gt)-local martingale. We denote by pµ the predictable projection of µ with respect to (Gt). We will show that M -p µ ∑ M, M remains a (Gt)-local martingale.
Let  be stopping time localizing M such that M  , the martingale M stopped at  , is bounded. To simplify notation we assume M  = M . Let 0  s < t, A  Gs and  > 0. Then

t

E(1A(Mt - Ms+)) = E 1A

µr d M, M r

s+

t

= E 1AE

µr d M, M r Gs

s+

t

= E 1AE

pµr d M, M r Gs

s+

t

= E 1A

pµr d M, M r

s+

(see Theorem 57, Chapter VI in [DM78]). By dominated convergence the left hand side of this equation converges to E(1A(Mt - Ms)) as   0. The right hand side converges by similar arguments. Hence we obtain
t
E(1A(Mt - Ms)) = E 1A pµr M, M r ,
s
which means that M - pµ ∑ M, M is a (Gt)-martingale.
We close this section by recalling some basic properties of information drifts.
Lemma 1.6. Suppose the filtration (Ft) is a finite utility filtration with respect to which the Doob-Meyer decomposition of S is given by S = M +  ∑ M, M . Let (Ht) be a filtration satisfying Ft  Ht for all t  [0, T ] and suppose that (Ht) has an information drift µ with respect to (Ft). Then the following properties hold true.
i) If µ belongs to L2(M ), then the maximal expected utility uH(x) is finite for all x > 0.
ii) The set of finite utility filtrations F is equal to the set of all filtrations containing (Ft) and possessing an information drift  with respect to (Ft) such that   L2(M ).
iii) If (Ht) is a finite utility filtration, then µ is orthogonal to LF2 (M ), the subspace of (Ft)predictable processes in L2(M ).
iv) If (Gt) is a filtration such that Ft  Gt  Ht for all t  [0, T ], then there is also an information drift  of (Gt) with respect to (Ft). More precisely,  is equal to the L2(M )-projection of µ onto the subspace of the (Gt)-predictable processes.
6

Proof. Properties i) and ii) are obvious. For property iii) let S = N +  ∑ N, N denote the Doob-Meyer decomposition of S relative to (Ht), and let   LF2 (M ). Since  is adapted to both (Ft) and (Ht), the integrals ( ∑ M ) and ( ∑ N ) are square integrable martingales
with expectation zero. Therefore,

T
E µ d M, M
0

=E
=E = 0.

TT
 d M, M -  d M, M
00
TT
 dM -  dN
00

Thus, µ is orthogonal to LF2 (M ). For property iv) we refer again to [AI05].

2 General enlargements
Assume again that the price process S is a semimartingale of the form
S = M +  ∑ M, M
with respect to a finite utility filtration (Ft). Moreover, let (Gt) be a filtration such that Ft  Gt, and let  be the information drift of (Gt) relative to (Ft). And for simplicity of notation suppose in this section that time horizon is infinite, i.e. T = . We shall aim at describing the relative information drift  by basic quantities related to the conditional probabilities of the larger -algebras Gt with respect to the smaller ones Ft, t  0. Roughly, modulo some tedious technical details to be specified below, the relationship is as follows. Suppose for all t  0 there is a regular conditional probability Pt(∑, ∑) of F given Ft, which can be decomposed into a martingale component orthogonal to M , plus a component possessing a stochastic integral representation with respect to M with a kernel function kt(∑, ∑). Then we shall see that, provided  is square integrable with respect to d M, M  P , the kernel function at t will be a signed measure in its set variable. Moreover, this measure is absolutely continuous with respect to the conditional probability, if restricted to Gt, and  coincides with their Radon-Nikodym density.
We shall even be able to show that this relationship also makes sense in the reverse direction. Roughly, if absolute continuity of the stochastic integral kernel with respect to the conditional probabilities holds, and the Radon-Nikodym density is square integrable, the latter will turn out to provide an information drift  in a Doob-Meyer decomposition of S in the larger filtration.
We shall finish the section with an illustration of this fundamental relationship by discussing some simple examples of particularly enlarged filtrations.
The discussion of the details of this fundamental relationship requires some care with the complexity of the underlying filtrations and state spaces. Of course, the need to work with conditional probabilities first of all confines us to spaces on which they exist. Let therefore (, F , P ) be a standard Borel probability space (see [Par77]) with a filtration (Ft0)t0 consisting of countably generated -algebras, and M a (Ft0)-local martingale. We will also deal
7

with the smallest right-continuous and completed filtration containing (Ft0), which we denote
by (Ft). We suppose that F0 is trivial and that every (Ft)-local martingale has a continuous modification. Since Ft0 is a subfield of a standard Borel space, there exist regular conditional probabilities Pt relative to the -algebras Ft0. Then for any set A  F the process

(t, )  Pt(, A)

is an (Ft0)-martingale with a continuous modification (see e.g. Theorem 4, Chapter VI in [DM78]). Note that the modification may not be adapted to (Ft0), but only to (Ft).
Furthermore it is no problem to assume that the processes Pt(∑, A) are modified in a way

such that Pt(, ∑) remains a measure on F for PM -almost all (, t), where PM is a measure

on  ◊ R+ defined by PM () = E

 0

1

(,

t)d

M, M

t,



F

 B+.

It is known that each of these martingales may be uniquely written (see e.g. [RY99],

Chapter V)

t
Pt(∑, A) = P (A) + ks(∑, A)dMs + LtA,
0

(7)

where k(∑, A) is (Ft)-predictable and LA satisfies LA, M = 0.

Now let (Gt0) be another filtration on (, F , P ) satisfying

Ft0  Gt0

for all 0  t  T . We assume that each -field Gt0 is generated by a countable number of sets, and denote by (Gt) the smallest right-continuous and completed filtration containing (Gt0). It is clear that each -field in the left-continuous filtration (Gt0-) is also generated by a countable number of sets. We claim that the existence of an information drift of (Gt) relative
to (Ft) for the process M depends on whether the following condition is satisfied or not.

Condition 2.1. kt(, ∑) Gt0- is a signed measure and satisfies

kt(, ∑)
Gt0-

Pt(, ∑)
Gt0-

for PM -a.a (, t).

Remark 2.2. Unfortunately, we have to distinguish between the filtrations (Ft0), (Gt0) and their extensions (Ft), (Gt). The reason is that the regular conditional probabilities considered exist only with respect to the smaller -fields. On the other hand, we use stochastic
integration techniques which were developed only under the assumption that the underly-
ing filtrations satisfy the usual conditions, and this necessitates working also with the larger
-fields.

Let us next state some essential properties of the Radon-Nikodym density process existing according to our condition.

Lemma 2.3. Suppose Condition 2.1 satisfied. Then there exists an (Ft  Gt)-predictable process  such that for PM -a.a. (, t)

t(, 

)=

dkt(, ∑) dPt(, ∑)

(
Gt0-

).

8

Remark 2.4. Note that t(, ∑) is Gt--measurable. This is due to the fact that the predictable -algebra does not change by taking the left-continuous version of the underlying filtration.

Proof. Let

tni

=

i 2n

for

all

n0

and

i  0.

We

denote

by

T

the

set

of

all

tin.

It

is

possible

to choose a family of finite partitions (Pi,n) such that

∑ for all t  T we have Gt0- = (Pi,n : i, n  0 s.t. tin = t), ∑ Pi,n  Pi+1,n,

∑ if i < j, n < m and i 2-n = j 2-m, then Pi,n  Pj,m.

We define for all n  0

tn(, 

)

=

i0

AP i,n

1]tni ,tin+1](t)1A(

) kt(, A) . Pt(, A)

Note that

kt(,A) Pt(,A)

is (Ft)-predictable and 1]tin,tni+1](t)1A( ) is (Gt)-predictable.

Hence the

product of both functions, defined as a function on 2 ◊ R+, is predictable with respect to

(Ft  Gt). It follows that each n, and thus

 = lim inf n
n

is (Ft  Gt)-predictable.
Now fix t  0. We claim that kt(, ∑) = ∑ t(,  )Pt(, d ), and hence that t(, ∑) is the density of kt(, ∑) with respect to Pt(, ∑), PM -a.s. For all n  0 let j = j(n) be the integer satisfying tnj < t  tjn+1 and denote by Qn the corresponding partition Pj,n. Observe that (Qn) is an increasing sequence of partitions satisfying

(Qn : n  0) = Gt0-

and hence

t(, 

)

=

lim inf
n

tn(,



)

=

lim inf
n

1A(

AQn

) kt(, A) Pt(, A)

=

dkt(, ∑) dPt(, ∑)

.
Gt0-

Lemma 2.5. If (t, ,  )  t(,  ) is (Ft  Gt)-predictable and bounded, then

t(,  ) Pt(, d ) d M, M t dP () =

t(, ) d M, M t dP ().

Proof. Let 0  r < s, A  Fr, B  Gr and t(,  ) = 1]r,s](t)1A()1B( ). 9

Then

t(,  ) Pt(, d ) d M, M t dP ()
s
= 1A()Pt(, B) d M, M t dP ()
r s
= 1A()1B() d M, M t dP ()
r
= t(, ) d M, M t dP (),

where the second equality holds due to results about optional projections (see Theorem 57, Chapter VI, in [DM78]). By a monotone class argument this can be extended to all bounded and (Ft  Gt)-predictable processes.

Theorem 2.6. Suppose Condition 2.1 is satisfied and  is as in Lemma 2.3. Then

t() = t(, )

is the information drift of (Gt) relative to (Ft).

Proof. Suppose  to be a stopping time such that M  is a martingale. For 0  s < t and A  Gs0 we have to show

t

E [1A(Mt - Ms )] = E 1A

u(, )

d

M, M

 u

.

s

For notational simplicity write M  = M and observe

E [1A(Mt - Ms)] = E [Pt(∑, A)(Mt - Ms)]

t
= E (Mt - Ms) ku(∑, A) dMu + E[(Mt - Ms)LtA]
0

t

= E ku(∑, A) d M, M u

s

t

=E

u(,  ) dPu(, d ) d M, M u

sA

t

= E 1A() u(, ) d M, M u ,

s

where we used Lemma 2.5 in the last equation.

Corollary 2.7. (Gt) is a finite utility filtration if and only if t2(,  ) Pt(, d ) d M, M t dP () < .

Proof. This follows immediately from Lemma 2.5.

We now look at the problem from the reverse direction. Starting with the assumption

that (Gt) is a finite utility filtration, which amounts to E

T 0

2 d

M, M

< , we show the

validity of Condition 2.1.

10

In the sequel, (Gt) denotes a finite utility filtration and  its predictable information drift,

i.e. ∑

M~ = M - t d M, M t

(8)

0

is a (Gt)-local martingale. To prove the main results (Theorems 2.10 and 2.12), we need the

following lemma.

Lemma 2.8. Let 0  s < t and P = {A1, . . . , An} be a finite partition of  into Gs0-measurable sets. Then

tn
E
s k=1

ku Pu

2
(∑, Ak) 1Ak d M, M u  4E

t
u2 d M, M u < .
s

Proof. Let P = {A1, . . . , An} be a finite Gs0-partition. An application of Ito's formula, in conjunction with (7) and (8), yields

n

[1Ak log Ps(∑, Ak) - 1Ak log Pt(∑, Ak)]

k=1

n t1 = k=1 - s Pu(∑, Ak) 1Ak dPu(∑, Ak)

1t

1

+ 2

s

Pu(∑, Ak)2 1Ak d P (∑, Ak), P (∑, Ak) u

=

n k=1

-

t s

ku Pu

(∑,

Ak

)

1Ak

dM~ u -

t s

ku Pu

(∑,

Ak )

1Ak u

d

M, M

u

-

t s

1 Pu(∑, Ak) 1Ak

dLAu k

+

1 2

t s

ku Pu

2
(∑, Ak) 1Ak d M, M u

1 +
2

t1 s Pu(∑, Ak)2

1Ak

d

LAk , LAk

u

(9)

Note that Pt(∑, Ak) log Pt(∑, Ak) is a submartingale bounded from below for all k. Hence the expectation of the left hand side in the previous equation is at most 0.
A priori it is not clear whether

n k=1

t s

ku Pu

(∑,

Ak

)

1Ak

dM~ u

is integrable or not. Consider therefore for all  > 0 stopping times defined by

k =

  / Ak inf{t  s : Pt(∑, Ak)  } else

and   = 1  . . .  n.
Observe that     as   0 and that the stopped process

n k=1

t  s

ku Pu

(∑,

Ak

)

1Ak

dM~ u

11

has expectation zero, since

 E

t  s

n k=1

ku Pu

(∑,

Ak

)

1Ak

dM~ u

2 

=E

t  n s k=1

ku Pu

2
(∑, Ak) 1Ak d M, M u



1 2 E

t  n
(ku)2 (∑, Ak) 1Ak d M, M u
s k=1



1 2 E

n k=1

t
d P (∑, Ak), P (∑, Ak) u
s

< .

Similarly, one can show that the expectation of

t  s

1 Pu(∑, Ak) 1Ak

dLuAk

vanishes. Consequently we may deduce from equation (9) and the Kunita-Watanabe inequality

n1 E
2
k=1

t  s

ku Pu

2
(∑, Ak) 1Ak d M, M u

n
E
k=1

t  s

ku Pu

(∑,

Ak

)

1Ak

u

d

M, M

u

E

t  n s k=1

ku Pu

2
(∑, Ak) 1Ak d M, M u

1 2
E

which implies

t 

1 2

u2 d M, M u ,

s

t  n
E
s k=1

ku Pu

2
(∑, Ak) 1Ak d M, M u  4E

t 
u2 d M, M u .
s

Now the proof may be completed by a monotone convergence argument.

Let T and (Pi,n)i,n0 be a family of partitions as in the proof of Lemma 2.3. We define

for all n  0

Ztn(, 

)

=

i0

AP i,n

1]tin,tin+1](t)1A(

) kt(, A) . Pt(, A)

Note that Zn is (Ft  Gt)-predictable. We are now able to prove a converse statement to Theorem 2.6. Observe first

Lemma 2.9. For PM -almost all (, t)   ◊ R+ the discrete process (Ztm(, ∑))m1 is an L2(Pt(, ∑))-bounded martingale.

12

Proof. Every statement in the sequel is meant to hold for PM -a.a. (, t)   ◊ R+. Let m  0, l  0 and j be the natural number such that ]tml +1, tml++11] ]tjm, tjm+1]. We start by proving that on ]tml +1, tml++11] we have

EPt(,∑)[Ztm+1(, ∑)|Pj,m] = Ztm(, ∑).

For this, let B  Pj,m and A1, . . . , Ak  Pl,m+1 such that A1  . . .  Ak = B. Note that

EPt(,∑)[1B(∑)Ztm+1(, ∑)]

=

E Pt (,∑)

k i=1

1Ai

(∑)

kt Pt

(,

Ai)

k

= kt(, Ai)

i=1

= kt(, B)

= EPt(,∑)[1B(∑)Ztm(, ∑)]

on ]tml +1, tml++11]. Consequently the process (Ztm(, ∑))m1 is a martingale (with respect to a filtration depending on t). The martingale property implies that the sequence (Ztn)2(,  ) Pt(, d )
is increasing, and hence, by monotone convergence,

sup E
n

(Ztn)2(,  ) Pu(, d ) d M, M t

= E sup (Ztn)2(,  ) Pu(, d ) d M, M t.
n

By Lemma 2.8 and Lemma 2.5 we have

sup E
n

(Zun)2(,  ) Pu(, d ) d M, M u

= sup E (Zun)2(, ) d M, M u
n

=

sup E
n i0

tni+1
1A()
tni APi,n

kt(, A) Pt(, A)

2
d M, M u

 4E u2 d M, M u < .

This shows that (Zn)n1 is an L2(Pt(, ∑))-bounded martingale.
We now will show that k can be chosen to be a signed measure. For this we identify Pt(, ∑) with another measure on a countable generator of Gt0-. We then apply the result that two Banach space valued measures are equal, if they coincide on a generator stable for finite intersections.
Theorem 2.10. The kernel k may be chosen such that
Gt0- A  kt(, A)  R
is a signed measure which is absolutely continuous with respect to Pt(, ∑)|Gt0-, for PM -a.a. (, t)   ◊ [0, ). This means that Condition 2.1 is satisfied.

13

Proof. Lemma 2.9 implies that (Ztm(, ∑))m1 is an L2(Pt(, ∑))-bounded martingale and hence, for a.a. fixed (, t), (Ztm(, ∑))m1 possesses a limit Z. It can be chosen to be (Ft 
Gt)-predictable. Take for example

Zt

=

lim inf
n

(Ztn



0)

+

lim sup(Ztn
n



0).

Now define a signed measure by

k~t(, A) = 1A( )Zt(,  )dPt(, d ).

Observe that k~t(, ∑) is absolutely continuous with respect to Pt(, ∑) and that we have for all A  Pj,m with j2-m  t
k~t(, A) = kt(, A)

for PM -a.a. (, t)   ◊ R+. One may also interpret Gt0- A  k~t(, A), as an

L2(M )-valued measure. By applying the stochastic integral operator, we obtain an L2()-valued

measure: Gt0-

A

t 0

k~s(,

A)dMs.

Moreover,

t
Pt(, A) = P (A) + k~s(, A) dMs + LtA()
0

(10)

for all A  j2-mt Pj,m. Since the LHS and both expressions on the RHS are measures
coinciding on a system which is stable for intersections, equation (10) holds for all A  Gt0-. Hence, by choosing kt(∑, A) = k~t(∑, A) for all A  Gt0-, the proof is complete.

Remark 2.11. Since k is determined up to PM -null sets, we may assume that kt(, ∑) is absolutely continuous relative to Pt(, ∑) everywhere.
We close this section with some examples showing how (well known) information drifts can be derived explicitly, based on the formalism of Theorem 2.6. To this end it is not always necessary to determine the signed measures kt(, ∑) on the whole -algebras Gt0, but only on some sub--fields. This is the case for example, if
Gt0 = Ft0  Ht0, 0  t  T,
where (Ht0) is some countably generated filtration on (, F ). Now suppose that kt(, ∑) is a signed measure on (Ht0-) satisfying

kt(, ∑)
Ht0-

Pt(, ∑)
Ht0-

for PM -a.a (, t). Then we can show with the arguments of the proof of Lemma 2.3 that there is an (Ft  Ht)-predictable process  such that PM -a.e.

t(, 

)

=

dkt(, ∑) dPt(, ∑)

(
Ht0-

).

The information drift of (Gt) relative to (Ft) is already determined by the trace of (t). For the corresponding analogue of Theorem 2.6 we shall give a more explicit statement.

14

Theorem 2.12. The process

t() = t(, )

is the information drift of (Gt) relative to (Ft).

Proof. Suppose T to be a stopping time such that M T is a martingale. For 0  s < t, A  Hs0 and B  Fs0 we have to show

t

E 1A1B(MtT - MsT ) = E 1A1B

u(, )

d

M, M

T u

.

s

For simplicity assume M T = M , and observe, like in the proof of Thereom 2.6,

E [1A1B(Mt - Ms)] = E [1BPt(∑, A)(Mt - Ms)]
t
= E 1A()1B() u(, ) d M, M u .
s

Example 2.13. Let (Wt) be the standard Wiener process and (Ft0) the filtration generated by (Wt). Moreover, let (Yt) be a Gaussian process independent of F1 such that for each pair s, t with 0  s < t the difference Yt - Ys is independent of Yt. We denote by wt the variance of Yt.
We enlarge our filtration by
Ht0 = (W1 + Ys : 0  s  t) = (W1 + Yt)  (Yt - Ys : 0  s  t),
and put Gt0 = Ft0  Ht0, 0  t  1. Now observe that for all C  (Yt - Ys : 0  s  t) and Borel sets B  B(R) we have

Pt(∑, {W1 + Yt  B}  C) = P (C) 1B(x + W1 - Wt + Yt) dP
x=Wt

= P (C) 1B(y + x)1-t+wt(y)dy
x=Wt
= P (C) 1-t+wt(y - Wt)dy, 0  t < 1,
B

where

v(y) =

1

1

e-

y2 2v

.

(2v) 2

Now observe that f (x, t) = P (C) B 1-t+wt(y - x)dy is differentiable in x and satisfies

 y-x

f (x, t) = P (C) x

B 1 - t + wt 1-t+wt (y - x) dy

for all 0  t < 1 and x  R. By Ito's formula

Pt(∑, {W1 + Yt  B}  C) = f (0, 0) +

t 0

 x f (Ws, s) dWs + At,

0



t

<

1,

15

where A is a process of bounded variation. Note that A is also a martingale, and thus A = 0. Hence

kt(∑, {W1 + Yt  B}  C)

=

P (C)

B

y - Wt 1 - t + wt

1-t+wt (y

-

Wt) dy

y+x-x = P (C) 1B(y + x) 1 - t + wt 1-t+wt (y) dy x=Wt()

=

{W1+YtB}C

W1(

) + Yt( ) - 1 - t + wt

Wt()

dPt(, d

)

As a consequence

t(, 

)

=

kt(, d ) Pt(, d )

Ht0

=

W1(

) + Yt( ) - Wt() , 1 - t + wt

and by Theorem 2.12,

Wt -

t 0

W1 1

+ -

Ys - Ws s + ws

ds,

0  t < 1,

is a martingale relative to (Gt).

Similar examples can be found in [CIKHN03], where the information drifts are derived in

a completely different way, though.

Example 2.14. Let (Wt) be the standard Wiener process and (Ft) the Wiener filtration. We use the abbreviation Wt = sup0st Ws and consider the filtration enlarged by the random variable G = 1[0,c](W1), c > 0. Again we want to apply Theorem 2.12 in order to obtain the
information drift of Gt = Ft  (G). To this end let Zt = suptr1(Wr - Wt) and denote by
pt the density of Zt, 0  t < 1. Now,

Pt(∑, G = 1) = P (Wt  Wt + Zt  c|Ft)

= 1[0,c](y  x + Zt)dP
x=Wt,y=Wt

c-x

= 1[0,c](y)
0

pt(z)dz

,

x=Wt,y=Wt

for all 0  t < 1. Note that F (x, y, t) = 1[0,c](y)

c-x 0

pt(z)dz

is differentiable in x for all

0  t < 1 and x  R, and by Ito's formula

Pt(∑, G = 1) = F (0, 0, 0) +

t 0

 x

F

(Ws,

Ws,

s)

dWs

+

At,

0  t < 1,

where A is a process of bounded variation. Hence

Similarly, we have

kt(∑, G = 1) =

 x

F

(Wt,

Wt,

t),

0  t < 1.

Pt(∑, G = 0) = H(Wt, Wt, t), 0  t < 1,

16

and where As a consequence

kt(∑, G = 0) =

 x

H

(Wt,

Wt

,

t),

0  t < 1,



H(x, y, t) = 1(c,)(y) + 1[0,c](y)

pt(z)dz.

c-x

t(,  )

=

kt(, d ) Pt(, d ) (G)

=

1{1}(G(

))

 x

log

F

(Wt(),

Wt(

),

t)

+

1{0}(G(

))

 x

log

H (Wt (),

Wt(

),

t),

0  t < 1.

3 Monotone convergence of information drifts

In the preceding section we established a general relationship between the information drift and the regular conditional probabilities of filtrations. In this framework the knowledge of the better-informed agent is described by a general enlarged filtration (Gt) of (Ft). We shall now consider the question whether this situation may be well approximated by "stepwise initial" enlargements, for which we take Ft  Gti- for t  [ti, ti+1), if the family (ti)0in is a partition of R+. One particularly important question in this context concerns the behavior of the information drifts along such a sequence of discretized enlargements. Of course we expect some convergence of the drifts. We shall establish this fact rigorously in the following section. In the present section, we shall prepare the treatment of this problem by solving a somewhat more general problem. Let (Gtn)nN be an increasing sequence of finite utility filtrations and supn uGn(x) be finite. We will show that the smallest filtration containing every (Gtn) is then also a finite utility filtration.
Since we will not deal with regular conditional probabilities in this section, it is not necessary to require our probability space (, F) to be standard.
We use the terminology of Revuz and Yor [RY99]: H2(Ft) denotes the set of L2-bounded continuous (Ft)-martingales, i.e. the space of continuous (Ft, P )-martingales M such that
sup E(Mt2) < .
t0
We need the following characterization of H2(Ft).
Lemma 3.1. (Proposition 1.23 in [RY99]) A continuous (Ft)-local martingale belongs to H2(Ft) if and only if the following two conditions hold
i) E(M02) <  ,
ii) E( M, M ) < .
The properties i) and ii) are independent of the filtration considered. This is due to the fact that the quadratic variation of M does not change under a new filtration (Gt) for which M is still a semimartingale. We therefore have

17

Lemma 3.2. Suppose M  H2(Ft). Let (Gt) be a filtration such that M is still a (Gt)-semimartingale. If
M = M~ + A
is a Doob-Meyer decomposition with respect to (Gt) with A0 = 0, then M~ belongs to H2(Gt).

Proof. Notice that M~0 = M0 and M, M = M~ , M~ . The claim follows now by applying Lemma 3.1 twice.
Now let M be a continuous (Ft)-local martingale and (Gtn)n1 an increasing sequence of filtrations, i.e. for all t  0 we have

Ft  Gt1  . . .  Gtn  Gtn+1  . . .

We assume that for all n  1 the process M is a (Gtn)-semimartingale with Doob-Meyer

decomposition of the form

∑
M = M n + µns d M, M s,
0

where µn is (Gtn)-predictable. We then have the following asymptotic property.

Lemma 3.3. If the processes (µn)nN converge to some µ in L2(M ), then
∑
M - µs d M, M s
0
is a local martingale with respect to Gt = n1 Gtn, t  0.

Proof. Suppose the stopping time  reduces M such that M  is a bounded martingale. Note that Lemma 3.2 implies that the stopped processes (M n) are (Gt)-martingales.
For simplicity we assume M  = M . Choose a constant C > 0 such that

|M |  C, and E (µsn)2 d M, M s  C2 for all n  1.
0
Now let  > 0, 0  s < t and A  Gs. It suffices to show

t
E [1A(Mt - Ms)] - E 1A µs d M, M s  .
s

We start by choosing n0 such that µn - µ L2(M)  4

 E( M, M )

for all n  n0.

Note that nn0 Gsn is an algebra generating the -algebra n1 Gsn = Gs = nn0 Gsn. Hence we can find a sequence (Ai)iN of sets in nn0 Gsn such that P (A Ai)  0. A

subsequence of (1Ai)iN converges to 1A almost surely and therefore we may choose n  n0

and A~  Gsn satisfying P (A~

A)



(

 4C

)2

and

t
E (1A - 1A~)2 M, M
s

1

2


 .

4C

18

Hence we have

E [1A(Mt - Ms)] - E 1A~(Mt - Ms)

 E (1A~ - 1A)(Mt - Ms)

 P (A~



 .

2

1
A) 2

(E(Mt

-

Ms)2)

1 2

By applying the Kunita-Watanabe inequality we get for n  n0

tt
E 1A µu d M, M u - E 1A~ µun d M, M u
ss

tt
 E 1A (µu - µnu) d M, M u + E (1A - 1A~) µun d M, M u
ss



E

t
1A d M, M

1 2
E


(µs - µsn)2 d M, M s

1 2

s0

+E

t
(1A - 1A~)2 d M, M

1 2
E


(µns )2 d M, M s

1 2

s0



(E M, M

1
)2

µ - µn

 L2(M ) + 4



 ,

2

and thus

t
E [1A(Mt - Ms)] - E 1A µu d M, M u
s

 E [1A(Mt - Ms)] - E 1A~(Mt - Ms)

t
+ E 1A~(Mt - Ms) - E 1A~ µnu d M, M u
s

tt
+ E 1A~ µnu d M, M u - E 1A µu d M, M u
ss



 + 0 + = .

22

We are now in a position to prove the main result of the section.

Theorem 3.4. If supn1

µn

2 L2(M )

<

,

then

(µn)

converges

in

L2(M )

to

a

process

µ.

Moreover,

∑

M - µ d M, M

0

is a local martingale with respect to Gt = n1 Gtn, t  0.

Proof. Set c = supn1

µn

2 L2

(M

)

.

Let

m



n



1,

and

note

that

µm -µn

is

the

information

drift of (Gtm) relative to (Gtn). Therefore, property iii) in Lemma 1.6 implies

µm

2 L2(M )

=

µn

2 L2(M )

+

µm - µn

2 L2

(M

)

.

19

Thus, c = limn

µn

2 L2(M )

and

µm - µn

2 L2(M )

=

µm

2 L2(M )

-

µn

2 L2(M )



c

-

µn

2 L2(M )



0

as n  . Therefore {µn}n1 is a Cauchy sequence in L2(M ). By completeness of L2(M ), there exists a unique (Gt)-predictable process µ0  L2(M ) such that limn µn = µ0 in L2(M ). By Lemma 3.3 the process M - µ0 d M, M is a (Gt)-local martingale.

4 Continuous and initial enlargements

In this section we relate general enlargements of filtrations to "initial enlargements" along discrete partitions of [0, T ], for finite horizon T. The knowledge of the insider is modeled by an arbitrary filtration (Gt)t[0,T ], satisfying Gt  Ft, 0  t  T . For s  [0, T ] we set

Gts =

Ft, Ft  Gs-,

t<s t  s.

Again the analysis of this section does not require our probability space (, F) to be standard.

Remark 4.1. In the case where the -field Gs-, s  [0, T ], is generated by a countable number of events, say (An)nN, the enlarged filtration Gts can be viewed as initial enlargement at time s in the classical sense. In that case Gs- = (1An : n  N) and one has for t  [0, T ],

Ft  Gs- = Ft  (1An : n  N).

The set {0, 1}N can be endowed with a metric so that it becomes a Polish space with corresponding Borel--field B({0, 1})N. Hence, the filtration (Gts) can be seen as initial enlargement at time s induced by the random variable G :   {0, 1}N,   (1An())nN. In particular, the standard theory of initial enlargements is applicable. See Jeulin, Yor [JY85].

In the following, we assume that (Gts) is for arbitrary s  [0, T ] a finite utility filtration. For 0  s  t  T we denote

0([0, s)

◊

(t,

T ])

=

F (s, t)

=

1 E
2

T t

µsr 2 d M, M r,

where µs is a (Gts)-information drift. So far 0 is defined only on the set J = {[0, s) ◊ (t, T ] : s  t}. As the next lemma shows 0 can be extended to a measure on the Borel sets of D = {(s, t)  R2 : 0  s < t  T }.

Lemma 4.2. There exists a unique measure  on the Borel sets B(D) of D satisfying |J = 0.

Proof. Uniqueness is an immediate consequence of the measure extension theorem. In order to show the existence of an extension it satisfies to verify the following property which essentially amounts to countable additivity on a generating semiring (see Elstrodt [Els96] Chapter II, Satz 3.8): For any (s, t)  D and any sequence (sn, tn)nN in D with sn  s,

20

tn  t and limn(sn, tn) = (s, t) we have limn F (sn, tn) = F (s, t). Moreover, F (sn, tn)  F (s, t) < .
Let sn, tn, s and t as above. Without loss of generality we assume that (sn) is monotonically increasing. For u  [t, T ] we consider the filtrations (Grsn)r[u,T ], n  N, over the time interval [u, T ]. The filtrations are monotonically increasing with nN Grsn = Grs, r  [u, T ]. Since (µsrn)r[u,T ] are (Grsn)-information drifts, it follows (by Lemma 1.6) that
T
E µs - µsn µsn d M, M = 0.
u
In particular,
TT
E µsn 2 d M, M  E µs 2 d M, M < .
ut
By Theorem 3.4 the processes (µsrn)r[u,T ] converge to the information drift (µrs)r[u,T ] in L2(M ; [u, T ]). Therefore, for any u  (t, T ],

TT

lim inf E

µsn 2 d M, M  E

µs 2 d M, M .

n

tn

u

Due to the continuity of M the right hand side of the previous equation tends to

E

T t

µs

2 d M, M

as u  t. Consequently, we obtain limn F (sn, tn) = F (s, t).

The measure  describes the utility increase by additional information. As will be shown

below, (D) is finite if and only if (Gt) is a finite utility filtration. We now approximate the general filtration (Gt) by filtrations that can be seen as successive
initial enlargements. Let  : 0 = s0  ∑ ∑ ∑  sn = T , n  N, be a partition of the interval [0, T ]. We let for r  [si, si+1), i = 0, . . . , n - 1,

Gr = Gsi-  Fr.

Proposition 4.3. For i = 0, . . . , n -1, let µsi be a (Grsi)-information drift and set µr := µsri for r  [si, si+1). Then µ is a Gt-information drift. Moreover,

1 2

T
µr 2 d M, M r = (D),
0

where D := {(s, t)  D : i  {0, . . . , n - 1} with s < si and t > si}.

Proof. It is straightforward to verify that µ is an information drift for (Gt). Moreover,

1 E
2

T 0

µr

2d

M, M

r

=

1 n-1 E
2

i=0

si+1 µrsi 2 d M, M r
si

1 n-1 =E
2
i=0

T
µsri 2 d M, M r - E
si

T
µsri 2 d M, M r
si+1

n-1
= ([0, si) ◊ (si, si+1]) = (D).

i=0

We can now state the main theorem of this section. 21

Theorem 4.4. Let n, n  N, be a sequence of partitions of the interval [0, T ] the mesh of which tends to 0. If (D) is finite, then the information drifts µn converge in L2(M ) to a
(Gt)-information drift µ. Moreover, the utility gain of the insider satisfies

u(Gt, x)

-

u(Ft, x)

=

1 E
2

T
µ2 d M, M
0

= (D).

If (D) is infinite, then so is the utility gain of the insider.

The proof of the theorem is based on the following proposition.

Proposition 4.5. If (D) < , then there exists a (Gt)-information drift µ. Moreover,

1 2

µ

2 L2(M )

=

(D).

Proof. Let n, n  N, be as in the above theorem with the additional assumption that

n+1 is a refinement of n for all n  N. Then one has Gtn  Gtn+1 for any t  [0, T ].

By Proposition 4.3,

1 2

µn

2 L2(M )

=

(Dn )



(D).

Due to Theorem 3.4 the information

drifts µn converge to a ( nN Gtn) = (Gt-)-information drift µ in L2(M ). Using monotone

convergence we obtain that

(D) = lim (Dn) = lim 1

n

n 2

µn

2 L2(M )

=

1 2

µ

2 L2

(M

).

Since every cadlag (Gt-)-martingale is as well a (Gt)-martingale, µ is a (Gt)-information drift.

Proof of Theorem 4.4. Assume that (D) is finite. Since the mesh of the partitions n tends to zero, one has limn 1Dn (x) = 1 for all x  D. Consequently, the dominated convergence theorem yields

lim (Dn) = (D).
n

(11)

We established the existence of a (Gt)-information drift µ in Proposition 4.5. Recall that by Lemma 1.6, the processes µn and µ - µn are orthogonal in L2(M ). Consequently,

µ - µn

2 L2(M )

=

µ

2 L2(M )

-

µn

2 L2(M

)

.

Due to (11) the right hand side of the previous equation converges to 0. Hence, µn converges to µ in L2(M ). The remaining statements are consequences of Proposition 4.5 and Proposition
1.4.

5 Additional utility and entropy of filtrations
In this section we consider the link between the additional expected logarithmic utility of a better informed agent and the entropy of the additional information he possesses. The additional utility was firstly expressed in terms of a relative entropy in [PK96] (p. 1103) for a particular example. More generally, [AIS98] discussed the link between the absolute
22

entropy of a random variable describing initially available additional information and the utility increment of better informed agents. Here we shall see that the expected logarithmic utility increment is given by an integral version of relative entropies of the -algebras of the filtration. This notion can best be understood as the limit of discrete entropy sums along a sequence of partitions of the trading interval as the mesh goes to 0. Alternatively, we are able to give an interpretation of the utility increment by Shannon information differences between the filtrations of the agents. In particular, we shall see that this differences are independent of any local martingales the filtrations may carry.
Suppose that the assumptions of Chapter 2 are satisfied. Moreover, we assume that M is a continuous local martingale satisfying the (PRP) relative to (Ft) which simply means that LA = 0. Equation (7) simplifies to

t
Pt(∑, A) = P (A) + ks(∑, A) dMs,
0
where k(∑, A) is as in Chapter 2. Let again (Gt0) be a filtration satisfying Ft0  Gt0 and being generated by countably many sets. To simplify notation we assume the filtration (Gt0) to be left-continuous. Let (Gt) be the smallest completed and right-continuous filtration containing (Gt0). In the following, we assume that (Gt) is a finite utility filtration and denote by µ its predictable information drift, i.e.
∑
M~ = M - µt d M, M t
0
is a (Gt)-local martingale. Recall that by Theorem 2.10 we may assume that kt(, ∑) is a signed measure. For a fixed r > 0 we define µr as the information drift of the initially enlarged filtration (Gtr), defined as in the beginning of the preceding chapter. For stating the main result we need the following lemma.

Lemma 5.1. Let 0  s < t and (Pm)m0 an increasing sequence of finite partitions such that (Pm : m  0) = Gs0. Then

t
lim E
m s APm

ku Pu

2
(∑, A) 1A d M, M u

=

t
E (µsu)2 d M, M u
s

and

lim E
m

t s APm

ku Pu

(∑,

A)

1A

µsu

d

M, M

u

=

E

t
(µus )2 d M, M u.
s

Proof. By Lemma 2.10 the process

Yum(,  ) =
AP m

ku Pu

(,

A)1A(

),

m  1,

is a L2-bounded martingale for PM -a.a. (, u)   ◊ [s, t]. Hence (Y m) converges PM -a.s.

to the density

u

=

ku(∑, d ) Pu(∑, d )

.
Gs0

23

By Theorem 2.6 we have

u(,  ) = µsu()

PM -a.s. on  ◊ [s, t] and hence the first result. In a similar way one can prove the second statement.

We next discuss the important concept of the additional information of a -field relative to a filtration.

Definition 5.2. Let A be a sub--algebra of F and R, Q two probability measures on F. Then we define the relative entropy of R with respect to Q on the -field A by



 HA(R Q) =

log

dR dQ

A

dR,

if R

Q

,

else.

Moreover, the additional information of A relative to the filtration (Fr) on [s, t] (0  s < t  T ) is defined by
HA(s, t) = HA(Pt(, ∑) Ps(, ∑)) dP ().

The following lemma establishes the basic link between the entropy of a filtration enlargement and additional logarithmic utility of a trader possessing this information advantage.

Lemma 5.3. For 0  s < t we have

1

HGs0 (s, t)

=

E 2

t
(µus )2 d M, M u.
s

Proof. Let (Pm)m0 be an increasing sequence of finite partitions such that (Pm : m  0) = Gs0. Recall that by equation (9)

[1A log Ps(∑, A) - 1A log Pt(∑, A)]

AP m

=

AP m

-

t s

ku Pu

(∑,

A)

1A

dM~ u

-

t s

ku Pu

(∑,

A)

1Aµu

d

M, M

u

1t +
2s

ku Pu

2
(∑, A) 1A d M, M u

Since M~ is a local martingale, we obtain by stopping and taking limits if necessary

E

AP m

Ps(∑,

A)

log

Pt(∑, A) Ps(∑, A)

=

E
AP m

t s

ku Pu

(∑,

A)

1A

µu

d

M, M

u-

1 2

t s

ku Pu

2
(∑, A) 1A d M, M u.

Note that in the previous line µ may be replaced by µs, because (µ - µs) is orthogonal to L2(M )(Gs) (see property iv) in Lemma 1.6). Applying Lemma 5.1 yields

1

lim HPm(s, t)
m

=

E 2

t
(µsu)2 d M, M u.
s

24

Fatou's Lemma implies

lim inf
m

HPm (s,

t)



HGs0 (s,

t).

On the other hand we have HPm(s, t)  HGs0(s, t), since Pm  Gs0, and thus

lim
m

HPm (s,

t)

=

HGs0 (s,

t),

which completes the proof.

Let us now return to the stepwise approximation of a filtration enlargement along a sequence of partitions of the trading interval by "initial enlargements", and define their respective information increment.

Definition 5.4. Let  : 0 = s0  ∑ ∑ ∑  sn = T , n  N, be a partition of the interval [0, T ] and let µ be the information drift of (Gr). The additional information of (Gr) relative to

(Fr) is defined as

n-1

H =

HGs0i (si, si+1).

i=0

Theorem 5.5. We have

1

lim H
||0

=

E 2

T
µ2u d M, M u.
0

Proof. This follows from Theorem 4.4 and Lemma 5.3.

Example 5.6. Let Gt0 = Ft0  (P), where P is a finite partition in FT . Then µ0 = µ and

by Lemma 5.3

1

HG00 (0, T )

=

E 2

T
µu2 d M, M u.
0

If F0 is trivial, then

HG00(0, T ) = - P (A) log P (A),
AP

which is the absolute entropy of the partition P. Thus, the additional logarithmic utility of an agent with information (Gt) is equal to the entropy of P. This example shows that there is a link between logarithmic utility and the so-called Shannon information.

Definition 5.7. Let X and Y be two random variables in some measurable spaces. The mutual information (or Shannon information) between X and Y is defined by

I(X, Y ) = H(PX,Y PX  PY ).

Now let Z be a third random variable. The conditional mutual information of X and Y given Z is defined by
I(X, Y |Z) = E H(PX,Y |Z PX|Z  PY |Z ) ,
provided the regular conditional probabilities exist.

25

If A is a sub--algebra of F, then we write idA for the measurable map (, F)  (, A),   . For two sub--algebras A and D we abbreviate

I(A, D) = I(idA, idD).

Since our probability space is standard, for any sub--fields A, D, E of F there exists a regular conditional probability PidA,idD|idE and we define
I(A, D|E) := I(idA, idD|idE ).

The mutual information was introduced by Shannon as a measure of information. It plays an important role in information theory (see, for instance, [Iha93]).

Theorem 5.8.

lim
||0

i

I(Gs0i , Fs0i+1 |Fs0i )

=

1 E
2

T
µ2u d M, M u.
0

Proof. Note that for three random variables X, Y and Z we have

dP(X,Y )|Z d(PX|Z  PY |Z )

=

dPX|(Y,Z) . dPX|Z

This property implies that one has for 0  s < t  T ,

I(Gs0, Ft0|Fs0) =

log dPidGs0 |idFt0 dP ( ) dP () dPidGs0 |idFs0

= log Pt(∑, d ) dP ( ) dP () Ps(∑, d ) Gs0

= HGs0 (s, t).

Thus the assertion is an immediate consequence of Theorem 5.5.

This result motivates the following notion.

Definition 5.9. The information difference of (Gr0) relative to (Fr0) up to time T is defined

as

A(G0, F 0) = lim
||0

I(Gs0i , Fs0i+1 |Fs0i ).

i

Remark 5.10. Note that we did not use M in our definition of the information difference of (Gr0) relative to (Fr0). However, by Theorem 5.8, the information difference may be represented in terms of any local martingale satisfying the (PRP).

Theorem 5.8 can be reformulated in the following way.

Theorem 5.11. The additional utility of an agent with information (Gt) is equal to the information difference of (Gr0) relative to (Fr0), i.e.
uG(x) - uF (x) = A(G0, F 0).

26

If (Gt) is initially enlarged by some random variable G, then the information difference of (Gr0) relative to (Fr0) coincides with the Shannon information between G and (FT0 ).
Lemma 5.12. Let Gt0 = Ft0  (G), where G is a random variable with values in some Polish space. Then
A(G0, F 0) = I(G, FT0 |F00).

Proof. Let 0  s  t. By standard arguments we have I(Gs0, Ft0|Fs0) = I(G, Ft0|Fs0) and
I(G, Ft0|F00) = I(G, (Ft0, Fs0)|F00) = I(G, Ft0|Fs0) + I(G, Fs0|F00)

(see e.g. [Iha93] Theorem 1.6.3.) By iteration we obtain for all partitions 

and hence the result.

I(Gs0i , Fs0i+1 |Fs0i ) = I(G, FT0 |F00),
i

Theorem 5.13. Let Gt0 = Ft0  (G), where G is a random variable with values in some Polish space. Then the additional logarithmic utility of an agent with information (Gt) is equal to the Shannon information between G and (FT0 ) conditioned on F0, i.e.
uG(x) - uF (x) = I(FT0 , G|F00).
In particular, if F00 is trivial, then the additional utility is equal to I(FT0 , G).
Proof. This follows from Lemma 5.12 and Theorem 5.8.

Remark 5.14. If Gt0 = Ft0  (G) and G is FT0 -measurable, then the mutual information I(FT0 , G|F00) is equal to the conditional absolute entropy of G (see also [AIS98]).
Example 5.15. Let (, F, P ) be the 1-dimensional canonical Wiener space equipped with the Wiener process (Wt)0t1. More precisely,  = C([0, 1], R) is the set of continuous functions on [0, 1] starting in 0, F the -algebra of Borel sets with respect to uniform convergence, P the Wiener measure and W the coordinate process. (Ft)0t1 is obtained by completing the natural filtration (Ft0)0t1. Suppose the price process S is of the form

St = exp(Wt + bt),

0  t  1,

with b  R. We want to calculate the additional utility of an insider knowing whether the price exceeds a certain level or not. More precisely, we suppose the insider to know the value of
G = 1(c,)(S1),
where c > 0 and S1 = max0t1 St. By Remark 5.14 the additional utility is equal to the entropy
H(G) = p log p + (1 - p) log(1 - p)

27

where

p = P (S1 > c).

This may be calculated via Girsanov's theorem. Namely we have

P (S1 > c)

=

P (t  [0, 1] : max Wt + bt > log c)
t[0,1]

=

1
exp

b log c - b2 s

| log c| exp

- | log c|2

ds.

0

2 2s3

2s

6 Mutual information estimates

In this final section we apply some results from information theory to derive estimates for the information of a better informed agent. This yields a priori estimates for the agent's additional expected logarithmic utility in the light of the preceding section. Among other facts, the differential entropy maximizing property of Gaussian laws will play a role. We adopt the notations of [Iha93].
Before we provide the information estimates, we summarize some basic facts of the mutual information (see [Iha93], Theorem 1.6.3). For random variables X, Y , Z in some Borel spaces, the following properties hold:
(I.1) I(X, Y |Z)  0 and, I(X, Y |Z) = 0 if and only if X and Y are independent given Z

(I.2) I(X, (Y, Z)) = I(X, Z) + I(X, Y |Z)

(I.3) If X is a continuous random variable with finite differential entropy, then I(X, Y ) = h(X) - h(X|Y ).

For some fixed integer d  N, let X be a FT0 -measurable Rd-valued random variable. Moreover, denote by Y a d-dimensional r.v. that is independent of the -field FT0 . We consider the enlarged filtration Gt0 = Ft0  (G), where G := X + Y .

Lemma 6.1. Suppose that the law of Y is absolutely continuous with respect to Lebesgue measure and has finite differential entropy

h(Y ) = -

dPY dd

(y)

log

dPY dd

(y)

dy.

Then

I(G, FT0 ) = h(X + Y ) - h(Y ).

(12)

Proof. Due to property (I.2), we have

I(G, FT0 ) = I(X + Y, X) + I(X + Y, FT0 |X).
Given X, the r.v.'s X + Y and idFT0 are independent. Therefore, (I.1) and (I.3) lead to I(G, FT0 ) = I(X + Y, X) = h(X + Y ) - h(X + Y |X) = h(X + Y ) - h(Y ).

Now assume the perturbation Y to be a Rd-valued centered Gaussian r.v. that is independent of FT0 .
28

Lemma 6.2. Suppose that X  L2(P ) and let CX and CY denote the covariance matrices of X and Y , respectively. Then

I (G,

FT0 )



1 2

log

det(CX + CY det(CY )

) .

(13)

Moreover, equality holds in equation (13) if X is Gaussian.

Proof. The distribution of Y is continuous with respect to Lebesgue measure and has finite

entropy. Therefore,

I(G, FT0 ) = h(X + Y ) - h(Y ).

Let CX and CY denote the covariance matrices of X and Y , respectively. Due to the independence of X and Y , the random variable X + Y has the covariance matrix CX+Y = CX + CY . Next recall that the normal distribution maximizes the differential entropy under a covariance constraint, i.e. h(X + Y )  h(Z), where Z is a centered Gaussian r.v. with covariance matrix

CX+Y . Therefore,

I(X, X + Y )  h(Z) - h(Y ).

Using the formula for the differential entropy of Gaussian measures (Theorem 1.8.1, [Iha93])

we obtain

h(Z) - h(Y ) =

1 2

log

(2e)d det(CX+Y )

-

1 2

log

(2e)d det(CY )

=

1

log

det(CX+Y

) .

2 det(CY )

If X is Gaussian, then h(X + Y ) = h(Z) and, hence, the second statement of the lemma follows.

Corollary 6.3. Assume that additionally to the assumptions of the above lemma, the equation Y = N is valid, where N is a d-dimensional standard normal r.v. and  > 0. Then

I (G,

FT0 )



1 2

d

log

j

+

 ,



j=1

where j (j = 1, . . . , n) denote the eigenvalues of CX .

Proof. The proof follows easily by computing the determinants in Lemma 6.2.

The proof of Lemma 6.2 is based on the fact that Gaussian distributions maximize the differential entropy under a constraint on the covariance structure. Let us recall the construction of entropy maximizing measures under a linear constraint.

Lemma 6.4. Let E  Rd be a measurable set, c > 0 and g : E  [0, ) a measurable map. Assume that there exist constants Z, t  0, such that the measure  defined by

d dd (x)

=

1 Z

e-tg(x),

is a probability measure satisfying E[g] = c. Then  is the unique probability measure max-

imizing the differential entropy among all continuous probability measures µ on E satisfying Eµ[g] = c.

29

The entropy maximization problem is equivalent to minimizing the relative entropy H(∑ d). Hence, the problem can be treated under more general constraints by using results of (Csisza¥r [Csi75], Theorem 3.1).
Proof. Let µ be a continuous probability measure on E with Eµ[g] = c. Then

H(µ

) = Eµ log dµ d

=

Eµ

log

dµ dd

+ Eµ log dd d

= -h(µ) + log Z + tEµg = -h(µ) + log Z + tEg

= -h(µ) - E log e-tg = -h(µ) + h(). Z

Since H(µ )  0 and H(µ ) = 0 iff µ = ,  is the unique maximizer of the differential entropy.

Remark 6.5. The above lemma can be used to derive similar results as obtained in Lemma

6.2. For instance, for E := R and g(x) := |x| one obtains that the two-sided exponential distribution maximizes the differential entropy under the constraint Eµg = c (c > 0). In

particular,

the

measure



with

d d

(x)

=

(2c)-1

e-|x|/c

satisfies

E[g] = c and h() = 1 + log(2c).

Now let X be a real-valued r.v. in L1(P ). Moreover, let 1 := E[|X - EX|] and Y be a two-sided exponential distribution with E|Y | =: 2. Then due to Lemma 6.1,

I(G, FT0 )  log

1 + 2 2

.

Example 6.6. We consider the classical stock market model with one asset. Let (Ft0)t[0,T ] be a Brownian filtration generated by the Brownian motion (Bt)t[0,T ] and denote by (Ft) its completion. The stock price is modeled by the process

St = S0 exp Bt + bt ,

where S0 > 0 is the deterministic stock price at time 0 and b  R. For some fixed times
t1, . . . , td  (0, T ] (d  N), let X := (Bti)i=1,...,d. We suppose that the insider bases his investment on the filtration Gt = s>t Fs  (G), where G = X + N and N is a standard normal r.v. in Rd that is independent of FT . Due to Lemma 6.2 the additional utility of the
insider is related to the eigenvalues of the matrix

 t1 t1 . . . t1

t1

 

...

t2 ...

...

t2

...

 

.



t1 t2 . . . td

Let us finish the section with an example for a general enlargement.

Example 6.7. We reconsider the classical stock market model of Example 6.6 with T := 1. The knowledge of the insider at time t is modeled by Gt = r>t Fr  ((Gs)s[0,r]), where

30

Gt := B1 + B~g(1-t), (B~t) is a Brownian motion independent of (Bt) and g : [0, 1]  [0, ) is a decreasing function. We are therefore in a setting similar to Example 2.13. We now calculate the utility increment from the perspective of the notion of information difference of filtrations. Let  be as in Section 4. For 0  s  t  1 we have

([0, s) ◊ (s, t]) = I((Gu)u[0,s], Ft0|Fs0) = I(Gs, Ft0|Fs0) = I(Gs, Bt|Fs0) + I(Gs, Ft0|Fs0, Bt) = I(B1 + B~g(1-s), Bt - Bs|Fs0) = I(B1 - Bs + B~g(1-s), Bt - Bs).

Using the formula for the differential entropy for Gaussian measures we obtain

([0, s) ◊ (s, t]) = h(B1 - Bs + B~g(1-s)) - h(B1 - Bt + B~g(1-s))
= 1 log(2e(1 - s + g(1 - s))) - 1 log(2e(1 - t + g(1 - s))) 22 1 1 - s + g(1 - s)
= 2 log 1 - t + g(1 - s)

Alternatively one can express ([0, s) ◊ (s, t]) as

([0, s) ◊ (s, t]) = 1 2

t1 s 1 - u + g(1 - s) du.

For a partition  : 0 = t0  ∑ ∑ ∑  tm = 1 (m  N) we consider D as in Section 4. One has

n
(D) = ([ti-1, ti) ◊ (ti, ti+1])

i=1

11

1

= 2

0

1 - u + g(1 - max{ti : ti  u}) du

Next, choose a sequence of refining partitions (n) such that their mesh tends to 0. Then

the term in the latter integral is monotonically increasing in n and convergent. Hence, one

obtains

lim (Dn) = 1 1 1 du.

n

2 0 1 - u + g(1 - u)

On the other hand,

lim
n

(Dn )

=

(D)

=

uG (x)

-

uF (x).

Consequently the insider has finite utility if and only if

1 0

1 1-u+g(1-u)

du

<



is

finite.

Now

suppose g(y) = Cyp for some C > 0 and p > 0. It is straightforward to show that the integral,

and hence the additional utility, is finite if and only if p  (0, 1). This equivalence follows

also from results in [CIKHN03], where the authors compute explicitly the information drift.

References
[ABS03] J. Amendinger, D. Becherer, and M. Schweizer. A monetary value for initial information in portfolio optimization. Finance Stoch., 7(1):29≠46, 2003.
31

[AI05]

S. Ankirchner and P. Imkeller. Finite utility on financial markets with asymmetric information and structure properties of the price dynamics. Preprint, 2005.

[AIS98]

J. Amendinger, P. Imkeller, and M. Schweizer. Additional logarithmic utility of an insider. Stochastic Process. Appl., 75(2):263≠286, 1998.

[Ank05]

S. Ankirchner. Information and Semimartingales. Ph.D. thesis, Humboldt Universita®t Berlin, 2005.

[Bau01]

F. Baudoin. Conditioning of brownian functionals and applications to the modelling of anticipations on a financial market. PhD thesis, Universit¥e Pierre et Marie Curie, 2001.

[BO03]

F. Biagini and B. Oksendal. A general stochastic calculus approach to insider trading. Preprint, 2003.

[Cam03] L. Campi. Some results on quadratic hedging with insider trading. Preprint, Univ. Pescara, Univ. P. et M. Curie, 2003.

[CIKHN03] J. Corcuera, P Imkeller, A. Kohatsu-Higa, and D. Nualart. Additional utility of insiders with imperfect dynamical information. Preprint, September 2003.

[Csi75]

I. Csiszar. I-divergence geometry of probability distributions and minimization problems. Ann. Probab., 3:146≠158, 1975.

[DH86]

D. Duffie and C. Huang. Multiperiod security markets with differential information: martingales and resolution times. J. Math. Econom., 15(3):283≠303, 1986.

[DM78]

C. Dellacherie and P.-A. Meyer. Probabilities and potential, volume 29 of NorthHolland Mathematics Studies. North-Holland Publishing Co., Amsterdam, 1978.

[DS95]

F. Delbaen and W. Schachermayer. The existence of absolutely continuous local martingale measures. Ann. Appl. Probab., 5(4):926≠945, 1995.

[Els96]

J. Elstrodt. Maﬂ- und Integrationstheorie. (Measure and integration theory). Springer-Lehrbuch. Berlin: Springer, 1996.

[GP98]

A. Grorud and M. Pontier. Insider trading in a continuous time market model. International Journal of Theoretical and Applied Finance, 1:331≠347, 1998.

[GV03]

D. Gasbarra and E. Valkeila. Initial enlargement: a Bayesian approach. Preprint, 2003.

[Iha93]

S. Ihara. Information theory for continuous systems. Singapore: World Scientific, 1993.

[Imk96]

P. Imkeller. Enlargement of the Wiener filtration by an absolutely continuous random variable via Malliavin's calculus. Probab. Theory Related Fields, 106(1):105≠135, 1996.

32

[Imk02] [Imk03] [IPW01] [JY85]
[Par77] [PK96] [RY99]

P. Imkeller. Random times at which insiders can have free lunches. Stochastics and Stochastics Reports, 74:465≠487, 2002.
P. Imkeller. Malliavin's calculus in insider models: additional utility and free lunches. Math. Finance, 13(1):153≠169, 2003. Conference on Applications of Malliavin Calculus in Finance (Rocquencourt, 2001).
P. Imkeller, M. Pontier, and F. Weisz. Free lunch and arbitrage possibilities in a financial market model with an insider. Stochastic Process. Appl., 92(1):103≠130, 2001.
Th. Jeulin and M. Yor, editors. Grossissements de filtrations: exemples et applications, volume 1118 of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 1985. Papers from the seminar on stochastic calculus held at the Universit¥e de Paris VI, Paris, 1982/1983.
K.R. Parthasarathy. Introduction to probability and measure. Delhi etc.: MacMillan Co. of India Ltd. XII, 1977.
I. Pikovsky and I. Karatzas. Anticipative portfolio optimization. Adv. in Appl. Probab., 28(4):1095≠1122, 1996.
D. Revuz and M. Yor. Continuous martingales and Brownian motion, volume 293 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer-Verlag, Berlin, third edition, 1999.

33

SFB 649 Discussion Paper Series
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Nonparametric Risk Management with Generalized Hyperbolic Distributions" by Ying Chen, Wolfgang H‰rdle and Seok-Oh Jeong, January 2005.
002 "Selecting Comparables for the Valuation of the European Firms" by Ingolf Dittmann and Christian Weiner, February 2005.
003 "Competitive Risk Sharing Contracts with One-sided Commitment" by Dirk Krueger and Harald Uhlig, February 2005.
004 "Value-at-Risk Calculations with Time Varying Copulae" by Enzo Giacomini and Wolfgang H‰rdle, February 2005.
005 "An Optimal Stopping Problem in a Diffusion-type Model with Delay" by Pavel V. Gapeev and Markus Reiﬂ, February 2005.
006 "Conditional and Dynamic Convex Risk Measures" by Kai Detlefsen and Giacomo Scandolo, February 2005.
007 "Implied Trinomial Trees" by Pavel CÌzek and Karel Komor·d, February 2005.
008 "Stable Distributions" by Szymon Borak, Wolfgang H‰rdle and Rafal Weron, February 2005.
009 "Predicting Bankruptcy with Support Vector Machines" by Wolfgang H‰rdle, Rouslan A. Moro and Dorothea Sch‰fer, February 2005.
010 "Working with the XQC" by Wolfgang H‰rdle and Heiko Lehmann, February 2005.
011 "FFT Based Option Pricing" by Szymon Borak, Kai Detlefsen and Wolfgang H‰rdle, February 2005.
012 "Common Functional Implied Volatility Analysis" by Michal Benko and Wolfgang H‰rdle, February 2005.
013 "Nonparametric Productivity Analysis" by Wolfgang H‰rdle and Seok-Oh Jeong, March 2005.
014 "Are Eastern European Countries Catching Up? Time Series Evidence for Czech Republic, Hungary, and Poland" by Ralf Br¸ggemann and Carsten Trenkler, March 2005.
015 "Robust Estimation of Dimension Reduction Space" by Pavel CÌzek and Wolfgang H‰rdle, March 2005.
016 "Common Functional Component Modelling" by Alois Kneip and Michal Benko, March 2005.
017 "A Two State Model for Noise-induced Resonance in Bistable Systems with Delay" by Markus Fischer and Peter Imkeller, March 2005.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

018 "Yxilon ≠ a Modular Open-source Statistical Programming Language" by Sigbert Klinke, Uwe Ziegenhagen and Yuval Guri, March 2005.
019 "Arbitrage-free Smoothing of the Implied Volatility Surface" by Matthias R. Fengler, March 2005.
020 "A Dynamic Semiparametric Factor Model for Implied Volatility String Dynamics" by Matthias R. Fengler, Wolfgang H‰rdle and Enno Mammen, March 2005.
021 "Dynamics of State Price Densities" by Wolfgang H‰rdle and Zdenk Hl·vka, March 2005.
022 "DSFM fitting of Implied Volatility Surfaces" by Szymon Borak, Matthias R. Fengler and Wolfgang H‰rdle, March 2005.
023 "Towards a Monthly Business Cycle Chronology for the Euro Area" by Emanuel Mˆnch and Harald Uhlig, April 2005.
024 "Modeling the FIBOR/EURIBOR Swap Term Structure: An Empirical Approach" by Oliver Blaskowitz, Helmut Herwartz and Gonzalo de Cadenas Santiago, April 2005.
025 "Duality Theory for Optimal Investments under Model Uncertainty" by Alexander Schied and Ching-Tang Wu, April 2005.
026 "Projection Pursuit For Exploratory Supervised Classification" by Eun-Kyung Lee, Dianne Cook, Sigbert Klinke and Thomas Lumley, May 2005.
027 "Money Demand and Macroeconomic Stability Revisited" by Andreas Schabert and Christian Stoltenberg, May 2005.
028 "A Market Basket Analysis Conducted with a Multivariate Logit Model" by Yasemin Boztu and Lutz Hildebrandt, May 2005.
029 "Utility Duality under Additional Information: Conditional Measures versus Filtration Enlargements" by Stefan Ankirchner, May 2005.
030 "The Shannon Information of Filtrations and the Additional Logarithmic Utility of Insiders" by Stefan Ankirchner, Steffen Dereich and Peter Imkeller, May 2005.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

