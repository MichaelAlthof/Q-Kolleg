BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2007-024
From Animal Baits to Investors' Preference: Estimating and Demixing of the Weight Function in Semiparametric Models
for Biased Samples
Ya'acov Ritov* Wolfgang Härdle**
* The Hebrew University of Jerusalem, Israel ** Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

From animal baits to investors' preference: Estimating and demixing of the
weight function in semiparametric models for biased samples
Ya'acov Ritov
Department of Statistics The Hebrew University of Jerusalem
Wolfgang K. Ha¨rdle 
CASE - Center for Applied Statistics and Economics Institute for Statistics and Econometrics Humboldt-Universita¨t zu Berlin
May 1, 2007
Abstract We consider two semiparametric models for the weight function in a biased sample model. The object of our interest parametrizes the weight function, and it is either Euclidean or non Euclidean. One of the models discussed in this paper is motivated by the estimation the mixing distribution of individual utility functions in the DAX market.
Key Words and Phrases: Mixture distribution. Inverse problem. Risk aversion. Exponential mixture. Empirical pricing kernel. DAX. Market utility function.
AMS subject classification: 62C05, 62G07, 62G30, 62G08, 62P05, 62P20, G2P12.
JEL subject classifications: C10, C14, D01, D81.
Partially supported by an ISF grant. This research was supported by Deutsche Forschungsgemein schaft through the SFB 649 Economic Risk.
1

1 Introduction

A sample X1, . . . , Xn is considered biased if it is sampled from a density p which is represented as

p(x) =

q(x)w(x) q(u)w(u) du

.

(1)

Here q is some `natural' pdf (probability density function) for the problem, representing the `true' underlying distribution, while w is a given weight function that biases the sample. In a standard example, X represents the severity of the disease, and q is the density of X among patients at admission to the hospital. However, it may be more convenient to take a random sample from the population of patients who are in the hospital at a given time. If the time of hospitalization is proportion to the severity of the case, then the sample is taken from the density p, which is equal to q `length biased' with w(x)  x. Vardi (1985) was the first to analyze systematically these type of models. Asymptotic theory was develop in Gill, Vardi and Wellner (1988). Gilbert, Lele, and Vardi (1999) extended the model to the situation where the weight function depends on some parameter, w(x) = w(x; f ). The large sample properties were discussed in Gilbert (2000).
Our paper is about estimating f , the parameter of the weight function, w(x) = w(x; f ). Two semiparametric models are discussed in this paper. The first model is an experimental design problem in which the concept of biased sample is introduced in a situation where a direct sample is in fact impossible. The model for q is semiparametric, while the parameter of interest, f , is Euclidean. In the second model we consider, q is taken as known, while the weight function is parametrized by a non-Euclidean parameter. This brings us to an inverse problem of estimating and demixing the weight.
The purpose of the first model is to design an experiment in order to evaluate the relative attractiveness of different baits. See for example Ji, Veitch, and Craig (1999). It can be described as follows. Suppose that traps are distributed with different baits. We do not know much about the animal population. We do not know even how many animals visited each trap. We know only the number and characteristics of the animals who were tempted by each of the baits and were captured. For simplicity we assume that each trap and bait can attract and catch unlimited number of animals. We want to estimate the attractiveness of each type of bait as a function of

2

the animal characteristics, and do that from the sample in which a datum is a pair of an animal and a bait.
The second model is motivated by research on risk aversion and proclivity, more precisely on the empirical pricing kernel (EPK). See Detlefsen, Ha¨rdle and Moro (2007) (hereafter DHM). The EPK describes the apparent utility behavior as function of individual investors utility function.
In this model q is the risk neutral density of assets pricing. It is derived from theoretical considerations. The density p on the other hand is the empirical (historical) prices. See Figure 1 (a) and (b) for an example. In asset pricing the EPK links risk neutral investor's behavior to individual utilities, which gives in our notation a semiparametric modeling of the weight function w. The integral function of the pricing kernel q/p is the utility function used by a representing individual. Knowing p and q yields the exact form of the utility function, cf. Ait-Sahalia and Lo (2000). The risk neutral (state price) density (SPD) q can be calculated from the market data on European options, in the DAX data-set. There are more than 5000 observations each day for maturity from 1 week to 2 years. The SPD can therefore be estimated very precisely. Much empirical research work has demonstrated the so called EPK paradox: The resulting utility function is partially concave and partially convex, more precisely of the Friedman and Savage type. See Friedman and Savage (1948).
It is assumed in DHM that the observed density of the DAX value has density of the form p(x) = cq(x)w(x; f ) where q  {q,   N  Rd} is theoretical derived risk neutral density, assumed to follow a given parametric function and c is a normalization factor. That is, of the type of equation (1). The weight function is theoretically derived to be given by 1/U (x), where U is the market utility function, and prime denotes derivative. The market utility function itself is assumed to be a function of the mixture of the individual investors:
x = U -1(u) = g(u; )f () d.

Here  denotes an investor type, f is the density of the investors' distribution, and the function g(·; ·) is the inverse utility function and it is considered known. A subject of type  has the inverse utility function g(·; ), or equivalently, the utility function u(·; ) satisfying g{u(x; ); }  x. The problem we consider in this paper is to find the density f . We obtain the representation:

p(x) = cq(x)

 u

g(u;

)f

()

d,

with x =

g(u; )f () d.

(2)

3

Distribution of the Switching Points (2000)

2

x 10-3 1

10 15 20

1.8

1.6 0.8

Probability*E-2

1.4 1.2 0.6

1 0.4
0.8

5

0.6
0.2 0.4

0

0.2
0 0.5

1 1.5 return

0 0.6 0.8 1 1.2 1.4 1.6 1.8 2
2 return

(a) (b)

0.5 1 1.5 Returns
(c)

2

Figure 1: The DAX data, 24/03/2000 half a year look ahead: (a) p, the historical density; (b) q, the risk neutral density; (c) The estimate of f , the mixing density. Figures are taken from DHM

See Figure 1 for an example taken from DHM of estimates of p, q, and f . We will investigate the estimation rate for a few utility functions. The
result is typical for inverse problems, in that slightly different assumption will yield completely different results. In fact, we will present three similar models, similar to those investigated in DHM, that exhibit three different type of behavior:
(i) There is no consistent estimator of f ;
(ii) f can be estimated at a regular nonparametric rate of n-;
(iii) f can be estimated but at a very slow rate.
We will also show a sort of uncertainty principle. The better we can estimate the function U -1(u), the worse we can demix it and estimate f . This is reasonable. We cannot estimate f well, when large differences in f have only minor impact on g(·; )f () d.
There are two unified themes to the paper. The first is the usefulness of the concept of biased sample of unknown weight function, even in models where this point of view is not automatic. The second is the technical analysis of inverse problem which starts with a naive estimator that not much is known about it, and then improving it. The structure of the rest of the paper is as follows. In Section 2 we consider a parametric model for the weight function, and show the simplicity of the result. Then, in Section 3 , we suggest an algorithm for calculating the generalized maximumlikelihood estimator (GMLE) for the semiparametric weight function of the

4

model suggested by DHM. Finally rates of convergence of the demixing estimator for the DHM's model are discussed in Section 4 , as well as of estimates of the mixture itself.

2 The parametric bait problem

Animals of type z are trapped by a bait of type u with probability which is proportional to w(z, u; f ). We do know about the animals only after they are caught by the traps. Our aim is to design an experiment and to estimate f . The solution in a nutshell is to distribute many different baits and compare the difference between the animals captured by the different baits, somewhat similar to what is done in case-control studies.
Let X = (U, Z) where U denotes the bait property (e.g., percentage of fat) and Z is a vector of measurement for each animal (e.g., gender, weight, and age). Baits with known (designed!) distribution g are spread, and are encountered by the animals. We assume therefore that the data is sampled from the joint density given by

p(u, z) =

w(u, z; f )g(u)h(z) w(u~, z~; f )g(u~)h(z~) µ. (u~) d(z~)

(3)

with respect to the product measure µ × , and where w(u, z; f ) is some parametric weight function, for example, if u is a scalar and z  Rd, we can
consider

w(u, z; f ) = exp{uf Tz}.

(4)

Our aim is to estimate f . The density h is an unknown distribution over Rd, d large.
Suppose we consider the weight function (4), then the likelihood equa-
tion for f is

0=

UiZi - n

uzeufTzg(u)h(z) dµ(u) d(z) eufTzg(u)h(z) dµ(u) d(z)

Assuming that Z has at least one Lebesgue continuous component, the GMLE (generalized maximum likelihood estimator) of its distribution is discrete with a point mass at each observation, so that we obtain

n
0 = UiZi - n
i=1

j

j

uzjeufTzj g(u)h^j dµ(u) eufTzj g(u)h^j dµ(u)

.

5

where h^j is the estimated mass at the point Zj. However, the likelihood equation for h is

0

=

1 h^ j

-n

eufTZj g(u) dµ(u)

n i=1

eufTZi g(u) dµ(u)h^i

Plugging this into the profiled log-likelihood equation for f we obtain

0

=

1 n

(f )

=

1 n

n

UiZi -

n

i=1 j=1

uzjeufTzj g(u) dµ(u) eufTzj g(u) dµ(u)

=

1 n

n

Zi

Ui - Ef (U |f TZi)

i=1

=

1 n

n i=1

T (Ui, Zi; f ),

say.

For example, in the simple case where g is uniform on the interval (a, b):

Ef (U |z) =

b a

ueuf

Tz

dz

b a

euf Tz

du

=

bebfTu - aeafTz ebf Tz - eafTz

-

1 f Tz

if f Tz = 0, and (a + b)/2 otherwise.

Generally, the derivative of Ef (U |f TZi) is - Varf (U |f Tz)ZZT. Hence is concave in f . This yields that the maximizer of is simple to find

and is asymptotically normal with asymptotic covariance function given

by E Varf (U |f Tz)ZZT

3 EPK: Model and an EM estimator

We consider now the EPK problem. We start now from (2) and we assume

that q is known. In practice, it is assumed only to belong to some parametric

family slower

t{hqan}.thHe opwareavmere,trwicewnilrlatdee,aalnidnththeeesfotilmloawteinogf

in rates which are  is based on much

larger sample than the estimates of the rest of the parameters. Therefore,

the assumption that  is known simplifies the discussion without a real

impact on the results.

Rewrite (2) as

p g(u; )f () dµ()

 u

g(u;

)f

()

dµ()

= cq g(u; )f () dµ()

 u

g(u;

)f

()

dµ()

2
,

(5)

6

where µ is some dominating measure (i.e., the Lebesgue or the counting meausres). Noting that the LHS of (5) is integrated to 1, we can solve for c and obtain

p

g(u; )f () dµ() =

q g(u; )f () dµ() q g(v; )f () dµ()

 u

g(u;

)f

()

dµ()

 u

g(v;

)f

()

dµ()

2 dv

The market utility U (x) = U (x; f ) is given by

x  g U (x; f );  f () dµ()  f U (x; f ) .

We obtain

p(x) = =

q(x)

 u

g(U

(x;

f

);



)f

()

dµ()

q(y)

 u

g(U

(y;

f

);

)f

()

dµ()

dy

q(x)f q(y)f

f-1(x) f-1(y)

dy .

(6)

The statistical model assumed by DHM is that we obtain a simple random sample from p, where p is parametrized in (6) by the non-Euclidean parameter f . A natural approach is to estimate f by the MLE or a variant of it, which we develop now. Note that f f (u) = g(u; ·), and by taking the gradient of x  g f-1(x);  f () dµ() we obtain

0 = g f-1(x); · + f f-1(x) f f-1(x).

The derivative of the log-likelihood is given therefore by

n
f () = i=1 f

1 f-1(Xi)

 u

g

f-1(Xi); 

-

f f

- nAf (),   supp f

=

n i=1

f

1 Ui

 u

g

Ui; 

-

f f

(Ui)g(Ui;

)

Ui = f-1(Xi),   supp f,

f-1(Xi) g - nAf (),

f-1(Xi); 

where Af () is the mean of the first term under f . Since the density of Ui is given by

rf (u) = p f (u) f (u) =

q f (u) q f (v)

f (u) f (v)

2
2 dv

.

7

We obtain that

Af () =

q f (u)

f

(u)

 u

g(u;



)

-

f

(u)g(u;

)

q f (v) f (v) 2 dv

du .

The model of random sample from the density p can be well approx-
imated as   0 by a Xi = f (Ui) + i, i = 1, . . . , n, where 1, . . . , n is a random sample from N (0, 2) independent from the random sample
U1, . . . , Un taken from the density rf . Now, the log-likelihood of the joint density is given by

n
f = log q f (Ui) + 2 log f (Ui)
i=1
where

-

nCf

-

1 22

n
(Xi - f (Ui))2,

i=1

Cf = log q f (v) f (v) 2 dv.

By a well known formula for the Bayes estimator in the Gaussian measure-
ment error model, under the above model, the distribution of f (Ui) - Xi given Xi is normal with mean 2fX (Xi)/fX (Xi) and a second moment equal to 4fX (Xi)/fX (Xi) + 2, where fX is the marginal density of Xi. At the limit as 2  0, the conditional expectation of the log-likelihood given the Xis amounts therefore to replacing Ui by f-1(Xi). We conclude that the limiting EM algorithm iterates therefore between

The E step: Ui  f-1(Xi), i = 1, . . . , n,

and

n

The M step: f  arg max

log q f (Ui) + 2 log f (Ui)

i=1

- nCf .

Let U = (U1, . . . , Un), X = (X1, . . . , Xn), and denote the E-step by U = f-1(X). The M-step can be solved by solving the likelihood equation:

0

=

M
f

(;

U)

n
=
i=1

q q

f (Ui) f (Ui)

g(Ui;

)

+

f

2 (Ui)

 u

g(Ui,

)

-

C f

()

,

(7)

8

for all   supp f , where

C f () =

q q

f (v) f (v)

g(v; ) +

2 f (v)

 u

g(v,



)

q

f (v)

q f (v) f (v) 2 dv

= Ef

q q

f (U ) f (U )

g(U ;

)

+

2 f (U )

 u

g(U,

)

= Ef Tf (U ; ) , say.

f (v) 2 dv

An approximate M-step (which is enough, since all we need in the M-step
aisnthapatptrhoexilmikaetleihNooedwtwoinll-Rbaepshtrsiocntlysoilnuctrieoanseodf )(7is),owbthaeinreedOpb(ycno)ntseidrmersining the Hessian of the log-likelihood are discarded. That is the term

n
f Tf (Ui; ) - Ef f Tf (U ; ) .
i=1
We consider therefore the algorithm:

fi+1

= fi + Hf-i1

M fi

·; f-i1(X)

,

i = 1, 2, . . . ,

where Hf : L2(µ)  L2(µ) is the operator given by:

Hf (, ) = covf Tf (U ; ) , Tf (U ; ) .

4 EPK: Rates of convergence

In the previous section we considered the MLE estimate of f . In this section we consider simple estimators of the type suggested by DHM. Using these estimators we will be able to discuss possible minimax rates of convergence. In essence, we start with some naive nonparametric estimator and improve it or demix it for f . One simple method for demixing the EPK is to start with (2) which can be written as

1=c

 u

g(u;

)f ()

d

q p

g(u; )f () d

=

c

 u

q p

g(u; )f () d .

Hence

q p

g(u; )f () d =  + u

9

for some  and , or

g(u; )f () d =

p q

-1
( + u).

The utility function of an individual is defined up to affine transformation.

To assure that it is well defined, we assume that that utility at a return of 1

is 0, and its derivative there is 1. In other words we assume that g(0, ) 

 u

g(0,

)



1.

Hence



=

p(1) q(1)



=

p (1) q(1)

-

p(1) q(1)

q (1) q(1)

.

We therefore want to solve

g(u; )f () d = (u),

(8)

for some . Since q is estimated as a parametric density (based on a much larger sample), and p can be estimated at a standard non-parametric rate based on a direct sample from p,  can as well be estimated at a regular density estimation rate. The analysis of this section starts with (8). We assume that  and its relevant derivatives can be estimated in some polynomial rate ^(i) -i  = Op(n-i) for some i > 0. The natural estimator suggested by DHM is given by the inverse function of a weighed density estimator. Under strict monotonicity and boundness, the inverse function inherits most properties from the density kernel estimator.
Note that model (8) looks like a linear model. For example if f is approximated by a finite distribution with point mass at 1, . . . , m, and we consider the equation at the k points u1, . . . , uk then we can write (8) as

m
^(ui) = jg(ui; j) + i,
j=1

i = 1, . . . , k.

(9)

(9) looks like a standard linear model, and indeed we suggest to estimate
f by solving it. However, it is not. Most linear model assumptions are vio-
lated. E.g., 1, . . . , k are not i.i.d. , they are not independent of the random u1, . . . , uk, which are in fact an estimated function of the observed values x1, . . . , xk.
The basic idea of this section is as follow. We assume that we have some
naive nonparametric estimator of . We then proceed to use the pseudo

10

linear model (9) to to estimate the mixing distribution and to improve the estimate of  itself. We show that this method yields the minimax rates.
How fast can f be estimated? In the rest of the section we present simple examples following DHM. These examples show that in a very similar models very different type of behavior can be obtained. It can be that (i) There is no consistent estimator of f ; (ii) f can be estimated at a regular nonparametric rate of n-; (iii) f can be estimated but at a very slow rate. Thus one can suspect that any optimistic result of demixing depends too heavily on assumptions, and are a priori not robust (at least in the minimax sense). In particular, any result should be checked to stand against different changes in the model.

4.1 Switching between two utilities
Following DHM assume that for x,  > 0:
U (x; ) = 2(1 - c)1-1/2{[x - ]+1/1  (x - c)1/2 } - 2(1 - c), (10)
where 2 > 1 > 1 are given, c < 0, and [x]+ = x1(x > 0). See Figure 2 .
Then g(u; ) = min{2{u + 2(1 - c)}2 + c, 1{u + 2(1 - c)}1 + },
where  = 2-1(1 - c)-1+1/2. To simplify and generalize the discussion, we consider the slightly more general case:

g(u; ) = g2(u)

- < u  h() ,  > 0,

g1(u) +   > u > h()

where

h-1 = g2 - g1

(11)

is a strictly increasing function. Note that g(u; ) is continuous in . Then (8) is translated to

h-1(u)

(u) =

f () d + g2(u)F h-1(u) + g2(u) 1 - F h-1(u) ,

where F is the cdf corresponding to the pdf f . Changing variables and considering (11)
s
 h(s) = f () d - sF (s) + g2 h(s) .

11

U(x)

14
12
10
8
6
4
2
0
-2 0 5 10 15 20 25 x
Figure 2: The utility function U (·; ) of (10) (1 = 2, 2 = 2.25, c = 2) for two different values of  (solid lines), and of (12) for two values (broken lines).
Taking derivative:
F (s) = h (s) g2 h(s) -  h(s) .
Hence estimating F at s is equivalent to the estimation of  at h(s). In other words, f (·) can be estimated at the same rate as the rate of the estimation of second derivative of , which in turn is essentially governed by the rate of estimation of the second derivative of p, which depends on the level of smoothness assumption we are willing to accept. Thus if we assume that p has s bounded derivatives, then f can be estimated with an Op(n-(s-2)/(2s+1)) error. See Silverman (1986) for a general review of density estimation.
4.2 Polynomial and exponential inverse utility function
The previous example was a relatively optimistic example. However, modest changes in the inverse utility function may create situations in which f can hardly estimated, or even not at all.
12

Suppose a CRRA (constant relative risk aversion) utility: g(u; ) = (-1)-1 (u + ) -  + 1, u  R,   R+,

where  is known. Note that g is scaled such that both its value and derivative at 0 are equal to 1. That is, we consider only one branch of (10). If  is an integer, then (·) is a function of only the first  moments of f , and hence there is no consistent estimator of f .
Seemingly, more and more moments are revealed as   . However, it is not clear that they could be estimated effectively. The limiting form of the inverse utility function, as    and /   is given by

g(u; )  -1(eu - 1) + 1.

(12)

The density f is now identified. For example, all its moments can be estimated, e.g., by if () d = (i+1)(0). We are going now to analyze this model in some detail. We will argue now that if f (·) is assumed to have two bounded derivatives, then its value at a point can indeed be estimated, but this can be done only in a very slow convergence rate, slower than any polynomial rate. To be more exact,we argue

Theorem 4.1 Assume that g is given by (12) and f is bounded and has two bounded derivatives. Suppose the minimax rate of estimation of  is n,   (0, 1/2). Then there is an estimator f^such that f^(s)-f (s) = Op(n- log log n/ log n) for some , and there is no estimator f~(s), such that f~(s)-f (s) = Op(n-/ log log n) for some .
The proof is given in Appendix A .

4.3 Smoothing the empirical estimate and an uncertainty principle
As in the previous subsections we start with a nonparametric ^. The purpose of this subsection is to show that a simple projection of this initial estimator yields a considerably better estimator.
We argued in Subsection 4.2 there that there is no reasonable estimator of f for g given in (12). Is this model useless? The surprising answer is no. Although f cannot be estimated per-se, many of its functionals can be estimated quite easily and quite well. For example, as mentioned, its moments. Similarly (u) considered as a simple linear functional can be

13

estimated quite easily. Suppose that f is supported on some compact interval [a, b]. Then one can approximate

m
(u) = iui + Rm(u)
i=1

where for some u~  (0, u):

0



Rm(u)

=

(m

1 +

1)! m+1(u~)

=

(m

1 +

1)!

b
meu~f () d
a



bmeub (m + 1)!

.

(13)

Generally speaking, the faster the coefficients  are converging to 0, the

easier it is to estimate  and the harder it is to estimate the mixing density

g. As (13) shows, we need only very few terms to approximate  quite

well. In fact we show that in this smooth case, where on one hand f can

be hardly estimated,  can be estimated almost at the parametric rate. This

is not an accident -- these are two faces of one phenomena. The shape

of the observable  hardly depends on f , and essentially depends only

on a few aspects of f . These aspects can be estimated well (and hence 

can be estimated well too). The other aspects can hardly be estimated and

hence f cannot be estimated in a reasonable rate. This yields an uncertainty

principle -- the more you are certain about  the less you are about f .

Recall that a function g is called completely monotone if (-1)kg(k) 

0, and it is called Bernstein function if its first derivative is completely

monotone. It is well known (Feller, 1996) g is completely monotone if, and

only if g(u) =

 0

e-u

dF

(

).

In other words,  is a Bernstein function.

Nonparametric maximum likelihood estimation for exponential mixture

(and hence completely monotone density) was discussed in Jewel (1982).

Balabdaoui and Wellner (2007) discussed the estimation of a k-monotone

density. We assume that at our disposal there is an estimate ^ = ^n. For any
u1, . . . , uk > 0, let (u1, . . . , uk)  Rk×k, where

ij(u1, . . . , uk) = cov ^(ui) , ^(uj) .

Consider the following assumption:

14

A1. For any n there is k = kn, and u1, . . . , uk  (c, d), 0 < c < d, such that the spectral radius of (u1, . . . , uk) is O(k/n) and maxi |E (ui) - (ui)|2 = O(log n/n).

Assumption A1 is satisfied by many nonparametric density and regression

estimators when they are strictly under-smooth. We care much more about bias than about variance of the original estimator ^. Thus, we have in mind

a kernel estimator with bandwidth of order n-1/4+. The spectral radius is

based on the assumptions that the estimator at points that are a multiply of

the bandwidth apart are (almost) independent, for example this is trivially

the case with kernel estimator with compact support. The relationships in

the assumption are derived from assuming that the bias of the estimator is

O(2), the variance is O(1/n), and k = O(-1).

Consider now the least squares regression of Y = ^(u1), . . . , ^(uk) T

on the design matrix Z  Rk×m, Zij = uji . That is ^ = (Z Z)-1Z Y , where

^  Rm. Finally let ~(u) =

m j=1

^j

uj

,

u

>

0.

We argue that the error

achieved by ~ is almost the parametric rate even although ^ achieves can

be estimated in strictly lower rate.

Theorem 4.2 Suppose g(u; )  -1(eu - 1) and that f is supported on a

compact interval. Assume A1 holds and m = mn = log n/ log log n. Then

k-1

k i=1

~(ui) - (ui)

2 = Op

(log n)2/n

.

Proof. Let 0 be the true value j0 = j-1f () d/j!. Write Y = Z + , where  include both the random error and the bias terms due to both the estimator and the truncation. The latter term is given in (13). By standard least squares results

k

k-1E

~(ui) - (ui) 2 = k-1E TZ(ZTZ)-1ZT

i=1

= k-1 trace Z(ZTZ)-1ZTE(T) .

Since Z(ZTZ)-1ZT is a projection matrix on a m-dimensional space, the RHS is bounded by the largest eigenvalue of E(T) times m/k. This has
three sources (variance and two biases) and hence

k
k-1E
i=1

~(ui) - (ui)

2=O

m k

k n

+

k

log n

n

+

k

bm m!

2

.

15

The factor k before the last two terms is due to the norm of the unit vector in Rk, and, the last term is by (13). The theorem follows by taking m =
log n/ log log n
A more general result can be based on an assumption like the following

A2. Assume that for some c, d and each  there are h,1, . . . , h,M() such that

M ()

sup min max
  c<u<d

g(u; ) -

j=1

j hj (u)

<

Note that clearly the assumption ensures the existence of (·) such that

maxc<u<d |g(u; ) -

M () j=1

j

()hj

(u)|

<

, but then there are also j

=

j()f () d, j = 1, . . . , M (), such that maxc<u<d |(u)-

M () j=1

j hj (u)|

<

.

The following theorem can be proved similarly to Theorem 4.2 :

Theorem 4.3 Suppose assumptions A1 and A2 hold. Define n by

n = arg min{M ()/n + }.


Let ~ be the least squares estimate of the regression of ^ on hn,1, . . . , hn,M(n).

Then k-1

k i=1

~(ui) - (ui)

2 = Op(n).

In practice, Theorems 4.2 and 4.3 may seem to be of a limited use -- a knowledge of the structure of the span of the individual utility functions is needed, and the regression is based on an identified efficient base, which may be not natural. For example, we used a polynomial base for the exponential utility function. The practical approach is an histogram or discrete approximation of f . We want now to discuss when such a procedure does yield an effective estimator, an estimator which is both statistically speaking efficient, but at the same time easy to compute and can be be used in off-the-shelf manner.
This is indeed the case. Let 1, . . . , M() be reasonably spaced points in the support of f . With the notation introduced after Assumption A2 , and by a similar argument, for a vector  on the simplex

M ()

M () M ()

sup jg(u; j) - j l(j)hl(u)  .

u j=1

j=1 l=1

Hence, one can use the base function g(·; 1), . . . , g(·; M()) as well.

16

A Appendix: Proof of Theorem 4.1 .

We start the proof with the negative result. The proof is standard. We exhibit a small perturbation that cannot be detected. The perturbed density should remain a probablity density function with bounded second derivative. It should be however very wiggly so that the exponential mixing would smooth it out to make it hardly detectable through . Very convenient candidates could be high derivatives of the normal density, but the supports of these functions are not bounded, while the support of f is bounded at least from below. We therefore use derivatives of approximations of the normal density. Here are the details.
Consider

m() = m(; c, d) =

1-

-c d

2

m1{  (c - d, c + d)}

for some c, d, where 1 denotes the indicator function. m is approximately
the normal pdf normalized improperly, cf. (24) below. Note that for k  m:

c+d
eum(k)() d = (-1)kuk
c-d

eum() d.

(14)

and

m(2k)(c) = (-1)kd-2k

m k

(2k)!

(15)

Write

m()

=

(1

-



- d

c )m(1

+



- d

c )m

and taking the derivative of the RHS:

m2k ( )

2k
= d-2k

2k i

(-1)i

m! (m -

i)!

(1

-

~)m-i

(m

m! - 2k

+

i)!

(1

+

~)m-2k+i

i=0

k
= d-2k (-1)iai,

say.

i=0

For simplicity we write ~ = ( - c)/d. Note that

(16)

ai+1 ai

=

2k - i m - i 1 + ~ i + 1 m - 2k + i + 1 1 - ~

17

Hence the sum in the RHS of (16) is of unimodal terms with alternating signs. Let l be the index where the maximum is achieved:

2k - l l+1

m

m-l - 2k + l

+

1

=

{1

+

O(1)} 1 1

- +

~~.

(17)

Then

al  al - (al+2j-1 - al+2j ) - (al-2j+1 - al-2j )
j=1 j=1
2k
= (-1)l (-1)iai
i=0
= al - al+1 + (al+2j - al+2j+1) - al-1 + (al-2j - al-2j-1)
j=1 j=1
 -al

(18)

where, if necessarily, the sequences are padded by zeros at the ends. But then for some C = O(1), C may vary from line to line:

al = (2k)!

m l

m 2k - l

(1 - ~)m-l(1 + ~)m-2k+l



C (2k)!

ll(m

-

m2m(1 - ~)m-l(1 + l)m-l(2k - l)2k-l(m

~)m-2k+l - 2k + l)m-2k+l

= C(2k)! (1 - ~2)m-2k

k(1 - ~) 2k - l

2k-l

k(1 + ~) l l

m 2k k

×

1

+

m

l -

l

m-l

1

+

m

2k - - 2k

l +

l

m-2k+l

(19)

To deal with the following terms of the RHS of (19) we assume that 0 < l  k. The case 2k > l  k is dealt similarly. The cases of l  {0, 2k} are simple

k(1 - ~) 2k-l 2k - l

k(1 + ~) l

l

=

k(1 - ~) 2(k-l) 1 + ~ 2k - l l

2k - l

1 - ~ l

 2k

1 + ~2k - l 1 - ~ l + 1

l

 3k

m - 2k + l + 1 m-l

l
,

by (17)

 3k

(20)

18

The next bound is easy,

1

+

m

l -

l

m-l

1

+

m

2k - - 2k

l +

l

m-2k+l < e2k,

(21)

since (1 + 1/x)x < ex for any x > 0. We conclude from (16), (18), (20), and (21):

m(2k)

  al  C(2k)!

c2

m k

2k

(22)

for c2 > 1. Let

m,k ( )

=

(2k

+

d2k 2)!(c1c2)2k

m(2k)()

where we take m = c1k . Note that by (22) (m2,)k is uniformly bounded, while by (15)

m,k(c)  c3k-2(c1c2)-2k

m k

 c-4 k

(23)

for some c4 > 1. However by (14)

-1 eu - 1 f () + m,k() d

=

(u)

+

d-2(m-k) (2k + 2)!c22k

u2k

b
m()eu d
a

= (u) + (-1)k = (u) + (-1)k

1 + O(1) 1 + O(1)

(2k+d222k)d!c222kk+u1 2k (2k + 2)!m1/2c22k

b
e-m(-c)2/d2
a
u2k euc .

eu

d

Hence if

d2k+1 (2k + 2)!m1/2(c1c2)2k

=

O(n-1/2),

or k log k - log n  , then one would not be able to test between f to f + m,k. In particular this happens when k = log n/ log log n. However, then, by (23), nm,k(c)   for any  > 0. This proves that f can be estimated in any n,  > 0 rate.

19

We move now to the positive result. We suggest an estimator of the
mixing density f whose rate of convergence is easy to evaluate. Of course,
the practical way would be the standard least squares as discussed in Sub-
section 4.3 , but then rates are difficult to evaluate. We suggest therefore in the proof a kernel estimator of g given by ^(u)K¯ (u) du for some K¯ given
below. Here are the details. If (u) = g(u; ) f () d, let s = s(u) = e-us((u)-1). Assume for
simplicity that by assumption f () = 0 for   (s0 - d, s0 + d). Then since

s(u) = eu(-s)-1f () d - e-us -1f () d s(k)(u) = ( - s)keu(-s)-1f () d - (-1)kske-us -1f () d,

then formally:

mmm

2d2

k

k=0

-1 d2

k s(2k) (u)

=

m 2d2

m(; s, d)eu(-s)-1f () d

-

m 2d2

m(s;

0,

d)e-us

-1f () d

where m(·) = m(·; s, d). Note that for any smooth bounded function h with two bounded derivatives:

m 2d2

m(; s, d)h() d 

m d2

  m( - s)/d h() d

 h(s) + O(m-1),

(24)

where  is the standard normal density. Hence

mmm

2d2

k

k=0

-1 d2

ks(2k)(u)  s-1(s)

as m  .

(25)

Let ^s be an estimator of s. Let K be a smooth kernel of order 2m, integrated to 1, and with bounded support kernel. Then by (25) we can

20

estimate f (s) by

f^(s) = s =s

mm
2d2
k=0
mm
2d2
k=0

m k
m k

= K¯ (u)^s(u) du

-1 k d2
-1 k d2

K(u)^s(2k)(u) du K(2k)(u)^s(u) du

(26)

where

K¯ (u)  s

mm 2d2

m k

k=0

-1 d2

k K (2k) (u)

Since we have already developed the machinery we pick

K(u) = m

2m 22

2m(u;

u0,

)

where m = 1 + O(1). Hence by (22)

K¯





s

m 2d

m

m k

k=0

cm k

2k(2k)! = O(cmmm).

(27)

If s can be estimated at a standard polynomial rate, ^-  = Op(n-), then we obtain from (26) and (27) that ^ induce an error of O(cmmm/n). To this we have to add the bias of O(m-1) as given by (24). The minimization of
the error estimate is obtained therefore of the order of the value at m when
these two terms are equal:

m log m -  log n = log m.

By taking m = mn =  log n/ log log n we achieve the rate of
f^(s) - f (s) = Op n- log log n/ log n ,
for any  < 1. We have shown that the optimal rate of convergence is nn for some n  0 slowly.

21

REFERENCES
Ait-Sahalia, Y. and Lo, A. (2000). Nonparametric risk-management and implied risk aversion. Journal of Econometrics, 94.
Balabdaoui, F. and Wellner, J. A. (2007). Estimation of a k-monotone density: limit distribution theory and the spline connection. Manuscript.
Detlefsen K., Ha¨rdle, ,W.K., and Moro, R. A. (2007) ,Empirical Pricing Kernels and Investor Preferences. Manuscript
Feller, W. (1966): An introcustion to Probability Theory and Its applications, vol. II Wiley, New-York.
Friedman, M. and Savage, L. P. (1948). The utility analysis of choices involving risk. Journal of Political Economy, 56, 279304.
Gilbert, P. B. (2000): Large sample theory of maximum likelihood estimates in semiparametric biased sampling models. The Annals of Statistics 28, 151­194.
Gilbert, P. B., Lele, S. R., and Vardi,Y.(1999). Maximum likelihood estimation in semipara- metric selection bias models with application to AIDS vaccine trials. Biometrika 86 27­43.
Jewell, N. P. (1982). Mixtures of exponential distributions. Ann. of Statis. 10, 479­482.
Ji, W, H., Veitch, C. R., and Craig, J. L. (1999). An evaluation of the efficiency of rodent trapping methods: The effect of trap arrangement, cover type, and bait, New Zealand Journal Of Ecology 23, 45­51.
Gill, R. D., Vardi, Y., and Wellner, J. A. (1988).Large sample theory of empirical distributions in biased sampling models. Ann.Statist. 16 1069 1112.
Silverman, B., (1986). Density Estimation. Chapman and Hall, London.
Vardi, Y. (1985). Empirical distributions in selection bias models. Ann.Statist. 13, 178-203.
22

SFB 649 Discussion Paper Series 2007
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Trade Liberalisation, Process and Product Innovation, and Relative Skill Demand" by Sebastian Braun, January 2007.
002 "Robust Risk Management. Accounting for Nonstationarity and Heavy Tails" by Ying Chen and Vladimir Spokoiny, January 2007.
003 "Explaining Asset Prices with External Habits and Wage Rigidities in a DSGE Model." by Harald Uhlig, January 2007.
004 "Volatility and Causality in Asia Pacific Financial Markets" by Enzo Weber, January 2007.
005 "Quantile Sieve Estimates For Time Series" by Jürgen Franke, JeanPierre Stockis and Joseph Tadjuidje, February 2007.
006 "Real Origins of the Great Depression: Monopolistic Competition, Union Power, and the American Business Cycle in the 1920s" by Monique Ebell and Albrecht Ritschl, February 2007.
007 "Rules, Discretion or Reputation? Monetary Policies and the Efficiency of Financial Markets in Germany, 14th to 16th Centuries" by Oliver Volckart, February 2007.
008 "Sectoral Transformation, Turbulence, and Labour Market Dynamics in Germany" by Ronald Bachmann and Michael C. Burda, February 2007.
009 "Union Wage Compression in a Right-to-Manage Model" by Thorsten Vogel, February 2007.
010 "On -additive robust representation of convex risk measures for unbounded financial positions in the presence of uncertainty about the market model" by Volker Krätschmer, March 2007.
011 "Media Coverage and Macroeconomic Information Processing" by Alexandra Niessen, March 2007.
012 "Are Correlations Constant Over Time? Application of the CC-TRIGt-test to Return Series from Different Asset Classes." by Matthias Fischer, March 2007.
013 "Uncertain Paternity, Mating Market Failure, and the Institution of Marriage" by Dirk Bethmann and Michael Kvasnicka, March 2007.
014 "What Happened to the Transatlantic Capital Market Relations?" by Enzo Weber, March 2007.
015 "Who Leads Financial Markets?" by Enzo Weber, April 2007. 016 "Fiscal Policy Rules in Practice" by Andreas Thams, April 2007. 017 "Empirical Pricing Kernels and Investor Preferences" by Kai Detlefsen,
Wolfgang Härdle and Rouslan Moro, April 2007. 018 "Simultaneous Causality in International Trade" by Enzo Weber, April
2007. 019 "Regional and Outward Economic Integration in South-East Asia" by
Enzo Weber, April 2007. 020 "Computational Statistics and Data Visualization" by Antony Unwin,
Chun-houh Chen and Wolfgang Härdle, April 2007. 021 "Ideology Without Ideologists" by Lydia Mechtenberg, April 2007. 022 "A Generalized ARFIMA Process with Markov-Switching Fractional
Differencing Parameter" by Wen-Jen Tsay and Wolfgang Härdle, April 2007.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "Time Series Modelling with Semiparametric Factor Dynamics" by Szymon Borak, Wolfgang Härdle, Enno Mammen and Byeong U. Park, April 2007.
024 "From Animal Baits to Investors' Preference: Estimating and Demixing of the Weight Function in Semiparametric Models for Biased Samples" by Ya'acov Ritov and Wolfgang Härdle, May 2007.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

