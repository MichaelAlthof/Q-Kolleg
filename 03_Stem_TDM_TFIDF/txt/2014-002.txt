BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2014-002
A Simultaneous Confidence Corridor for
Varying Coefficient Regression with Sparse
Functional Data
Lijie Gu* Li Wang** Wolfgang Karl H‰rdle*** Lijian Yang*/****
* Soochow University, China ** University of Georgia, USA *** Humboldt-Universit‰t zu Berlin, Germany **** Michigan State University, USA This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk". http://sfb649.wiwi.hu-berlin.de
ISSN 1860-5664 SFB 649, Humboldt-Universit‰t zu Berlin
Spandauer Straﬂe 1, D-10178 Berlin

A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data
1

Lijie Gu Center for Advanced Statistics and Econometrics Research
Soochow University Suzhou 215006, China email: gulijie@suda.edu.cn
Li Wang Department of Statistics
University of Georgia Athens, GA 30602 email: lilywang@uga.edu
Wolfgang K. Ha®rdle C.A.S.E. ≠ Center for Applied Statistics and Economics
Humboldt-Universita®t zu Berlin Unter den Linden 6
10099 Berlin, Germany email: haerdle@wiwi.hu-berlin.de and
Lee Kong Chian School of Business, Singapore Management University Lijian Yang
Center for Advanced Statistics and Econometrics Research Soochow University
Suzhou 215006, China email: yanglijian@suda.edu.cn and
Department of Statistics and Probability Michigan State University
East Lansing, MI 48824 email: yang@stt.msu.edu
2

Author's Footnote:
Lijie Gu is Ph.D. student, Center for Advanced Statistics and Econometrics Research, Soochow University, Suzhou 215006, China (E-mail: gulijie@suda.edu.cn). Li Wang is Associate Professor, Department of Statistics, University of Georgia, Athens, GA 30602 (E-mail: lilywang@uga.edu). Wolfgang K. H®ardle is Professor, C.A.S.E. ≠ Center for Applied Statistics and Economics, Humboldt-Universita®t zu Berlin, Unter den Linden 6, 10099 Berlin, Germany, and Distinguished Visiting Professor, Lee Kong Chian School of Business, Singapore Management University (E-mail: haerdle@wiwi.hu-berlin.de). Lijian Yang is Director, Center for Advanced Statistics and Econometrics Research, Soochow University, Suzhou 215006, China, and Professor, Department of Statistics and Probability, Michigan State University, East Lansing, MI 48824 (E-mail: yanglijian@suda.edu.cn; yang@stt.msu.edu). This work is supported in part by the Deutsche Forschungsgemeinschaft through the CRC 649 "Economic Risk", the US National Science Foundation awards DMS 0905730, 1007594, 1106816, 1309800, Jiangsu Specially-Appointed Professor Program SR10700111, Jiangsu Province Key-Discipline Program (Statistics) ZY107002, National Natural Science Foundation of China award 11371272, and Research Fund for the Doctoral Program of Higher Education of China award 20133201110002.
3

Abstract We consider a varying coefficient regression model for sparse functional data, with time varying response variable depending linearly on some time independent covariates with coefficients as functions of time dependent covariates. Based on spline smoothing, we propose data driven simultaneous confidence corridors for the coefficient functions with asymptotically correct confidence level. Such confidence corridors are useful benchmarks for statistical inference on the global shapes of coefficient functions under any hypotheses. Simulation experiments corroborate with the theoretical results. An example in CD4/HIV study is used to illustrate how inference is made with computable p-values on the effects of smoking, preinfection CD4 cell percentage and age on the CD4 cell percentage of HIV infected patients under treatment. Keywords: B spline, confidence corridor, Karhunen-Lo`eve L2 representation, knots, functional data, varying coefficient. JEL Classification: C14, C23
4

1. INTRODUCTION Functional data are commonly encountered in biomedical studies, epidemiology and social science, where information is collected over a time period for each subject. In many longitudinal studies, repeated measurements are often collected at few irregular time points. Data of this type are frequently referred to as sparse longitudinal or sparse functional data. See, for example, James, Hastie and Sugar (2000), James and Sugar (2003), Yao, Mu®ller and Wang (2005a), Hall, Mu®ller and Wang (2006), and Zhou, Huang and Carroll (2008).
In longitudinal study, often, interest lies in studying the association between the covariates and the response variable. In recent years, there has been an increasing interest in nonparametric analysis of longitudinal data to enhance flexibility, see e.g., Yao and Li (2013). The varying coefficient model (VCM) proposed by Hastie and Tibshirani (1993) strikes a delicate balance between the simplicity of linear regression and the flexibility of multivariate nonparametric regression and has been widely applied in various settings, for instance, the Cobb-Douglas model for GDP growth in Liu and Yang (2010), and the longitudinal model for CD4 cell percentages in AIDS patients in Wu and Chiang (2000), Fan and Zhang (2000) and Wang, Li and Huang (2008). See Fan and Zhang (2008) for an extensive literature review of VCM.
To examine whether the association changes over time, Hoover et al. (1998) proposed the following varying coefficient model

Y (t) = 0(t) + X(t)T(t) + (t), t  T ,

(1)

where X(t) = (X1(t), . . . , Xd(t))T are covariates at time t, (t) = (1(t), . . . , d(t))T are functions of t, and (t) is a mean zero process. Model (1) is a special case of functional linear models, see Ramsay and Silverman (2005) and Wu, Fan and Mu®ller (2010).
The coefficient functions l(t)'s in model (1) can be estimated by, for example, kernel method in Hoover et al. (1998), basis function approximation method in Huang, Wu and Zhou (2002), polynomial spline method in Huang, Wu and Zhou (2004) and smoothing spline method in Brumback and Rice (1998). Fan and Zhang (2000) proposed a two-step method

5

to overcome the computational burden of the smoothing spline method.

For some longitudinal studies, the covariates are independent of time, and their observa-

tions are cross-sectional. Take for instance the longitudinal CD4 cell percentage data among

HIV seroconverters. This dataset contains 1817 observations of CD4 cell percentages on 283

homosexual men infected with the HIV virus. Three of the covariates are observed at the

time of HIV infection and hence by nature independent of the measurement time and fre-

quency: Xi1, the i-th patient's smoking status; Xi2, the i-th patient's centered pre-infection

CD4 percentage; and Xi3 the i-th patient's centered age at the time of HIV infection. A fourth predictor, however, is time dependent: Tij, the time (in years) of the j-th measurement of CD4 cell on the i-th patient after HIV infection; while the response Yij is also time dependent: the j-th measurement of the i-th patient's CD4 cell percentage at time Tij. Wu and Chiang (2000), Fan and Zhang (2000) and Wang, Li and Huang (2008) all contain

detailed descriptions and analysis of this dataset.

A feasible VCM for multivariate functional data such as the above takes the form

d Yij = il (Tij) Xil +  (Tij) ij, 1  i  n, 1  j  Ni,
l=1

(2)

where the measurement errors (ij)ni=,N1,ij=1 satisfy E (ij) = 0, E(i2j) = 1, and {il(t), t  T } 
are i.i.d copies of a L2 process {l(t), t  T }, i.e., E T l2(t)dt < +, l = 1, . . . , d. The common mean function of processes {l(t), t  T } is denoted as ml(t) = E{l(t)}, l = 1, . . . , d.

The actual data set consists of {Xi, Tij, Yij}, 1  i  n, 1  j  Ni, in which the i-th subject is observed Ni times, the time independent covariates for the i-th subject are Xi = (Xil)dl=1, 1  i  n, and the random measurement time Tij  T = [a, b]. The aforementioned data

example is called sparse functional as the number of measurements Ni for the i-th subject

is relatively low. (In the above CD4 example actually at most 14). In contrast, for a dense

functional data limn min1in Ni = . For the CD4 cell percentage data, we introduce a fourth time independent covariate, the

baseline Xi0  1, and denote by ml (t), l = 0, 1, 2, 3, the coefficient functions for baseline CD4 percentage, smoking status, centered pre-infection CD4 percentage and centered age,

6

respectively. Figures 2-5 contain spline estimates of the ml (t), 0  l  3, and simultaneous confidence corridors (SCC) at various confidence levels.
In previous works the theoretical focus has mainly been on consistency and asymptotic normality of the estimators of the coefficient functions of interest, and the construction of pointwise confidence intervals. However, as demonstrated in Fan and Zhang (2000), this is unsatisfactory as investigators are often interested in testing whether some coefficient functions are significantly nonzero or varying, for which a SCC is needed. Take for instance, Figure 3, which shows both the 95% and 20.277% SCC of m1 (t) contain the zero line completely, thus with a very high p-value of 0.79723 the null hypothesis of m1 (t)  0, t  T is not rejected. More details are in Section 6.
Construction of computationally simple SCCs with exact coverage probability is known to be difficult even with independent cross-sectional data; see, Wang and Yang (2009) and related earlier work H®ardle and Luckhaus (1984) on uniform consistency. Most earlier methods proposed in the literature restrict to asymptotic conservative SCCs. Wu, Chiang and Hoover (1998) developed asymptotic SCCs for the unknown coefficients based on local polynomial methods, which are computationally intensive, as the kernel estimator requires solving an optimization problem at every point. Huang, Wu and Zhou (2004) proposed approximating each coefficient function by a polynomial spline and developed spline SCCs, which are simpler to construct, while Xue and Zhu (2007) proposed maximum empirical likelihood estimators and constructed SCCs for the coefficient functions. All these SCCs are Bonferroni-type variability bands according to Hall and Titterington (1988). The idea is to invoke pointwise confidence intervals on a very fine grid of [a, b], then adjust the level of these confidence intervals by the Bonferroni method to obtain uniform confidence bands, and finally bridge the gaps between the grid points via smoothness conditions on the coefficient curve. However, to use these bands in practice, one must have a priori bounds on the magnitude of the bias on each subinterval as well as a choice for the number of grid points. Chiang, Rice and Wu (2001) proposed a bootstrap procedure to construct confidence intervals. However, theoretical properties of their procedures have not yet been developed.
7

In this paper, we derive SCCs with exact coverage probability for the coefficient functions ml(t), l = 1, . . . , d, in (3) via extreme value theory of Gaussian processes and approximating coefficient functions by piecewise-constant splines. The results represent the first attempt at developing exact SCCs for the coefficient functions in VCM for sparse functional data. Our simulation studies indicate the proposed SCCs are computationally efficient and have the right coverage probability for finite samples. Our work parallels Zhu, Li and Kong (2012) which established asymptotic theory of SCC in the case of VCM for dense functional data. It is important to mention as well that the linear covariates in Zhu, Li and Kong (2012) are time dependent, which does not complicate the problem as they work with dense data instead of the sparse data we concentrate on.
We organize our paper as follows. Section 2 describes spline estimators, and establish their asymptotic properties for sparse longitudinal data. Section 3.1 proposes asymptotic pointwise confidence intervals and SCCs constructed from piecewise constant splines. Section 3.2 describes actual steps to implement the proposed SCCs. In Section 4 we provide further insights into the estimation error structure of spline estimators. Section 5 reports findings from a simulation study. A real data example appears in Section 6. Proofs of technical lemmas are in the Appendix and Supplementary Materials.

2. SPLINE ESTIMATION AND ASYMPTOTIC PROPERTIES

For a functional data {Xi, Tij, Yij}, 1  i  n, 1  j  Ni, denote the eigenvalues and

eigenfunctions sequences of its covariance operator Gl (s, t) = cov {l(s), l(t)} as {k,l}k=1,

{k,l(t)}k=1,

in

which

1,l



2,l



∑

∑

∑



0,


k=1

k,l

<

,

and

{k,l}k=1

form

an

orthonor-

mal basis

of

L2 (T ).

It

follows

from spectral

theory

that

Gl

(s,

t)

=


k=1

k,lk,l(s)k,l

(t).

For any l = 1, . . . , d, the i-th trajectory {il(t), t  T } allows the Karhunen-Lo`eve L2 rep-

resentation (Yao,

Mu®ller and Wang,

2005b):

il(t) =

ml(t)

+


k=1

ik,lk,l(t),

where the

random coefficients ik,l are uncorrelated with mean 0 and variances 1, and the functions

k,l

=

 k,lk,l,

thus

Gl(s,

t)

=


k=1

k,l(s)k,l

(t),

and

the

response

measurements

(2)

can

8

be represented as follows

d d 

Yij = ml (Tij) Xil +

ik,lk,l (Tij) Xil +  (Tij) ij.

l=1 l=1 k=1

(3)

Without loss of generality, we take T = [a, b] to be [0, 1]. Following Xue and Yang (2006),

we approximate each coefficient function by the spline smoothing method. To describe the

spline functions, one can divide the finite interval [0, 1] into (Ns + 1) equal subintervals

J = [J , J+1), J = 0, . . . , Ns - 1, Ns = [Ns, 1]. A sequence of equally-spaced points {J }JN=s 1, called interior knots, are given as 0 = 0 < 1 < ∑ ∑ ∑ < Ns < 1 = Ns+1. Let J = Jhs for 0  J  Ns + 1, where hs = 1/ (Ns + 1) is the distance between neighboring knots. We denote by G(-1) = G(-1) [0, 1] the space of functions that are constant on each

subinterval J , and the B-spline basis of G(-1), as {bJ (t)}NJ=s 0, which are simply indicator

functions of intervals J , bJ (t) = IJ (t), J = 0, 1, . . . , Ns. For any t  [0, 1], define its

location index as J(t) = Jn(t) = min {[t/hs] , Ns} so that t  J(t).

Next we define the space of spline coefficient functions on T ◊ Rd as

{} d
M = g (t, x) = gl(t)xl : gl(t)  G(-1), t  T , x = (x1, . . . , xd)T  Rd ,

l=1

and

propose

estimating

the

multivariate

function

d
l=1

ml(t)xl

by

d n Ni

m^ (t, x) = m^ l(t)xl = argmin

{Yij - g (Tij, Xi)}2 .

l=1 gM i=1 j=1

(4)

Let Y2 (t, x) be the conditional variance of Y given T = t and X = x = (x1, . . . , xd)T  Rd

d 2Y (t, x) = Var(Y |T = t, X = x ) = Gl (t, t) x2l + 2(t).
l=1

Next for any t  [0, 1], let

[

n(t) = c-J(2t),n{n E(N1)}-1 E XXT

Y2 (u, X) f (u) du

E +

{N1(N1 - E N1

1)}

d
l=1

Xl2

J (t)

J (t) ◊J (t)

Gl

(u,

v)

f

(u)

f

(v)

] dudv

,

(5)

9

where Further denote

1 cJ,n = E bJ2 (T ) = b2J (t)f (t)dt, J = 0, . . . , Ns.
0
n(t) = H-1n(t)H-1 = {2n,ll (t)}dl,l=1 ,

(6) (7)

where 2n,ll(t) are later shown to be the asymptotic covariances between m^ l(t) and m^ l(t).

Theorem 1. Under Assumptions (A1)≠(A6) in Appendix A, for any t  [0, 1], as n  ,

-n 1/2 (t) {m^ (t) - m (t)} -L N (0, Id◊d) ,

where m^ (t) = (m^ 1(t), . . . , m^ d(t))T is the estimate of m(t) = (m1(t), . . . , md(t))T. Furthermore, for any l = 1, . . . , d and   (0, 1),

lim
n

P

{n-,1ll(t)

|m^ l(t)

-

ml(t)|



} Z1-/2

=

1

-

.

Remark

1.

Note

that

n(t)

=

{ n2 ,ll

(t)}dl,l

=1

in

(7)

is

complicated

to

compute

in

practice.

The next proposition suggests that, for any t  [0, 1], n(t) in (5) can be simplified by

~ n(t)



E

[ XXT 2Y (t, X) f (t)hsn E(N1)

{ 1

+

E

N1 (N1 - E N1

1)

d
l=1

Xl2Gl (t, t) Y2 (t, X)

f (t)hs

}]

.

(8)

Denote the supremum norm of a function  on [a, b] by  = supt[a,b] |(t)|. For any matrix A = (aij), define A = max |aij|, where the maximum is taken over all the elements of A, while for a matrix function A(t) = (aij(t)), A = supt[a,b] A(t).

Proposition 1. Under Assumptions (A2)≠(A6) in Appendix A, there exists a constant c > 0 such that as n  , n(t) - ~ n(t) = O (n-1hrs-1) = O (n-c) .

To derive the maximal deviation distribution of estimators m^ l(t), l = 1, . . . , d, let

{}

QNs+1 () = bNs+1 - a-N1s+1 log

- 1 log(1 - ) 2

,   (0, 1)

(9)

aNs+1 = {2 log (Ns + 1)}1/2 ,

()

bNs+1

=

aNs+1

-

log

2a2Ns+1 2aNs+1

.

(10)

10

Theorem 2. Under Assumptions (A1)≠(A6) in Appendix A, for l = 1, . . . , d and any  

(0, 1),

{}

lim P
n

sup n-,1ll(t) |m^ l(t) - ml(t)|  QNs+1 ()
t[0,1]

= 1 - ,

where n,ll(t) and QNs+1 () are given in (7) and (9), respectively.

3. ASYMPTOTIC CONFIDENCE REGIONS In this section we construct the confidence regions for functions ml(t), l = 1, . . . , d.
3.1 Asymptotic Confidence Intervals and SCCs Theorems 1 and 2 allow one to construct pointwise confidence intervals and SCCs for components m^ l(t), l = 1, . . . , d. The next corollary provides the theoretical underpinning upon which SCCs can be actually implemented, see subsection 3.2.
Corollary 1. Under Assumptions (A1)-(A6) in Appendix A, for any l = 1, . . . , d and   (0, 1), as n  ,

(i) an asymptotic 100 (1 - ) % pointwise confidence interval for ml(t), t  [0, 1], is m^ l(t)± n,ll(t)Z1-/2, with n,ll(t) given in (7), while Z1-/2 is the 100 (1 - /2)th percentile of the standard normal distribution.

(ii) an asymptotic 100 (1 - ) % SCC for ml(t), with QNs+1 () given in (9), is m^ l(t) ± n,ll(t)QNs+1 (), t  [0, 1].

3.2 Implementation

In the following we describe procedures to construct the SCCs and the pointwise intervals

given in Corollary 1. For any data set (Tij, Yij, Xil)in=,N1,ij,d=1,l=1 from model (3), the spline

estimators m^ l(t), l = 1, . . . , d, are obtained by (4), and the number of interior knots is taken

to

be

Ns

=

[cNT1/3(log(n))],

in

which

NT

=

n
i=1

Ni

is

the

total

sample

size,

[a]

denotes

the

integer part of a, and c is a positive constant.

11

To construct the SCCs, one needs to evaluate the functions 2n,ll(t), l = 1, . . . , d, which

are the diagonal elements of matrix n(t) in (7). Based on Proposition 1, one can estimate each unknowns f (t), 2Y (t, x), Gl (t, t) and matrix H and then plug these estimators into the

formula of the SCCs; see Wang and Yang (2009).

The

number

of

interior

knots

for

pilot

estimation

of

f (t),



2 Y

(t, x),

and

Gl

(t, t)

is

taken

to

be

Ns

=

[] 0.5n1/3 ,

and

hs

=

1/ (1

+

Ns).

The histogram pilot estimator of the density

function

f (t)

is

f^(t)

=

NT-1hs -1

n
i=1

Ni
j=1

We now discuss the estimation of n (t)

bJ (t) (Tij ). in (5). Defining

Rij

=

( Yij

-

d
l=1

)2 m^ (Tij)Xil ,

1  j  Ni, 1  i  n, the estimator of 2Y (t, x) is

d Ns

Ns

d

^2Y (t, x) =

^J,lbJ (t)xl2 + µ^J bJ (t) = G^l (t, t) x2l + ^2(t),

l=1 J=0

J =0

l=1

where {^0,1, . . . , ^Ns,d, µ^0, . . . , µ^Ns}T are solutions of the following least squares problem:

( ^0,1,

.

..

, ^Ns,d,

µ^0,

..

.

,

µ^Ns )T

=

{ n Ni

d Ns

Ns

}2

argmin

Rij -

J,lbJ (Tij)Xi2l - µJ bJ (Tij) .

( )0,1,...,µNs TR(Ns+1)(d+1) i=1 j=1

l=1 J=0

J =0

The matrix n (t) is estimated by substituting f (t), Gl (t, t) and Y2 (t, x) with f^(t), G^l (t, t)

and ^Y2 (t, x). Define ^n (t) 

[ n-1

n

Xil

Xil

^2Y

(t,

Xi

)

{ f^(t)hs

NT

}-1

◊

{ 1

i=1
(
+

n
i=1

Ni2

NT

-

) 1

d
l=1

G^l

(t,

t)

^Y2 (t, Xi)

Xi2l

f^(t)hs

}]d
l,l

=1

.

The following proposition provides the consistent rate of ^n(t) to n(t).

Proposition 2. Under Assumptions (A1)-(A6) in Appendix A, there exists a constant c > 0

such that as n  , ^n(t) - n(t) = Op (n-c).


Proposition 2 implies that n(t) can be replaced by ^n(t) with a negligible error. Define

a

d

◊

d

matrix

H^

=

{n-1

n
i=1

XilXil }ld,l=1,

then

n(t)

can

be

estimated

well

by

^ n(t)

=

12

{^2n,ll(t)}dl,l=1 = H^ -1^n (t) H^ -1. Therefore, as n  , l = 1, . . . , d, the SCCs

m^ l(t) ± ^n,ll(t)QNs+1 () ,

(11)

with QNs+1 () given in (9), and the pointwise intervals m^ l(t)±^n,ll(t)Z1-/2 have asymptotic confidence level 1 - .

4. DECOMPOSITION

In this section, we describe the representation of the spline estimators m^ l(t), l = 1, . . . , d, in (4), then break the estimation error m^ l(t) - ml(t) into three terms by the decomposition of Yij in model (3). Although such representation is not needed for applying the procedure describe in Section 3.2 to analyze data, it sheds insights into the proof of the main theoretical results in Section 2.
We consider the following rescaled B-spline basis {BJ (t)}JN=s 0 for G(-1):

BJ (t)  bJ (t) (cJ,n)-1/2 , J = 0, . . . , Ns.

(12)

It is easily verified that E{BJ (T )}2 = 1 for J = 0, 1, . . . , Ns, and BJ (t)BJ(t)  0 for J = J.

By simple linear algebra, the spline estimator m^ l(t) defined in (4) equals

m^ l(t) = Ns ^J,lBJ (t) = c-J(1t/),2n^J(t),l , l = 1, . . . , d,
J =0

(13)

where

the

coefficients

^

=

( ^

T0 ,

.

.

.

,

^

T Ns

)T

with

^ J

=

( ^J,1,

.

.

.

,

^J,d)T

being

the

solution

of

the following least squares problem

{ n Ni

d Ns

}2

^ = argmin

Yij -

J,lBJ (Tij) Xil .

( )= 0,1,...,Ns,d TRd(Ns+1) i=1 j=1

l=1 J=0

(14)

In the following let Y = (Y11, . . . , Y1N1, . . . , Yn1, . . . , YnNn)T be the collection of all the Yij's. Let B(t) = (B0(t), . . . , BNs(t))T and Xi = (Xi1, . . . , Xid)T be two vectors of dimension

(Ns + 1) and d, respectively. Denote

D = (B(T11)  X1, . . . , B(T1N1)  X1, . . . , B(Tn1)  Xn, . . . , B(TnNn)  Xn)T , 13

(15)

a NT ◊ ((Ns + 1) d) matrix, where "" denotes the Kronecker product. Solving the least squares problem in (14), we obtain

^ = (DTD)-1 (DTY) .

(16)

Denote x = (x1, . . . , xd)T, thus equation (4) can be rewritten as

d m^ l(t)xl = (B(t)  x)T (DTD)-1 (DTY) .

(17)

l=1

According

to

(15),

one

has

DTD

=

n
i=1

Ni
j=1

{ B(Tij )B(Tij )T



} XiXiT ,

in

which

ma-

{}

trix B(Tij)B(Tij)T = diag B02(Tij), . . . , BN2 s(Tij) . So matrix DTD should be a block diag-

onal matrix, and we write NT-1DTD = diag {V^ 0, . . . , V^ Ns}, where

{ n Ni

}d

V^ J = NT-1

BJ2 (Tij )XilXil

.

i=1 j=1

l,l=1

(18)

On the other hand,

we

have

DTY

=

n
i=1

Ni
j=1

{B(Tij

)



Xi} Yij.

Thus,

^

=

( ^

T 0

,

.

.

.

,

^

T Ns

)T

can be easily calculated using

{ n Ni

}d

^ J = V^ J-1 NT-1

BJ (Tij)XilYij

, J = 0, . . . , Ns.

i=1 j=1

l=1

(19)

Then the functions m(t) = (m1(t), . . . , md(t))T can be simply estimated by

m^ (t)

=

(m^ 1(t),

.

.

.

,

m^ d(t))T

=

cJ-(1t/),2n

( ^ J (t),1 ,

.

.

.

,

^J

)T
(t),d

=

cJ-(1t/),2n^ J(t).

(20)

Projecting the relationship in model (3) onto the space of spline coefficient functions on T ◊Rd as M, we obtain the following important decomposition:

d d d d m^ l(t)xl = m~ l(t)xl + ~l(t)xl + ~l(t)xl,
l=1 l=1 l=1 l=1
where for any l = 1, . . . , d,

(21)

Ns

m~ l(t) =

~J,lBJ (t) = c-J(1t/),2n~J(t),l,

J =0

(22)

14

~l(t) = Ns ~J,lBJ (t) = c-J(1t/),2n~J(t),l, ~l(t) = Ns ~J,lBJ (t) = c-J(1t/),2n~J(t),l,

J =0

J =0

(23)

and the vectors (~J,l, J = 0, . . . , Ns, l = 1, . . . , d)T, (~J,l, J = 0, . . . , Ns, l = 1, . . . , d)T, and

(~J,l, J

=

0, . . . , Ns, l

=

1, . . . , d)T

are

solutions

to

(14)

with

Yij

replaced

by

d
l=1

ml

(Tij) Xil,

d
l=1


k=1

ik,lk,l

(Tij )

Xil,

and



(Tij )

ij ,

respectively.

Furthermore, under Assumption (A5) we can decompose m^ l(t) as

m^ l(t) = m~ l(t) + ~l(t) + ~l(t), l = 1, . . . , d.

(24)

The next two propositions concern the functions m~ l(t), ~l(t), ~l(t), l = 1, . . . , d, given in (22) and (23). Proposition 3 gives the uniform convergence rate of m~ l(t) to ml(t). Proposition 4 provides the asymptotic distribution for the maximum of the normalized error terms.

Proposition 3. Under Assumptions (A1), (A2) and (A4)≠(A6) in Appendix A, the functions m~ l(t), l = 1, . . . , d satisfy supt[0,1] sup1ld |m~ l(t) - ml(t)| = Op(hs).

Proposition 4. Under Assumptions (A2)≠(A6) in Appendix A, for   R, and n,ll(t),

aNs+1, bNs+1 as given in (7) and (9),

{}

lim P
n

sup -n,1ll(t) ~l(t) + ~l(t)   /aNs+1 + bNs+1
t[0,1]

= exp (-2e- ) .

5. SIMULATION

To illustrate the finite-sample performance of the spline approach, we generate data from

the following model

{ 2

}{

3

}

Yij = m1 (Tij) + ik,1k,1 (Tij) Xi1 + m2 (Tij) + ik,2k,2 (Tij) Xi2

k=1

k=1

+ (Tij) ij, 1  i  n, 1  j  Ni,

where T  U [0, 1], X1  N (0, 1), X2  Binomial[1, 0.5], k,1  N (0, 1), k = 1, 2, k,2  N (0, 1), k = 1, 2, 3,   N (0, 1), and Ni is generated from a discrete uniform distribution from 2, . . . , 14, for 1  i  n. For the first component, we take m1(t) = sin {2 (t - 1/2)},
15

 1,1(t) = -2 cos { (t - 1/2)} / 5, 2,1(t) = sin { (t - 1/2)} / 5, thus 1,1 = 2/5, 2,1 = 1/10. For the second component, we take m2(t) = 5 (t - 0.6)2, 1,2(t) = 1, 2,2(t) = 
2 sin (2t), 3,2(t) = 2 cos (2t), thus 1,2 = 2,2 = 3,2 = 1. The noise level is chosen to be  = 0.5, 1.0, and the number of subjects n is taken to be 200, 400, 600, 800.
We consider the confidence levels 1 -  = 0.95 and 0.99. Table 1 reports the coverage of the SCCs as the percentage out of the total 500 replications for which the true curve was covered by (11) at the 101 points {k/100, k = 0, . . . , 100}.
[Table 1 about here.]
In the above SCC construction, the number of interior knots Ns is determined by the sample size n and a tuning constant c as described in Section 3.2. We have experimented with c = 0.3, 0.5, 0.8, 1.0 in this simulation study. The simulation results in Table 1 reflect that the coverage percentages depend on the choice of c, however, the dependency becomes weaker when sample sizes increase. For large sample sizes n = 600, 800, the effect of the choice of c on the coverage percentages is insignificant. Because Ns varies with Ni, for 1  i  n, the data-driven selection of an "optimal" Ns remains an open problem. At all noise levels, the coverage percentages for the SCC (11) are very close to the nominal confidence levels 0.95 and 0.99 for c = 0.5. Note that since E N1 = 8, the total sample size NT  8 ◊ 200, 8 ◊ 400, 8 ◊ 600, 8 ◊ 800 which explains the closeness of coverage percentages in Table 1 to the nominal levels. These large NT's are realistic as we believe they are common for real data. For instance, the CD4 cell percentage data in Section 6 has NT = 1817.
For visualization of actual function estimates, Figure 1 shows the true curve, the estimated curve, the asymptotic 95% SCC and the pointwise confidence intervals at  = 0.5 with n = 200. The same plot for n = 600 has shown significantly narrower SCC and pointwise confidence intervals as expected, but is not included to save space.
[Figure 1 about here.]
16

6. REAL DATA ANALYSIS To illustrate our method, we return to the CD4 cell percentage data discussed in Section 1 for further analysis. Since the actual visit times Tij are irregularly spaced and vary from year 0 to year 6, we first transform the times by Zij = FNT (Tij), where FNT is the empirical cdf of times {Tij}in=,N1,ij=1. Then the Zij-values are distributed fairly uniformly. We have set a slightly smaller number of interior knots Ns = [0.3NT1/3(log(n))] to avoid singularity in solving the least squares problem.
The left plots of Figures 2, 3, 4 and 5 depict the spline estimates, the asymptotic 95% SCCs, the pointwise confidence intervals for ml (t), l = 0, 1, 2, 3, respectively. The horizontal solid line represents zero. Based on the shape of the SCCs, we are interested in testing the following hypotheses:
H00 : m0 (t)  a + bt, for some a, b  R v.s. H10 : m0 (t) = a + bt, for any a, b  R; H01 : m1 (t)  0 v.s. H11 : m1 (t) = 0, for some t  [0, 6]; H02 : m2 (t)  c for some c > 0 v.s. H12 : m2 (t) = c, for any c > 0; H03 : m3 (t)  0 v.s. H13 : m3 (t) = 0, for some t  [0, 6].
[Figure 2 about here.]
[Figure 3 about here.]
Asymptotic p-values are calculated for each pair of hypotheses as ^0 = 0.99072, ^1 = 0.79723, ^2 = 0.25404, ^3 = 0.10775. Apparently, none of the null hypothesis is rejected.
The right plots of Figures 2, 3, 4 and 5 show the spline estimates, the 100(1 - ^l)% SCCs and the pointwise confidence intervals, and estimates of ml (t) under H0l, l = 0, 1, 2, 3. From these figures, one can see the baseline CD4 percentage of the population is a decreasing linear function of time and greater than zero over the range of time. The effects of smoking status and age at HIV infection are insignificant, while the pre-infection CD4 percentage is positively proportional to the post-infection CD4 percentage. These findings are consistent with the observations in Wu and Chiang (2000), Fan and Zhang (2000) and Wang, Li and
17

Huang (2008), but are put on rigorous standing due to the quantification of type I errors by computing asymptotic p-values relative to the SCCs.

APPENDIX A Throughout this section, an  bn means limn bn/an = c, where c is some nonzero constant. For functions an(t), bn(t), an(t) = U {bn(t)} means an(t)/bn(t)  0 as n   uniformly for t  [0, 1], and an(t) = U {bn(t)} means an(t)/bn(t) = O(1) as n   uniformly for t  [0, 1]. We use Up(∑) and Up(∑) if the convergence is in the sense of uniform convergence in probability.

A.1 Technical Assumptions

We define the modulus of continuity of a continuous function  on [a, b] by  (, ) =

maxt,t[a,b],|t-t| |(t) -  (t)|. For any r  (0, 1], denote the collection of order r H~older

continuous function on [0, 1] by

{}

C0,r [0, 1] =



:

0,r

=

sup
t= t,t,t[0,1]

|(t) -  (t)| |t - t|r

<

+

,

in which 0,r is the C0,r-seminorm of . Let C [0, 1] be the collection of continuous function on [0, 1]. Clearly, C0,r [0, 1]  C [0, 1] and, if   C0,r [0, 1], then  (, )  0,r r.
The following regularity assumptions are needed for the main results.

(A1) The regression functions ml(t)  C0,1 [0, 1], l = 1, . . . , d.

(A2)

The

set

of

random

variables

( Tij

,

ij

,

Ni

,

ik,l

,

Xil

)n,Ni,,d
i=1,j=1,k=1,l=1

is

a

subset

of

variables

( Tij ,

ij ,

Ni,

ik,l,

Xil

),,,d
i=1,j=1,k=1,l=1

consisting

of

independent

random

variables,

in

which

all Tij's i.i.d with Tij  T , where T is a random variable with probability density

function f (t); Xil's i.i.d for each l = 1, . . . , d; Ni's i.i.d with Ni  N , where N > 0 is

a positive integer-valued random variable with E{N 2r}  r!crN , r = 2, 3, . . ., for some

constant

cN

> 0.

Variables

( ik,l

),,d
i=1,k=1,l=1

and

(ij )i=,1,j=1

are

i.i.d

N (0, 1).

(A3) The functions f (t), (t) and k,l(t)  C0,r [0, 1] for some r  (0, 1] with f (t)  [cf , Cf ], (t)  [c, C], t  [0, 1], for constants 0 < cf  Cf < +, 0 < c  C < +.

18

(A4)

For

l

=

1, . . . , d,


k=1

k,l

 < +, and Gl(t, t)  [cG,l, CG,l], t  [0, 1], for con-

stants 0 < cG,l  CG,l < +.

(A5) There exist constants 0 < cH  CH < + and 0 < c  C < +, such that cHId◊d  H = {Hll}dl,l=1 = E (XXT)  CHId◊d. For some  > 4, l = 1, . . . , d,
c  E |Xl|8+  C.
() (A6) As n  , the number of interior knots Ns = O n for some   (1/3, 1/2) while
{} Ns-1 = O n-1/3 (log(n)) -1/3 . The subinterval length hs  Ns-1.

Assumptions (A1)≠(A3) are common conditions used in the literature; see for example,

Ma, Yang and Carroll (2012). Assumption (A1) controls the rate of convergence of the

spline approximation m^ l, l = 1, . . . , d. The requirement of Ni in Assumption (A2) ensures

that the observation times are randomly scattered, reflecting sparse and irregular designs.

Assumption

(A4)

guarantees

that

the

random

variable


k=1

ik,lk,l(t)

absolutely

uniformly

converges. Assumption (A5) is analog to Assumption (A2) in Liu and Yang (2010), ensuring

that the Xil's are not multicollinear. Assumption (A6) describes the requirement of the growth rate of the dimension of the spline spaces relative to the sample size.

A.2 Proofs of Propositions 1≠4

Proof of Proposition 1. By Assumption (A3) on the continuity of functions k,l(t),

2(t) and f (t) on [0, 1] and Assumption (A4), for any t, u  [0, 1] satisfying |t - u| hs,



|Gl(t, t) - Gl(u, u)| 

2k,l(t) - k2,l(u)

 2

k,l





( k,l,

) hs



Chrs .

k=1

k=1

Furthermore,

 () {Gl(t, t)f (t) - Gl(u, u)f (u)} du  Ch1s+r = O hs1+r ,
J (t)



{ Gl(t,

t)f

2(t)

-

Gl

(u,

v)

f

(u)

f

} (v)

dudv

 Chs2+r = O (hs2+r) ,

J (t) ◊J (t)



{2(t)f

(t)

-

2

(u)

f

} (u)

du

 Ch1s+r = O (h1s+r) .

J (t)

19

According to the definition of CJ,n in (6), 



CJ,n =

f (x)dx = f (J )hs +

{f (x) - f (J )}dx,

[J ,J+1]

[J ,J+1]

(A.1)

thus, |CJ,n - f (J )hs|  w(f, hs)hs for all J = 0, . . . , Ns. Therefore,

n(t)

=

{ f (t)hs

+

U

(hs1+r )}-2

(n E N1)-1

E

[{ 

2 Y

(t, X) f

(t) hs}+

Up

(h]s1+r )

E +
[

{N1(N1 - E N1

1)}

d
l=1

Xl2Gl

(t,

t)

f

2 (t) {

h2s

+

Up

(h2s +r )

XXT

=

E ◊

XXT2Y (t, X) {f (t)hsn E

d
l=1

Xl2Gl

(t,

t)

f

(t)

hs

}

Y2 (t, X)

N1}-1 {1 + Up

1 + E {N1(N1 - 1)} ] E N1
(hrs )} = ~ n(t) + U (n-1

hrs -1 )

,

establishing the proposition.

Proof of Proposition 2. The result follows from standard theory of kernel and spline

smoothing, as in Wang and Yang (2009), thus omitted.

Proof of Proposition 3. According to the result on page 149 of de Boor (2001),

there exist functions gl  G(-1) [0, 1] that satisfies ml - gl = O (hs) for l = 1, . . . , d. By

the definition of m~ l (t) in (22),

m~ (t)

=

(m~ 1(t),

.

.

.

,

m~ d(t))T

=

cJ-(1t/),2n

( ~ J (t),1 ,

.

.

.

,

~J

)T
(t),d

=

c-J(1t/),2n~ J(t),

where

~ J

=

V^ J-1

{ NT-1

n
i=1

Ni
j=1

BJ

(Tij

)Xil

d

}d ml (Tij )Xil

for V^ J defined in (18).

l=1

l=1

Let g~(t) = (g~1 (t) , . . . , g~d (t))T, then we have

m~ l

(t)

-

g~l

(t)

=

c-J(1t/),2nV^ J-(1t)

[ 1
NT

n
i=1

Ni
j=1

BJ(t)(Tij )Xil

d
l=1

{ml (Tij )

-

gl (Tij )}

]d Xil
l=1

.

Observing that g~l  gl as gl  G(-1) [0, 1], there is a decomposition similar to (24), m~ l (t) =

m~ l (t) - g~l (t) + gl (t), l = 1, . . . , d. By (A.1), one has supt[0,1] cJ(t),n = O (hs). Next E |BJ (Tij)| = CJ-,n1/2  bJ (x)f (x)dx 

hs1/2, (

thus

supt)[0,1]

BJ (t) (Tij )

= Op(hs1/2).

Then it is easy to show that m~ l - g~l =

Op h-s 1/2h1s/2hs = Op (hs). Hence, for l = 1, . . . , d, m~ l - ml  m~ l - g~l+ml - gl

= Op (hs) , which completes the proof.

20

Note that BJ (t)  bJ c-J,1n/2, t  [0, 1], so the terms ~l(t) and ~l(t), l = 1, . . . , d, defined in

(23) are

~(t)

=

( ~1(t),

.

.

.

,

~d(t))T

=

cJ-(1t/),2n

( ~ J

(t),1,

.

.

.

,

~ J

(t),d

)T

=

c-J(1t/),2n~ J(t),

~(t)

=

(~1(t), . . . , ~d(t))T

=

cJ-(1t/),2n

( ~J

(t),1,

.

.

.

,

~J

)T
(t),d

=

c-J (1t/),2n ~ J (t) ,

(A.2) (A.3)

where

{ n Ni

d 

}d

~ J = V^ J-1 NT-1

BJ (Tij)Xil

 ik,l k,l (Tij ) Xil

,

i=1 j=1

l=1 k=1

{ n Ni

}d

~J = V^ J-1 NT-1

BJ (Tij)Xil (Tij) ij

.

l=1

i=1 j=1

l=1

According to Lemma B.3, the inverse of the random matrix V^ J can be approximated by

that of a deterministic matrix H = E(XXT). Substituting V^ J with H in (A.2) and (A.3),

we define the random vectors

^(t) ^(t)

= =

cJ-(1t/),2nH-1 cJ-(1t/),2nH-1

{ 1
NT {
1
NT

n Ni

d 

BJ(t)(Tij )Xil

 ik,l k,l

i=1 j=1
n Ni

l=1 k=1
}d

BJ(t)(Tij)Xil (Tij) ij .

i=1 j=1

l=1

}d (Tij ) Xil ,
l=1

(A.4) (A.5)

Proof

of

Proposition

4.

Given

(Tij , Ni, Xil)ni=,N1,ij,d=1,l=1,

let



2 l

,n

(t)

and

2l,n(t)

be

the

conditional variances of ^l(t) and ^l(t) defined in (A.4) and (A.5), respectively. Define

l(t)

=

{ 2l,n(t)

+

}-1/2



2 l

,n(t)

{ ^l(t)

+

} ^l(t)

.

(A.6)

By Lemma B.7, l(t) is a Gaussian process consisting of (Ns + 1) standard normal variables

{ J,l

}Ns
J =0

such

that

l(t)

=

 J (t),l

for

t



[0, 1].

Thus,

for

any





R,

()

()

P sup |l(t)|   /aNs+1 + bNs+1 = P | max{0,l, ..., Ns,l}|   /aNs+1 + bNs+1 .

t[0,1]

By Theorem 1.5.3 in Leadbetter, Lindgren and Rootz¥en (1983), if 0, ..., Ns are i.i.d. standard normal r.v.'s, then for   R

P

( |

max{0,

...,

 Ns }|





/aNs

+

bNs )



exp(-2e-

)

.

21

Next by Lemma 11.1.2 in Leadbetter, Lindgren and Rootz¥en (1983),

(

)(

)

P

|

max{0,l, ..., Ns,l}|  4
| 2
0J<J Ns

 E

/aNs+1 + bNs+1 J,lJ,l|(1 - | E

-P J,lJ



|
,l

|m2)a-x1{/20e,x.p..,{-Ns(}|/aNs+/a1 N+s+b1N+s+1b)N2s}+1. 1 + E J,lJ,l

According to Lemma B.7, there exists a constant C > 0 such that sup0J= JNs E J,lJ,l 

Chs for large n. Thus, as n  ,

(

)(

)

P | max{0,l, ..., Ns,l}|   /aNs+1 + bNs+1 - P | max{0, ..., Ns}|   /aNs+1 + bNs+1  0.

Therefore, for any   R,

()

lim P
n

sup |l(t)|   /aNs+1 + bNs+1
t[0,1]

= exp (-2e- ) .

By Lemma B.8, we have (

)

aNs+1 sup -n,1ll(t) ^l(t) + ^l(t) - sup |l(t)|

t[0,1]

t[0,1]

{}

= Op log (Ns + 1) (nhs)-1/2 (log(n))1/2 = Op (1) .

(A.7)

On the other hand, Lemma B.4 ensures that (

)

aNs+1 sup -n,1ll(t) ~l(t) + ~l(t) - sup n-,1ll(t) ^l(t) + ^l(t)

t[0,1]

t[0,1]

=

Op

{ (log

(Ns

+

1)

nhs)1/2n-1hs-3/2

} log(n)

=

Op

{n-1/2hs-1(log

(Ns

+

1))1/2

} log(n)

=

Op

(1)

.

Then the proof follows from (A.7) and Slutsky's Theorem.

A.3 Proof of Theorem 1

For

any

vector

a

=

(a1,

.

.

.

,

ad)T



Rd,

E

[d
l=1

al

{ ^l

(t)

+

^l

}] (t)

=

0.

Using

the

conditional

independence of ^l(t), ^l(t) on (Tij, Ni, Xil)ni=,N1,ij,d=1,l=1, we have

[ d

{

}

]

Var al ^l (t) + ^l (t) (Tij, Ni, Xil)Nj=i,1n,i,d=1,l=1

l=1

d d

{

}

= alal E ^l (t) ^l (t) + ^l (t) ^l (t) (Tij, Ni, Xil)Nj=i,1n,i,d=1,l=1

l=1 l=1

= aT {,n(t) + ,n(t)} a .

22

Meanwhile, Assumption (A2) the conditional distribution of

entails that [ aT {,n(t)

for any t  [0, 1], given

+

,n(t)}

a]-1/2

d
l=1

al

{(Tij, Ni, ^l (t) +

Xil)Nj=}i,1n,i,d=1,l=1, ^l (t) is a s-

tandard normal distribution. So we have

[aT

{,n(t)

+

,n(t)}

a]-1/2

d

al

{ ^l

(t)

+

^l

} (t)



N

(0,

1)

.

l=1

Using (B.9), we have as n  

[aTn(t)a]-1/2

d

al

{ ^l

(t)

+

^l

} (t)

-L

N

(0,

1)

.

l=1

Therefore

[ aT

n(t)a]-1/2

d
l=1

al

{m^ l

(t)

-

ml (t)}

-L

N

(0, 1)

follows

from

(24),

Proposi-

tion 3, Lemma B.4 and Slutsky's Theorem. Applying Cram¥er-Wold's device, we obtain

n-1/2 (t) {m^ l (t) - ml (t)}dl=1 -L N (0, Id◊d), and consequently, -n,1ll(t) {m^ l(t) - ml(t)} -L

N (0, 1) for any t  [0, 1] and l = 1, . . . , d.

A.4 Proof of Theorem 2

By

Proposition {

3,

m~ l

-

ml

=

Op

(hs), }

l

=

1, . . {

.

,

d,

so

}

aNs+1 sup n-,1ll(t) |m~ l(t) - ml(t)| = Op (nhs)1/2 (log (Ns + 1))1/2hs = Op (1) .

t[0,1]

According to (24), it is easy to show that {

}

aNs+1 sup -n,1ll(t) |m^ l(t) - ml(t)| - sup n-,1ll(t) ~l(t) + ~l(t) = Op (1) .

t[0,1]

t[0,1]

Meanwhile, Pro{position(4 entails that, for any   R, ) }

lim P
n

aNs+1 sup n-,1ll(t) ~l(t) + ~l(t) - bNs+1
t[0,1]



= exp (-2e- ) .

Thus Slutsky's{Theore(m implies that

)}

lim P
n

aNs+1 sup -n,1ll(t) |m^ l(t) - ml(t)| - bNs+1
t[0,1]



= exp (-2e- ) .

{}

Let  = - log

-

1 2

log

(1

-

)

, the definition of QNs+1 () in (9) entails

lim
n

P

{ml(t)

 {

m^ l(t)

±

n,ll(t)QNs+1

()

,

t



[0,

1]}

}

= lim P
n

sup n-,1ll(t) |m^ l(t) - ml(t)|  QNs+1 ()
t[0,1]

= 1 - .

Theorem 2 is proved.

23

B. SUPPLEMENTARY MATERIALS Supplement to "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data": Supplement containing the details of theoretical proofs referenced in the main article. vcmfdaband.xpl: XploRe-package containing code to perform estimations and SCCs for the coefficient functions.
REFERENCES [1] Brumback, B. and Rice, J. A. (1998), "Smoothing Spline Models for the Analysis of
Nested and Crossed Samples of Curves (with Discussion)," Journal of the American Statistical Association, 93, 961≠994.
[2] Chiang, C.-T., Rice, J. A. and Wu, C. O. (2001), "Smoothing Spline Estimation for Varying Coefficient Models with Repeatedly Measured Dependent Variables," Journal of the American Statistical Association, 96, 605≠619.
[3] de Boor, C. (2001). A Practical Guide to Splines, New York: Springer-Verlag.
[4] Fan, J. and Zhang, J. T. (2000), "Functional Linear Models for Longitudinal Data," Journal of the Royal Statistical Society Series B, 62, 303≠322.
[5] Fan, J. and Zhang, W. Y. (2000), "Simultaneous Confidence Bands and Hypothesis Testing in Varying-coefficient Models," Scandinavian Journal of Statistics, 27, 715≠731.
[6] Fan, J. and Zhang, W. Y. (2008), "Statistical Methods with Varying Coefficient Models," Statistics and its Interface, 1, 179≠195.
[7] Hall, P., Mu®ller, H. G., and Wang, J. L. (2006), "Properties of Principal Component Methods for Functional and Longitudinal Data Analysis," Annals of Statistics, 34, 1493≠ 1517.
24

[8] Hall, P. and Titterington, D. M. (1988), "On Confidence Bands in Nonparametric Density Estimation and Regression," Journal of Multivariate Analysis, 27, 228≠254.
[9] H®ardle, W and Luckhaus, S. (1984), "Uniform Consistency of a Class of Regression Function Estimators," Annals of Statistics, 12, 612≠623.
[10] Hastie, T. and Tibshirani, R. (1993), "Varying-coefficient Models," Journal of the Royal Statistical Society Series B, 55, 757≠796.
[11] Hoover, D. R., Rice, J. A., Wu, C. O. and Yang, L.-P. (1998), "Nonparametric Smoothing Estimates of Time-varying Coefficient Models with Longitudinal Data," Biometrika, 85, 809≠822.
[12] Huang, J. Z., Wu, C. O. and Zhou, L. (2002), "Varying-coefficient Models and Basis Function Approximations for the Analysis of Repeated Measurements," Biometrika, 89, 111≠128.
[13] Huang, J. Z., Wu, C. O. and Zhou, L. (2004), "Polynomial Spline Estimation and Inference for Varying Coefficient Models with Longitudinal Data," Statistica Sinica, 14, 763≠788.
[14] James, G. M., Hastie, T., and Sugar, C. (2000), "Principal Component Models for Sparse Functional Data," Biometrika, 87, 587≠602.
[15] James, G. M. and Sugar, C. A. (2003), "Clustering for Sparsely Sampled Functional Data," Journal of the American Statistical Association, 98, 397≠408.
[16] Leadbetter, M. R., Lindgren, G., and Rootz¥en, H. (1983), Extremes and Related Properties of Random Sequences and Processes, New York: Springer-Verlag.
[17] Liu, R. and Yang, L. (2010), "Spline-backfitted Kernel Smoothing of Additive Coefficient Model," Econometric Theory, 26, 29≠59.
25

[18] Ma, S., Yang, L. and Carroll, R. J. (2012), "A Simultaneous Confidence Band for Sparse Longitudinal Regression," Statistica Sinica, 22, 95≠122.
[19] Ramsay, J. O. and Silverman, B. W. (2005), Functional Data Analysis, Second Edition, New York: Springer.
[20] Wang, L., Li, H., and Huang, J. Z. (2008), "Variable Selection in Nonparametric Varying-coefficient Models for Analysis of Repeated Measurements," Journal of the American Statistical Association, 103, 1556≠1569.
[21] Wang, L. and Yang, L. (2009), "Polynomial Spline Confidence Bands for Regression Curves," Statistica Sinica, 19, 325≠342.
[22] Wu, C. O. and Chiang, C.-T. (2000), "Kernel Smoothing on Varying Coefficient Models with Longitudinal Dependent Variable," Statistica Sinica, 10, 433≠456.
[23] Wu, C. O., Chiang, C.-T. and Hoover, D. R. (1998), "Asymptotic Confidence Regions for Kernel Smoothing of a Varying-coefficient Model with Longitudinal Data," Journal of the American Statistical Association, 93, 1388≠1402.
[24] Wu, Y., Fan, J. and Mu®ller, H. G. (2010), "Varying-coefficient Functional Linear Regression," Bernoulli, 16, 730--758.
[25] Xue, L. and Yang, L. (2006), "Additive Coefficient Modelling via Polynomial Spline," Statistica Sinica, 16, 1423≠1446.
[26] Xue, L. and Zhu, L. (2007), "Empirical Likelihood for a Varying Coefficient Model with Longitudinal Data," Journal of the American Statistical Association, 102, 642≠654.
[27] Yao, W. and Li, R. (2013), "New Local Estimation Procedure for a Non-parametric Regression Function for Longitudinal Data," Journal of the Royal Statistical Society Series B, 75, 123≠138.
26

[28] Yao, F., Mu®ller, H. G., and Wang, J. L. (2005a), "Functional Linear Regression Analysis for Longitudinal Data," Annals of Statistics, 33, 2873≠2903.
[29] Yao, F., Mu®ller, H. G., and Wang, J. L. (2005b), "Functional Data Analysis for Sparse Longitudinal Data," Journal of the American Statistical Association, 100, 577≠590.
[30] Zhou, L., Huang, J., and Carroll, R. J. (2008), "Joint Modelling of Paired Sparse Functional Data Using Principal Components," Biometrika, 95, 601≠619.
[31] Zhu, H., Li, R. and Kong, L. (2012), "Multivariate Varying Coefficient Model for Functional Responses. Annals of Statistics, 40, 2634≠2666.
27

Y -3 -2 -1 0 1 2 3 4
Y -4 -3 -2 -1 0 1 2 3 4 5

0 0.5 1 0 0.5 1 XX
Figure 1: Plots of 95% SCC (11) (upper and lower solid), pointwise confidence intervals (dashed), the spline estimator (thin), and the true function (middle thick) at  = 0.5, n = 200 for m1(left) and m2(right).
28

Figure 2: Plots of (a) 95% SCC (upper and lower solid), pointwise confidence intervals
(dashed) and the spline estimator m^ 0 (middle solid) for baseline effect; and (b) the same except with confidence level 1 - ^0 and the estimated m0 under H00 (solid linear).

(a) Baseline CD4

(b) Baseline CD4

Baseline CD4 0.1 0.2 0.3 0.4

Baseline CD4 0.1 0.2 0.3 0.4

0

0

0123456 Year

0123456 Year

(a) Smoking Effect

(b) Smoking Effect

Coeff. of Smoke -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
Coeff. of Smoke -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4

0123456 Year

0123456 Year

Figure 3: Plots of (a) 95% SCC (upper and lower solid), pointwise confidence intervals
(dashed) and the spline estimator m^ 1 (middle solid) for smoking effect; and (b) the same except with confidence level 1 - ^1 and the estimated m1 under H01 (solid linear).

29

Figure 4: Plots of (a) 95% SCC (upper and lower solid), pointwise confidence intervals
(dashed) and the spline estimator m^ 2 (middle solid) for pre-infection CD4 effect; and (b) the same except with confidence level 1 - ^2 and the estimated m2 under H02 (solid linear).

(a) PreCD4 Effect

(b) PreCD4 Effect

0.01 0.02 0.03

0.01 0.02 0.03

0

Coeff. of PreCD4

0

Coeff. of PreCD4

-0.01

-0.01

-0.02

-0.02

0123456 Year

0123456 Year

(a) Age Effect

(b) Age Effect

0.01 0.02 0.03

0.01 0.02 0.03

0

Coeff. of Age

0

Coeff. of Age

-0.01

-0.01

-0.02

-0.02

0123456 Year

0123456 Year

Figure 5: Plots of (a)95% SCC (upper and lower solid), pointwise confidence intervals
(dashed) and the spline estimator m^ 3 (middle solid) for age effect; and (b) the same except with confidence level 1 - ^3 and the estimated m3 under H03 (solid linear).

30

Table 1: Coverage percentages of the SCCs for functions m1 (left) and m2 (right), based on 500 replications.

 n 1 -  c = 0.3

c = 0.5

c = 0.8

c=1

200

0.950 0.990

0.950, 0.952 0.990, 0.998

0.944, 0.948 0.990, 0.990

0.920, 0.904 0.976, 0.984

0.886, 0.884 0.968, 0.974

1.0

400 600

0.950 0.990 0.950 0.990

0.944, 0.948 0.996, 0.984 0.934, 0.962 0.992, 0.996

0.950, 0.930 0.990, 0.988 0.954, 0.946 0.992, 0.986

0.922, 0.912 0.984, 0.988 0.930, 0.952 0.988, 0.990

0.908, 0.904 0.974, 0.966 0.930, 0.924 0.984, 0.990

800

0.950 0.990

0.936, 0.934 0.998, 0.996

0.960, 0.966 0.994, 0.994

0.950, 0.964 0.986, 0.992

0.956, 0.934 0.988, 0.988

200

0.950 0.990

0.936, 0.948 0.988, 0.994

0.952, 0.942 0.992, 0.990

0.916, 0.900 0.972, 0.974

0.912, 0.890 0.972, 0.972

0.5

400 600

0.950 0.990 0.950 0.990

0.916, 0.930 0.994, 0.984 0.924, 0.948 0.996, 0.994

0.936, 0.932 0.992, 0.988 0.952, 0.954 0.994, 0.986

0.928, 0.916 0.996, 0.988 0.926, 0.958 0.984, 0.990

0.904, 0.898 0.978, 0.976 0.936, 0.938 0.990, 0.994

800

0.950 0.990

0.942, 0.900 0.996, 0.998

0.950, 0.960 0.996, 0.994

0.942, 0.962 0.990, 0.996

0.960, 0.938 0.992, 0.988

31

Supplement to "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data"
1Lijie Gu, 3Li Wang, 4,5Wolfgang K. H®ardle and 1,2Lijian Yang 1Soochow University, 2Michigan State University, 3University of Georgia
4Humboldt-Universit®at zu Berlin, 5Singapore Management University

In this document, we have collected a number of technical lemmas and their proofs. The technical lemmas are used in the proofs of Propositions 1≠4 in the paper.

Lemma B.1. (Bosq (1998), Theorem 1.2). Suppose that {i}in=1 are i.i.d with E(1) =

0, 2 = E 21, and Then for each n

there > 1,

exists c > 0 such t > 0, P (|Sn| 

that for r 
nt) 

= 3, 4, . . ., E (
2 exp -t2 (4

+|12|rct/cnr-2)r-!1E),12in<w+hich.

Sn

=

n
i=1

i.

Lemma B.2. Under Assumptions (A2)≠(A6), we have

An,1

=

sup
0J Ns ,1l,l d

BJ 

Xl

,

BJ

Xl

NT-

BJ

Xl

,

BJ

Xl



BJ Xl, BJ Xl BJ Xl, BJ Xl

 

= Op 

log (n)  , nhs

where for any J = 0, . . . , Ns and l, l = 1, . . . , d,

BJ Xl, BJ Xl NT

=

NT-1

n
i=1

Ni
j=1

BJ2

(Tij

)XilXil

,

BJ Xl, BJ Xl = E {BJ2 (Tij)XilXil } = Hll.

1

Proof. E (ij,J )2 =

EL{et jN=ii,1J

= i,J},l,2l BJ2 (Tij)

=

Ni
j=1

BJ2

(Tij

)Xil

E (XilXil )2  h-s 1.

Xil , Next

then E i,J = E N1Hll  1 and define a sequence Dn = n with

(4

+

/2)

>

1

and

 log

(n)Dnn-1/2h-s 1/2



0,

n1/2h1s /2Dn-(3+/2)



0,

which

necessitates

 > 2 according to Assumption (A5). We make use of the following truncated and tail

decomposition

Xill

=

XilXil

=

X Dn
ill,1

+ XiDlln,2,

where

X Dn
ill,1

=

XilXil I {|XilXil | > Dn},

X Dn
ill,2

=

XilXil I {|XilXil |  Dn}.

Define corre-

spondingly the truncated and tail parts of i,J as i,J,m = BJ2(Tij)XiDlln,m, m = 1, 2. According

to Assumption (A5), for any l, l = 1, . . . , d,

 P
n=1

{|XnlXnl |

>

Dn}




n=1

E |XnlXnl |4+/2 Dn4+/2



 C Dn-(4+/2)
n=1

<

.

By

Borel-Cantelli

Lemma,

one

has

Ni
j=1

BJ2 (Tij

)XiDlln,1

=

0, a.s..

So

we

obtain

n sup n-1 i,J,1

= Oa.s. (n-k) ,

k  1,

J,l,l

i=1

and

{}

E i,J,1

=

E (XiDlln,1) E

Ni BJ2 (Tij)

j=1

 Dn-(3+/2) E |XilXil |4+/2 E N1 E BJ2 (Tij )  cDn-(3+/2).

Next we considerate the truncated part i,J,2. For large n, E (i,J,2) = E (i,J )-E (i,J,1)  1,

E (i,J,2)2 = E (i,J )2 - E (i,J,1)2  h-s 1. Define i,J,2 = i,J,2 - E (i,J,2), then E i,J,2 = 0,

and

E (i,J,2)2

=

{ Ni

}2

E (i,J,2)2 - (E i,J,2)2 = E

BJ2 (Tij )XiDlln,2 - U (1)

j=1

=

E

(XiDlln,2)2

E

{ Ni

}2 BJ2 (Tij)

-

U

(1)

.

j=1

Note that

E

(XiDlln,2)2

E

{ Ni

BJ2

(Tij

}2 )



{ E

(Xill

)2

-

E

(XiDlln,1)2}

E

{ Ni

BJ4

} (Tij )

j=1



{ E

(Xill )2

-

U

} (1)

E

N1

E

j=1
BJ4 (Tij).

2

Thus, there exists c such that for large n, E (i,J,2)2  c E (Xill)2 hs-1. Next for any r > 2

E i,J,2 r

= =

E |i,{J,2 - E (i,J,2)|r 

2r-1

E

X Dn
ill,2

rE

Ni

2r-1

(E

|i,J,2|r
r

+ |E }

BJ2(Tij) + U (1)

(i,J,2

)|r

)

=

 2r-1 E

X Dn
ill,2

r

E

j=1  r1+∑∑∑+rNi =r 
0r1,∑∑∑ ,rNi r

( r1

∑

r ∑∑

) rNi

Ni
j=1

E

BJ2rj

  (Tij )

+

 U (1)

,

then there exists C > 0 such that for any r > 2 and large n,

[{

}]

E i,J,2 r

 2r-1 Dnr-2 E (Xill )2 E

Ni N1r max E BJ2rj (Tij)

+ U(1)



2r-1

[Dnr-2

E

(Xill

)2

(E

N1r )

CB

j=1
hs1-r +

U

] (1)

 2rDnr-2 (cNr r!)1/2 CBh2s-rc- 1 E (i,J,2)2



( C

Dn

hs-1)r-2

r!

E

(i,J,2)2

,

which for r

implies

that

{ i,J,2

}n
i=1

satisfies

> 2 and any large enough 

Cram¥er's {
> 0, P

condition.

n-1

n
i=1

Applying i,J,2  

Lemma

B.1

to

n
i=1

}i,J,2,

(nhs)-1/2 (log(n))1/2 is

bounded by

{}

2 exp

-2 (log(n)) 4 + 2CDnh-s 1 (log(n))1/2 n-1/2h1s/2

 2n-8.

Hence

{

}

 n

P sup n-1 i,J,2   (nhs)-1/2 (log(n))1/2 < .

n=1

0J Ns ,1l,l d

i=1

Thus,

supJ,l,l

n-1

n
i=1

i,J,2

{} = Oa.s. (nhs)-1/2 (log(n))1/2 as n   by Borel-Cantelli

Lemma. Furthermore,

n

sup n-1 i,J - E i,J

J,l,l

i=1

n n

 sup n-1 i,J,1 + sup n-1 i,J,2 + sup |E i,J,1|

=

J,l,l
Ua.s.

i=1
(n-k) +

Oa.s.

J,l,l
{ (nhs)-1/2

i=1 } (log(n))1/2

J,l,l
+U

(Dn-(3+/2))

{}

= Oa.s. (nhs)-1/2 (log(n))1/2 .

3

Finally, we notice that

sup
J,l,l

BJ Xl, BJ XlNT - BJ Xl, BJ Xl

= sup
J,l,l

(nNT-1) n-1 n i,J - (E N1)-1 E i,J
i=1

n n

 sup (E N1)-1 (n E N1) NT-1 - 1 n-1 i,J + sup (E N1)-1 n-1 i,J - E i,J

=

J,l,l
Op (n-1/2)

+

Oa.s.

{ (nhs)-1/2

i=1 } (log(n))1/2

=

J,l,l
{ Op (nhs)-1/2

i=1 } (log(n))1/2

,

{} and BJ Xl, BJ Xl = Hll = U (1). Hence, An,1 = Op (nhs)-1/2 (log(n))1/2 .

For the random matrix V^ J defined in (18), the lemma below shows that its inverse can

be approximated by the inverse of a deterministic matrix H = E(XXT).

Lemma B.3. Under Assumptions (A2) and (A4)≠(A6), for any J = 0, . . . , Ns, we have

{} V^ J-1 = H-1 + Op (nhs)-1/2 (log(n))1/2 .

(B.1)

Proof. By Lemma B.2, we have {}
V^ J - H = Op (nhs)-1/2 (log(n))1/2 .

Using the fact that for any matrices A and B,

(A + hB)-1 = A-1 - hA-1BA-1 + O(h2),

we obtain (B.1). The next lemma implies that the difference between ~ (t) and ^ (t) and the difference
between ~(t) and ^(t) are both negligible uniformly over t  [0, 1].

Lemma B.4. Under Assumption (A2)-(A6), for ~(t), ~(t) given in (A.2), (A.3) and ^(t), ^(t) given in (A.4), (A.5), as n  , we have

sup ~ (t) - ^ (t)

t[0,1]



sup ~ (t) - ^ (t)

= =

Op

{n-1h-s 3/2

} log(n)

,

Op

{n-1h-s 3/2

} log(n)

.

t[0,1]

(B.2) (B.3)

4

Proof. Comparing the equations of ~(t) and ^(t) given in (A.2) and (A.4), we let

1 n Ni

d 

n d n

NT

i=1

BJ (Tij)Xil

 ik,l k,l (Tij ) Xil

j=1 l=1 k=1

=

NT l=1 i=1 i,J,l,l.

where

i,J,l,l

=

i

=

[ n-1 XilXil


k=1

{Ni
j=1

BJ

(Tij

)k,l

}] (Tij ) ik,l .

Note that E i

=

0

and

()

2 i,n

=

E

i2 

(Tij , Ni, Xil)ni=,N1,ij,d=1,l=1 {

 Ni

}2

= n-2 XilXil

BJ (Tij)k,l (Tij) 

k=1 j=1

{}

 Ni

 n-2

Xi2l

X2
il

Ni

BJ2 (Tij )k2,l (Tij )

{ k=1 j=1

}

Ni

= n-2

Xi2l

X2
il

Ni

BJ2 (Tij)Gl (Tij, Tij)

j=1



C

n-2

hs-1Xi2l

X2
il

Ni2.

Given

(Tij , Ni, Xil)ni=,N1,ij,d=1,l=1,

{ 

-1 i,n

i

}n
i=1

are

i.i.d

N

(0, 1).

It is easy to show that for

any large enough  > 0,

P

  

|nin=1

i=1

i| 2 i,n



  log(n)

(Tij ,

Ni,

  Xil )ni=,N1,ij,d=1,l=1 



2

exp

{ -12 2

} log(n)



2n-8,

 n
P  i
i=1





{ C

log(n) n-1 nhs

n }1/2

Xi2l

X2
il

Ni2

i=1

 (Tij , Ni, Xil)ni=,N1,ij,d=1,l=1  2n-8.

Note

that

n-1

n
i=1

Xi2l

X2
il

Ni2

=

Op

(1),

hence

{ 

n

P sup

i,J,l,l

n=1

0JNs,1l,ld i=1

}   (nhs)-1/2 (log(n))1/2 < .

Thus, supJ,l,l |in=1 ma. Furthermore,

{

i,J,l,l| = Op (nhs)-1/2

supJ,l

nNT-1

d
l=1

n
i=1

} (log(n))1/2 as n   by Borel-Cantelli Lem-
{} i,J,l,l = Op (nhs)-1/2 (log(n))1/2 . Finally,

according to Lemma B.1, we obtain (B.2). (B.3) is proved similarly.

5

Denote the inverse matrix of H by H-1 = {zll}ld,l=1. For any l = 1, . . . , d, we rewrite the l-th element of ^l(t) and ^l(t) in (A.4) and (A.5) as the following

^l(t) = c-J(1t/),2nNT-1 d n  Rik,,J(t),l,lik,l ,
l=1 i=1 k=1
^l(t) = c-J(1t/),2nNT-1 n N Rij,,J(t),lij ,
i=1 j=1

(B.4) (B.5)

where for any 0  J  Ns,

( d

){ Ni

}

Rik,,J,l,l =

zll Xil Xil

BJ (Tij) k,l (Tij) ,

l=1

j=1

(B.6)

Further denote

( d

)

Rij,,J,l =

zll Xil BJ (Tij)  (Tij) .

l=1

( d

)2

Sill =

zll Xil Xil , sll = E (Sill ) ,

l=1

1  l, l  d.

(B.7) (B.8)

Lemma B.5. Under Assumptions (A2)-(A6), for Rik,,J,l,l, Rij,,J,l in (B.6), (B.7),

( 

)

[

E

R2
ik,,J,l,l

= c-J,1nsll (E N1)

bJ (u) Gl (u, u) f (u) du

k=1 

]

+ E {N1(N1 - 1)} bJ (u) bJ (v) Gl (u, v) f (u) f (v) dudv ,

 E Ri2j,,J,l = cJ-,1nzll bJ (u) 2 (u) f (u) du,

for 0  J  Ns and 0  l, l  d. In addition, there exist 0 < cR < CR < , such that

( 

)

cRsll  E

R2
ik,,J,l,l

 CRsll ,

cR  E Ri2j,,J,l  CR,

k=1

for 0  J  Ns, 0  l, l  d, and as n  

n 

( 

)

{

}

An, = sup n-1

R2
ik,,J,l,l

-

E

R2
ik,,J,l,l

= Oa.s. (nhs)-1/2 (log(n))1/2 ,

J,l,l

i=1 k=1

k=1

n Ni

{}

An, = sup NT-1

Ri2j,,J,l - E Ri2j,,J,l = Oa.s. (nhs)-1/2 (log(n))1/2 .

J,l i=1 j=1

6

Proof. By independence of {Tij}j=1, {Xil}dl=1 , Ni, the definition of BJ and (B.8),

( 

)

{  Ni

}2

E

R2
ik,,J,l,l

= E (Sill ) E

BJ (Tij ) k,l (Tij )

k=1

k=1 j=1

Ni Ni

= sll E

BJ (Tij) BJ (Tij ) Gl (Tij , Tij )

j={1 j=1 

= sllcJ-,1n (E N1) bJ (u) Gl (u, u) f (u) du



}

+ E {N1(N1 - 1)} bJ (u) bJ (v) Gl (u, v) f (u) f (v) dudv ,

thus

there

exist

constants

0

<

cR

<

CR

<



such

that

cRsll



E

(
k=1

) R2
1k,,J,l,l



CRsll ,

0  J  Ns, 0  l, l  d.

If

sll

= 0,

one

has Sill

= 0,

almost

surely.

Hence

n-1

n
i=1


k=1

R2
ik,,J,l,l

= 0,

almost

surely.

In

the

case

of

sll

>

0,

let

 i,J

=

 i,J,l,l

=


k=1

R2
ik,,J,l,l

for

brevity.

Under

Assumption (A5), it is easy to verify that

0

<

s2
ll



E (Sill )2



d3

d E |zll Xil Xil |4



d3

d

zll

{ E

|Xil |8

E |Xil |8}1/2

<

.

l=1

l=1

So for large n,

E

( 

i,J

)2

=

 

( Ni Ni

)2

E (Sill )2

BJ (Tij) BJ (Tij ) Gl (Tij, Tij ) 

j=1 j=1



E

(Sill )2

1 4

c2
G,l

{ Ni
E

BJ

}4 (Tij )



c

E

Ni BJ4

(Tij )



ch-s 1,

j=1 j=1

and

E

( 

i,J

)2



{ Ni

}4

E (Sill )2 4CG2 ,l E

BJ (Tij)

[ j=1 ] Ni
 c E N13 E BJ4 (Tij) N1  c E N14 E BJ4 (Tij)  ch-s 1.

j=1

Define a sequence Dn = n that satisfies  (2 + /4) > 1, Dnn-1/2hs-1/2(log(n))1/2  0, n1/2h1s/2Dn-(1+/4)  0, which requires  > 4 provided by Assumption (A5). We make use of

7

the following truncated and tail decomposition

where

d d

Sill =

z z X X X2
ll ll il il il

=

S Dn
ill,1

+

S Dn
ill

,2,

l=1 l=1

S Dn
ill,1

=

d

d

zll

zll

Xil

Xil

X2
il

I

{

X X X2
il il il

} > Dn ,

l=1 l=1

S Dn
ill,2

=

d

d

zll

zll

Xil

Xil

X2
il

I

{

X X X2
il il il

}  Dn .

l=1 l=1

Define correspondingly the truncated and tail parts of i,J as

Ni Ni

 i,J,m

=

S Dn
ill,m

BJ (Tij) BJ (Tij ) Gl (Tij, Tij ) ,

j=1 j=1

According to Assumption (A5), for any l, l, l = 1, . . . , d,

m = 1, 2.

 P

{

X X X2
nl nl nl

n=1

>

} Dn




n=1

E

|Xnl

Xnl

X2
nl

|2+/4

Dn2+/4



 C Dn-(2+/4)
n=1

<

.

Borel-Cantelli Lemma implies that

{ P

N () ,

Xnl

X X2
nl nl

()

}  Dn for n > N () = 1,

{ P

N () ,

X X X2
il il il

()

}  Dn, i = 1, . . . , n for n > N () = 1,

{ P

N

() ,

I

{

X X X2
il il il

()

}} > Dn = 0, i = 1, . . . , n for n > N () = 1.

Furthermore, one has

{}

n Ni Ni

n-1

S Dn
ill,1

BJ (Tij) BJ (Tij ) Gl (Tij, Tij ) = 0,

i=1 j=1 j=1

a.s.

Therefore, one has
Notice that E (SiDlln,1)

sup

n

n-1

 i,J,1

= Oa.s. (n-k) ,

J,l,l

i=1

k  1.

[

d d

{

=E

zll

zll

Xil

Xil

X2
il

I

X X X2
il il il

l=1 l=1



d d

Dn-(1+/4)

z zll ll E

X X X2
il il il

2+/4

l=1 l=1

 cDn-(1+/4).

] } > Dn

8

So for large n,

{}

() E i,J,1

=

E (SiDlln,1) E

Ni Ni BJ (Tij ) BJ (Tij ) Gl (Tij, Tij )

j=1 j=1

{ Ni

}2

 cDn-(1+/4)2CG,l E

BJ (Tij)



cDn-(1+/4)

E

(N12

)

E

j=1
BJ2 (Tij

)

 cDn-(1+/4).

( ) () ( )

Next we considerate the truncated part i,J,2. For large n, E i,J,2 = E i,J -E i,J,1  1,

E

( 

i,J,2

)2

=

E

( 

i,J

)2

-

E

( 

)2
i,J,1



hs-1.

Define



 i,J,2

=

 i,J,2

-

() E i,J,2 ,

then

E



 i,J,2

=

0,

and there exist c, C > 0 such that for r > 2 and large n,

E

( 

 )2
i,J,2

=

E

S Dn
ill,2

2E

Ni

Ni BJ

(Tij )

BJ

(Tij ) Gl

(Tij ,

Tij )

2

-

( E

 i,J,2)2

j=1 j=1



{ E |Sill |2 - E

S Dn
ill,1

}
2

1 4

c2
G,l

E

{ Ni

BJ

}4 (Tij )

-

U (1)

{ j=1 }



{ E

|Sill |2

-

} U (1)

1 4

c2
G,l

E

Ni BJ4 (Tij)

- U (1)

j=1



1 2

E

|Sill |2

1 4

c2
G,l

E N1

E BJ4

(Tij )

-

U (1)

 c E |Sill |2 h-s 1,

and

E



 i,J,2

r

= =

() E i,{J,2 - E i,J,2

2r-1

E

S Dn
ill,2

rE

r



2r-1

( E

 i,J,2

r

Ni Ni BJ (Tij) BJ

+

() E i,J,2

r)

(Tij ) Gl (Tij , Tij )

r

+

} U (1)



j=1 j=1

{ Ni

}2r



 2r-1 (cDn)r-2 E |Sill |2 (2CG,l )r E

BJ (Tij) + U (1)

j=1



2r-1

[ (cDn

)r-2

E

|Sill |2

(2CG,l )r

( E

N12r )

CB hs1-r

+

] U (1)



2r

(cDn)r-2

(2CG,l

)r

crN

r!CB

hs2-r

c-1

E

( 

 )2
i,J,2



( C

Dnhs-1)r-2

r!

E

( 

 )2
i,J,2

,

9

which

implies that

{ 

 }n
i,J,2 i=1

satisfies Cram¥er's

condition.

Applying

Lemma B.1

to

n
i=1



 i,J,2

,

for r > 2 and any large enough  > 0,

{}

n

P n-1



 i,J,2

 (nhs)-1/2(log(n))1/2

i=1 {

}

 2 exp

-2 log(n) 4 + 2C Dnh-s 1 (log(n))1/2 n-1/2hs1/2

 2n-8.

Hence

{

}

 n

P sup n-1



 i,J,2

  (nhs)-1/2 (log(n))1/2

< .

n=1

J,l,l

Thus,

supJ,l,l

n-1

n
i=1



 i,J,2

i=1 {

}

= Oa.s. (nhs)-1/2 (log(n))1/2 as n   by the Borel-

Cantelli lemma. Furthermore, we have

An,



sup

n

n-1

 i,J,1

+ sup

n

n-1



 i,J,2

+ sup

() E i,J,1

=

J,l,l
Ua.s.

i=1
(n-k) +

Oa.s.

J,l,l
{ (nhs)-1/2

i=1 } (log(n))1/2

J,l,l
+U

(Dn-(1+/4))

{}

= Oa.s. (nhs)-1/2 (log(n))1/2 .

The properties of Rij,,J,l are obtained similarly.

Next define two d ◊ d matrices

{ d n  Ni

}2

,n(t) = c-J(1t),nNT-2

BJ(t)(Tij )k,l (Tij )

X2
il

Xi

XTi

,

l=1 i=1 k=1 j=1

n Ni

,n(t) = cJ-(1t),nNT-2

BJ2(t)(Tij)2 (Tij) XiXTi .

i=1 j=1

Lemma B.6. For any t  R, the conditional covariance matrices of ^ (t) and ^ (t) on

(Tij , Ni, Xil)in=,N1,ij,d=1,l=1 are {}
,n(t) = E ^ (t) ^T (t) (Tij, Ni, Xil)in=,N1,ij,d=1,l=1 = H-1,n(t)H-1, {}
,n(t) = E ^ (t) ^T (t) (Tij, Ni, Xil)in=,N1,ij,d=1,l=1 = H-1,n(t)H-1,

and with n(t) defined in (7), sup {,n(t) + ,n(t)} - n(t) = Oa.s. {n-3/2h-s 3/2(log(n))1/2} .
t[0,1]

(B.9)

10

Proof. Note that

{

^ (t) ^T (t) = c-J(1t),nH-1

1 n Ni

d 

NT2

i=1

BJ(t)(Tij )Xil

 ik,l k,l (Tij ) Xil

j=1 l=1 k=1

n Ni

d 

}d

◊

BJ(t)(Tij )Xil

 ik,l k,l (Tij ) Xil

H-1.

i=1 j=1

l=1 k=1

l,l=1

Thus,

{}

,n(t) = E ^ (t) ^T (t) 

(Tij , Ni, Xil)in=,N1,ij,d=1,l=1 {

d n  Ni

= c-J(1t),nH-1 }2



◊ NT-2

BJ(t)(Tij )k,l (Tij )

X2
il

Xi

XiT

H-1

l=1 i=1 k=1 j=1

= H-1,n(t)H-1.

Similarly, we can derive the conditional covariance matrix of ^ (t). Next let

{ Ni

}2

ik,,J,l,l,l =

BJ (Tij )k,l (Tij )

X2
il

Xil

Xil

,

j=1

ij,,J,l,l = BJ2 (Tij )2 (Tij ) XilXil .

Similar to the proof of Lemma B.5,

( 

)

E ik,,J,l,l,l

=

c-J,1n

E

(Xi2l

Xil

Xil

)

[ (E

N1

)



Gl (u, u) f (u) du+

k=1

 J

]

+ E {N1(N1 - 1)}

Gl (u, v) f (u) f (v) dudv

J ◊J



E ij,,J,l,l = cJ-,1n E (XilXil ) 2 (u) f (u) du,
J

,

and as n  ,

n 

(

){

}

sup n-1

ik,,J,l,l,l - E

ik,,J,l,l,l = Oa.s. (nhs)-1/2 (log(n))1/2 ,

J,l,l,l

i=1 k=1

k=1

n Ni

{}

sup NT-1

ij,,J,l,l - E ij,,J,l,l = Oa.s. (nhs)-1/2 (log(n))1/2 .

J,l,l

i=1 j=1

11

Furthermore,  =

()

n 



sup NT-2

ik,,J,l,l,l - n-1 (E N1)-2 E

ik,,J,l,l,l

J,l,l,l
sup
J,l,l,l

n-1

i=1 k=1
(E N1)-2

{

( n E N1 )2 NT

-

1

(

k=1

n 

n-1

ik,,J,l,l,l

i=1 )k=}1

n 



+ n-1

ik,,J,l,l,l - E

ik,,J,l,l,l

Oa.s. {ni-=31/2kh=s-11/2(log(n))1/2} , k=1

and

n Ni

sup NT-2

ik,,J,l,l - (n E N1)-1 E ik,,J,l,l

J,l,l

i=1 j=1

{

 sup (n E N1)-1
J,l,l

n E N1 - 1 NT

n Ni

NT-1

ik,,J,l,l

i=1 j=}1

n Ni

+ NT-1

ik,,J,l,l - E ik,,J,l,l

= Oa.s. {n-3/i2=h1s-j1=/21(log(n))1/2} .

Notice that

{( d 

)

}d

n(t) = H-1cJ-(1t),n (n E N1)-1 (E N1)-1 E

ik,,J(t),l,l,l + E ij,,J(t),l,l

l=1 k=1

l,l=1

◊H-1,

,n(t) + ,n(t)

{ d n 

n Ni

}d

= H-1c-J(1t),nNT-2

ik,,J(t),l,l,l +

ij,,J (t),l,l

H-1,

l=1 i=1 k=1

i=1 j=1

l,l=1

and (A.1) implies supt[0,1] cJ(t),n = O (hs). Hence (B.9) holds.

Given

(Tij, Ni,

Xil )in=,N1,ij,d=1,l=1 ,

let



2 l

,n

(t)

and

2l,n(t)

be

the

conditional

variances

of

^l(t)

and ^l(t) defined in (A.4) and (A.5), respectively. Lemma B.6 implies that

sup
t[0,1]



2 l

,n

(t)

+

2l,n(t)

-

2n,ll(t)

= Oa.s. {n-3/2h-s 3/2(log(n))1/2} .

(B.10)

12

Lemma B.7. Under Assumptions (A2)-(A6), for l = 1, . . . , d, l(t) defined in (A.6) is

a

Gaussian

process

consisting

of

(Ns + 1)

standard

normal

variables

{ J,l

}Ns
J =0

such

that

l(t) = J(t),l for t  [0, 1], and there exists a constant C > 0 such that for large n,

sup0J= JNs E J,lJ,l  Chs.

{}

Proof. For any fixed l = 1, . . . , d and 0  J  Ns, L J,l (Tij, Ni, Xil)in=,N1,ij,d=1,l=1 {}
N (0, 1) by Assumption (A2), so L J,l = N (0, 1), for 0  J  Ns.

=

Next we derive the upper bound for sup0J=JNs E J,lJ,l . Let

d n 

RØ,J(t),l = NT-1

Ri2k,,J (t),l ,l ,

l=1 i=1 k=1

n Ni

RØ,J(t),l = NT-1

Ri2j,,J (t),l ,

i=1 j=1

then we have

l,n(t)

=

{ cJ-(1t),nNT-2

d

n 

}1/2

R2
ik,,J (t),l ,l

=

{ cJ-(1t),nNT-1RØ,J

}1/2
(t),l

,

l=1 i=1 k=1

l,n(t)

=

{ c-J (1t),n NT-2

n

Ni

}1/2 Ri2j,,J (t),l

=

{ cJ-(1t),n

NT-1RØ,J

}1/2
(t),l

.

i=1 j=1

For J = J, by (B.7) and the definition of BJ ,

( d

)2

Rij,,J,lRij,,J,l =

zll Xil BJ (Tij) BJ (Tij) 2 (Tij) = 0,

l=1

along with the conditional independence of ^l(t), ^l(t) on (Tij, Ni, Xil)ni=,N1,ij,d=1,l=1, and inde-

pendence of ik,l, Tij, Ni, {Xil}dl=1, 1  j  Ni, 1  i  n, k = 1, 2, . . .,

() E J,lJ,l

=

E [(RØ,J,l + RØ{,J(,l)-1/2(RØ,J,l + RØ,J,l)-1/2 ) (

)

d n 

d n 

◊NT-1 E

R ik,,J,l,l ik,l

R ik,,J,l,l ik,l

( l=1 i=1 k=)1 (

)l=1 i=1 k=1

}]

n Ni

n Ni

+ Rij,,J,lij

Rij,,J,lij (Tij , Ni, Xil)ni=,N1,ij,d=1,l=1

i=1 j=1

i=1 j=1

= E Cn,J,J,l,

in which

{}

d n 

Cn,J,J,l = (RØ,J,l + RØ,J,l)-1/2(RØ,J,l + RØ,J,l)-1/2 NT-1

R Rik,,J,l,l ik,,J,l,l .

l=1 i=1 k=1

13

Note that according to definitions of Rik,,J,l,l, Rij,,J,l, and Lemma B.5, for 0  J  Ns

RØ,J(t),l + RØ,J(t),l  RØ,J(t),l  ERi2j,,J,l - An,  cR - An,,

   2

P

 inf
0J =J  Ns

{(RØ,J,l

+

RØ ,J,l )(RØ ,J  ,l

+

RØ,J



,l

} )



cR

-



log(n)    1 - 2n-8. nhs

Thus for large n, with probability  1-2n-8, the denominator of Cn,J,J,l is uniformly greater

than cR2 /4. On the other hand, we consider the numerator of Cn,J,J,l.

( d n 

)

 

( d n d

)2

E NT-1

R Rik,,J,l,l ik,,J,l,l

l=1 i=1 k=1
( Ni Ni

= E NT-1
l=1 i=1
)}

zll Xil Xil
l=1

◊ BJ (Tij) BJ (Tij ) Gl (Tij, Tij )  hs.

j=1 j=1

Applying Bernstein's inequality, there exists C0 > 0 such that, for large n,

(
P sup
0J= JNs

d n 

NT-1

R Rik,,J,l,l ik,,J,l,l

l=1 i=1 k=1

)  C0hs  1 - 2n-8.

Putting the above together, for large n, C1 = C0 (cR2 /4)-1, ()
P sup |Cn,J,J,l|  C1hs  1 - 4n-8.
0J =J  Ns

Note that as a continuous random variable, sup0J= JNs |Cn,J,J,l|  [0, 1] , thus

( ) 1 (

)

E sup |Cn,J,J,l| = P sup |Cn,J,J,l| > u du.

0J= JNs

0 0J= JNs

() For large n, C1hs < 1 and then E sup0J=JNs,l |Cn,J,J| is

 C1hs {

} 1 {

}

P sup |Cn,J,J,l| > u du + P sup |Cn,J,J,l| > u du

0 0J= JNs,l

C1 hs

0J =J  Ns ,l

 C1hs

1



1du +

4n-8du  C1hs + 4n-8  Chs

0 C1hs

for some C > 0 and large enough n. The lemma now follows from

()

sup |E (Cn,J,J,l)|  E

sup |Cn,J,J,l|  Chs.

0J= JNs

0J =J  Ns

This completes the proof of the lemma.

14

Lemma

B.8.

Under

Assumptions {

(A2)-(A6), }

for

l(t), n,ll(t), l

=

1, . . . , d,

defined

in

(A.6)

and (7), one has n,ll(t)-1 ^l(t) + ^l(t) - l(t) = |rn,l(t) - 1| |l(t)|, where rn,l(t) =

{ }1/2 n-,1ll(t) 2l,n(t) + 2l,n(t) , and as n  ,

{} sup {aNs+1 |rn,l(t) - 1|} = Oa.s. (nhs)-1/2 (log (Ns + 1) log(n))1/2 .
t[0,1]

Proof. By Lemma B.5, n2,ll(t) in (7) can be rewritten as

{ ( )}

d 

n2,ll(t) = cJ-(1t),n (n E N1)-1 (E N1)-1

E

R2
ik,,J (t),l ,l

+ E Ri2j,,J(t),l

l=1

k=1

 n-1hs-1.

Hence, according to (B.10) and (10),

{{

}1/2

}

sup {aNs+1 |rn,l(t) - 1|}
t[0,1]

=

sup aNs+1 t[0,1] {

-n,1ll(t)



2 l

,n

(t)

+

2l,n(t)

{}

-1 }



sup aNs+1 -n,2ll(t) t[0,1] {

2l,n(t)

+



2 l

,n(t)

-1

}

=

sup aNs+1n-,2ll(t) t[0,1] {

2l,n(t) + 2l,n(t) - n2,ll(t) }

= Oa.s. (nhs)-1/2 (log (Ns + 1) log(n))1/2 .

This completes the proof.

15

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Principal Component Analysis in an Asymmetric Norm" by Ngoc Mai Tran, Maria Osipenko and Wolfgang Karl H‰rdle, January 2014.
002 "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data" by Lijie Gu, Li Wang, Wolfgang Karl H‰rdle and Lijian Yang, January 2014.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

