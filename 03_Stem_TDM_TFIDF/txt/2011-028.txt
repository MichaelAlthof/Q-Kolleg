BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2011-028
Asymptotic equivalence and sufficiency for volatility estimation under microstructure noise
Markus Reiﬂ*
* Humboldt-Universit‰t zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Asymptotic equivalence and sufficiency for volatility
estimation under microstructure noise
Markus Reiﬂ
Institute of Mathematics Humboldt-Universit®at zu Berlin mreiss@mathematik.hu-berlin.de
Abstract The basic model for high-frequency data in finance is considered, where an efficient price process is observed under microstructure noise. It is shown that this nonparametric model is in Le Cam's sense asymptotically equivalent to a Gaussian shift experiment in terms of the square root of the volatility function . As an application, simple rateoptimal estimators of the volatility and efficient estimators of the integrated volatility are constructed. Key words and Phrases: High-frequency data, integrated volatility, spot volatility estimation, Le Cam deficiency, equivalence of experiments, Gaussian shift. AMS subject classification: 62G20, 62B15, 62M10, 91B84 JEL subject classification: C14, C58
Financial support from the Deutsche Forschungsgemeinschaft via SFB 649 "O® konomisches Risiko", Humboldt-Universita®t zu Berlin, is gratefully acknowledged.
1

2 Markus Reiﬂ

1 Introduction

In recent years volatility estimation from high-frequency data has attracted a lot of attention in financial econometrics and statistics. Due to empirical evidence that the observed transaction prices of assets cannot follow a semi-martingale model, a prominent approach is to model the observations as the superposition of the true (or efficient) price process with some measurement error, conceived as microstructure noise. The main features are already present in the basic model of observing

Yi = Xi/n + i, i = 1, . . . , n,

(1.1)

with an efficient price process Xt =

t 0

(s)

dBs,

B

a

standard

Brownian

motion,

and i  N (0, 2) all independent. The aim is to perform statistical inference on the

volatility function  : [0, 1]  R+, e.g. estimating the so-called integrated volatility

1 0

2(t) dt

over

the

trading

day.

The mathematical foundation on the parametric formulation of this model has

been laid by ? who prove the interesting result that the model is locally asymptotically normal (LAN) as n  , but with the unusual rate n-1/4, while without microstructure noise the rate is n-1/2. Starting with ?, the nonparametric model

has come into the focus of research. Mainly three different, but closely related ap-

proaches have been proposed afterwards to estimate the integrated volatility: multi-

scale estimators (?), realized kernels or autocovariances (?) and preaveraging (?).

Under various degrees of generality, especially also for stochastic volatility, all au-

thors provide central limit theorems with convergence rate n-1/4 and an asymptotic

variance involving the so-called quarticity

1 0

4(t)

dt.

Recently,

also

the

problem

of

estimating the spot volatility 2(t) itself has found some interest (?).

The aim of the present paper is to provide a thorough mathematical understand-

ing of the basic model, to explain why statistical inference is not so canonical and

to propose a simple estimator of the integrated volatility which is efficient. To this

end we employ Le Cam's concept of asymptotic equivalence between experiments.

Asymptotic equivalence for volatility estimation

3

In fact, our main theoretical result in Theorem 6.2 states under some regularity conditions that observing (Yi) in (1.1) is for n   asymptotically equivalent to observing the Gaussian shift experiment
dYt = 2(t) dt + 1/2n-1/4 dWt, t  [0, 1],
with Gaussian white noise dW . Not only the large noise level 1/2n-1/4 is apparent, but also a non-linear (t)-form of the signal, from which optimal asymptotic variance results can be derived. Note that a similar form of a Gaussian shift was found to be asymptotically equivalent to nonparametric density estimation (?). A key ingredient of our asymptotic equivalence proof are the results by ? on asymptotic equivalence for generalized nonparametric regression, but also ideas from ? and ? play a role. Moreover, fine bounds on Hellinger distances for Gaussian measures with different covariance operators turn out to be essential.
Roughly speaking, asymptotic equivalence means that any statistical inference procedure can be transferred from one experiment to the other such that the asymptotic risk remains the same, at least for bounded loss functions. Technically, two sequences of experiments E n and G n, defined on possibly different sample spaces, but with the same parameter set, are asymptotically equivalent if the Le Cam distance (E n, G n) tends to zero. For Ei = (Xi, Fi, (Pi )), i = 1, 2, by definition, (E1, E2) = max((E1, E2), (E1, E2)) holds in terms of the deficiency (E1, E2) = infM sup M P1 - P2 T V , where the infimum is taken over all randomisations or Markov kernels M from (X1, F1) to (X2, F2), see e.g. ? for details. In particular, (E1, E2) = 0 means that E1 is more informative than E2 in the sense that any observation in E2 can be obtained from E1, possibly using additional randomisations. Here, we shall always explicitly construct the transformations and randomisations and we shall then only use that (E1, E2) sup P1 - P2 T V holds when both experiments are defined on the same sample space.
The asymptotic equivalence is deduced stepwise. In Section 2 the regressiontype model (1.1) is shown to be asymptotically equivalent to a corresponding white

4 Markus Reiﬂ

noise model with signal X. Then in Section 3, a very simple construction yields a Gaussian shift model with signal log(2(∑) + c), c > 0 some constant, which is asymptotically less informative, but only by a constant factor in the Fisher information. Inspired by this construction, we present a generalisation in Section 4 where

the information loss can be made arbitrarily small (but not zero), before applying

nonparametric local asymptotic theory in Section 5 to derive asymptotic equiva-

lence with our final Gaussian shift model for shrinking local neighbourhoods of the

parameters. Section 6 yields the global result, which is based on an asymptotic

sufficiency result for simple independent statistics.

Extensions and restrictions are discussed in Section 7 before we use the theoret-

ical insight to construct in Section 8 a rate-optimal estimator of the spot volatility

and an efficient estimator of the integrated volatility by a locally-constant approx-

imation. Remarkably, the asymptotic variance is found to depend on the third

moment

1 0

3(t)

dt

and

for

non-constant

2(∑)

our

estimator

outperforms

previous

approaches applied to the basic model. Constructions needed for the proof are pre-

sented and discussed alongside the mathematical results, deferring more technical

parts to the Appendix, which in Section 9.1 also contains a summary of results on

white noise models, the Hellinger distance and Hilbert-Schmidt norm estimates.

2 The regression and white noise model

In the main part we shall work in the white noise setting, which is more intuitive to handle than the regression setting, which in turn is the observation model in practice. Let us define both models formally. For that we introduce the H®older ball

C(R) := {f  C([0, 1]) | f C

|f (x) - f (y)|

R} with f C = f  + sup
x=y

|x - y|

.

2.1 Definition. Let E0 = E0(n, , , R, 2) with n  N,  > 0,   (0, 1), R > 0, 2 0 be the statistical experiment generated by observing (1.1). The volatility 2

Asymptotic equivalence for volatility estimation

5

belongs to the class

S (, R, 2) := 2  C(R) min 2(t) 2 .
t[0,1]
Let E1 = E1(, , R, 2) with  > 0,   (0, 1), R > 0, 2 0 be the statistical experiment generated by observing

dYt = Xt dt +  dWt, t  [0, 1],

with Xt =

t 0

(s)

dBs

as

above,

independent

standard

Brownian

motions

W

and

B and 2  S (, R, 2).

From ? it is well known that the white noise and the Gaussian regression model 
are asymptotically equivalent for noise level  = / n  0 as n  , provided the signal is -H®older continuous for  > 1/2. Since Brownian motion and thus also our price process X is only H®older continuous of order  < 1/2 (whatever  is), it is not clear whether asymptotic equivalence can hold for the experiments E0 and E1. Yet, this is true. Subsequently, we employ the notation An Bn if An = O(Bn) and An  Bn if An Bn as well as Bn An and obtain:

2.2 Theorem. For any  > 0, 2 0 and , R > 0 the experiments E0 and E1 
with  = / n are asymptotically equivalent ; more precisely:

(E0(n,

,

,

R,

2),

E1

 (/ n,

h,

,

R,

2

))

R-2n-.

Interestingly, the asymptotic equivalence holds for any positive H®older regularity  > 0. In particular, the volatility 2 could be itself a continuous semimartingale, but such that X conditionally on 2 remains Gaussian. As the proof in Section 9.2 of the appendix reveals, we construct the equivalence by rate-optimal approximations of the anti-derivative of 2 which lies in C1+. Similar techniques have been used by ? and ?, but here we have to cope with the random signal for which we need to bound the Hilbert-Schmidt norm of the respective covariance operators. Note further that the asymptotic equivalence even holds when the level of the microstructure noise  tends to zero, provided 2n   remains valid.

6 Markus Reiﬂ
3 Less informative Gaussian shift experiments
From now on we shall work with the white noise observation experiment E1, where the main structures are more clearly visible. In this section we shall find easy Gaussian shift models which are asymptotically not more informative than E1, but already permit rate-optimal estimation results. The whole idea is easy to grasp once we can replace the volatility 2 by a piecewise constant approximation on small blocks of size h. That this is no loss of generality, is shown by the subsequent asymptotic equivalence result, proved in Section 9.3 of the appendix.

3.1 Definition. Let E2 = E2(, h, , R, 2) be the statistical experiment generated by observing

dYt = Xth dt +  dWt, t  [0, 1],

with Xth =

t 0

(

s

h) dBs,

s h :=

s/h h for h > 0 and h-1  N, and indepen-

dent standard Brownian motions W and B. The volatility 2 belongs to the class

S (, R, 2).

3.2 Proposition. Assume  > 1/2 and 2 > 0. Then for   0, h = o(1/2) the experiments E1 and E2 are asymptotically equivalent ; more precisely:
(E1(, , R, 2), E2(, h, , R, 2)) R-3/2h-1/2.
In the sequel we always assume h = o(1/2) to hold such that we can work equivalently with E2. Recall that observing Y in a white noise model is equivalent to observing ( em dY )m 1 for an orthonormal basis (em)m 1 of L2([0, 1]), cf. also Subsection 9.1 below. Our first step is thus to find an orthonormal system (not a basis) which extracts as much local information on 2 as possible. For any  

Asymptotic equivalence for volatility estimation

7

L2([0, 1]) with  L2 = 1 we have by partial integration

11

1

(t)dYt = (t)Xth dt +  (t) dWt

00

0

1

= (1)X1h - (0)X0h - (t)( t h) dBt + 
0

1 1/2
= 2(t)2( t h) dt + 2 

0

(t) dWt (3.1)

where (t) = -

1 t

(s)

ds

is

the

antiderivative

of



with

(1)

=

0

and





N (0,

1)

holds. To ensure that  has only support in some interval [kh, (k + 1)h], we require

 to have support in [kh, (k + 1)h] and to satisfy (t) dt = 0. The function k

with supp(k) = [kh, (k + 1)h], k L2 = 1, k(t) dt = 0 that maximizes the

information load k2(t) dt for 2(kh) is given by (use Lagrange theory)

 k(t) = 2h-1/2 cos (t - kh)/h 1[kh,(k+1)h](t), t  [0, 1].

(3.2)

The L2-orthonormal system (k) for k = 0, 1, . . . , h-1 - 1 is now used to construct Gaussian shift observations. In E2 we obtain from (3.1) the observations

1/2
yk := k(t) dYt = h2-22(kh) + 2 k, k = 0, . . . , h-1 - 1,

(3.3)

with independent standard normal random variables (k)k=0,...,h-1-1. Observing (yk) is clearly equivalent to observing

zk := log(yk2h-22) - E[log(k2)] = log 2(kh) + 2h-22 + k

(3.4)

for k = 0, . . . , h-1 - 1 with k := log(k2) - E[log(k2)]. We have found a nonparametric regression model with regression function
log(2(∑) + 2h-22) and h-1 equidistant observations corrupted by non-Gaussian, but centered noise (k) of variance 2. To ensure that the regression function does not change under the asymptotics   0, we specify the block size h = h() = h0 with some fixed constant h0 > 0.
It is not surprising that the nonparametric regression experiment in (3.4) is equivalent to a corresponding Gaussian shift experiment. Indeed, this follows readily

8 Markus Reiﬂ

from results by ? who in their Section 4.2 derive asymptotic equivalence already

for our Gaussian scale model (3.3). Note, however, that their Fisher information

should

be

I ()

=

1 2

-2

and

we

thus

have

asymptotic

equivalence

of

(3.3)

with

the

Gaussian regression model

wk

=

1 2

log(2(kh)

+

h0-22)

+

k ,

k = 0, . . . , h-1 - 1,

where k  N (0, 1) i.i.d. Since by the classical result of ? the Gaussian regression is equivalent to the corresponding white noise experiment (note that log(2(∑) + h-0 22) is also -H®older continuous), we have already derived an important and far-reaching result.

3.3 Theorem. For  > 1/2 and 2 > 0 the high frequency experiment E1(, , R, 2) is asymptotically more informative than the Gaussian shift experiment G1(, , R, 2, h0) of observing

dZt

=

1 2

log

2(t) + h-0 22

dt + h01/21/2dWt,

t  [0, 1].

Here h0 > 0 is an arbitrary constant and 2  S (, R, 2).

3.4 Remark. Moving the constants from the diffusion to the drift part, the experiment G1 is equivalent to observing

dZ~t = (2h0)-1/2 log(2(t) + h0-22) dt + 1/2dWt, t  [0, 1].

(3.5)

The Gaussian shift experiment is nonlinear in 2 which is to be expected. Writing



=

 / n

gives

us

the

noise

level

1/2n-1/4

which

appears

in

all

previous

work

on

the model E0.

To quantify the amount of information we have lost, let us study the LAN-

property of the constant parametric case 2(t) = 2 > 0 in G1. We consider the
local alternatives 2 = 02 + 1/2 for which we obtain the Fisher information Ih0 = (2h0)-1h40/(2 + h2002)2. Maximizing over h0 yields h0 = 30-1 and the Fisher
information is at most equal to

sup Ih0 = 0-333/2/(32)  0.05170-3.
h0 >0

Asymptotic equivalence for volatility estimation

9

By

the

LAN-result

of

?

for

E0

the

best

value

is

I (0 )

=

1 8

0-3

which

is clearly 

larger.

Note, however, that the relative (normalized) efficiency is already 33/2/(32) 

1/8

0.64, which means that we attain about 64% of the precision when working with G1

instead of E0 or E1.

4 A close sequence of simple models

In order to decrease the information loss in G1, we now take into account higher frequencies in each block [kh, (k + 1)h]. In a frequency-location notation (j, k) we consider for k = 0, 1, . . . , h-1 - 1, j 1

 jk(t) = 2h-1/2 cos(j(t - kh)/h)1[kh,(k+1)h](t), t  [0, 1].

(4.1)

This gives the corresponding antiderivatives



jk(t) =

2h j

sin(j(t

-

kh)/h)1[kh,(k+1)h](t),

t  [0, 1].

Not only the (jk) and (jk) are localized on each block, also each single family of functions is orthogonal in L2([0, 1]). Working again on the piecewise constant

experiment E2, we extract the observations

yjk :=

1
jk(t) dYt =

h2-2j-22(kh) + 2

1/2
jk, j

0

1, k = 0, . . . , h-1 - 1, (4.2)

with jk  N (0, 1) independent over all (j, k). The same transformation as before leads for each j 1 to the regression model for k = 0, . . . , h-1 - 1

zjk := log(yj2k) - log(h2-2j-2) - E[log(j2k)] = log(2(t) + 2h-22j2) + jk. (4.3)

Applying the asymptotic equivalence result by ? for each independent level j separately, we immediately generalize Theorem 3.3.

4.1 Theorem. For  > 1/2 and 2 > 0 the high frequency experiment E1(, , R, 2) is asymptotically more informative than the combined experiment

10 Markus Reiﬂ

G2(, , R, 2, h0, J) of independent Gaussian shifts

dZtj

=

1 2

log(2(t) + h-0 22j2) dt +

h10/21/2dWtj ,

t  [0, 1], j = 1, . . . , J,

with independent Brownian motions (W j)j=1,...,J and 2  S (, R, 2). The constants h0 > 0 and J  N are arbitrary, but fixed.

4.2 Remark. Let us again study the LAN-property of the constant parametric case 2(t) = 2 > 0 for the local alternatives 2 = 02 + 1/2. We obtain the Fisher information

Ih0,J

=

J
(2h0)-1h04(2j2
j=1

+ h2002)-2

=

J j=1

h-0 1 2(2(jh0-1)2

+ 02)2 .

In the limit J   and h0   we obtain by Riemann sum approximation

 dx

1

lim
h0 

lim
J 

Ih0

,J

=

0

2(2x2 + 02)2 = 803 .

This is exactly the optimal Fisher information, obtained by ? in this case. Note, however, that it is not at all obvious that we may let J, h0  , in the asymptotic equivalence result. Moreover, in our theory the restriction h = o(1/2) is necessary, which translates into h0 = o((1-2)/2). Still, the positive aspect is that we can come as close as we wish to an asymptotically almost equivalent, but much simpler

model.

5 Localisation
We know from standard regression theory (?) that in the experiment G1 we can estimate 2  C in sup-norm with rate ( log(-1))/(2+1), using that the logfunction is a C-diffeomorphism for arguments bounded away from zero and infinity. Since E1 is for  > 1/2 asymptotically more informative than G1, we can therefore localize 2 in a neighbourhood of some 02. Using the local coordinate s2 in 2 = 02 + vs2 for v  0 we define a localized experiment, cf. ?.

Asymptotic equivalence for volatility estimation

11

5.1 Definition. Let Ei,loc = Ei,loc(0, , , R, 2) for 0  S (, R, 2) be the statistical subexperiment obtained from Ei(, , R, 2) by restricting to the parameters 2 = 02 + vs2 with v = /(2+1) log(-1) and unknown s2  C(R).

We shall consider the observations (yjk) in (4.2) derived from E2,loc and multiplied by j/h. The model is then a generalized nonparametric regression family in the sense of ?. On the sequence space (X , F ) = (RN, B N) we consider for    = [2, R] the Gaussian product measure

P = N 0,  + h-0 22j2 .
j1

(5.1)

The parameter  plays the role of 2(kh) for each k. By independence and the

result for the one-dimensional Gaussian scale model, the Fisher information for 

is given by

I() :=
j

1

1 2( + h-0 22j2)2

=

h0 83/2

1 + 41/2h0e-21/2h0 - e-41/2h0 - 2

(1 - e-21/2h0 )2

1/2h0

,

(5.2)

where the series is evaluated in Section 9.6 using Fourier analysis. Since we shall

later let h0 tend to infinity, an essential point is the asymptotics I()  h0. We split our observation design {kh | k = 0, . . . , h-1} into blocks Am = {kh | k =
(m - 1) , . . . , m - 1}, m = 1, . . . , ( h)-1, of length such that the radius v of our nonparametric local neighbourhood has the order of the parametric noise level (I() )-1/2 in each block:

v  (I() )-1/2   h0-1v-2.

For later convenience we consider odd and even indices k separately, assuming that h-1 and are even integers. This way, for each block m observing (yjkj/h) for j 1 and k  Am, k odd respectively k even, can be modeled by the experiments

E3o,dmd =

X /2, F  /2,

P02 (k/n)+v s2 (k/n)
kAm odd

,
s2 C (R)

E3e,vmen =

X /2, F  /2,

P02 (k/n)+v s2 (k/n)
kAm even

,
s2 C (R)

(5.3) (5.4)

12 Markus Reiﬂ

where all parameters are the same as for E2,loc. Using the nonparametric local asymptotic theory developed by ? and the independence of the experiments (E3o,dmd)m (resp. (E3e,vmen)m), we are able to prove in Section 9.4 the following asymptotic equivalence.

5.2 Proposition. Assume  > 1/2, 2 > 0 and h0  -p with p  (0, 1 - (2)-1) such that (2h)-1  N. Then observing {yj,2k+1 | j 1, k = 0, . . . , (2h)-1 - 1} in experiment E2,loc is asymptotically equivalent to the local Gaussian shift experiment G3,loc of observing

1 dYt = 803/2(t)

1- 2 0(t)h0

1/2
vs2(t) dt + (2)1/2dWt,

t  [0, 1],

(5.5)

where the unknown s2 and all parameters are the same as in E2,loc. The Le Cam

distance tends to zero uniformly over the center of localisation 02  S (, R, 2).

The same asymptotic equivalence result holds true for observing {yj,2k | j

1, k = 0, . . . , (2h)-1 - 1} in experiment E2,loc.

Note that in this model, combining even and odd indices k, we can already infer the LAN-result by ?, but we still face a second order term of order h0-1v in the drift. This term is asymptotically negligible only if it is of smaller order than

the noise level 1/2. To be able to choose h0 sufficiently large, we have to require a

larger Ho®lder smoothness of the volatility.



5.3

Corollary.

Assume



>

1+ 17 8

 0.64,

2

>0

and

(2h)-1



N.

Then

observ-

ing {yj,2k+1 | j 1, k = 0, . . . , (2h)-1 - 1} in experiment E2,loc is asymptotically

equivalent to the local Gaussian shift experiment G4,loc of observing

dYt

=

1 803/2

(t)

v

s2(t)

dt

+

(2)1/2dWt,

t  [0, 1],

(5.6)

where the unknown s2 and all parameters are the same as in E2,loc. The Le Cam

distance tends to zero uniformly over the center of localisation 02  S (, R, 2).

The same asymptotic equivalence result holds true for observing {yj,2k | j

1, k = 0, . . . , (2h)-1 - 1} in experiment E2,loc.

Asymptotic equivalence for volatility estimation

13



Proof.

For  >

1+ 17 8

the choice of h0 = -p

for

some

p



(

1 4+2

,

2-1 2

)

is

possible

and ensures that h = o(1/2) holds as well as h-0 2 = o(v-2). Therefore the

Kullback-Leibler divergence between the observations in G3loc and in G4loc evaluates

by the Cameron-Martin (or Girsanov) formula to

-1

11 0 803(t)

1- 2 0(t)h0

1/2
-1

2
v2s4(t) dt

-1h02v2.

Consequently, the Kullback-Leibler and thus also the total variation distance tends

to zero.

In a last step we find local experiments G5,loc, which are asymptotically equiv-

alent to G4,loc and do not depend on the center of localisation 02. To this end we

use a variance-stabilizing transform, based on the Taylor expansion

 2x1/4

=

2x01/4

+

1 8

x-0 3/4(x

-

x0)

+

O((x

-

x0)2)

which holds uniformly over x, x0 on any compact subset of (0, ). Inserting x = 2(t) = 02(t) + vs2(t) and x0 = 02 from our local model, we obtain

2(t) =

20(t)

+

1 8

0-3/2(t)vs2(t)

+

O(v2).

(5.7)

Since v2 = o(1/2) holds for  > 1/2, we can add the uninformative signal

201/2(t) to Y

 in G4,loc, replace the drift by 21/2(t) and still keep convergence

of the total variation distance, compare the preceding proof. Consequently, from

Corollary 5.3 we obtain the following result.



5.4

Corollary.

Assume



>

1+ 17 8

 0.64,

2

>0

and

(2h)-1



N.

Then

observ-

ing {yj,2k+1 | j 1, k = 0, . . . , (2h)-1 - 1} in the experiment E2,loc is asymptotically

equivalent to the local Gaussian shift experiment G5,loc of observing

dYt = 2(t) dt + (2)1/2 dWt, t  [0, 1],

(5.8)

where the unknown is 2 = 02 + vs2 and all parameters are the same as in E2,loc. The Le Cam distance tends to zero uniformly over the center of localisation 02  S (, R, 2).

14 Markus Reiﬂ
The same asymptotic equivalence result holds true for observing {yj,2k | j 1, k = 0, . . . , (2h)-1 - 1} in experiment E2,loc.

6 Globalisation

The globalisation now basically follows the usual route, first established by ?. Essential for us is to show that observing (yjk) for j 1 is asymptotically sufficient in E2. Then we can split the white noise observation experiment E2 into two independent sub-experiments obtained from (yjk) for k odd and k even, respectively. Usually, a white noise experiment can be split into two independent subexperiments
 with the same drift and an increase by 2 in the noise level. Here, however, this does not work since the two diffusions in the random drift remain the same and thus independence fails.
Let us introduce the L2-normalized step functions

0,k(t) := (2h)-1/2 1[(k-1)h,kh](t) - 1[kh,(k+1)h](t) , k = 1, . . . , h-1 - 1, 0,0(t) := h-1/21[0,h](t).

We obtain a normalized complete basis (jk)j 0,0 k h-1-1 of L2([0, 1]) such that observing Y in experiment E2 is equivalent to observing
1
yjk := jk(t) dYt, j 0, k = 0, . . . , h-1 - 1.
0
Calculating the Fourier series, we can express the tent function 0,k with 0,k = 0,k and 0,k(1) = 0 as an L2-convergent series over the dilated sine functions jk and j,k-1, j 1:

0,k(t) = (-1)j+1j,k-1(t) + jk(t), k = 1, . . . , h-1 - 1.

j1

j1

(6.1)

We also have 0,0(t) = 2 j 1 j,0(t). By partial integration, this implies (with

Asymptotic equivalence for volatility estimation

15

L2-convergence)
1
0,k := 0,k, X = - 0,k(t) dX(t) = (-1)j+1j,k-1 + jk,
0 j1 j1
where jk := jk, X

for k 1 and similarly 0,0 = 2 j 1 j,0. This means that the signal 0,k in y0,k can be perfectly reconstructed from the signals in the yj,k-1, yjk. For jointly Gaussian random variables we obtain the conditional law in E2

L (jk | yjk) = N

Var(j k ) Var(yj k )

yj k ,

2 Var(jk) Var(yj k )

Given the results by ? and our less informative Gaussian shift experiment G1 for  > 1/2, 2 > 0, there is an estimator ^2 based on (y1,k)k in E2 with

lim
0

inf
 2 S

P2,(

^2 - 2



Rv) = 1,

(6.2)

where v = /(2+1) log(-1) as in the definitions of the localized experiments. We can thus generate independent N (0, 1)-distributed random variables jk to
construct from (yjk)j 1,k

~jk

:=

Var (j k ) Var (yj k )

yjk

+

 Var(jk)1/2 Var (yj k )1/2

jk

,

where the variance Var is the expression for Var where all unknown values 2(kh)

are replaced by the estimated values ^2(kh). From this we can generate artificial observations (y~0,k) such that the conditional law L ((y~0,k)k | (~j,k)k) coincides with

L ((y0,k)k | (0,k)k), which is just a multivariate normal law with mean zero and

tri-diagonal covariance matrix 2( 0,k, 0,k )k,k .

In Section 9.5 we shall prove that the Hellinger distance between the families

of centered Gaussian random variables Y := {yjk | j 0, k = 0, . . . , h-1 - 1} and

Y~ := {y~0,k | k = 0, . . . , h-1 - 1}  {yjk | j 1, k = 0, . . . , h-1 - 1} tends to zero,



provided

h-0 1v2

=

o(),

which

is

possible

when



>

1+ 4

5

with

the

choice

h0

=

-p

for

some

p



(

1 2+1

,

2-1 2

).

16 Markus Reiﬂ



6.1

Proposition.

Assume



>

1+ 5 4



0.81,

2

>

0

and

h-1

an

even

inte-

ger. Then the experiment E2 is asymptotically equivalent to the product experi-

ment E2,odd  E2,even where E2,odd is obtained from the observations {yj,2k+1 | j

1, k = 0, . . . , (2h)-1 - 1} and E2,even from the observations {yj,2k | j 1, k =

0, . . . , (2h)-1 - 1} in experiment E2.

This key result permits to globalize the local result. In the sequel we always



assume



>

1+ 4

5

and

2

>

0.

We

start

with

the

asymptotic

equivalence

between

E2 and E2,odd  E2,even. Using again an estimator ^2 in E2,odd satisfying (6.2) we can

localize the second factor E2,even around ^2 and therefore by Corollary 5.4 replace

it by experiment G5,loc, see Theorem 3.2 in ? for a formal proof. Since G5,loc does

not depend on the center ^2, we conclude that E2 is asymptotically equivalent to

the product experiment E2,odd  G5 where G5 has the same parameters as E2 and is

given by observing Y in (5.8). Now we use an estimator ^2 in G5 satisfying (6.2),

whose existence is ensured by ?, to localize E2,odd. Corollary 5.4 then allows again

to replace the localized E2,odd-experiment by G5 such that E2 is asymptotically

equivalent to the product experiment G5  G5. Finally, taking the mean of the

independent observations (5.8) in both factors, which is a sufficient statistics, (or,

abstractly, due to identical likelihood processes) we see that G5  G5 is equivalent 
to the experiment G0 of observing dYt = 2(t) dt +  dWt, t  [0, 1]. Our final

result then follows from the asymptotic equivalence between E0 and E1 as well as

between E1 and E2.



6.2

Theorem.

Assume



>

1+ 4

5



0.81

and

, 2, R

>

0.

Then

the

regression

experiment E0(n, , , R, 2) is for n   asymptotically equivalent to the Gaussian

shift experiment G0(n-1/2, , R, 2) of observing

dYt = 2(t) dt + 1/2n-1/4 dWt, t  [0, 1],

(6.3)

for 2  S (, R, 2).

Asymptotic equivalence for volatility estimation

17

7 Discussion

Our results show that inference for the volatility in the high-frequency observation

model under microstructure noise E0 is asymptotically as difficult as in the well understood Gaussian shift model G0. Remark that the constructions in ?, ? rely

on preliminary estimators at the boundary of suitable blocks, while we require

supp jk = [kh, (k + 1)h] to obtain independence among blocks. In this context

Proposition 6.1 shows asymptotic sufficiency of observing only the pinned process

Xt

-

(k+1)h-t h

Xkh

-

t-kh h

X(k+1)h,

t



[kh, (k

+

1)h],

on

each

block

due

to

(t +

)jk(t) dt = 0 for j 1, ,   R. Naturally, the (jk)j 1 form exactly the

eigenfunctions of the covariance operator of the Brownian bridge.

It is interesting to note that both, model E0 and model G0, are homogeneous

in the sense that factors from the noise (i.e. the dWt-term) can be moved to the drift term and vice versa such that for example high volatility can counterbalance

a high noise level  or a large observation distance 1/n. Another phenomenon is

that observing E0 m-times independently, in particular with different realisations

of the process X, is asymptotically as informative as observing E0 with m2 as many

observations: both experiments are asymptotically equivalent to dYt = 2(t)dt +

m1/21/2n-1/4dWt. Similarly, by rescaling we can treat observations on intervals

[0, T ] with T > 0 fixed: Observing Yi = XiT/n + i, i = 1, . . . , n, in E0 with

Xt =

t 0

(s)

dBs,

t



[0,

T

],

is

under

the

same

conditions

asymptotically

equivalent

to observing

dYu = 2(T u) du + 1/2T -1/4n-1/4 dWu, u  [0, 1],

or equivalently,

dY~v = 2(v) du + 1/2T 1/4n-1/4 dWv, v  [0, T ].

Concerning the various restrictions on the smoothness  of the volatility 2, one might wonder whether the critical index is  = 1/2 in view of the classical asymptotic equivalence results (?, ?). In our approach, we still face the second order

18 Markus Reiﬂ

term in (5.5) and using the localized results, a much easier globalisation yields for  > 1/2 only that E0 is asymptotically not less informative than observing

dYt = F (2(t)) dt + 1/2n-1/4dWt, t  [0, 1],

with F (x) = 1x(y1/2 - 2h-0 1)1/2y-1dy/8, which includes a small, but nonnegligible second-order term since h0 cannot tend to infinity too quickly.

On the other hand, it is quite easy to see that for  1/4 asymptotic equiva-

lence fails. In the regression model E0 with n observations we cannot distinguish be-

tween Xn(t) =

t 0

n(t)

dBt

with

n2 (t)

=

1 + n-1/4

cos(nt),

n2 C1/4 = 2 + n-1/4,

and standard Brownian motion (2 = 1) since Xn(i/n)-Xn((i-1)/n)  N (0, 1/n)

i.i.d. holds. On the other hand, we have

1 0

(

 2n(t) - 2)2 dt  n-1/2, which

shows that the signal to noise ratio in the Gaussian shift G0 is of order 1 and a

Neyman-Pearson test between n2 and 1 can distinguish both signals with a posi-

tive probability. This different behaviour for testing in E0 and G0 implies that both

models cannot be asymptotically equivalent for  = 1/4. Note that ? merely re-

quire  1/4 for their LAN-result, but our counterexample is excluded by their 
parametric setting. In conclusion, the behaviour in the zone   (1/4, (1 + 5)/4]

remains unexplored.

8 Applications

Let us first consider the nonparametric problem of estimating the spot volatility

2(t). From our asymptotic equivalence result in Theorem 6.2 we can deduce, at

least for bounded loss functions, the usual nonparametric minimax rates, but with

the

number

n

of

observations

replaced

by

 n

provided

2



C

for



>

 (1 + 5)/4

as the mapping (t)  2(t) is a C-diffeomorphism for volatilities 2 bounded

away from zero. Since the results so far obtained only deal with rate results, it is even

simpler to use our less informative model G1 or more concretely the observations (yk) in (3.3) which are independent in E2, centered and of variance h2-22(kh)+2.

Asymptotic equivalence for volatility estimation

19

With h =  a local (kernel or wavelet) averaging over -22yk2 - 2 therefore yields rate-optimal estimators for classical pointwise or Lp-type loss functions.

For later use we choose h =  in E2 and propose the simple estimator

^b2(t)

:=

 2b

(-22yk2 - 2)

k:|k-t| b

for some bandwidth b > 0. Since k2 is 2(1)-distributed, it is standard (?) to show that with the choice b  ( log(-1))1/(2+1) we have the sup-norm risk bound

E[ ^b2 - 2 2] ( log(-1))2/(2+1),

especially we shall need that ^b2 is consistent in sup-norm loss. In terms of the regression experiment E0 we work (in an asymptotically equiv-
alent way) with the linear interpolation Y^ of the observations (Yi), see the proof

of Theorem 2.2. By partial integration we can thus take for any j, k

1 n i/n

yj0k := - jk(t)Y^ (t) dt =

-

jk(t) dt (Yi - Yi-1),

0 i=1 (i-1)/n

(8.1)

setting Y0 := 0. Note that we have the uniform approximation yj0k =

-1 n

n i=1

j k (i/n)(Yi

- Yi-1) + O(h-1/2n-1)

due

to

jk 

(2h)-1/2. We see the

relationship with the pre-averaging approach. The idea of using disjoint averages is

present in ?, where in our terminology Haar functions are used as k. They were aware of the fact that discretized sine functions would slightly increase the Fisher

information (personal communication, see also their discussion after Corollary 2),

but they have not used higher frequencies. Since we use the concrete coupling by linear interpolation to define yj0k in E0 and
since convergence in total variation is stronger than weak convergence, all asymp-

totics for probabilities and weak convergence results for functionals F ((yjk)jk) in E2 remain true for F ((yj0k)jk) in E0, uniformly over the parameter class. The formal argument for the latter is that whenever Pn - Qn T V  0 and PnXn  P weakly for some random variables Xn we have for all bounded and continuous g

EQn [g(Xn)] = EPn [g(Xn)] + O( g  Pn - Qn T V ) -n--- EP[g(X)].

20 Markus Reiﬂ

Thus, for  > 1/2, 2 > 0 and b  (n-1/2 log n)-1/(2+1) the estimator

~n2 (t)

:=

 2b n

(n-22(yk0)2 - 2)

k:|kn-1/2-t| b

satisfies in the regression experiment E0

(8.2)

lim
n

2

inf
S (,R,2

)

P

2

,n

(n/(4+2)

(log

n)-1

~n2 - 2



R) = 1.

(8.3)

The asymptotic equivalence can be applied to construct estimators for the inte-

grated volatility

1 0

2(t)dt

or

more

generally

p-th

order

integrals

1 0

p(t)dt

using

the approach developed by ? for white noise models like G0. In our notation their

Theorem 7.1 yields an estimator ^p,n of

1 0

p(t)dt

in

G0

such

that

E2

^p,n -

1 p(t) dt - 1/2n-1/4 2p

1
p-1/2(t) dWt

2

= o(n-1/2)

00

holds uniformly over 2  S (, R, 2) for any , R, 2 > 0 since the functional

(∑) 

1 0

p

(t)dt

is

smooth

on

L2.

A

LAN-result

shows

that

asymptotic

nor-

mality with rate n-1/4 and variance 2p2

1 0

2p-1(t) dt

is

minimax

optimal.

Spe-

cializing to the case p = 2 for integrated volatility, the asymptotic variance is

8

1 0

3(t)

dt.

It

should

be

stressed

here

that

the

existing

estimation

procedures

for

integrated volatility are globally sub-optimal for our idealized model in the sense

that their asymptotic variances involve the integrated quarticity

1 0

4(t) dt

which

can at most yield optimal variance for constant values of 2, because otherwise

1 0

4(t)

dt

>

1 0

3(t)

dt

4/3

follows

from

Jensen's

inequality.

The

fundamental

reason is that all these estimators are based on quadratic forms of the increments

depending on global tuning parameters, whereas optimizing weights locally permits

to attain the above efficiency bound as we shall see.

Instead of following these more abstract approaches, we use our analysis to

construct a simple estimator of the integrated volatility with optimal asymptotic

variance. First we use the statistics (yjk) in E2 and then transfer the results to E0 using (yj0k) from (8.1).

Asymptotic equivalence for volatility estimation

21

On each block k we dispose in E2 of independent N (0, h2j-2-22(kh) + 2)observations yjk for j 1. A maximum-likelihood estimator ^2(kh) in this exponential family satisfies the estimating equation

^2(kh) = wjk(^2)h-2j22(yj2k - 2),

j1

where wjk(2) :=

(2(kh) + h-0 22j2)-2 l 1(2(kh) + h0-22l2)-2

.

(8.4) (8.5)

This can be solved numerically, yet it is a non-convex problem (personal communi-

cation by J. Schmidt-Hieber). Classical MLE-theory, however, asserts for fixed h, k and consistent initial estimator ~n2 (kh) that only one Newton step suffices to ensure asymptotic efficiency. Because of h  0 this immediate argument does not apply

here, but still gives rise to the estimator

h-1 -1

IV  :=

h wjk(~n2 )h-2j22(yj2k - 2)

k=0 j 1

of the integrated volatility IV :=

1 0

2(t)

dt.

Assuming

the

L-consistency

~n2 -

2   0 in probability for the initial estimator, we assert in E2 the efficiency

result

1
-1/2(IV  - IV ) -L N 0, 8 3(t) dt .
0

To prove this, it suffices by Slutsky's lemma to show

h-1 -1

1

-1/2

h wjk(2)h-2j22(yj2k - 2) -L N 0, 8 3(t) dt ,

k=0 j 1

0

sup|wjk(~n2 ) - wjk(2)| wjk(2) ~n2 - 2 .
jk

(8.6) (8.7)

The second assertion (8.7) follows from inserting the Lipschitz property that

W (x) := (x + h-0 22j2)-2 satisfies |W (x)| W (x)|x - y| uniformly over x, y 2 > 0.

W (x) and thus |W (x) - W (y)|

For the first assertion (8.6) note that in E2 the estimator IV  is unbiased and

Var wjk(2)h-2j22(yj2k - 2) =
j1

2 j 1(2(kh) + h-0 22j2)-2

22 Markus Reiﬂ

such that by formula (9.14) and Riemann sum approximation as h0   (with

arbitrary speed)

h-1 -1
-1 Var(IV ) =
k=0

j

2hh0 1(2(kh) + h0-22j2)-2



8

1
3(t) dt.
0

Due to the independence and Gaussianity of the (yjk) we deduce also

4
E wjk(2)h-2j22(yj2k - E[yj2k])
j1

2
Var wjk(2)h-2j22(yj2k - 2)
j1

such that the central limit theorem under a Lyapounov condition with power p = 4

(e.g. ?) proves assertion (8.6), assuming h  0 and h0  . A feasible estimator

is obtained by neglecting frequencies larger than some J = J():

h-1-1 J

IV ,J :=

h wjJk(~n2 )h-2j22(yj2k - 2)

k=0 j=1

where wjJk(2) :=

(2(kh) + h0-22j2)-2

J l=1

(2(kh)

+

h0-22l2)-2

.

(8.8) (8.9)

A simple calculation yields E[|IV ,J -IV |2] (h0/J)3 such that for h0/J  0

convergence in probability implies again by Slutsky's lemma

1
-1/2(IV ,J - IV ) -L N 0, 8 3(t) dt .
0
By the above argument, weak convergence results transfer from E2 to E0 and we

obtain the following result where we give a concrete choice of the initial estimator,

the block size h and the spectral cut-off J (we just need some consistent estimator ~n2 , h2n1/2  0 as well as hn1/2   and J -1 = o(h-1n-1/2)).

8.1 Theorem. Let yj0k for j 1, k = 0, h-1 - 1 be the statistics (8.1) from model E0. For h  n-1/2 log(n) and J/ log(n)   consider the estimator of integrated

volatility

h-1-1 J

IV n :=

h wjJk(~n2 )h-2j22((yj0k)2 - n-1)

k=0 j=1

with weights wjJk from (8.9) and the initial estimator ~n2 from (8.2). Then IV n is

asymptotically efficient in the sense that

1
n1/4(IV n - IV ) -L N 0, 8 3(t) dt as n  ,
0

Asymptotic equivalence for volatility estimation

23

provided 2 is strictly positive and -H®older continuous with  > 1/2.
This might serve as a benchmark for more general models, whereas we, in the spirit of ?, focus on elucidating the underlying fundamental structures. In particular, we should dispense with the Gaussianity of the microstructure noise (i) as well as with the deterministic nature of the volatility 2. The analysis in both cases, however, cannot simply rely on model E2, since E0 is non-Gaussian. Different tools are required.

9 Appendix

9.1 Gaussian measures, Hellinger distance and HilbertSchmidt norm

We gather basic facts about cylindrical Gaussian measures, the Hellinger distance

and their interplay.

Formally, we realize the white noise experiments, as L2-indexed Gaussian vari-

ables, e.g. in experiment E1 we observe for any f  L2([0, 1])

1t

1

Yf := f, dY := f (t) (s)dB(s) dt +  f (t) dWt.

00

0

Canonically,

we

thus

define

P,

on

the

set



=

L2 ([0,1])
R

with

product

Borel

-

algebra F = BL2([0,1]) (realizing a cylindrical centered Gaussian measure). Its

covariance structure is given by

E[Yf Yg] = Cf, g , f, g  L2([0, 1]),

with the covariance operator C : L2([0, 1])  L2([0, 1]) given by

1
Cf (t) =
0

tu
2(s) ds f (u) du + 2f (t), f  L2([0, 1]).
0

Note that C is not trace class and thus does not define a Gaussian measure on L2([0, 1]) itself.

24 Markus Reiﬂ

In the construction, it suffices to prescribe (Yem )m 1 for an orthonormal basis

(em)m 1 and to set



Yf :=

f, em Yem .

m=1

This way, we can define P, equivalently on the sequence space  = RN with

product -algebra F = B N. This is useful when extending results from finite

dimensions.

The Hellinger distance between two probability measures P and Q on (, F ) is

defined as

H(P, Q) =

p() -

q()

2µ(d)

1/2
,



where µ denotes a dominating measure, e.g. µ = P + Q, and p and q denote the

respective densities. The total variation distance is smaller than the Hellinger dis-

tance:

P - Q T V H(P, Q).

(9.1)

The identity H2(P, Q) = 2 - 2 pqdµ implies the bound for finite or countably

infinite product measures

H2 Pn, Qn
nn

H2(Pn, Qn).
n

(9.2)

Moreover, the Hellinger distance is invariant under bi-measurable bijections T :



since

with

the

densities

p



T -1,

q



T -1

of

the

image

measures

T
P

and

T
Q

with respect to µT we have

H2(PT , QT ) = ( p  T -1 -


q  T -1)2dµT =

 (p

-

q)2dµ

=

H 2 (P,

Q).



(9.3)

For the one-dimensional Gaussian laws N (0, 1) and N (0, 2) we derive

H2(N (0, 1), N (0, 2)) = 2 - 8/(2 + 1) 2(2 - 1)2.

For the multi-dimensional Gaussian laws N (0, 1) and N (0, 2) with invertible

covariance

matrices

1, 2



d◊d
R

we

obtain

by

linear

transformation

and

inde-

Asymptotic equivalence for volatility estimation

25

pendence, denoting by 1, . . . , d the eigenvalues of -1 1/221-1/2:

H2(N (0, 1), N (0, 2)) = H2(N (0, Id), N (0, 1-1/22-1 1/2))

d
2(k - 1)2.
k=1

The last sum is nothing, but the squared Hilbert-Schmidt (or Frobenius norm) of

-1 1/22-1 1/2 - Id such that

H2(N (0, 1), N (0, 2))

2

1-1/2(2 - 1)1-1/2

2 H

S

.

(9.4)

Observing that (9.2) and (9.3) also apply to Gaussian measures on the sequence

space RN, the bound (9.4) is also valid for (cylindrical) Gaussian measures N (0, i) with self-adjoint positive definite covariance operators i : L2([0, 1])  L2([0, 1]).
The Hilbert-Schmidt norm of a linear operator A : H  H on any separable

real Hilbert space H can be expressed by its action on an orthonormal basis (em)

via

A

2 HS

=

Aem, en 2,

m,n

which for a matrix is just the usual Frobenius norm. For self-adjoint operators A, B

with | Av, v | | Bv, v | for all v  H we use the eigenbasis (em) of A and obtain

A

2 HS

=

Aem, em 2

Bem, en 2 =

B

2 H

S

.

m m,n

Furthermore, it is straight-forward to see for any bounded operator T

(9.5)

T A HS T A HS , AT HS T A HS

(9.6)

with the usual operator norm T of T . Finally, for integral operators Kf (x) =

1 0

k(x,

y)f (y)

dy

on

L2([0, 1])

it

is

well

known

that

K HS = k L2([0,1]2).

(9.7)

For two Gaussian laws with different mean vectors µ1, µ2 and with the same invertible covariance matrix  we can similarly use the transformation -1/2 and the scalar case H2(N (m1, 1), N (m2, 1)) = 2(1 - e-(m1-m2)2/8) (m1 - m2)2/4 to conclude by independence

H2(N (µ1, ), N (µ2, ))

1 4

-1/2(µ1 - µ2)

2.

(9.8)

26 Markus Reiﬂ

Combining (9.4) and (9.8) we obtain by the triangle inequality the bound

H2(N (µ1, 1), N (µ2, 2))

1-1/2 (µ1 -µ2 )

2+

1-1/2 (2 -1 )1-1/2

2 HS

.

(9.9)

9.2 Proof of Theorem 2.2
 We first show that E1 is asymptotically at least as informative as E0 for  = / n
 and  > 0. From E1 with  = / n we can generate the observations (statistics)

(2i+1)/2n

(2i+1)/2n

Y~i := n

dYt = n

Xtdt + ~i, i = 1, . . . , n - 1,

(2i-1)/2n

(2i-1)/2n

11

Y~n := 2n

dYt = 2n

Xtdt + ~n,

(2n-1)/2n

(2n-1)/2n

with ~i = n(W(2i+1)/2n - W(2i-1)/2n)  N (0, 2) and similarly ~n  N (0, 2), all independent. In contrast to standard equivalence proofs, it turns out to be essential here to take Y~i as a mean symmetric around the point i/n. Since (Yi) and (Y~i) are defined on the same sample space, using inequality (9.1) it suffices to prove that the Hellinger distance between the law of (Yi) and the law of (Y~i) tends to zero as n tends to infinity.

For the integrated volatility function we introduce the notation

t
a(t) := 2(s) ds, 0 t 1.
0

For notational convenience we also set a(1 + s) := a(1 - s) for s > 0. The covariance matrix Y of the centered Gaussian vector (Yi) is given by

kYl := E[YkYl] = a(k/n) + 21(k = l), 1 k l n.

Similarly, the covariance matrix Y~ of the centered Gaussian vector (Y~i) is given

by

(2k+1)/2n

Yk~l := E[Y~kY~l] = n

a(t) dt + 21(k = l), 1 k l n,

(2k-1)/2n

where for k = l = n we used the convention for a(1 + s) above. We bound the

Hellinger distance using consecutively (9.4), Y 2 Id in (9.5) and (9.2), a Taylor

Asymptotic equivalence for volatility estimation

27

expansion for a and treating the case k = l = n by a Lipschitz bound separately:

H2(L (Yi, i = 1, . . . , n), L (Y~i, i = 1, . . . , n))

2

(Y )-1/2(Y

- Y~ )(Y )-1/2

2 HS

2-4 Y~ - Y

2 HS

4-4

(2k+1)/2n

2

n (a(t) - a(k/n)) dt

1kln

(2k-1)/2n

n (2k+1)/2n

2

4-4 O(R2n-2) + n

n

(a (k/n)(t - k/n) + O(Rn-1-)) dt

k=1

(2k-1)/2n

= 4-4 O(R2n-2) + O(R2n2-2-2)

= O(-4R2n-2).

Consequently, by (9.1) the total-variation and thus also the Le Cam distance be-

tween the experiments of observing (Yi) and of observing (Y~i) tends to zero for

n  , which proves that the white noise experiment E1 is asymptotically at least

as informative as the regression experiment E0.

To show the converse, we build from the regression experiment E0 a contin-

uous time observation by linear interpolation. To this end we introduce the lin-

ear B-splines (or hat functions) bi(t) = b(t - i/n) with b(t) = min(1 + nt, 1 -

tn)1[-1/n,1/n](t) and set

nn

n

Y^t := Yibi(t) = Xi/nbi(t) + ibi(t),

i=1 i=1

i=1

t  [0, 1].

Note that (Y^t ) is a centered Gaussian process with covariance function

nn

c^(t, s) := E[Y^t Y^s ] =

a((i  j)/n)bi(t)bj(s) + 2 bi(t)bi(s), 0 t, s

i,j=1

i=1

1.

For any f  L2([0, 1]) we thus obtain

E[ f, Y^

n
2] = a((i  j)/n) f, bi
i,j=1 n
a((i  j)/n) f, bi
i,j=1

f, bj f, bj

n
+ 2 f, bi 2
i=1
+ 2n-1 f 2,

28 Markus Reiﬂ

because nbi = 1 yields by Jensen's inequality f, nbi 2 f 2, nbi and we have i bi 1. This means that the covariance operator C^ induced by the kernel c^ is

smaller than

n

Cf (t) :=

a((i  j)/n) f, bj bi(t) + 2n-1f (t), f  L2([0, 1])

i,j=1

in the sense that C^ - C is positive (semi-)definite. Now observe that C is the

covariance operator of the white noise observations

dYØt =

n

Xi/nbi(t)

+

 n

dWt

,

t  [0, 1].

i=1

(9.10)

Hence, we can generate these observations from (Y^t ) by randomisation, i.e. by adding uninformative N (0, C - C^)-noise to Y^ . Now it is easy to see that observing

YØ in (9.10) and Y from E1 is asymptotically equivalent, since in terms of the respec-

tive covariance operators, using again (9.4), (9.5) and (9.2), the squared Hellinger

distance satisfies

H2(L (YØ ), L (Y ))

2

(CY )-1/2(C - CY )(CY )-1/2

2 HS

2-4n2

1

1n

2

a(t  s) - a((i  j)/n)bi(t)bj(s) dtds

00

i,j=1

11
= 2-4n2

n2
(a(t  s) - a((i  j)/n))bi(t)bj(s) dtds,

0 0 i,j=0

where for the last line we have used

n i=0

bi(t)

=

1

and

a(0)

=

0.

Since

bi(t)

=

0

can only hold when i - nt  {0, 1}, the -Ho®lder regularity of 2 implies for

t s - 1/n:

n2
(a(t  s) - a((i  j)/n))bi(t)bj(s)

i,j=0

= 1 (a ( nt /n)(t - (k + nt )/n) + O(Rn-1-))bk+ nt (t)bl+ ns (s) 2

k,l=0

= O(R2n-2-2) +

a ( nt /n)

1
(t - (k +

2
nt )/n)bk+ nt (t)

k=0

= O(R2n-2-2).

Asymptotic equivalence for volatility estimation

29

A symmetric argument gives the same bound for s t - 1/n. For |t - s| < 1/n we use only the Lipschitz continuity of a to obtain the bound O(R2n-2). Altogether we have found
H2(L (YØ ), L (Y )) 2-4n2 O(R2n-2-2) + n-1O(R2n-2) = O(-4R2n-2),
which together with the transformation in the other direction shows that the Le Cam distance between E0 and E1 is of order O(-2Rn-).

9.3 Proof of Proposition 3.2
The main tool is Proposition 9.1 below. Together with the H®older bound
|2( s h) - 2(s)| Rh, s  [0, 1],
it implies that for fixed  the observation laws in E1 and E2 have a Hellinger distance of order Rh-3/2-1/2. By inequality (9.1) this translates to the total variation and thus to the Le Cam distance.

9.1 Proposition. For  > 0 and continuous  : [0, 1]  (0, ) consider the law

,
P

generated

by

dYt =

t
(s)dB(s) dt +  dWt, t  [0, 1],
0

with independent Brownian motions B and W . Then the Hellinger distance between two laws P1, and P2, satisfies

H (P1,, P2,)

12 - 22



max
t[0,1]

1-3(t)

-1/2.

Proof.

The covariance operator C

of

,
P

is for f, g  L2([0, 1]) given by

Cf, g = E[ f, dY g, dY ] = E[ f, X g, X ] + 2 f, g = F G2 + 2 f g.

For covariance operators corresponding to 1, 2 we have with F (t) = -

1 t

f

(s)ds

30 Markus Reiﬂ

by twofold partial integration

| (C1 - C2 )f, f | = =

1 1 ts
(12 - 22)(u) duf (t)f (s) ds dt
000
1
F (u)2(12 - 22)(u) du
0
1
12 - 22  F (u)2 du = 12 - 22  CBM f, f
0

with CBM g(t) :=

1 0

(t



s)g(s) ds,

the

covariance

operator

of

standard

Brownian

motion. Using further the ordering C1 mint 12(t)CBM + 2 Id and (9.5), (9.2)

we obtain

C-11/2(C2 - C1 )C-11/2 HS

12 - 22  C-11/2CBM C-11/2 HS

12 - 22



(min
t

12

(t)CBM

+

2

Id)-1/2CBM

(min
t

12

(t)CBM

+

2

Id)-1/2

HS

= 12 - 22  F (CBM ) HS ,

employing functional calculus with F (x) = (mint 12(t)x + 2)-1x. The spectral 
properties of CBM imply that F (CBM ) has eigenfunctions ek(t) = 2 sin((k -

1/2)t), k

1,

with

eigenvalues

k

=

,4
4 mint 12(t)+(2k-1)222

whence

its

Hilbert-

Schmidt norm is of order maxt 1-3(t)-1/2. This yields the result.

9.4 Proof of Proposition 5.2

We only consider the case of odd indices k, both cases are treated analogously. The result of Theorem 6.1 in conjunction with Theorem 5.2 of ? establishes that E3o,dmd and the Gaussian regression experiment G3,m of observing

Yk = vs2(kh) + I(02(kh))-1/2k, k  Am odd, k  N (0, 1) i.i.d. (9.11)

are equivalent to experiments E~3,m = (Y , G , (P~ms2 )s2C(R)) and G~3,m (Y , G , (Q~ ms2 )s2C(R)), respectively, on the same space (Y , G ) such that

=

sup H2(P~sm2 , Q~ ms2 )
s2 C (R)

-2

(9.12)

Asymptotic equivalence for volatility estimation

31

holds for all  < 1.
To be precise, it must be checked that the regularity conditions R1 - R3 of ? are satisfied for all values . One complication is that in our parametric model the probabilities P and the Fisher information I() depend on h0 which tends to infinity. Yet, inspecting the proofs it becomes clear that the results remain valid if (a) the conditions R1-R3 hold for varying models, but with uniform constants and (b) the Fisher information is renormalized by the localisation such that the parametric rate -1/2 (in our block length notation) is attained. From the fact that P is the product of one-dimensional exponential family models we easily check condition R1 for  = 1 and condition R2 for any  > 0. Both conditions hold uniformly over h0 once the score l has been renormalized through multiplication by h0-1/2. In (5.2) we have already calculated the Fisher information and we infer directly condition R3 that h0-1I() is uniformly bounded away from zero and infinity. We thus infer (9.12).
In view of the independence among the experiments (E3o,dmd)m and equally among the experiments (G3,m)m we infer from (9.12) and (9.2)

sup H 2((mh=)1-1 P~ms2 , (mh=)1-1 Q~ sm2 )
s2 C (R)

( h)-1 -2

-1v2h20v4.

Since we assume h0 = o((1-2)/2), the right-hand side tends to zero provided

-1 + 2 

(1 - 2) 4 + +=

-

>0

2 + 1  2 + 1 (2 + 1)

holds. Since  < 1 is arbitrary, this is always satisfied for  < 1. In the case  = 1 we use h0 -p for some p < 1/2. We have derived asymptotic equivalence between the product experiments mE~3l,omc and mG~3,m. A fortiori, applying the ? result, this leads to asymptotic equivalence between observing (yjk) in experiments E2,loc and the corresponding Gaussian shift models of observing

dYt = I(02(t))1/2vs2(t) dt + (2h)1/2dWt, t  [0, 1].

(9.13)

32 Markus Reiﬂ

From the explicit form (5.2) of the Fisher information we infer for h0  

23/2 h0

I ()

-

1 4

+

1 21/2h0

e-h0 .

Consequently, by the polynomial growth of h0 in -1, the Kullback-Leibler diver-

gence between the observation laws from (9.13) and the model G3,loc converges to

zero. This gives the result.

9.5 Proof of Proposition 6.1

Since the observations yjk for j 1 are the same in Y and Y~, we can work conditionally on those. Moreover, it suffices to consider only the event  := { ^2 - 2  Rv} because the squared Hellinger distance satisfies (with obvious notation)

H2(L (Y ), L (Y~)) = E[H2(L ((y0k)k | (yjk)j 1,k), L ((y~0k)k | (yjk)j 1,k))] E[H2(L ((y0k)k | (yjk)j 1,k), L ((y~0k)k | (yjk)j 1,k))1 ] + 2 P()

with P()  0. Conditional on (yjk)j 1,k, both laws are Gaussian, (y0,k)k has mean µ with

µ0 = 2
j

1

Var(j k ) Var(yj k )

yjk

,

µk =
j

Var(j k ) 1 Var(yjk)

(-1)j+1yj,k-1 + yjk

,

k

and covariance matrix  with


2ck    
k,k = ckk     0,

j

1

2 Var(jk) Var(yj k )

+

2,

j

1

2 Var(jk) Var(yj k )

-

2 2

,

if k = k, if |k - k| = 1, otherwise,

1,

where ck := 1  (2 - k)  {1, 2}. Conditional mean µ~ and covariance matrix ~ of (y~0k)k have the same representation, but replacing Var each time by Var.
From the tri-diagonal structure of  and from

j

Var(j k ) 1 Var(yjk)  j

1

(h0/j)2 (h0/j)2 +

1



h0,

h0  ,

Asymptotic equivalence for volatility estimation

33

we infer  (2h0 + 2) Id h Id in matrix order. Combining this with the Hellinger bound (9.9) we arrive at the estimate

E[H2(L ((y0k)k | (yjk)j 1,k), L ((y~0k)k | (yjk)j 1,k))]

E

µ - µ~ 2 h

+

 - ~

2 HS

2h2

Var(jk) - Var(jk) 2 Var(yjk) + j 1,k Var(yjk) Var(yjk) h j 1,k

2 Var(jk) - 2 Var(jk) 2-2h-2.

Var(yj k )

Var (yj k )

The function G(z) :=

jk 2z jk 2z+2

has derivative

G (z)

=

jk 22 ( jk 2z+2)2

and thus

satisfies uniformly over all z bounded away from zero |G(w)-G(z)|

.jk 22|w-z|
( jk 2+2)2

Inserting |2 - 02| v and jk  h/j, we thus find the uniform bound on 

Var(jk) - Var(jk) 2 Var(yjk) Var(yjk)

v24h4/j4 (2 + h2/j2)4



v2 min(h0/j, j/h0)4.

Putting the estimates together, we arrive at

H2(L (Y ), L (Y~))

v2
j

min(h0/j, j/h0)4
1,k

1

+ h02/j2 h0

+

1 h02

+ P()

2v2h-1 min(h0/j, j/h0)2h-0 1 + P()
j1

 v2h-0 1-1 + P()

such that the Hellinger distance tends to zero uniformly if h0-1v2 = o(), which is ensured by our choice of h0. This implies asymptotic equivalence of observing Y and Y~ and thus of experiment E2 and of just observing (yjk)j 1,k in E2. By independence the latter is equivalent to E2,odd  E2,even.

9.6 An explicit series representation

We aim at deriving the formula

(2

3 + 2j2)2

=

1 + 4e-2 - e-4 4(1 - e-2)2

-

1 2

j1

(9.14)

for any  > 0. We employ Fourier techniques and consider the Fourier coefficients

of g(x) = e-x/:

g^(j) := (2)-1/2

2
g(x)eijxdx =

(1 - e-2)

,

0 2( - ij)

j  Z.

34 Markus Reiﬂ

For the 2-periodic convolution g  g(x) = xe-x/ + (2 - x)e-(2+x/) we obtain the Fourier coefficient as a product:
g  g(j) = 2(1 - e-2)2 . 2( - ij)2

The Parseval formula therefore yields

12 (2 + 2j2)2 = 3(1 - e-2)4

|g



g(j)|2

=

3(1

2 - e-2)4

gg

2 L2

.

jZ

jZ

We

infer

that

(

 

)3

gg

2 L2

equals

3

2
(1 - e-2)2x2 + 42e-4 + 4(e-2 - e-4)x e-2x/dx

0

= e-2 + (1 - e-4)/4 (1 - e-2)2

and thus obtain

3 2 (2 + 2j2)2 = (1 - e-2)2

e-2 + (1 - e-4)/4

.

jZ

Using the symmetry in j, we establish (9.14).

Acknowledgement
I am grateful to Marc Hoffmann, Mark Podolskij and Johannes Schmidt-Hieber for very useful discussions.

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Localising temperature risk" by Wolfgang Karl H‰rdle, Brenda LÛpez Cabrera, Ostap Okhrin and Weining Wang, January 2011.
002 "A Confidence Corridor for Sparse Longitudinal Data Curves" by Shuzhuan Zheng, Lijian Yang and Wolfgang Karl H‰rdle, January 2011.
003 "Mean Volatility Regressions" by Lu Lin, Feng Li, Lixing Zhu and Wolfgang Karl H‰rdle, January 2011.
004 "A Confidence Corridor for Expectile Functions" by Esra Akdeniz Duran, Mengmeng Guo and Wolfgang Karl H‰rdle, January 2011.
005 "Local Quantile Regression" by Wolfgang Karl H‰rdle, Vladimir Spokoiny and Weining Wang, January 2011.
006 "Sticky Information and Determinacy" by Alexander Meyer-Gohde, January 2011.
007 "Mean-Variance Cointegration and the Expectations Hypothesis" by Till Strohsal and Enzo Weber, February 2011.
008 "Monetary Policy, Trend Inflation and Inflation Persistence" by Fang Yao, February 2011.
009 "Exclusion in the All-Pay Auction: An Experimental Investigation" by Dietmar Fehr and Julia Schmid, February 2011.
010 "Unwillingness to Pay for Privacy: A Field Experiment" by Alastair R. Beresford, Dorothea K¸bler and Sˆren Preibusch, February 2011.
011 "Human Capital Formation on Skill-Specific Labor Markets" by Runli Xie, February 2011.
012 "A strategic mediator who is biased into the same direction as the expert can improve information transmission" by Lydia Mechtenberg and Johannes M¸nster, March 2011.
013 "Spatial Risk Premium on Weather Derivatives and Hedging Weather Exposure in Electricity" by Wolfgang Karl H‰rdle and Maria Osipenko, March 2011.
014 "Difference based Ridge and Liu type Estimators in Semiparametric Regression Models" by Esra Akdeniz Duran, Wolfgang Karl H‰rdle and Maria Osipenko, March 2011.
015 "Short-Term Herding of Institutional Traders: New Evidence from the German Stock Market" by Stephanie Kremer and Dieter Nautz, March 2011.
016 "Oracally Efficient Two-Step Estimation of Generalized Additive Model" by Rong Liu, Lijian Yang and Wolfgang Karl H‰rdle, March 2011.
017 "The Law of Attraction: Bilateral Search and Horizontal Heterogeneity" by Dirk Hofmann and Salmai Qari, March 2011.
018 "Can crop yield risk be globally diversified?" by Xiaoliang Liu, Wei Xu and Martin Odening, March 2011.
019 "What Drives the Relationship Between Inflation and Price Dispersion? Market Power vs. Price Rigidity" by Sascha Becker, March 2011.
020 "How Computational Statistics Became the Backbone of Modern Data Science" by James E. Gentle, Wolfgang H‰rdle and Yuichi Mori, May 2011.
021 "Customer Reactions in Out-of-Stock Situations ≠ Do promotion-induced phantom positions alleviate the similarity substitution hypothesis?" by Jana Luisa Diels and Nicole Wiebach, May 2011.
SFB 649, Ziegelstraﬂe 13a, D-10117 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
022 "Extreme value models in a conditional duration intensity framework" by Rodrigo Herrera and Bernhard Schipp, May 2011.
023 "Forecasting Corporate Distress in the Asian and Pacific Region" by Russ Moro, Wolfgang H‰rdle, Saeideh Aliakbari and Linda Hoffmann, May 2011.
024 "Identifying the Effect of Temporal Work Flexibility on Parental Time with Children" by Juliane Scheffel, May 2011.
025 "How do Unusual Working Schedules Affect Social Life?" by Juliane Scheffel, May 2011.
026 "Compensation of Unusual Working Schedules" by Juliane Scheffel, May 2011.
027 "Estimation of the characteristics of a LÈvy process observed at arbitrary frequency" by Johanna Kappus and Markus Reiﬂ, May 2011.
028 "Asymptotic equivalence and sufficiency for volatility estimation under microstructure noise" by Markus Reiﬂ, May 2011.
SFB 649, Ziegelstraﬂe 13a, D-10117 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

