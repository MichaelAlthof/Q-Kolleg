BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2016-051
Dynamic Topic Modelling for Cryptocurrency Community Forums
Marco Linton* Ernie Gin Swee Teo*² Elisabeth Bommes*³ Cathy Yi-Hsuan Chen*³ Wolfgang K. Härdle*³
* University of York, United Kingdom *² Singapore Management University, Singapore
*³Humboldt-Universität zu Berlin, Germany This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Dynamic Topic Modelling for Cryptocurrency Community Forums 
Marco Linton1, Ernie Gin Swee Teo2, Elisabeth Bommes3, Cathy Yi-Hsuan Chen3, and Wolfgang Karl H¨ardle2,3
1University of York, Heslington, York YO10 5DD, United Kingdom 2School of Business, Singapore Management University, 50 Stamford Road,
Singapore 178899 3Ladislaus von Bortkiewicz Chair of Statistics, C.A.S.E. ­ Center for Applied Statistics and Economics, Humboldt-Universitt zu Berlin, Unter den Linden 6,
10099 Berlin, Germany
November 24, 2016
Abstract
Cryptocurrencies are more and more used in official cash flows and exchange of goods. Bitcoin and the underlying blockchain technology have been looked at by big companies that are adopting and investing in this technology. The CRIX Index of cryptocurrencies hu.berlin/CRIX indicates a wider acceptance of cryptos. One reason for its prosperity certainly being a security aspect, since the underlying network of cryptos is decentralized. It is also unregulated and highly volatile, making the risk assessment at any given moment difficult. In message boards one finds a huge source of information in the form of unstructured text written by e.g. Bitcoin developers and investors. We collect from a popular crypto currency message board texts, user information and associated time stamps. We then provide an indicator for fraudulent schemes. This indicator is constructed using dynamic topic modelling, text mining and unsupervised machine learning. We study how opinions and the evolution of topics are connected with big events in the cryptocurrency universe. Furthermore, the predictive power of these techniques are investigated, comparing the results to known events in the cryptocurrency space. We also test hypothesis of self-fulling prophecies and herding behaviour using the results.
JEL classification: C19, G09, G10 Keywords: Dynamic Topic Modelling, Cryptocurrencies, Financial Risk Financial support from Deutsche Forschungsgemeinschaft (DFG) via CRC 649 "Economic Risk" is gratefully acknowledged.

1 Introduction
Cryptocurrencies such as Bitcoin have become more mainstream over the years with big companies adopting and investing in the technology. Once seen to be the domain of technophiles and radicals, cryptocurrencies are now widely traded on many exchanges throughout the world. Governments have also discussed the possibilities of adopting cryptocurrencies as a means to offer digital currency. The underlying network (called the blockchain) of cryptocurrency is decentralised, unregulated and highly volatile, making its situation at any given moment difficult to assess. On the other hand, an almost bottomless source of information can be found in the form of unstructured text written by cryptocurrency users on the internet. Crowd wisdom found in such networks can be a powerful indicator of major events affecting cryptocurrencies. We attempt to take advantage of this to analyse and assign quantitative meaning to such resources.
Early academic statistical analysis of Bitcoin includes Cheah and Fry (2015) and Cheung et al. (2015), both looked at speculative bubbles using Bitcoin price data. More related to this paper are works that looked at social media information and search engine data such as Kristoufek (2013), Mai et al. (2015) and Matta et al. (2015).
Utilizing techniques from dynamic topic modelling (DTM), text mining and machine learning, we pull data from a popular cryptocurrency forum and attempt to detect events such as new trends in currencies, fraudulent schemes or legal and economic issues. The DTM technique, as a type of unsupervised learning, is demanded when the taxonomy is unclear. Some important topics may be left out if one does a subjective judgement for taxonomy. The DTM is designed for summarizing the unknown but important features in the world. In addition to "discover" and "quantify" the hidden topics, the DTM is able to characterize the evolution of the hidden topics, which may be useful of evaluating the importance and persistence. Specifically, we collect user information and text associated with time stamps and apply unsupervised dynamic topic modelling, studying how opinions and the evolution of topics are connected with big events in the cryptocurrency universe. Furthermore, the predictive power of these techniques are investigated, comparing the results to known events in the cryptocurrency space.We also test hypothesis of self-fulfilling prophecies and herding behaviour using the results. For example, Smailovi´c et al. (2013) were able to improve predictive power for stock markets by using sentiment derived from Twitter feeds. Cryptocurrency discussion forums tend to be very responsive and sensitive to events; this makes it a suitable candidate to test the predictive ability of dynamic topic modelling.
2 Data
A good, consistent and representative source of information regarding the cryptocurrency community can be found on talk forums such as bitcointalk.org. Acquiring the data from this platform requires deploying a web scraper to download the relevant html pages from the server and extract the embedded information. Good practices of web scraping were
1

used to ensure there was no risk of overloading servers such as waiting fifteen seconds between each request and respect for the robots.txt protocol. Information regarding thread ids, post ids, usernames, time stamps, post titles, post texts, quotes of other posts and links were collected and stored in a database. There are three main discussion boards which was used in this study, they are "Bitcoin", "Economy" and "Alternative Cryptocurrencies". The two remaining discussion boards were "Other" which was discarded as it mainly deal with non-related topics and "Local" which is also discarded as discussions are in local languages. Each of the main discussion boards were divided into subforums such as "Trading Discussions" and "Scam Accusations". In total there were little under 200 subforums, half a million different threads with over 15 million posts (including local discussion). For the purpose of our study, we concentrate on the Bitcoin discussion subforum.
Knowledge is power so the more information we have, the better. Aside from this, the main motivations behind collecting these bits of information are as follows: Thread ids and post ids are used to uniquely identify posts and the thread they come from; usernames are used to associate each post with an agent in order to create a graph for herding and social network analysis; time stamps are used to classify posts into time slices for the dynamic topic model; post titles and post texts are used in conjunction to form a document for the dynamic topic model; links and quotes are used in order to analyse how posts relate to each other and other websites which is useful for herding and social network analysis.
3 Topic Modelling
We apply topic modelling to these forums in order to model trends in the community and to see how real life events effect the topics discussed and vice versa. The most commonly used model to model topics in machine learning is LDA (Latent Dirichlet Allocation) by Blei et al. (2003).
This model, however, makes the assumption that all documents modelled are exchangeable and therefore the aspect of time is completely lost and the idea of detecting events becomes pointless. Therefore, the model we use is the dynamic topic model proposed by Blei and Lafferty (2006), which is a variant of LDA that analyses documents in a set of predetermined discrete time slices and assumes topics evolve smoothly from slice to slice with Gaussian noise.
LDA is a generative probabilistic model for text, however it has also been applied successfully to other types of discrete data sets such as images. This model differs from most as it is completely unsupervised, therefore removing the bottleneck of having to acquire a trained model, and the problem it tries to solve is not classification into topics, but rather assigning topic distributions to documents. These properties mean that it is ideal to apply to large quantities of unstructured text where it would be impossible to obtain reliable training data to produce a model and simply classifying documents into topics would produce confusing and unrealistic results. Bao and Datta (2014)apply the LDA
2

method to extract the risk types (meaningful topics) in Security Exchange Commission 10-K forms, and find many plausible and meaningful risk types that have been left out in a supervised learning scheme proposed by Huang and Li (2011). The inferred topics from a supervised learning only cover 78% of topic pools.
The Dirichlet distribution is defined on a (k - 1) dimensional simplex

k
k = q  Rk : qi = 1, qi  o, i - 1, 2, . . . , k .
i=1

(1)

It can be thought of as a distribution of random probability mass/density functions (pdf). An excellent example based introduction can be found in Frigyik et al. (2010).

Definition 1 Let Q be a real value in k and suppose that   Rk, i > 0 and define

0 d=ef T 1. Then Q has a Dir() distribution with pdf f (q; ) =

(0) (i)

k
qii
i=1

-1.

i=1

Figure 1: Plots of sample pmfs drawn from Dirichlet distributions for various values of  XFGtdmDirichlet
Density plots are given in Figure 1 for different . Given a document with a certain word distribution, the task is obviously to determine  from the set of documents.
The gamma function is a generalization of the factorial function, (s) = s(s - 1) with (1) = 1. The mean of a Dir() random variable is EQ = /0. Note that  determines the "location" of words in documents, a "small"  creates sharp peaks on defined locations. You may think of the document that has been written by the poet in the flim "Shining", in the described Dir() framework, there is just one "big" peak of the words at "all work and no play makes Jack a dull boy". With just k = 2 words in a document the Dir() reduces to the Beta distribution with pdf

f (x; a, b) = (a + b) xa-1(1 - x)b-1. (a) + (b)

(2)

For  = (a, b)T with Q = (X, 1 - X)  Dir() for X  Beta(a, b).

3

In a Bayesian context, employed here entirely for numerical and computational reasons, one finds that the multinomial distribution with pdf

f (x; n, q) =

n!
k
xi

!

k
qixi
i=1

,

x,

q



Rk

i=1

(3)

is a so called conjugate prior.

As the binomial distribution (for k = 2) is the conjugate prior for the Beta distribution,

one finds that if (X | q)  M ultR(n, q) and Q  Dir(), then (Q | X = x)  Dir( + x). Again we refer for a proof of this to Frigyik et al. (2010).

The basic idea of a static Topic Model (TM) is to take a document as a sample of

words generated by a Dir() distribution, where  represents the topic. More precisely it

is assumed that a document is generated via the following imaginary random process:

1. For each topic k, draw a distribution over words k  Dirv() 2a. For each document d, draw topic proportions d from over the (k - 1) simplex 2b. For each word Wd,n within the document: i. Draw a topic assignment Zd,n  M ult(d), Zd,n  {1, ..., k} ii. Draw a word Wd,n  M ult(zd,n), Wd,n  {1, ..., V } z is a vector of , one for each topic.  is a matrix of word|topic parameters. The number of topics is assumed known beforehand though determining the number of

topics (clusters) is rather challenging in unsupervised learning. One can easily find some

methods being proposed for estimating the number of topics automatically, but one has

to be aware of several restrictions. Firstly, Wallach et al. (2010) find that the estimated

numbers of topics are strongly model-dependent. Besides, merely using fit statistics such

as perplexity may be problematic due to a negative relation between the best fitted model

and the substantive fit (Chang et al. (2009)). To balance the substantive fit and statistical

fit, Bao and Datta (2014) propose strategic procedures - Firstly, employing statistical fit

to reduce the set of candidate models with different numbers of topics. Relying on the

predefined perplexity, one can optimize the predictive power of model. In their case, the

numbers can be chosen as 30, 40 and 50 in terms of perplexity and a converge in the

range [30,50] is shown. Secondly, the substantive fit for semantic coherence is compared

among the competing models. To be specific, the model precision in word intrusion task is

evaluated. It's so called "semantic validation". The semantic coherence of topics perhaps

is the most useful indicator w.r.t the quality of topics, reflecting to how well the topic

matches a human concept through a list of keywords. The number, 30, is therefore chosen

due to its best semantic coherence performance.

Let us provide an example that sheds some light on this generation mechanism. Sup-

pose that the "word universe" corresponds to the most frequent words in the NASDAQ

analysis study by Zhang et al. (2016) and Bommes et al. (2017), as given in Table ??.

The idea is now that different topics have different word distribution as given by

M ult(z). Suppose there were k = 2 topics/sectors, corresponding to "finance" and "IT"

4

Word free well gold best fool strong like top better motley

Freq. (in k) 649 238 235 207 200 196 172 167 162 152

Freq. for Top 5 Sectors 10 9 1 9 5 5 5 3 0 2

Table 1: Most frequent words used in NASDAQ articles

and further suppose that the distribution of words over topics are generated by Dir(). To be precise, for k = 2, the Dirichlet distribution boils down to a Beta() distribution. It could be the case that for the topic "finance", the third most frequent word "gold" is more concentrated. Whereas, for the topic "IT", concentration would be more around the words "fool" and "motley". See figure 2 below for an illustration that shows the random outcomes 1 and 2. In such as scenario, we would prefer a different word distribution for each these topics.

Freq Freq

300 1 250 200 150 100
50 0

300 2 250 200 150 100
50 0

free well gold best fool strong like top better motley free well gold best fool strong like top better motley

Words for topic "Finance"

Words for topic "IT"

Figure 2: Distribution of words by topic (1 and 2)

XFGdtmWDistr

Step 2bi. now refers to the random mechanism that a word to be written down is drawn from 1 or 2. Suppose that the first has to be drawn from 1 since Z1,1 = 1, for d = 1 (1st document) and n = 1 (first word). So a random outcome as described in Step 2bii. could be the word W1,1 = "gold" (the word with the second highest frequency in 1. For the next word (n = 2), Z1,2 could take the value 1 again and now W1,2 = "strong" could be the outcome. A third word could be via Z1,3 = 2, W1,3 = "free", and so on. The task of TM is now to invert this mechanism and calibrate the observed documents to the parameters of the Dir and M ult distributions.
The problem of static TM though is that there is no timeline, an issue that is of course necessary for the questions we would like to study here. The dynamic topic model, on

5

the other hand models each time slice with LDA, but its parameters  and  are chained together in a state space model which evolves with Gaussian noise:

t,k|t-1,k  N (t-1,k, 2I)

(4)

t,k|t-1,k  N (t-1,k, 2I)

(5)

Like this we get a smooth evolution of topics from slice to slice. The state space diagram describes the model well:

Figure 3: State space diagram of the dynamic topic model
Due to the nonconjugacy of the Gaussian and multinomial distributions, exact inference is intractable so the authors present two methods for approximate inference using variational methods: variational Kalman filtering and variational wavelet regression.
4 Preprocessing
Preprocessing steps make a big difference to the outcome of topic models. Especially when working in the domain of a forum where thousands of users post everyday, most likely without looking words up in the dictionary or worrying about the correctness of their grammar, we will find many spelling mistakes, slang and proper names that aren't going to be simple to handle. Therefore, a natural approach to preparing the data appropriately would be to use a POS tagging algorithm coupled with a tokeniser to infer from context what words have which function. Stop words will appear multiple times in each sentence without conveying any meaning and therefore are removed and so are functional words, verbs, adjectives and adverbs leaving us only with nouns, proper nouns and foreign words. In this way we have all the most important information from each post without losing out on non-standard vocabularies that arise in the community. To combat typos, the words occurring in fewer than 10 documents were removed and to get rid of generic words, the words appearing in more than 10% of the documents were also removed. In the end, from a dictionary of 500,000 words, we obtained one of 10,000 meaningful words. Once we had
6

the cleaned text, the preparation for the dynamic topic model (code by Gerrish) consisted of converting the corpus to a sparse matrix representation whereby each line represented a document and was in the following form:
N unique words word id : word count word id : word count..... Also a file containing information about the time slices was prepared of the following format: N time slices N docs slice 1 N docs slice 2 ... Where N denotes number of documents in the corresponding slice. On top of these necessary files, for each corpus a file containing metadata, a dictionary file and a vocabulary file were also produced. The metadata file contains a header describing the fields and then each line represents a document with the following informations: thread id, post id, date time, username, post text, post quotes and post links. This will come in handy for information retrieval and herding analysis. The dictionary file is a python dictionary object which maps ids to words and contains word count information. The vocabulary file is a human readable file where each line is a word from the dictionary and its position maps to its key.
5 Trends
As mentioned in the introduction, the data acquired from the forum was divided into subforums. The main subforums by posting volume are: `Economics', `Bitcoin Discussion', `Altcoin Discussion' and `Speculation'. The dynamic topic model was run on these subforums and in addition also with the subforum 'Scam Accusations'. The commonly used 50/k heuristic by Griffiths and Steyvers (2004) for the alpha parameter was chosen and a varying number of topics were modelled. All models were run with weekly data over the 2009/11/22 (when the forum was created) to 2016/08/06 period.
Each topic in the hidden structure is represented as a distribution over words and therefore the most human interpretable way of understanding what a topic is about is to look at the most probable words in each distribution. An example representation can be found in Table ?? in which some topics are shown for the last time slice in the Bitcoin Discussion subboard. Each time slice will have it's own similar representation. While the words may change over time as new trends emerge and fall, the topic will intuitively remain the same. For example, in the table shown we can see that topic 50 is about Bitcoin mining, but the top words in the first time slice are rather different even though we would still assign the same topic label to it; cpu, difficulty, proof, mining, adjustment, proof-ofwork, power, attack were the top words in 2009 in topic 50, demonstrating how Bitcoin mining has evolved to cope with the increasing mining difficulty. In fact we can directly compare different mining hardware and how they were relevant over different periods of
7

Topic Number 1 2 5 7
12 18 20 23 24 30 32 33 35 38 42 45 48 50

Most Probable Words value, gold, bar, dollar, rate, demand, interest, asset business, casino, house, trust, gambling, run, strategy, player government, control, criminal, law, study, regulation, state, rule use, service, option, cash, good, spend, fiat, convert account, payment, fund, card, paypal, party, merchant, credit score, online, pay, shop, bill, product, purchase, phone wallet, key, paper, computer, storage, code, data, secure price, trade, market, trader, drop, volume, sell, stock trading, term, hold, buy, pump, dump, earn, gamble exchange, bitfinex, lesson, cryptocurrency, crash, platform, altcoins, popularity investment, risk, invest, aim, impact, salary, making, way year, altcoins, end, today, adoption, prediction, happen, trend transaction, block, fee, chain, confirmation, hour, minute, hardfork altcoin, company, loss, hack, scam, hacker, scammer, road bank, system, security, fiat, banking, role, function, institution ethereum, split, advantage, issue, side, change, fork, core forum, post, topic, member, bitcointalk, thread, index, php mining, miner, network, power, pool, cost, reward, electricity

Table 2: Notable topics from 50 topic model on Bitcoin Discussion subforum from 2016/07/31 to 2016/08/06

time in Figure 4. As we can see, in topic 50 the word CPU was very prominent initially and all the
others were non-existent. Then when the network grew to an extent that the quantity of Bitcoins produced by CPU mining were worth less than what it cost to operate, GPU mining came into play. Another stride in mining hardware was the usage of application specific integrated circuits (asic). The first asic mining hardware project called the `Avalon Project' was announced in 2012 on the forum and the peak in the third plot in January 2013 corresponds to the release of their first chip. In the fourth plot we see the timeline of Antminer, a brand of asics considered to be the current top of the line. As expected we can see a positive trend over the last years with peaks in discussion around releases of new models.
As an up and coming and fast growing technology, Bitcoin has had its fair share of issues. In fact, due to its unregulated nature and uncertainty of legality or legitimacy as currency in most corners of the world, the cryptocurrency history is laden with high profile hacks, ponzi schemes and scam websites. Many of these go undetected for months until a certain point where gradually complaints start to stack up and a realisation or confirmation of the events takes place.
Probably the biggest example of such an event in Bitcoin history is the insolvency of the MtGox Bitcoin exchange in 2014. MtGox originally started off in 2007 as a platform for trading Magic: The Gathering Online trading cards which is where it got its name (Magic: The Gathering eXchange). In 2010, however, it was rebranded as one of the first exchanges where people could buy and sell Bitcoins. The exchange grew gradually and watched the price of Bitcoin go from less than USD0.1 in 2010 to parity with the US dollar

8

p(w = cpu | k = 50)

CPU 0.2

0.1

0.0 11/2009

06/2011

12/2012

07/2014

02/2016

ASICS

0.04 0.03 0.02 0.01 0.00
11/2009

06/2011

12/2012

07/2014

02/2016

p(w = antminer | k = 50)

p(w = gpu | k = 50)

GPU

0.03

0.02

0.01

0.00 11/2009

06/2011

12/2012

07/2014

02/2016

Antminer

0.002

0.001

0.000 11/2009

06/2011

12/2012

07/2014

02/2016

p(w = asics | k = 50)

Figure 4: Comparison of word evolution for different mining technologies 22/11/2009 - 06/08/2016 XFGdtmMining

in 2011. At this point however, the owner of MtGox decided to sell the exchange in order to dedicate himself to `other projects'. An internal email dating back from after the sale of the exchange revealed that already 80,000 Bitcoins (worth over $60,000 at the time) had already been missing before any of the public fiascos had occurred and had never been recovered. However, it was only three months later that a major event occurred. 60,000 accounts were exposed publicly and a compromised MtGox auditors account was used to create huge sell orders and crash the Bitcoin price from $17.51 to $0.01. As a result of this event the site was down for a week and many of the exposed accounts were used to steal coins from other bitcoin services due to password reuse. However, unlike many other Bitcoin services, MtGox managed to recover its reputation and became the largest Bitcoin exchange, handling 70% of all trades worldwide. Fast forwarding to 2013, when their real problems began, in June withdrawals of US dollars were suspended and even though a couple of weeks later in July it had been announced that withdrawals had fully resumed, as of September few withdrawals had successfully been completed. Complaints piled up over the next few months and on 7 February 2014 all Bitcoin withdrawals had been suspended for good. On the 24th of February all activities had halted, the website went offline and a leaked internal crisis management document claimed that 744,408 Bitcoins (worth almost half a billion dollars) had been lost and the company was insolvent.
As we can see, MtGox has had a roller coaster of a past with repeated security issues and poor management and has therefore been a major topic of discussion among users of the main Bitcoin forum. The main topics in which MtGox arises are predictively topic 23 about Bitcoin trading and markets and topic 38 about scams and hacks. Naturally the word/topic probability plot in Figure 5 reflects this and we can see peaks corresponding to the main events. In topic 38 there is a clear peak in mid 2011 during the first hack and in February 2014 also. Meanwhile in topic 23 there is a gradual peak starting in mid 2013

9

when the transaction issues first occurred and trailing off at the same time MtGox starts to gain momentum in topic 38.

p(w = mtgox | k = 23)

Topic 23
0.02

0.01

0.00 11-2009

02-2011

04-2012

07-2013

Topic 38

09-2014

12-2015

p(w = mtgox | k = 38)

0.3
0.2
0.1
0.0 11-2009

02-2011

04-2012

07-2013

09-2014

12-2015

Figure 5: MtGox word evolution 22/11/2009 - 06/08/2016 XFGdtmMtGox
MtGox is only one example of the many scams and hacks resulting in huge losses that have occurred over the years and it is because of this that cryptocurrencies get a bad rap. Many services have come and gone, but none quite so spectacularly as MtGox.
Currency exchanges, mining hardware manufacturers, technology startups, mining pools and many other cryptocurrency related services have almost infallibly been victims of hacks and inside jobs, revealed as ponzi schemes, virus promoters etc. As soon as such events occur or are discovered, we would expect there to be gradual buildups or sudden explosions of discussion on the forum depending on the situation. In general, we would expect any event in the Bitcoin universe to be discussed on the forum and therefore be a part of the inferred generative process of the topic structure.
We want to evaluate the effectiveness of topic models in discerning these types of events. In our MtGox example, the word probabilities over time are characterised by relatively flat probabilities in general and spikes at the time of events. We can take advantage of this structure and hypothesise that it extends to other events. First we must validate this against other events. A curated list of Bitcoin services which have been victims of hacks or perpetrators of scams have been compiled over the years in a thread on bitcointalk.org (https://bitcointalk.org/index.php?topic=576337.0). This list will form our basis for event discovery validation. This could be done for other types of events however the most complete information can be found regarding scam/hack events since they are of relevance and interest to all involved with Bitcoin. We look at the topic prominence for this set of words and see if the model correctly partitions them in a scam/hack topic.

10

Number of Topics k 10 20 30 40 50

µ -185.74 -204.28 -176.46 -202.10 -205.83

 66.62 65.57 52.80 68.99 63.17

Table 3: Topic coherence statistics

6 Choosing k and Analysis

The choice of the number of topics has been an issue ever since topic models were first introduced in 2003. For this particular study, we used the Umass coherence metric by Mimno et al. (2011) to evaluate which number of topics was optimal. This method involves taking the top N words for each topic and taking measures of their occurrences and cooccurrences in the corpus. Formally it is defined as:

N-1 N D(wi, wj ) +

i=1 j=i+1

D(wi)

(6)

Where wi and wj are the ith and jth ranked words in a given topic respectively and D(w) is the number of documents in which word w occurs. We set N = 20.
It has been shown to correlate well with human interpretations of what constitutes a coherent topic. In addition, the metric does not require external validation, simplifying the procedure and making it more versatile. To make the repeated training of models viable, we calculated Umass coherence on a subsample of 100 weeks of data. In Table ?? we can see the results of the coherence evaluation. We have taken the arithmetic mean and standard deviation of the output values over the 100 chained LDA models; higher values mean more human understandable topics. Clearly our model is optimal when we choose 30 for the k parameter since on average the topics are more coherent and stable over time. We also observe that lower numbers of k are more coherent than higher values, but are also less stable over time. While this method does a good job at finding the number of topics more attuned to human intuition, we would also like to study how this effects event detection.
The generative process described now gives us a multi-layer interpretation of the data. We have K topics with D documents and W words. Each topic can be described by a vector of length W of word/topic probabilities. Each document can be described by a vector of length K of topic/document probabilities. Each topic changes over each of the T time slices and therefore each topic/document distribution acquires a different meaning depending on where it is in the timeline.
Say we have a particular word w in our vocabulary we would like to learn something about. The best way to do this is to look at the word probabilities over a certain time slice in the topics. We can call this concept the word prominence and we would like to

11

maximize this in order to find the most relevant topic.

arg

max
k

tj

1 -

ti

tj t=ti

p(w|k,

ti)

(7)

Once we have found this topic (or topics if we want to find several), looking at the

topics top words will allow us to discover in which context this term is discussed the most.

We can also plot the evolution of the probability over time of this particular word in this

topic and see when it was most used, when it came into use or passed out of use. Quite

often words with same spelling but different meaning (homonyms) occur or words that can

be discussed in different contexts (for example price could be present in a stock market

topic or in a groceries topic). Whereas usually it wouldn't be a simple task to discern

these words, topic models account for them very nicely and provide a useful perspective.

In addition to analysing the word/topic distribution we can also take a look at the

topic/document distributions and determine in which time slice which topics were `hotter'

and which were `colder' and identify trend starters. The hotter a topic k at time t, the

more documents are going to exhibit higher mixtures of the topic. The inverse is true for

colder topics. We can define the topic temperature as follows by Hall et al. (2008):

1

p(k|d)p(d|t) =

p(k|d)

d:td=t

Dt d:td=t

(8)

Where Dt is the number of documents in time slice t and td is the date document d was written.

7 Detection
From the list of events acquired from the forum, all those solely concerning individuals or causing losses of fewer than 1000 Bitcoins were removed. As a consequence of this procedure, we were left with 33 different Bitcoin services (and 37 different events). For each word we determine which topics the word achieves a topic prominence larger than a certain threshold. Typically, any given word will only appear in a handful of topics and most in just 1 or 2. Even though a certain topic may not have anything to do with a chosen word, topic models have the property that the probability of a word occurring in a topic is never 0, albeit negligible. Therefore we use a very low empirically tested threshold to determine which topics to test and discard the noisy ones. Then we analyse the topic prominence of the words conditioned on topics through time and determine an event occurring to be when its upper control limit is breached. I.e. when:

p(w|k, ti+1) > µ(p(w|k, t1:i)) + 3 (p(w|k, t1:i))

(9)

Table ?? contains the information regarding of our events and the dates they occurred. We compared these events against those detected in our model using the method described

12

Number of Events

and have marked with an asterisk those that went undetected. Most of the events causing losses of circa 2000 Bitcoins and under (indicated) went
undetected and almost all of those causing larger losses were identified. As hypothesized in the previous section, the large majority of these events were found to be in a single topic (topic 38), demonstrating the effectiveness of topic models in discriminating event types and providing an indicator for future such events.
This event detection algorithm was also run on our 10, 30 and 50 topic models. For the varying number k we can see what effect it has on our event distribution in Figure 6. With the number of topics considered to be most coherent, our events are grouped mainly into a single topic. On the other hand, the less coherent topics are composed of many junk topics in the higher k case, or more general topics in the lower, therefore resulting in inconsistency in the experiment. A lower k results in fewer detections as our topics will each be less relevant and a higher k results in many junk topics and detections across more topics.
20
15
10
5
0 1 3 4 6 10 12 14 21 23 30 38
Figure 6: Event partitioning over varying k parameters, 10 topics (no filling), 30 topics (dashed filling), 50 topics model (densely dashed filling)
XFGdtmEvents
In addition, for each event we can observe the impact it has on the topic structure by measuring the deviation of the topic temperature from the mean at the time in which it occurred. Since our timeline and number of time slices is large and we are using a symmetric Dirichlet prior, our topics are going to be rather general and fixed through time and the change in temperature between different times won't be significant. However, one can note in Figure 7 that all values are positive at the times the events occurred and appreciate the event hierarchy that follows.
8 Conclusion
In the above piece of work we have introduced and explained topic models. A dataset has been created from user posts on bitcointalk.org by using web scraping; then text-mining techniques were used to prepare the data for dynamic topic modelling and consequently a
13

Event Ubitex* (1,138b) Allinvain MtGox Mybitcoin Bitomat Mooncoin Bitscalper Linode Betcoin* (3,171b) Bitcoinica Btc-e Kronos Bitcoin Savings and Trusts Bitfloor Btcguild* (1,254b) OkPay (main victim of 2013 Fork) Ziggap* (1,708b) Just-Dice Basic-Mining* (2,131b) Silkroad2 Vircurex* (1,454b) GBL Bips* (1,294b) Picostocks* (5,896b) MtGox Flexcoin Cryptorush Mintpal Silkroad2 Bitstamp Bter Cryptsy Shapeshift Gatecoin* Bitfinex

Dates 2011-04 to 2011-07 2011-06-13 2011-06-19 2011-06-20, 2011-07 2011-07-26 2011-09-11 2012-01 to 2012-03 2012-03-01 2012-04-11 2012-04-12, 2012-07-13 2012-07-13 2012-08 2012-08-28 2012-09-04 2013-03-10 2013-03-11 2013-02 to 2013-04 2013-07-15 2013-10 2013-10-02 2013-10-05 2013-10-26 2013-11-17 2013-11-29 2014-02-24 2014-03-02 2014-03-11 2014-10-14 2014-11-06 2015-01-04 2015-02-14 2016-01-01 2016-04 2016-05-13 2016-08-03

Topic None 23 23 23 23 23 23 23 None 23 12 23 23 23 None 30 None 23 None 23 None 12 None None 23 23 23 23 23 23, 25 23 23 23 None 12

Table 4: Events in chronological order, an asterisk means undetected in the 50 topic model

14

p(k | t) - p(k)

Bitcoinica Silkroad Linode
MtGox 11 Bitomat
MtGox 14 Mooncoin Silkroad2
Kronos Bitfinex Bitstamp
GBL MyBitcoin Cryptorush
Flexcoin Bter
Bitscalper BS&T
2013 Fork Cryptsy Btc-e Allinvain
Shapeshift Just-Dice
Bitfloor

0.015 0.010 0.005 0.000
Figure 7: Plot of ordered topic temperatures at time of event with k being the event topic and t being the time of the event
XFGdtmTemperature walk through of all the steps for constructing such a model has been provided. We have presented a study and exploration of the popular cryptocurrency forum in this framework and employed an event detection technique to capture the effect of high profile scamming and hacking on the community. The number of topics parameter has been shown to be optimal for event detection when it accords with a measure of topic coherence. In addition, the constructed model partitions almost all of the events above a certain severity in a single topic.
15

References
Bao, Y., Datta, A. (2014). Simultaneously discovering and quantifying risk types from textual risk disclosures; Management Science, 60(6): 1371­1391.
Blei D., Ng A. Y., Jordan M. I. and Lafferty J. (2003). Latent Dirichlet Allocation; Journal of machine Learning research, 3(Jan): 993 ­ 1022.
Blei D. and Lafferty J. (2006). Dynamic Topic Models; Proceedings of the 23rd international conference on Machine learning (AMC).
Bommes E., Chen C.Y., H¨ardle W.K. (2017). Textual sentiment and sector-specific reaction; Forthcoming.
Chang, J., Boyd-Graber, J.L., Wang, C. and Blei, D.M. (2009). Reading tea leaves: How humans interpret topic models; Advances in neural information processing systems, 288 ­ 296.
Cheah, E. T. and Fry, J. (2015). Speculative bubbles in Bitcoin markets? An empirical investigation into the fundamental value of Bitcoin; Economics Letters, 130: 32 ­ 36.
Cheung, A., Roca, E. and Su, J. J. (2015). Crypto-currency bubbles: an application of the Phillips-Shi-Yu (2013) methodology on Mt. Gox bitcoin prices; Applied Economics, 47(23): 2348 ­ 2358.
Frigyik B. A., Kapila A. and Gupta M.R. (2010). Introduction to the Dirichlet Distribution and Related Processes; Technical Report, Department of Electrical Engineering, University of Washington.
Griffiths T. and Steyvers M. (2004). Finding Scientific Topics; Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl1): 5228 - 5235.
Hall D., Jurafsky D. and Manning C. (2008). Studying the History of Ideas Using Topic Models; Proceedings of the Conference on Empirical Methods in Natural Language Processing: 363 ­ 371.
Huang, K. W. and Li, Z.L. (2011). A multilable text classification algorithm for labeling risk factors in SEC form 10-K; ACM Transactions on Management Information Systems (TMIS), 2(3): 18.
Kristoufek, L. (2013). BitCoin meets Google Trends and Wikipedia: Quantifying the relationship between phenomena of the Internet era; Scientific Reports, 3: 3415.
Mai, F., Bai, Q., Shan, Z., Wang, X. S. and Chiang, R. H. (2015). The Impacts of Social Media on Bitcoin Performance; Proceedings of the Thirty Sixth International Conference on Information Systems (ICIS 2015).
16

Matta, M., Lunesu, I. and Marchesi, M. (2015). Bitcoin spread prediction using social and web search media; Proceedings of DeCAT.
Mimno D., Wallach H. M., Talley E., Leenders M. and McCallum A. (2011). Optimizing Semantic Coherence in Topic Models; Proceedings of the Conference on Empirical Methods in Natural Language Processing: 262 ­ 272.
Smailovi´c, J., Grcar, M., Lavrac, N. and Znidarsic, M. (2013). Predictive sentiment analysis of tweets: A stock market application; Human-Computer Interaction and Knowledge Discovery in Complex, Unstructured, Big Data. Springer: 77 ­ 88.
Wallach, H.M., Jensen, S.T., Dicker, L.H. and Heller, K.A. (2010). An alternative prior process for nonparametric Bayesian clustering; Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), 9: 892 ­ 899.
Zhang J.L., H¨ardle W.K., Chen C.Y. and Bommes E. (2016). Distillation of News Flow Into Analysis of Stock Reactions; Journal of Business and Economic Statistics, 34: 547 ­ 563.
17

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001
002 003
004 005 006
007 008 009 010
011 012
013 014
015 016 017
018 019 020

"Downside risk and stock returns: An empirical analysis of the long-run and short-run dynamics from the G-7 Countries" by Cathy Yi-Hsuan Chen, Thomas C. Chiang and Wolfgang Karl Härdle, January 2016. "Uncertainty and Employment Dynamics in the Euro Area and the US" by Aleksei Netsunajev and Katharina Glass, January 2016. "College Admissions with Entrance Exams: Centralized versus Decentralized" by Isa E. Hafalir, Rustamdjan Hakimov, Dorothea Kübler and Morimitsu Kurino, January 2016. "Leveraged ETF options implied volatility paradox: a statistical study" by Wolfgang Karl Härdle, Sergey Nasekin and Zhiwu Hong, February 2016. "The German Labor Market Miracle, 2003 -2015: An Assessment" by Michael C. Burda, February 2016. "What Derives the Bond Portfolio Value-at-Risk: Information Roles of Macroeconomic and Financial Stress Factors" by Anthony H. Tu and Cathy Yi-Hsuan Chen, February 2016. "Budget-neutral fiscal rules targeting inflation differentials" by Maren Brede, February 2016. "Measuring the benefit from reducing income inequality in terms of GDP" by Simon Voigts, February 2016. "Solving DSGE Portfolio Choice Models with Asymmetric Countries" by Grzegorz R. Dlugoszek, February 2016. "No Role for the Hartz Reforms? Demand and Supply Factors in the German Labor Market, 1993-2014" by Michael C. Burda and Stefanie Seele, February 2016. "Cognitive Load Increases Risk Aversion" by Holger Gerhardt, Guido P. Biele, Hauke R. Heekeren, and Harald Uhlig, March 2016. "Neighborhood Effects in Wind Farm Performance: An Econometric Approach" by Matthias Ritter, Simone Pieralli and Martin Odening, March 2016. "The importance of time-varying parameters in new Keynesian models with zero lower bound" by Julien Albertini and Hong Lan, March 2016. "Aggregate Employment, Job Polarization and Inequalities: A Transatlantic Perspective" by Julien Albertini and Jean Olivier Hairault, March 2016. "The Anchoring of Inflation Expectations in the Short and in the Long Run" by Dieter Nautz, Aleksei Netsunajev and Till Strohsal, March 2016. "Irrational Exuberance and Herding in Financial Markets" by Christopher Boortz, March 2016. "Calculating Joint Confidence Bands for Impulse Response Functions using Highest Density Regions" by Helmut Lütkepohl, Anna StaszewskaBystrova and Peter Winker, March 2016. "Factorisable Sparse Tail Event Curves with Expectiles" by Wolfgang K. Härdle, Chen Huang and Shih-Kang Chao, March 2016. "International dynamics of inflation expectations" by Aleksei Netsunajev and Lars Winkelmann, May 2016. "Academic Ranking Scales in Economics: Prediction and Imdputation" by Alona Zharova, Andrija Mihoci and Wolfgang Karl Härdle, May 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

021 022
023 024
025
026 027
028 029 030
031
032 033
034 035
036
037 038
039

"CRIX or evaluating blockchain based currencies" by Simon Trimborn and Wolfgang Karl Härdle, May 2016. "Towards a national indicator for urban green space provision and environmental inequalities in Germany: Method and findings" by Henry Wüstemann, Dennis Kalisch, June 2016. "A Mortality Model for Multi-populations: A Semi-Parametric Approach" by Lei Fang, Wolfgang K. Härdle and Juhyun Park, June 2016. "Simultaneous Inference for the Partially Linear Model with a Multivariate Unknown Function when the Covariates are Measured with Errors" by Kun Ho Kim, Shih-Kang Chao and Wolfgang K. Härdle, August 2016. "Forecasting Limit Order Book Liquidity Supply-Demand Curves with Functional AutoRegressive Dynamics" by Ying Chen, Wee Song Chua and Wolfgang K. Härdle, August 2016. "VAT multipliers and pass-through dynamics" by Simon Voigts, August 2016. "Can a Bonus Overcome Moral Hazard? An Experiment on Voluntary Payments, Competition, and Reputation in Markets for Expert Services" by Vera Angelova and Tobias Regner, August 2016. "Relative Performance of Liability Rules: Experimental Evidence" by Vera Angelova, Giuseppe Attanasi, Yolande Hiriart, August 2016. "What renders financial advisors less treacherous? On commissions and reciprocity" by Vera Angelova, August 2016. "Do voluntary payments to advisors improve the quality of financial advice? An experimental sender-receiver game" by Vera Angelova and Tobias Regner, August 2016. "A first econometric analysis of the CRIX family" by Shi Chen, Cathy YiHsuan Chen, Wolfgang Karl Härdle, TM Lee and Bobby Ong, August 2016. "Specification Testing in Nonparametric Instrumental Quantile Regression" by Christoph Breunig, August 2016. "Functional Principal Component Analysis for Derivatives of Multivariate Curves" by Maria Grith, Wolfgang K. Härdle, Alois Kneip and Heiko Wagner, August 2016. "Blooming Landscapes in the West? - German reunification and the price of land." by Raphael Schoettler and Nikolaus Wolf, September 2016. "Time-Adaptive Probabilistic Forecasts of Electricity Spot Prices with Application to Risk Management." by Brenda López Cabrera , Franziska Schulz, September 2016. "Protecting Unsophisticated Applicants in School Choice through Information Disclosure" by Christian Basteck and Marco Mantovani, September 2016. "Cognitive Ability and Games of School Choice" by Christian Basteck and Marco Mantovani, Oktober 2016. "The Cross-Section of Crypto-Currencies as Financial Assets: An Overview" by Hermann Elendner, Simon Trimborn, Bobby Ong and Teik Ming Lee, Oktober 2016. "Disinflation and the Phillips Curve: Israel 1986-2015" by Rafi Melnick and Till Strohsal, Oktober 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

040 041 042 043 044 045 046 047 048 049 050 051

"Principal Component Analysis in an Asymmetric Norm" by Ngoc M. Tran, Petra Burdejová, Maria Osipenko and Wolfgang K. Härdle, October 2016. "Forward Guidance under Disagreement - Evidence from the Fed's Dot Projections" by Gunda-Alexandra Detmers, October 2016. "The Impact of a Negative Labor Demand Shock on Fertility - Evidence from the Fall of the Berlin Wall" by Hannah Liepmann, October 2016. "Implications of Shadow Bank Regulation for Monetary Policy at the Zero Lower Bound" by Falk Mazelis, October 2016. "Dynamic Contracting with Long-Term Consequences: Optimal CEO Compensation and Turnover" by Suvi Vasama, October 2016. "Information Acquisition and Liquidity Dry-Ups" by Philipp Koenig and David Pothier, October 2016. "Credit Rating Score Analysis" by Wolfgang Karl Härdle, Phoon Kok Fai and David Lee Kuo Chuen, November 2016. "Time Varying Quantile Lasso" by Lenka Zbonakova, Wolfgang Karl Härdle, Phoon Kok Fai and Weining Wang, November 2016. "Unraveling of Cooperation in Dynamic Collaboration" by Suvi Vasama, November 2016. "Q3-D3-LSA" by Lukas Borke and Wolfgang K. Härdle, November 2016. "Network Quantile Autoregression" by Xuening Zhu, Weining Wang, Hangsheng Wang and Wolfgang Karl Härdle, November 2016. "Dynamic Topic Modelling for Cryptocurrency Community Forums" by Marco Linton, Ernie Gin Swee Teo, Elisabeth Bommes, Cathy Yi-Hsuan Chen and Wolfgang Karl Härdle, November 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

