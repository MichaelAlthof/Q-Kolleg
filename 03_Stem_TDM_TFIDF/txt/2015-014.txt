BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2015-014
Generalized Exogenous Processes in DSGE: A Bayesian Approach
Alexander Meyer-Gohde* Daniel Neuhoff**
* Universit‰t Hamburg, Germany ** Humboldt-Universit‰t zu Berlin, Germany This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Generalized Exogenous Processes in DSGE: A Bayesian Approach 

Alexander Meyer-Gohdeß

Daniel Neuhoff 

This Version: March 17, 2015

Abstract

The Reversible Jump Markov Chain Monte Carlo (RJMCMC) method can enhance Bayesian DSGE estimation by sampling from a posterior distribution spanning potentially nonnested models with parameter spaces of different dimensionality. We use the method to jointly sample from an ARMA process of unknown order along with the associated parameters. We apply the method to the technology process in a canonical neoclassical growth model using post war US GDP data and find that the posterior decisively rejects the standard AR(1) assumption in favor of higher order processes. While the posterior contains significant uncertainty regarding the exact order, it concentrates posterior density on hump-shaped impulse responses. A negative response of hours to a positive technology shock is within the posterior credible set when noninvertible MA representations are admitted.

JEL classification: C11; C32; C51; C52

Keywords: Bayesian analysis; Dynamic stochastic general equilibrium model; Model evaluation; ARMA; Reversible Jump Markov Chain Monte Carlo

We are grateful to Michael Burda, Fabio Canova, Marco Del Negro, and Helmut Lu®ktepohl as well as to partici-
pants of the 2014 CEF, 2014 SFB-649 Annual Motzen Meeting, 2014 ESEM, 2014 CFE and of research seminars at
the HU Berlin and LMU Munich for useful comments, suggestions, and discussions. We are thankful for the research
assistance provided by Tobias Ko®nig. This research was supported by the DFG through the SFB 649 "Economic Risk."
Any and all errors are entirely our own. Universita®t Hamburg, Professur fu®r Volkswirtschaftslehre insb. Wachstum und Konjunktur, Von-Melle-Park 5,
20146 Hamburg, Germany; Tel.: +49-40-42838 3996; E-Mail: alexander.meyer-gohde@wiso.uni-hamburg.de ßHumboldt-Universita®t zu Berlin, Institut fu®r Wirtschaftstheorie II, Spandauer Straﬂe 1, 10178 Berlin, Germany;
Tel.: +49-30-2093 5720; E-Mail: alexander.meyer-gohde@wiwi.hu-berlin.de Humboldt-Universita®t zu Berlin, Institut fu®r Wirtschaftstheorie II, Spandauer Straﬂe 1, 10178 Berlin, Germany;
Tel.: +49-30-2093 5667; E-Mail:daniel.neuhoff@gmail.com

1 Introduction
Despite recent advances in improving the fit of DSGE models to the data, misspecification remains. In his Nobel Prize Lecture, Sims (2012, p. 1202) observes that "DSGEs could be made to fit better by adding parameters allowing more dynamics in the disturbances." Likewise, Del Negro and Schorfheide (2009) identify three approaches to deal with misspecification in rational expectations models: ignore it, generalize the stochastic driving forces, or relax the cross-equation restrictions. Apart from Smets and Wouters (2007) who have the price-markup disturbance follow an ARMA(1,1) process, Del Negro and Schorfheide (2009) who let government expenditures follow an AR(2) instead of an AR(1) process, or Justiniano, Primiceri, and Tambalotti (2008) who replace the ARMA(1,1) specification for the wage and price markup shocks in the Smets and Wouters (2007) model with AR(1) shocks in a robustness exercise, the DSGE literature has not yet provided a systematic framework to address the approach to misspecification of generalizing stochastic driving forces. We fill this gap by providing a Bayesian approach to estimating the order as well as the parameters of generalized ARMA representations of exogenous driving forces within DSGE models.
To accomplish the task, we adopt the Reversible Jump Markov Chain Monte Carlo (RJMCMC) methodology as pioneered by Green (1995).1 RJMCMC provides samples from a posterior distribution spanning several, not necessarily nested, models with parameter spaces of potentially different dimensionality. In our case, each model is identified by a specific set of orders for the lag polynomials of the autoregressive and moving average components of the disturbances, each leading to a different parameter space. This approach provides a framework for the systematic exploration of the fit of DSGE models using different structures for the shock processes which provides a computationally feasible alternative to estimating all different possible combinations of shock orders individually. Additionally, it allows us to quantify posterior model uncertainty and its consequences for impulse responses and correlation structures while being agnostic regarding the order of the underlying shock processes.2
1Markov Chain Monte Carlo (MCMC) methods have become increasingly popular for the estimation of DSGE models in recent years. See Ferna¥ndez-Villaverde and Rubio-Ram¥irez (2004), An and Schorfheide (2007) for a methodological review, and Herbst and Schorfheide (2014) for a textbook treatment.
2If multiple shocks are kept independent while generalizing their individual autocorrelation patterns, the resulting estimates admit a structural interpretation of the shocks that can guide the researcher in identifying those dimensions along which the model requires the most additional internal propagation. It may, furthermore, be possible to construct
1

The RJMCMC method rests on modifying the proposal ratios in the acceptance probability by inflating parameter vectors to common dimensionality in order to circumvent the dimensionality mismatch induced by sampling for ARMA processes of different orders. In our analysis of US post war GDP data we find that RJMCMC provides point estimates of the ARMA orders with a reliability comparable to traditional order selection criteria such as the Akaike Information Criterion (AIC), the corrected Akaike Information Criterion (AICC), and the Schwarz Criterion (SC). While the posterior mode models are AR(2) and ARMA (4,5) for first differenced and HP-filtered data respectively, RJMCMC is of primary interest for its posterior distribution over different ARMA orders and not for its point estimates of the orders. We find that the HP filtered GDP data is associated with substantial posterior model uncertainty, as testified to by the dispersed posterior over models provided by our RJMCMC analysis.
We then turn to a prototypical DSGE model, Hansen's (1985) specification of the neoclassical growth model, and relax the traditional AR(1) assumption imposed on the exogenous technology process. After confirming that RJMCMC would correctly identify the ARMA order using synthetic data generated from an AR(1) technology process, we turn to HP filtered US post war GDP data and estimate the order and parameters of the technology process. We find that the data prefers higher order exogenous processes--at the mode, ARMA(3,0), but with substantial posterior density associated with other higher order specifications, such as ARMA(2,2). The resulting posterior impulse responses are hump-shaped, reflecting common wisdom in the macroeconomics literature3 and differing thus qualitatively from the responses to the traditional AR(1) process. From a DSGE likelihood perspective, there is, without a commensurate prior specification, no reason to prefer invertible or "fundamental" representations in the presence of MA terms; in sampling from the covariance equivalent representations for draws of the order with nonzero MA order, we find a downward shift in the amplitude of the impulse responses as well as an overall increase in the posterior uncertainty regarding the impulse responses of endogenous variables to a technology shock. Strikingly, we cannot exclude the possibility of a negative response of hours to a positive technology shock.
model selection criteria based on the comparison of the spectrum of variables of interest derived from estimates of the posterior with the spectrum using only pure white noise shocks giving a measure of how much structure has to be added to the model outside of economic theory, an idea along the lines of Watson (1993).
3See especially, Cogley and Nason (1995).
2

Our approach can be considered a Bayesian Model Averaging (BMA) method for providing impulse responses and moments under model uncertainty, in that we weigh these statistics from different models with their respective posterior probabilities. While there are certainly alternatives to our approach, for example selecting the model with the highest maximized likelihood or using model selection criteria like the Akaike Information Criterion, BMA allows us to incorporate model uncertainty into the inference of any statistic of interest. The RJMCMC algorithm allows us to explore the posterior adaptively, which would allows for a more efficient means of sampling across models than a brute force BMA approach of generating samples from the posterior of each model (for us, ARMA order combinations p and q) and then weighting according to Bayes factors. The BMA paradigm was put forth by Leamer (1978) and interest in this approach has since increased with the advent of more powerful MCMC samplers. For an overview see Hoeting, Raftery, Madigan, and Volinsky (1999) who also document an improved out-of-sample forecasting performance using BMA, which is also found by Madigan and Raftery (1994) in the context of graphical models. Kass and Raftery (1995) provide a discussion of Bayesian model selection and averaging. A recent application of RJMCMC to instrumental variable regression is presented by Koop, Leon-Gonzalez, and Strachan (2012) and Raftery, Madigan, and Hoeting (1997) discuss the merits of BMA in the context of linear regression models. In a DSGE context, Wolters (2015) uses BMA to provide meta forecasts using multiple estimated DSGE models and Strachan and Van Dijk (2013) use BMA with VARs to assess the empirical support for structural breaks and the long-run and equilibria restrictions implied by a prototypical DSGE model. Our analysis is close in spirit to theirs, yet whereas they apply BMA to estimate VARs restricted commensurate with a DSGE model or provide forecasts using estimated DSGE models, we apply BMA to estimate the DSGE model itself.
This paper is organized as follows: We first introduce our methodology and shortly illustrate the method by constructing a sampler for a univariate autoregressive model of unknown order. Afterwards, we present the results of a small Monte Carlo study designed to gauge the power of the method for identifying univariate autoregressive moving-average models using synthetic data derived from estimated ARMA models of post war US GDP data. Lastly, we apply the method to the neoclassical growth model, using synthetic AR (1) as well as post war US data, and analyze the posterior model uncertainty and its consequences for posterior impulse responses and correlations.
3

2 Reversible Jump MCMC for ARMA Processes
2.1 Reversible Jump Markov Chain Monte Carlo
In this paper, we adapt and apply the Reversible Jump Markov Chain Monte Carlo (RJMCMC) methodology pioneered by Green (1995). RJMCMC generalizes the Metropolis-Hastings algorithm (Hastings 1970) to allow for moves between parameter spaces of varying dimensionality while maintaining detailed balance.4 This transdimensionality allows for inference on a posterior distribution spanning several, not necessarily nested, models. In the following, we will illustrate the mechanics of RJMCMC starting with a short description of conventional Metropolis-Hastings samplers to fix ideas before turning to the construction of a sampler for univariate autoregressive models of unknown order using an RJMCMC approach.5
2.2 Conventional Metropolis-Hastings Samplers
Markov Chain Monte Carlo (MCMC) methods in general provide samples from some probability distribution of interest by constructing a Markov chain whose stationary distribution is this distribution of interest. A Markov chain with the sequence of states 1, 2, . . . is specified in terms of the distribution for the initial state 1 and the transition kernel K(∑) that provides the conditional distribution of a state i+1 given the current state i. That is, the probability that i+1 is in some set A  Rd given that the current state of the chain is i is given by (1) K(, A) = P(i+1  A|i = ) A distribution  is invariant for some Markov chain if the transition kernel of the chain satisfies
(2) K(, A)()d = ()d
A
4A more extensive treatment of Metropolis-Hastings samplers can be found in Chib and Greenberg (1995). See also Tierney (1998) for a comparison of RJMCMC and conventional Metropolis-Hastings kernels. Another popular MCMC method is the Gibbs sampler which is a special case of Metropolis-Hastings samplers and ultimately RJMCMC samplers. See Gelfand and Smith (1990) for a review and comparison of Gibbs samplers as well as importance samplers and stochastic substitution and Troughton and Godsill (1998) for application to autoregressive models. Geweke (1998) provides an overview over Bayesian methods and their applications in economics.
5Several authors have applied RJMCMC to the problem of estimating univariate autoregressive (moving average) models, e.g., Brooks, Giudici, and Roberts (2003), Brooks and Ehlers (2004), and Ehlers and Brooks (2008).Relatedly, different approaches to statistical models of varying dimensionality have emerged; such as birth-death Markov Chain Monte Carlo, based on continuous time birth-death processes, as initiated by Stephens (2000) and applied to the analysis of autoregressive moving-average models by Philippe (2006). A summary and comparison of these methods can be found in Cappe`, Robert, and Ryde`n (2003).
4

for all subsets A of the state space. The task in MCMC is to construct a kernel such that the distribution of interest  is invariant with respect to the Markov chain defined by K(). The expression in (2), however, is not practically useful for the construction of an appropriate kernel, as verifying (2) would involve integration over the unknown distribution  being sought.
One widely used approach to overcome this hurdle are Metropolis-Hastings samplers:6 acceptreject samplers for which proposals for a new state of the chain are drawn from some distribution  to be chosen by the researcher and then accepted with an appropriately derived probability . Here, the stronger condition of reversibility or detailed balance is imposed, which guarantees that  is invariant for the Markov chain. This condition holds if a sequence of two states (,  ) has the same distribution as the reversed subchain ( , ) whenever ,   . I.e., if
(3)  () K (, B) d =   K  , A d
AB
for all subsets A, B  Rd. Condition (3) is more easily verified and can thus provide a starting point for the construction of a sampler.
A general Metropolis-Hastings algorithm can be written as follows: Let again  denote a state of the Markov chain, in the case of Bayesian inference in the context of model estimation, the state is just the vector of model parameters and the distribution of interest is the posterior distribution
(4) ()  L()()
where  denotes the vector of model parameters, L is the likelihood of the data given the model and its parameters and  is the prior over the model parameters. To obtain N samples from the posterior distribution, the following algorithm is run

Metropolis-Hastings
1. Set the (arbitrary) initial state 0 of the Markov chain 2. For i = 1 to N
(a) Set  = i-1 (b) Propose a new state from some proposal distribution ( |)

(c) Accept draw with probability (,  ) = min (1, )

with

=

L( ) L()

◊

( ) ()

◊

(| ) ( |)

Likelihood Ratio Prior Ratio Proposal Ratio
(d) If the draw is accepted set i =  . If the draw is rejected set i = 

6Laid out in Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller (1953) and generalized in Hastings (1970).

5

This algorithm defines a transition kernel such that the Markov chain has the desired invariant
distribution. The sequence of states of the chain is then a sample from this distribution of interest. The acceptance probability  corrects for differences between the proposal distribution  and the distribution of interest.7
The kernel in the above is given by
(5) K(, B) = ( |)(,  )d + 1 - ( |)(,  )d Ω
BB Probability of moving to set B Probability of rejecting the move and B
where Ω = 1 if   B and zero otherwise giving the probability of moving to some subset B of
the parameter space conditional on the chain currently being at . The crux when constructing the kernel is to define the appropriate acceptance probability  and the proposal distribution  so
as to satisfy the detailed balance condition and thereby guarantee the convergence of the Markov
chain to the desired probability distribution. Indeed, plugging in the formulation of the kernel from (5) into (3) gives an expression from which, given the proposal distribution  the appropriate acceptance probability  can be readily derived using Peskun's (1973) recipe.

2.3 Reversible Jump MCMC: AR(p) Order and Parameter Sampling

We will derive our transdimensional random walk sampler implementation of the RJMCMC with

a univariate zero-mean normally distributed AR(p) model of unknown order for illustration. Our

derivation follows the exposition of Waagepetersen and Sorensen (2001). Such an AR(p) model is

defined as

(6) yt = P1pyt-1 + P2pyt-2 + . . . + Pppyt-p + t, t  N 0, 2

Pip are the coefficients of the lag polynomial of order p associated with the i'th lag and t is a

zero-mean stochastic disturbance. Denote by Pp

P1p,

P2p,

.

.

.

,

P

p p

the vector of parameters of

the AR(p) model.8 We would like to construct a posterior distribution over the orders, p, and

associated parameters, Pp, given observations on yt.

It is sensible to interpret the order of the lag polynomial p as a model indicator. We will

use the terms model indicator and polynomial or lag order interchangeably. The aim is now to

7Note, that in the case of a standard random walk Metropolis-Hastings sampler with symmetric proposals, i.e. a
Metropolis sampler, the proposal ratio reduces to one. 8The part of the parameter vector associated with the standard deviation of the disturbance t,  will be left implicit
in the exposition of this section to maintain the focus on the order, p.

6

construct a sampler for the joint posterior distribution over the different models indexed by p and their parameters. The strategy closely resembles that for Metropolis-Hastings samplers. Indeed, Metropolis-Hastings samplers are a special case in the RJMCMC framework. It is expositionally convenient to express the state of the Markov chain as (7)  = (p, Pp) explicitly including the order of the autoregressive polynomial p in the state.
The detailed balance condition poses the main obstacle to the transdimensional sampling's construction of a joint posterior distribution over potentially nonnested models with parameter spaces of varying dimensionality. Recall the detailed balance condition (3), (8)  () K (, B) d =   K  , A d
AB
Unlike in the foregoing section, the dimension of  can change. I.e., the state space of the Markov chain spans parameter spaces with differing dimensionality--for a sampler for AR(p) models of unknown order, when p changes so does the number of parameters. Here, the usual strategy for the derivation of the acceptance probability will fail. Green (1995) modifies the proposals in such a way that the integrals on both sides of the detailed balance condition are over spaces of the same dimensionality by introducing an auxiliary proposal variable u together with a mapping g pp that maps the auxiliary proposal u and the current state of the chain to the new proposed state. The mapping gpp is chosen such that the dimensionality of the integrals on both sides of the equation is inflated to some higher common dimensionality.
In order to be able to easily verify adherence to detailed balance for a move from a state (p, Pp) to (p , Pp ) the vectors of Markov chain states and the random auxiliary proposal variables (Pp, u) and (Pp , u ) must be of equal dimension. This dimension matching condition ensures that (Pp|p)pp (Pp, u) and (Pp |p )p p(Pp , u ) are "joint densities on spaces of equal dimension," (Waagepetersen and Sorensen 2001, p. 54) allowing an application of a change of variables in the detailed balance equation to facilitate the construction of the transition kernel of the Markov chain. Here, pp (Pp, u) is the proposal density for the auxiliary variable u going from an AR model of order p to one with order p which may also depend on the current parameter vector Pp. The proposed new order p is drawn from some p(p |p) and the joint proposal density is () = pp (Pp, u)p(p |p).
7

In our implementation of the method, we use the following differentiable bijection for g pp

(9)

Pp u

= gpp (Pp, u) =

A(p, p )p ◊p Ip◊p

Ip ◊p 0p◊p

Pp u

where (10)

A(p, p ) = 0IIpp(pI◊◊p-pp◊p0p)◊pp◊(p-p )

if p > p
if p < p if p = p

This mapping leads to the transdimensional analog of a full-site updating random walk sampler.

Proposals for "newly born" parameters, i.e., those Pip for i = p + 1, . . . , p , are centered around

zero. If p < p the parameter vector is truncated and proposals for these parameters are centered

around their previous values. For p = p this mapping gives a standard random walk sampler.

The detailed balance condition holds if9

(11)  () Q , Bp dPp =   Q  , Ap dPp
Ap Bp
for all subsets Ap and Bp of the parameter spaces associated with autoregressive polynomials of
order p and p respectively and where

Q , Bp = ( |p, Pp)pp (,  )d
Bp
is the first part of the kernel in (5), i.e. the part of the conditional distribution of  associated with

acceptance of the proposal.

Implementing the change of variables with the mapping g pp , the detailed balance condition is satisfied if

(12)  () p(p |p)pp pp (Pp, u) =   p(p|p )p pp p(gpp (Pp, u))

where the details of the derivation can be found in the appendix.

Following Peskun (1973), we set the acceptance probability, pp , as large as possible,10

(13) pp = min 1, pp (,  )

with

(14)

pp ,  =

L( ) L()

( ) p(p|p )p p(gpp (Pp, u)) () p(p |p)pp (Pp, u)

Likelihood Ratio Prior Ratio

Proposal Ratio

Having chosen an appropriate acceptance probability to maintain detailed balanced, we can now

implement the procedure. The resulting sequence of states approximates the joint posterior over

9See also Waagepetersen and Sorensen (2001). 10Which, as noted by Green (1995), is "optimal in the sense of reducing the autocorrelation of the chain."

8

all models indexed by their order p and the corresponding parameter vectors.
RJMCMC Algorithm 1. Set the initial state 0 of the Markov chain 2. For i = 1 to N (a) set  = i-1 (b) Propose a visit to model p with probability p(p |p) (c) Sample u from pp (Pp, u) (d) Set (P , u ) = gpp (Pp, u) (e) Accept draw with probability  = min 1, pp (,  ) pp is defined as in (14) (f) If the draw is accepted set i =  . If the draw is rejected set i = 
The application to moving average models follows by analogy and the extension to autoregressive moving average (ARMA) models is straightforward. One simply defines the model indicator as a two-element vector, proposing not only visits to some model with autoregressive order p but also for a new order for the MA-polynomial q .
For many applications, it is desirable to restrict the parameter spaces of ARMA processes to ensure stationarity and/or invertibility.11 To constrain sampling to these invertible and stationary regions of the parameters spaces of each model, we reparametrize the AR (and MA) polynomial in terms of its (inverse) partial autocorrelations (PACs). Details are in the appendix.
3 RJMCMC ARMA Order and Parameter Estimation: Monte Carlo Evidence
We examine the performance of the RJMCMC method for ARMA processes of unknown order introduced in the foregoing section by carrying out two Monte Carlo experiments. For both experiments, we compare the model chosen by the posterior mode of our RJMCMC algorithm with the choices that follow from using the Akaike Information Criterion (AIC), the corrected Akaike Information Criterion (AICC), and the Schwarz Criterion (SC). We orient the Monte Carlo exper-
11For the DSGE application in sections 4 and 5, we will require stationarity of the exogenous driving forces. In section 5, we will examine the consequences of imposing or not imposing invertibility on MA components, should they exist, on impulse responses.
9

iments around the same post war US per capita real GDP data12 that will inform our DSGE model in the following section by applying our RJMCMC algorith to obtain 3,000,000 draws from the posterior distribution of first-differenced and demeaned quarterly observations of the logarithm of US per capita real GDP as well as 7,000,000 draws from the posterior distribution of the cyclical component of US GDP extracted using a Hodrick-Prescott filter with the smoothing parameter set to 1600 for the period from 1947:1 - 2013:3. The first Monte Carlo is carried out by taking every 30,000th draw from the posterior for first differences and the second with every 70,000th draw from the posterior for HP-filtered data, giving 100 different models each, and then for each generating 250 observations using the corresponding model and parameter values.
Figure 1: Posterior over the orders p, q for first differenced data For first-differenced data, the model at the mode is an AR(2), with the posterior mean parameters conditional on the AR(2) model being
yt = 0.3184yt-1 + 0.1297yt-2 + t; t  N(0, 0.9025)
12 We take 1947:1-2013:3 real GDP from the NIPA tables, expressed on a per capita basis using the BLS series on the civilian noninstitutional population. Both data sets were downloaded from the St. Louis Federal Reserve's FRED database.
10

The posterior over models can be found in figure (1). Note that there is a substantial amount of posterior uncertainty regarding the model with textbook representations such as Blanchard and Fischer's (1989, p. 9) ARMA(2,2) estimated on first differenced log GNP comfortably in the posterior distribution over models.
Figure 2: Posterior over the orders p, q for two-sided HP-filtered data With HP-filtered data, the model at the mode is an ARMA(4,5), with the posterior mean parameters conditional on posterior mode model given by
yt =0.6027yt-1 + 0.5304yt-2 + 0.0861yt-3 - 0.4196yt-4 + . . . + t + 0.3786 t-1 - 0.2556 t-2 - 0.5812 t-3 - 0.2706 t-4 - 0.2154 t-5 t  N(0, 0.7551)
Figure (2) shows the posterior distribution over the orders p, q for the HP-filtered data. Clearly, there is significant posterior uncertainty regarding the model reflected in the dispersion of posterior density spread over many more models than was the case with first differenced data. This is consistent with relatively high orders for the lag polynomials preferred at the posterior mode with
11

many neighboring models mimicking the covariance structure of the model model.

We implement RJMCMC by generating 1,500,000 draws from the posterior, discarding the

first 1,000,000 as burn-in, and identifying the model at the mode in (p, q). The first state of the

chain was set to white noise with unit standard deviation, i.e. p = q = 0 where p denotes the

autoregressive order, q the moving average order, and  = 1. Our metric for model choice is in

accordance with a 1 - 0 loss function, selecting the model at the mode of the posterior distribution

over (p, q). It should be noted that one of the strengths of our method is the ability to quantify

posterior uncertainty over models directly, such that model uncertainty can be incorporated in the

calculation of posterior credible sets over impulse responses, correlations structures, or the like,

providing more than just a point estimate of the model order.

We compare the model choice of our method with the choices that follow from minimizing the

Akaike Information Criterion (AIC), the corrected Akaike Information Criterion (AICC), and the

Schwarz Criterion (SC).13 These are defined as

AIC = 2k - 2 ln(L^),

AICC

=

AIC

+

2k(k + 1) n-k-1

,

SC = -2 ln(L^) + k ln(n)

with k being the number of model parameters and n the number of observations. L^ denotes the

maximized likelihood value of a model, i.e., for given ARMA orders p and q.

3.1 Priors and Proposals

Table 1 summarizes the priors and proposals used in the Monte Carlo study. We choose a uniform

Variable p q
AR PAC MA inverse PAC : Standard Deviation t

Prior U(0,10) U(0,10) TN(0,0.25) TN(0,0.25) IG(1,1)

Proposal LaplaceD(p,2) LaplaceD(q,2) TN(PAC,0.0025) TN(PAC,0.0025) TN(,0.0025)

Table 1: Prior and Proposal Distribution for Monte Carlo Experiment

prior over the AR and MA orders, restricting the highest allowed order to 10 for both the AR and MA polynomials. Proposals for the AR and MA orders are taken to follow a discretized Laplace
13Calculations for the three standard measures were carried out using the R package auto.arima.

12

distribution, LaplaceD(, b), with location parameter, , and shape parameter, b, such that (15) p(p |p)  exp(-b|p - p |) with p , p  [0, 1, . . . , 10] (16) q(q |q)  exp(-b|q - q |) with q , q  [0, 1, . . . , 10] For the (inverse) partial autocorrelations, our prior is a truncated normal distribution, TN(, , -1, 1), with location parameter, , and dispersion , and truncations at 1 and -1, imposing invertibility and stationarity. With these proposal distributions, we center the (inverse) partial autocorrelations around their previous values and new (inverse) partial autocorrelations are centered around zero.

0.01
0.008
0.006
0.004
0.002
0 0 1 2 3 4 5 6 7
q8
9 10

10

9

8

7

6

5

4

3

2 1

p

0

Figure 3: Implied prior over the orders p, q
All three standard information criteria penalize for the number of parameters in the model. This feature is also present in the posterior of our RJMCMC method with proper priors over the (inverse) partial autocorrelations. Increasing the order of, say, an autoregressive model and setting the new parameter to zero gives a model identical to the previous one with lower order; hence, does not change the likelihood. Yet, the posterior with the additional parameter is penalized as the prior probability assigned to the value of the new parameter is smaller than one, yielding a posterior probability lower than with the original, lower order. Even though the prior on the orders is uniform the prior resulting from the combination of the prior over the orders and the prior over
13

the parameters can be thought of as behaving implicitly like a prior of exponential form as shown in figure (3).

3.2 Likelihood

For the ARMA (p, q) model introduced in (A-12), we employ the Kalman filter to evaluated the

log likelihood, ln L {yt}tT=1 ;  , as a sequence of conditional log likelihoods

(17)

ln L {yt}Tt=1 ; 

T

=

ln L

yt|

yj

t-1 ; 
j=1

t=1

= -1 T 2 t=1

ln t

+

2t t

+

ln (2)

where the last equality follows from the assumption of normality; the sample size is T = 100; t is

the innovation in the current observation, t

of this innovation, t

E

2t |

t-1
y j j=1 .

yt - E

yt|

yj

t-1 j=1

; and t the conditional variance

The innovation and its conditional variance are recovered from the Kalman filter recursion14

where we follow Harvey (1993, p. 96) in setting up the recursion for ARMA(p,q) processes.15 The

state equation is

(18) wt+1 = Awt + R t, t  N(0, 2)

and the observation equation is given by

(19) yt = Zwt

where

(20)

Z = 1 01◊m-1 ,

A = PPmpmp,-q,q1

Im-1 01◊m-1

,

Pmp,-q1 = P1p,q . . . Pmp,-q1 ,

R = 1 Q1p,q . . . Qmp,q

for m = max(p, q + 1).

3.3 Results
We report the proportion of correctly identified models in table 2. The RJMCMC method outperforms the set of traditional information criteria in all cases except for the model at the posterior mode of HP-filtered data. An increase in the number of the draws from the posterior could further improve the performance of our implementation.
With the generally higher order processes obtained from the posterior obtained using HPfiltered data all methods identify the correct model only in very few cases. This is not surprising
14See, e.g., Anderson and Moore (1979). 15See de Jong and Penzer (2004) for an overview of alternate state space formulations of ARMA models.
14

Method RJMCMC
AIC AICC
SC

First Differences 0.23 0.08 0.09 0.18

HP-Filter 0.05 0.03 0.02 0.01

Table 2: Proportion of Correctly Identified Models

as the autocorrelation structure of ARMA models of higher orders may be very close even if the orders of the lag polynomials differ and the likelihood is therefore rather flat across models. This was reflected likewise in the posterior distribution over models in the estimation using post war US in figure (2). However, RJMCMC enables the characterization of the resulting uncertainty regarding model selection choices and the posterior therefore provides the researcher with a tool to gauge the extent of model uncertainty.
Of course, the ability of the method to estimate the parameters of the model along with the order of the model is of importance. Figure 4 reports the recursive means of the parameter draws of the model parameters conditional on p = 2 and q = 0 from a chain from experiment 1 where the model was correctly identified. These values clearly converge close to the values underlying the data generating process.

0.4 0.38 0.36 0.34 0.32
0.3 0.28 0.26
0

Conditional Recursive Mean AR Parameter 1
2 4 6 8 10 12 14 16 x 104

Conditional Recursive Mean AR Parameter 2 0.21
0.2
0.19
0.18
0.17
0.16
0.15 0 2 4 6 8 10 12 14 16 x 104

Conditional Recursive Mean Sigma 0.96
0.94
0.92
0.9
0.88
0.86
0.84 0 2 4 6 8 10 12 14 16 x 104

Figure 4: Recursive Parameter Means from the Conditional Posterior

In conclusion, our method exhibits roughly the same or better performance as classical methods concerning order identification while providing a complete posterior distribution over parameters and model orders that can be used for the posterior analysis of statistics of interest. We are interested in posterior statistics of DSGE models such as impulse responses and correlation structures and will now turn to a DSGE setting and apply the RJMCMC method there.
15

4 Neoclassical Growth Model

As a baseline model to examine how the RJMCMC model can be applied to a DSGE model, we
consider Hansen (1985) specification of the neoclassical growth model. In this simple model, the
social planner's problem is to maximize the discounted lifetime expected utility of a representative
household given by

(21) E0 t ln (ct) + ln (1 - lt) , 0 <  < 1
t=0
with ct representing consumption and lt hours;   (0, 1) is the subjective discount factor of the
household and  weights the utility of leisure, 1 - lt, in the household's utility function. The social
planner faces the resource constraint

(22) ct + it = yt

where investment, it, contributes to the accumulation of capital, kt, through

(23) kt = (1 - ) kt-1 + it

with the depreciation rate, , and where production, yt is neoclassical and given by

(24)

yt = ezt kt-1lt1-

with zt being stationary stochastic productivity. Hansen (1985) assumed a highly autocorrelated AR(1) process--with the autoregressive parameter set to 0.95-- following Kydland and Prescott

(1982). Relaxing this assumption will be the focus of our investigation.

The (25)

first

order

conditions

of the 1= ct

social planner's

Et



1 ct+1

1

-



problem + ezt+1

are
lt+1 kt

g1i-ven

by

(26)

 1 - lt

=

1 ct

(1 - ) ezt

kt-1  lt

An equilibrium is defined by the equations (22) through (26) along with a specification for the

stochastic productivity process, zt.

L

1 3

Steady state employment 1/3 of total time endowment

 0.36

Capital share

 0.025

Depreciation rate for capital

R 1.01

One percent real interest rate per quarter

Table 3: Model Calibration

16

In this exercise, we will take the parameters of Hansen's (1985) calibration of all parameters outside the specification of the stochastic productivity process, zt, as given. This will allow us to concentrate on the contribution of the RJMCMC algorithm in estimating the order and parameters of the exogenous process. The calibrated parameters reported in table 3 deliver standard values for parameters, imposing , e.g., that about one third of agents' time endowment is spent in employment activities, capital contributes a little more than one third to production. As we will consider arbitrary ARMA processes for zt, the model does not fit canonical DSGE linear problem statements, e.g., Klein (2000), which allow for straightforward calculation of the likelihood function. While we could redefine the model to include the entire state vector induced by the ARMA exogenous process as endogenous variables to bring the model into the canonical form, doing so would significantly increase the computation costs involved in the QZ decomposition for the state transition and the Sylvester equation for the impact matrix of shocks. In the appendix, we provide an extension of multivariate DSGE linear solution methods to arbitrary vector ARMA exogenous driving forces.
5 Estimation Results for the Neoclassical Growth Model Model
We carry out two exercises using the neoclassical growth model model as presented above. First, in order to check whether the method could pick up the correct underlying process for a technology shock in this model, we generated 250 observations of synthetic data using the AR(1) process as reported by Hansen (1985) in his original study. Second, we estimate the order and parameters of the technology shock process for the model using US GDP data, treated with the HP filter as in Hansen's (1985) original study.
5.1 Priors and Proposals
The priors and proposals for the shock process orders and parameters are reported in table 4. The priors remain the same as in the Monte Carlo study, while the dispersion parameters of the
proposals were tuned using short pilot runs to increase the efficiency of the RJMCMC algorithm.
17

Variable p q
AR PAC MA PAC


Prior U(0,10) U(0,10) TN(0,0.25) TN(0,0.25) IG(1,1)

Proposal LaplaceD(p,2.2) LaplaceD(q,2.2) TN(PAC,0.0016) TN(PAC,0.0016) TN(,0.0025)

Table 4: Priors and Proposals for RBC Model Estimation

5.2 Synthetic AR(1) Data
For this exercise we generated 250 realizations for the technology shock according to the AR(1) specification and calibration in Hansen (1985) (27) zt = 0.95zt-1 + t
We then fed the resulting series for zt into the linearized RBC model and applied our method to the resulting synthetic data on output, yt, generating 650.000 draws discarding the first 100.000 draws as burn in. Standard visual measures over the chains indicated convergence. Figure 5 shows the posterior distribution over the orders for the disturbance. The method places an overwhelming majority of the posterior weight on the AR(1) model--obviously correctly identifying the AR(1) data generating process for the productivity process with observations on output, yt.
This result gives us further confidence that, if the real world process for the productivity shock were AR(1), it would be correctly identified by the RJMCMC method we propose.
5.3 US GDP Data: Estimates
We now address what US postwar GDP data can reveal about the productivity shock in Hansen's (1985) model. We estimated the productivity shock process using HP-filtered quarterly US GDP per capita as in Hansen (1985) taking his original calibration and value of 1600 for the smoothing parameter in the HP filter as given.16 In applying the RJMCMC method introduced in section 2, we generated 4.000.000 draws discarding the first 1.000.000 draws as burn in. The HP filter was applied to the DSGE model when evaluating the likelihood, thus treating the data and the model with the same filter.17
16See footnote 12 for details on the data series. 17See section A.6 for details.
18

Figure 5: Posterior over the Orders for the Shock Process, Synthetic AR(1) Data from (27)
Figure 6 shows the posterior over (p, q) for this exercise. The model at the mode is ARMA(3,0) and the baseline AR(1) specification of Hansen (1985) is clearly rejected. There is much more substantial uncertainty regarding the correct shock process than in the Monte Carlo exercises above. The prior posterior plots in figure (7) are indicative that our results are not being overly driven by our choice of priors, likewise confirmed by comparing the posteriors over orders in figure 6 to the implied priors in figure 3.
Figure 8 reports recursive means of the first AR parameter for three chains with differing initial states for the orders of the ARMA polynomial for the technology shock, calculated both conditional on the model at the mode of the posterior as well as unconditional means. Inspection suggests that all three chains have converged. It is not clear, however, whether these standard graphical or other formal measures of convergence, e.g., Brooks and Gelman (1998), apply without adaptation in transdimensional analyses, see e.g., Fan and Sisson (2011). In any case, the posterior statistics, such as impulse responses, that we will examine are not indicative of a lack of convergence.
19

Figure 6: Posterior over the Orders for the Shock Process

40 35 30 25 20 15 10
5 0 0.9

AR PAC 1
Conditional Posterior Prior

0.95

x

1

1.05

9 8 7 6 5 4 3 2 1 0 -0.5

-0.4

AR PAC 2

Conditional Posterior Prior

-0.3 -0.2 x

-0.1

0

AR PAC 3 12
Conditional Posterior Prior 10
8
6
4
2
0 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 x

Figure 7: Priors and Posteriors for Partial Autocorrelations

20

Unconditional
Empirical Averages AR Parameter 1 1.4
1.2
1
0.8
0.6
0.4
0.2 Initial Order (0,10)
0 Initial Order (0,0) Initial Order (10,0)
-0.2 0 0.5 1 1.5 2 2.5 3 3.5 4 x 106

1.4 1.2
1 0.8 0.6 0.4 0.2
0 -0.2
0

Conditional
Conditional Empirical Average AR Parameter 1
Initial Order (0,0) Initial Order (0,10) Initial Order (10,0) 123456789
x 105

Figure 8: Convergence Diagnostics

Table 5 reports point estimates for the shock process parameters taken from the posterior distribution conditional on (p, q) = (3, 0). Additionally, the first two autocorrelations of the exogenous process, zt, implied by these point estimates are given. The first autocorrelation is higher than, though consistent with, the choice of Hansen (1985) following Kydland and Prescott (1982) to model the technology process with a near unit root.

Parameter AR(1)
AR(2)
AR(3)

(1) (2)

Mean 1.1689 (0.04) -0.0732 (0.06) -0.1224 (0.04) 0.5873 (0.08) 0.9804 0.9528

Median 1.1681
-0.0725
-0.1215
0.5733
0.9810 0.9542

Hansen 0.95
N/A
N/A
0.712
0.95 0.9025

Table 5: Posterior Point Estimates and Autocorelations

5.4 US GDP Data: Correlation Structure
We now examine the variance and correlation structures implied by our posteriors and compare these with the data and the statistics implied by our baseline AR(1) model implied by Hansen
21

(1985).18 The posterior matches the structure of the second moments of output quite well. As we estimated with real per capita GDP data, this is reassuring and indicates that the procedure does indeed provide a substantial improvement in fit.

Data Hansen Posterior Mode Model Posterior Mode 90% Posterior Credible Set

2.8491 3.2574

2.8332

2.8182

2.1074 -- 4.0965

Table 6: Standard Deviation of Output, in %

The standard deviations of output are in table 6. Both the standard deviation of model at the posterior mode of the ARMA order and parameter space and the posterior mode of the standard deviations line up very close to the statistic in the data, whereas the statistic of Hansen (1985) shows greater a difference from the value in the data. The 80% posterior credible set shows the extent of posterior uncertainty, which here is great enough to encompass all the point values reported.
1
0.8
0.6
0.4
0.2
0
-0.2
Data -0.4 Hansen
Posterior Mode Model -0.6 Posterior Mode
Posterior 10% Bound -0.8 Posterior 90% Bound
-1 0123456 j

Correlation Coefficient

Figure 9: Comparison of Autocorrelations of Output
18Following Hansen (1985), we calculate the second moments for his model using an HP filtered (with the smoothing parameter, , set to 1600) version of model.
22

The first six autocorrelations tell a more certain story, however, and can be found in figure 9. Again, both the autocorrelations of the model at the posterior mode of the ARMA order and parameter space and the posterior mode of the autocorrelations match the statistic in the data very closely. The AR(1) structure imposed by Hansen (1985) forces a compromise, with the initial autocorrelation are somewhat lower and the later values somewhat higher than in the data.
The fit as implied by the point estimates of our posterior with respect to our observable series output is reassuring in that our application of the RJMCMC method is successfully doing what it should. With a mean zero normally distributed process, the second moments describe the stochastic properties of the process and our posterior brings the second moments of output from the RBC model closer to the data by selecting appropriate ARMA processes.
5.5 US GDP Data: Impulse Responses
With a posterior distribution over both models--i.e., orders p and q--and their parameters for the ARMA technology process, we plot impulse responses taking posterior uncertainty about the model into account. In the presence of MA components, this requires us to take a stand on which covariance equivalent representation we choose.19 We will first examine the invertible or fundamental impulse responses associated with the posterior distribution. Then, we will allow the possibility of nonfundamental representations by sampling with a noninformative prior from the admissible (i.e., real valued) covariance equivalent representations and examine the resulting impulse responses.
In figure 10, we plot the impulse responses to a one standard deviation technology shock. We plot the invertible impulse associated with the model at the posterior mode of the ARMA order and parameter space against the pointwise posteriors (mode and 80% credible set) over all impulse responses weighted by posterior probabilities. To guarantee invertibility, we sample from the inverse partial autocorrelations analogously to our sampling from the partial autocorrelations for the AR components that guarantees stationarity. We also include the impulse response with Hansen's (1985) AR(1) technology assumption in the plot. The data driven selection of the specification of the shock process implies a different dynamic behavior of the model compared to Hansen's
19See Lippi and Reichlin (1994), Ferna¥ndez-Villaverde, Rubio-Ram¥irez, Sargent, and Watson (2007), and Alessi, Barigozzi, and Capasso (2011) for more on different MA representations in macroeconomic modeling.
23

calibration. Our RJMCMC procedure identifies hump-shaped impulse responses, a salient feature of the data identified in many empirical studies; e.g., Cogley and Nason (1995) identify a hump shaped response of output to transitory technology shocks using both an SVAR and a VEC model. In essence, the sluggishness of output in the data that is captured by frictions in more sophisticated models, see especially Sims (1998) for an early assessment, is relegated to the exogenous process by our procedure.
We now move beyond imposing fundamentalness in the sampled MA components. In admitting nonfundamental or noninvertible MA representations, we acknowledge that the covariance structure associated with our posterior distribution potentially implies several possible different structural representations. For an invertible or fundamental moving average representation, the roots, qi, of the MA polynomial (28) i (1/) qi + i,1qi-1 . . . + i,q must all be contained within the unit circle. That is, there exists no  such that i () = 0 where ||  1.20 We follow Lippi and Reichlin (1994) and engage in a root-flipping procedure to construct admissible covariance equivalent representations. We proceed as follows.
Sampling From Admissible Covariance Equivalent Representations 1. For a given draw of order q > 0 for the MA component of the exogenous process, factor the MA polynomial as (29) 1 + i,1L . . . + i,qLqi = (1 - 1L) (1 - 2L) . . . 1 - qi L 2. Enumerate all possible combinations of root flips, discarding any combination that would flip only one of complex conjugate pair of roots21 3. Draw an integer n  {0, 1, . . . , n~} from a uniform distribution, where n~ is the number of admissible combinations of root flips 4. Flip the roots according to the combination enumerate with n, where a draw of 0 indicates that no root is flipped (i.e., the invertible or fundamental representation is drawn.
For example, if n = 10 is drawn and the number 10 was associated with flipping roots 2 and 3, the MA polynomial for calculating impulse responses becomes
11 (30) i (L) = (-2) (-3) 1 - 2 L 1 - 3 L (1 - 1L) (1 - 4L) . . . 1 - qi L Drawing the covariance equivalent representation from a uniform distribution over all admissible covariance equivalent representations puts equal weight on each admissible representation, reflect-
20See, e.g., Hamilton (1994).
24

1.6 1.4 1.2
1 0.8 0.6 0.4 0.2
0 0
1.4 1.2
1 0.8 0.6 0.4 0.2
0 0
0.05 0.04 0.03 0.02 0.01
0 -0.01 -0.02
0
5 4 3 2 1 0 -1
0

Output
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound 5 10 15 20 25 30 35 40
Consumption
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40
Interest
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40
Investment
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40

1.4 1.2
1 0.8 0.6 0.4 0.2
0 0
1.2 1
0.8 0.6 0.4 0.2
0 -0.2
0
1 0.8 0.6 0.4 0.2
0 0

Capital
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound 5 10 15 20 25 30 35 40
Labor
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40
Technology
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40

Figure 10: Impulse Responses to a One Standard Deviation Technology Shock Invertibility of MA Components Imposed

25

ing our flat prior over the different representations over which DSGE theory is noninformative. Figure 11 contains the pointwise posteriors (mode and 80% credible set) over all impulse re-
sponses weighted by posterior probabilities and drawn, potentially, from nonfundamental covariance equivalent representations as outlined above. We plot these pointwise posteriors against the invertible representation of the model at the posterior mode over ARMA orders and their parameter values and against the impulse response with Hansen's (1985) AR(1) technology assumption. The admission of non-fundamental representations increases our uncertainty over the dynamic response of variables to a technology innovation, spreading the bounds of the 80% credible sets apart. Most of this spread is downward so that the number of periods for which the 80% credible set covers exclusively positive responses to a technology shock is greatly reduced.
Admitting non-fundamental moving average representations places a negative response of hours to a positive technology shock is contained in the credible set. Hence even this simplest real business cycle model with an estimated technology shock process can recreate this stylized observation of Gal¥i (1999) and Francis and Ramey (2005). The conclusion, therefore, that the stochastic growth model is unable to generate this response to technology shocks would require a strong prior against the noninvertible moving average representations, e.g., against news shocks and policy announcement shocks. Though the majority of the posterior mass still lies in a region where the response of hours to technology is conventional, in line with the results in Chari, Kehoe, and McGrattan (2008) and Uhlig (2004).
In sum, the posterior mode model and the posterior distribution over impulse responses, both fundamental and admitting the possibility of non-fundamental moving average representations, as markedly different than those implied by the AR(1) assumption in Hansen's (1985) original study. The data clearly favors hump-shaped impulse responses and cannot rule out a drop in hours in response to a positive technology shock.
6 Conclusion
In this paper we present a novel approach to addressing misspecification in DSGE models. We relax the assumptions usually placed on the structure of exogenous processes, standard practice being AR(1) processes, and estimate generalized, ARMA(p, q) processes of unknown orders. Since
26

Output
2 Hansen Posterior Mode Model Posterior Mode IRF
1.5 Posterior IRF 10% Bound Posterior IRF 90% Bound
1

0.5

0

-0.5 0
1.4 1.2
1 0.8 0.6 0.4 0.2
0 0
0.06 0.05 0.04 0.03 0.02 0.01
0 -0.01 -0.02
0
5 4 3 2 1 0 -1 -2 -3
0

5 10 15 20 25 30 35 40
Consumption
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40
Interest
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40
Investment
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound
5 10 15 20 25 30 35 40

1.2 1
0.8 0.6 0.4 0.2
0 -0.2
0
1.5
1
0.5

Capital
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound 5 10 15 20 25 30 35 40
Labor
Hansen Posterior Mode Model Posterior Mode IRF Posterior IRF 10% Bound Posterior IRF 90% Bound

0

-0.5

-1 0 5 10 15 20 25 30 35 40
Technology
Hansen Posterior Mode Model 1 Posterior Mode IRF Posterior IRF 10% Bound 0.8 Posterior IRF 90% Bound
0.6
0.4
0.2
0 0 5 10 15 20 25 30 35 40

Figure 11: Impulse Responses to a One Standard Deviation Technology Shock Invertibility of MA Components Not Imposed

27

theory provides no guidance on autocorrelation patterns of exogenous variables and the order of the these processes in DSGE models is seldom if ever estimated, the usual choice of the AR(1) structure on exogenous processes often lacks any empirical support. Our method treats the ARMA orders of shock processes as additional parameters to be estimated, enabling the researcher to identify those shock process structures which bring the model closer to the data.
The impulse responses implied by the estimated ARMA process for the technology shock using US GDP data with Hansen's (1985) specification of the canonical stochastic neoclassical growth model are markedly different than those generated under the original calibration. Our posterior clearly identifies hump-shaped impulse responses and cannot rule out a drop in hours in response to a positive technology shock.
Our method has the advantage that it will ultimately enable the analysis of a joint posterior over different specifications of the exogenous processes including their parameters as well as parameters of the model, as we are investigating in work in progress. This allows for the quantification of posterior uncertainty regarding the model parameters and all parameters of the exogenous processes including their orders, while maintaining the interpretability of these processes as structural. If one interprets the richer shock structure preferred by our method as a means of controlling for misspecification and, insofar as this misspecification is taken to be policy invariant, the generalized shocks should improve the accuracy of policy experiments while at the same time improving the fit of the model, as indicated in Del Negro and Schorfheide (2009).
28

References
Alessi, L., M. Barigozzi, and M. Capasso (2011): "Non-Fundamentalness in Structural Econometric Models: A Review," International Statistical Review, 79(1), 16≠47.
An, S., and F. Schorfheide (2007): "Bayesian Analysis of DSGE Models," Econometric Reviews, 26(2-4), 113≠172.
Anderson, B. D. O., and J. B. Moore (1979): Optimal Filtering. Prentice-Hall, Inc.
Barndorff-Nielsen, O., and G. Schou (1973): "On the Parametrization of Autoregressive Models by Partial Autocorrelations," Journal of Multivariate Analysis, 3, 408≠419.
Blanchard, O. J., and S. Fischer (1989): Lectures on Macroeconomics. MIT Press, Cambridge, MA.
Blanchard, O. J., and C. M. Kahn (1980): "The Solution of Linear Difference Models under Rational Expectations," Econometrica, 48(5), 1305≠1311.
Brooks, S. P., and R. Ehlers (2004): "Bayesian Analysis of Order Uncertainty in ARIMA Models," Discussion paper, Federal University of Paran, Brazil; University of Cambridge, UK.
Brooks, S. P., and A. Gelman (1998): "General Methods for Monitoring Convergence of Iterative Simulations," Journal of Computational and Graphical Statistics, 7(4), 434≠455.
Brooks, S. P., P. Giudici, and G. O. Roberts (2003): "Efficient construction of reversible jump Markov chain Monte Carlo proposal distributions," Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(1), 3≠39.
Cappe`, O., C. P. Robert, and T. Ryde`n (2003): "Reversible jump, birth-and-death and more general continuous time Markov chain Monte Carlo samplers," Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(3), 679≠700.
Chari, V. V., P. J. Kehoe, and E. R. McGrattan (2007): "Business Cycle Accounting," Econometrica, 75(3), 781≠836.
(2008): "Are structural VARs with long-run restrictions useful in developing business cycle theory?," Journal of Monetary Economics, 55, 1337≠1352.
Chib, S., and E. Greenberg (1995): "Understanding the Metropolis-Hastings Algorithm," The American Statistician, 49(4), 327≠335.
Cogley, T., and J. M. Nason (1995): "Output Dynamics in Real-Business-Cycle Models," American Economic Review, 85(3), 492≠511.
Cu¥rdia, V., and R. Reis (2010): "Correlated Disturbances and U.S. Business Cycles," NBER Working Papers 15774, National Bureau of Economic Research, Inc.
de Jong, P., and J. Penzer (2004): "The ARMA Model in State Space Form," Statistics & Probability Letters, 70(1), 119 ≠ 125.
Del Negro, M., and F. Schorfheide (2009): "Monetary Policy Analysis with Potentially Misspecified Models," American Economic Review, 99(4), 1415≠50.
29

Dennis, Jr., J. E., J. F. Traub, and R. P. Weber (1976): "The Algebraic Theory of Matrix Polynomials," SIAM Journal on Numerical Analysis, 13(6), 831≠845.
Ehlers, R., and S. P. Brooks (2008): "Adaptive Proposal Construction for Reversible Jump MCMC," Scandinavian Journal of Statistics, 35(4), 677≠690.
Fan, Y., and S. Sisson (2011): Handbook of Markov Chain Monte Carlochap. 3, pp. 67≠87. CRC Press.
Ferna¥ndez-Villaverde, J., and J. F. Rubio-Ram¥irez (2004): "Comparing dynamic equilibrium models to data: a Bayesian approach," Journal of Econometrics, 123(1), 153 ≠ 187.
Ferna¥ndez-Villaverde, J., J. F. Rubio-Ram¥irez, T. J. Sargent, and M. W. Watson (2007): "ABCs (and Ds) of Understanding VARs," American Economic Review, 97(3), 1021≠1026.
Francis, N., and V. A. Ramey (2005): "Is the technology-driven real business cycle hypothesis dead? Shocks and aggregate fluctuations revisited," Journal of Monetary Economics, 52(8), 1379≠1399.
Gal¥i, J. (1999): "Technology, Employment, and the Business Cycle: Do Technology Shocks Explain Aggregate Fluctuations?," American Economic Review, 89(1), 249≠271.
Gelfand, A. E., and A. F. M. Smith (1990): "Sampling-Based Approaches to Calculating Marginal Densities," Journal of the American Statistical Association, 85(410), pp. 398≠409.
Geweke, J. (1998): "Using simulation methods for Bayesian econometric models: inference, development, and communication," Staff Report 249, Federal Reserve Bank of Minneapolis.
Golub, G. H., and C. F. Van Loan (1996): Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, and London, UK, 3 edn.
Green, P. J. (1995): "Reversible Jump Markov Chain Monte Carlo," Biometrika, 82, 711≠732.
Hamilton, J. D. (1994): Time Series Analysis. Princeton University Press, Princeton.
Hansen, G. D. (1985): "Indivisible Labor and the Business Cycle," Journal of Monetary Economics, 16(3), 309≠327.
Harvey, A. C. (1993): Time Series Models. Harvester Wheatsheaf, London, UK, 2 edn.
Hastings, W. (1970): "Monte Carlo Sampling Methods using Markov Chains and Their Applications," Biometrika, 57, 97≠109.
Herbst, E., and F. Schorfheide (2014): "Bayesian Inference for DSGE Models," Mimeo.
Higham, N. J., and H.-M. Kim (2000): "Numerical Analysis of a Quadratic Matrix Equation," IMA Journal of Numerical Analysis, 20, 499≠519.
Hodrick, R. J., and E. C. Prescott (1997): "Postwar U.S. Business Cycles: An Empirical Investigation," Journal of Money, Credit and Banking, 29(1), 1≠16.
Hoeting, J. A., A. E. Raftery, D. Madigan, and C. T. Volinsky (1999): "Bayesian Model Averaging: A Tutorial," Statistical Science, 14(4), 382≠417.
30

Jones, M. (1987): "Randomly Choosing Parameters from the Stationarity and Invertibility Region of Autoregressive-Moving Average Models," Journal of the Royal Statistical Society, Series C (Applied Statistics), 36, 134≠138.
Justiniano, A., G. E. Primiceri, and A. Tambalotti (2008): "Investment Shocks and Business Cycles," Discussion paper, Federal Reserve Bank of Chicago.
Kass, R. E., and A. E. Raftery (1995): "Bayes Factors," Journal of the American Statistical Association, 80(430), 773≠795.
King, R. G., and S. T. Rebelo (1993): "Low frequency filtering and real business cycles," Journal of Economic Dynamics and Control, 17(1-2), 207≠231.
Klein, P. (2000): "Using the Generalized Schur Form to Solve a Multivariate Linear Rational Expectations Model," Journal of Economic Dynamics and Control, 24(10), 1405≠1423.
Koop, G., R. Leon-Gonzalez, and R. Strachan (2012): "Bayesian model averaging in the instrumental variable regression model," Journal of Econometrics, 171(2), 237 ≠ 250, Bayesian Models, Methods and Applications.
Kydland, F. E., and E. C. Prescott (1982): "Time to Build and Aggregate Fluctuations," Econometrica, 50(6), 1345≠70.
Lan, H., and A. Meyer-Gohde (2014): "Solvability of Perturbation Solutions in DSGE Models," Journal of Economic Dynamics and Control, 45(C), 366≠388.
Lancaster, P. (1966): Lambda-Matrices and Vibrating Systems. Pergamon Press, Oxford.
Leamer, E. E. (1978): Specification Searches: Ad Hoc Inference with Nonexperimental Data. Wiley.
Leeper, E. M., and C. A. Sims (1994): "Toward a Modern Macroeconomic Model Usable for Policy Analysis," in NBER Macroeconomics Annual, ed. by S. Fischer, and J. J. Rotemberg, vol. 9, pp. 81≠118. MIT Press Books.
Lippi, M., and L. Reichlin (1994): "VAR analysis, nonfundamental representations, blaschke matrices," Journal of Econometrics, 63(1), 307≠325.
Madigan, D., and A. E. Raftery (1994): "Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam's Window," Journal of the American Statistical Association, 89(428), 1535≠1546.
Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller (1953): "Equation of State Calculations by Fast Computing Machines," The Journal of Chemical Physics, 21(6), 1087≠1092.
Meyer-Gohde, A. (2010): "Linear Rational-Expectations Models with Lagged Expectations: A Synthetic Method," Journal of Economic Dynamics and Control, 34(5), 984≠1002.
Monahan, J. (1984): "A note on enforcing stationarity in autoregressive-moving average models," Biometrika, 71, 403≠404.
Peskun, P. (1973): "Optimum Monte-Carlo sampling using Markov chains," Biometrika, 60, 607≠ 612.
31

Philippe, A. (2006): "Bayesian analysis of autoregressive moving average processes with unknown orders," Computational Statistics & Data Analysis, 51, 1904≠1923.
Raftery, A. E., D. Madigan, and J. A. Hoeting (1997): "Bayesian Model Averaging for Linear Regression Models," Journal of the American Statistical Association, 92(437), 179≠191.
Sargent, T. J. (1987): Macroeconomic Theory. Academic Press, San Diego, CA, 2nd edn.
Schmitt-Grohe¥, S., and M. Uribe (2010): "Evaluating the Sample Likelihood of Linearized DSGE Models without the Use of the Kalman Filter," Economics Letters, 109(3), 142≠143.
Sims, C. A. (1998): "Stickiness," Carnegie-Rochester Conference Series on Public Policy, 49(1), 317≠356.
(2001): "Solving Linear Rational Expectations Models," Computational Economics, 20(12), 1≠20.
(2012): "Statistical Modeling of Monetary Policy and Its Effects," American Economic Review, 102(4), 1187≠1205.
Smets, F., and R. Wouters (2007): "Shocks and Frictions in US Business Cycles: A Bayesian DSGE Approach," American Economic Review, 97(3), 586≠606.
Stephens, M. (2000): "Bayesian analysis of mixture models with an unknown number of componentsan alternative to reversible jump methods," The Annals of Statistics, 28(1), 40≠74.
Strachan, R. W., and H. K. Van Dijk (2013): "Evidence On Features Of A Dsge Business Cycle Model From Bayesian Model Averaging," International Economic Review, 54(1), 385≠402.
Tierney, L. (1998): "A note on Metropolis-Hastings kernels for general state spaces," The Annals of Applied Probability, 8(1), 1≠9.
Troughton, P., and S. J. Godsill (1998): "A Reversible Jump Sampler for Autoregressive Time Series, Employing Full Conditionals to Achieve Efficient Model Space Moves," pp. 2257≠2260.
Uhlig, H. (1999): "A Toolkit for Analysing Nonlinear Dynamic Stochastic Models Easily," in Computational Methods for the Study of Dynamic Economies, ed. by R. Marimon, and A. Scott, chap. 3, pp. 30≠61. Oxford University Press.
(2004): "Do Technology Shocks Lead to a Fall in Total Hours Worked?," Journal of the European Economic Association, 2(2-3), 361≠371.
Waagepetersen, R., and D. Sorensen (2001): "A Tutorial on Reversible Jump MCMC with a View toward Applications in QTL-mapping," International Statistical Review, 69, 49≠61.
Watson, M. (1993): "Measures of Fit for Calibrated Models," Journal of Political Economy, 101, 1011≠1041.
Wolters, M. H. (2015): "Evaluating Point and Density Forecasts of DSGE Models," Journal of Applied Econometrics, 30(1), 74≠96.
32

A Appendices

A.1 Detailed Derivation of Inflated Proposal Mapping

To choose an appropriate mapping gpp , it is useful to break the mapping into two parts according

to the desired parameters Pp and the auxiliary parameters u. The mapping gpp is given by

(A-1)

(Pp , u ) = gpp (Pp, u) = (g1pp (Pp, u), g2pp (Pp, u))

and its inverse

(A-2)

(Pp, u) = g-p1p (Pp , u ) = gp p(Pp , u ) = (g1p p(Pp , u ), g2p p(Pp , u )

Start with g1pp . Suppose now that the current state of the Markov chain is at  = (p, Pp).

Now with probability p(p |p), a move to the model with order p is proposed. Conditional on this

proposal, we draw u from some proposal distribution  pp (u). Then, we introduce a deterministic

mapping g1pp that maps the current state and the auxiliary proposal u to the proposed new state

such that (p , Pp ) = (p , g1pp (Pp, u)). Note that u is not part of the state of the chain.

Additionally, we have to find g2pp . In order to be able to easily verify adherence to detailed

balance for a move from a state (p, Pp) to (p , Pp ) = (p , g1pp (Pp, u)) the vectors of Markov chain

states and the random auxiliary proposal variables (Pp, u) and (Pp , u ) must be of equal dimension

and requiring gpp to be a differentiable bijection lets us use a simple change-of-variables in the detailed balance equation. I.e., the kernel of the chain is now defined in terms of the auxiliary

variable u together with the model indicator and the parameter vectors.

Armed with this structure it is now straightforward to derive the appropriate acceptance probability. The detailed balance condition holds if22

(A-3)

 (p|y)  (Pp|p, y) Q , Bp dPp =  p |y  Pp |p , y Q  , Ap dPp

Ap Bp

for all subsets Ap and Bp of the parameter spaces associated with autoregressive polynomi-

als of order p and p respectively. The posterior distribution (|y) is factorized as (|y) =

(p|y)(Pp|p, y) and

Q , Bp = ( |)(,  )d
Bp
= p(p |p) Ω(g1pp (Pp, u)  Bp )pp (Pp, g1pp (Pp, u)pp (Pp, u)du

22See also Waagepetersen and Sorensen (2001).

33

The left hand side of (A-3) is then

(A-4) (A-5)

 (|y) Q , Bp dPp =

Ω(Pp  Ap, g1pp (Pp, u)  Bp ) (p|y)  (Pp|p, y) ◊

Ap

p(p |p)pp (Pp, g1pp (Pp, u)pp (Pp, u)dPpdu

and the right hand side reads

(A-6) (A-7)

  |y Q  , Ap dPp =
Bp

Ω(Pp  Bp , g1p p(Pp , u )  Ap) p |y  Pp |p , y ◊

p(p|p )p p(Pp , g1p p(Pp , u ))p p(Pp , u )dPp du

where ( |) is again factorized as p(p| p)pp (Pp, u). The fact that gpp is a differentiable bijection

together with the dimension matching conditions enables a change of variable in (A-6) leading to

1(g1pp (Pp, u)  Bp , Pp  Ap) p |y  g1pp (Pp, u)|p , y p(p|p )

(A-8)

◊p p(g1pp (Pp, u), Pp)p p(g1pp (Pp, u), g2pp (Pp, u))|gpp (Pp, u)|dPpdu

where dPp du = |gpp (Pp, u)|dPpdu and |gpp (Pp, u)| is the determinant of the Jacobian of gpp . By inspection of (A-4) and (A-8), the reversibility condition (A-3) is satisfied if

 (p|y)  (Pp|p, y) p(p |p)pp (Pp, g1pp (Pp, u))pp (Pp, u) =

 p |y  g1pp (Pp, u)|p , y p(p|p )p p(g1pp (Pp, u), Pp)◊

(A-9)

p p(g1pp (Pp, u), g2pp (Pp, u))|gpp (Pp, u)|

Choosing the acceptance probability as large as possible, we have

(A-10)

pp = min 1, pp (,  )

with (A-11)

pp ,  =

L( ) L()

( ) ()



p(p|p )p p(gpp (Pp, u)) p(p |p)pp (Pp, u)

|gp

p

(Pp, u) |

Likelihood Ratio Prior Ratio

Proposal Ratio

With our mapping gpp , in (9), |gpp (Pp, u)| is equal to one and (A-11) reduces to (14).23

A.2 Imposing Stationarity and Invertibility on ARMA(p,q) Sampling

To constrain sampling to these invertible and stationary regions of the parameters spaces of each model, we follow Barndorff-Nielsen and Schou (1973), Monahan (1984) and Jones (1987) and
reparametrize the AR (and MA) polynomial in terms of its (inverse) partial autocorrelations (PACs).
23The posterior  is here written factorized as the product of likelihood and prior L()() for correspondence with the general formulation of the detailed balance condition ( 3).

34

If the (inverse) partial autocorrelations are between -1 and 1 the process is (invertible) stationary.
First, we generalize the AR(p) model to an ARMA(p,q) as follows
(A-12) yt = P1p,qyt-1 + P2p,qyt-2 + . . . + Ppp,qyt-p + t + Q1p,q t-1 + . . . + Qqp,q t-q, t  N 0, 2
In order to recover the coefficients of the AR polynomials, the following algorithm is run
Recovering AR Coefficients from PACs 1. Introduce pk = p1(k), . . . , p(kk) , k = 1, . . . , p 2. Draw r = r1, . . . , rp, for ri  (0, 1) partial autocorrelations 3. Set p(11) = r1 4. Run the recursion pi(k) = p(ik-1) - rk p(kk--i1), i = 1, for . . . , k - 1 with pk(k) = rk for k = 2, . . . , p 5. Set Pp = p(p)

The MA coefficients are recovered analogously, where the inverse partial autocorrelations substitute for the partial autocorrelations, ri, in the foregoing. Ultimately, instead of proposing AR(MA) parameters directly, (inverse) partial autocorrelations are proposed in their place from which the parameters are then recovered. This will obviously necessitate the formulation of priors over (inverse) partial autocorrelations instead of parameters.

A.3 Class of DSGE Models with VARMA(p,q) Processes

We will consider linear(ized) DSGE models that can be expressed compactly as

(A-13)

0 = Et AXt+1 + BXt + CXt-1 + D Zt

nx ◊1

nz ◊1

where the vector Xt collects the endogenous variables and the vector Zt the exogenous variables.

Instead of the standard assumption of independent AR(1) processes for the elements of the vector

Zt,24 we shall allow each element in Zt to be driven by an independent ARMA(p,q) process, whose

orders p and q along with whose parameters we shall estimate using the RJMCMC algorithm

developed in section 2.

The method laid out in section 2 extends straigthforwardly to multiple autoregressive moving averages of finite order.25 Specifically, we assume that each exogenous process can be represented

24Notable exceptions are Cu¥rdia and Reis (2010) and Chari, Kehoe, and McGrattan (2007), who let their vector of
disturbances follow a vector AR(1) process, and Del Negro and Schorfheide (2009) and Smets and Wouters (2007), who let two of their seven disturbances follow ARMA(1,1).
25We will examine multiple ARMA processes instead of VARMA (vector autoregressive moving averages) both to

35

as a finite order ARMA26

(A-14) zi,t = i,1zt-1 + i,2zi,t-2 . . . + i,pi zi,t-pi + i,0 i,t + i,1 i,t-1 . . . + i,qi i,t-qi , i,t  N 0, i2

We assume that the processes in (A-14) are stationary and invertible, as we summarize in the

following

Assumption A.1. The roots of the polynomial

(A-15)

i () pi - i,1pi-1 + i,2pi-2 . . . + i,pi

are all inside the unit circle. That is, there exists no  such that i () = 0 where ||  1.

Expressed in vector form, the exogenous processes can be collected as

(A-16)

Zt = P1Zt-1 + P2Zt-2 . . . + PpZt-p + I t + Q1 t-1 . . . + Qq t-q, t  N (0, ) nz ◊1

where p is the highest autoregressive order (p = max ({pi})) and q the highest moving average order

(q = max ({qi})) among the exogenous processes. The covariance matrix  is diagonal, collecting

the variances of the individual processes along the diagonal-- diag 21, 22, . . . , 2nz . The

stationarity and invertibility of the individual processes in assumption A.1 transfers to the vector

process (A-16), as we state formally as

Lemma A.2. The latent roots of the  matrix

(A-17)

Inz p - P1p-1 + P2p-2 . . . + Pp

That is, there exists no  such that det (P ()) = 0 where ||  1.

Proof. Follows directly from assumption A.1.

A.4 Recursive Solution for DSGE Models with VARMA(p,q) Processes

We will solve for a recursive solution for the endogenous variables in the model (A-13) using a method of undetermined coefficients approach. Given (A-16) and (A-13), the state variables of the model are

(A-18)

Xt-1, Zt, Zt-1, . . . , Zt-(p~-1), t, t-1, . . . , t-(q-1)

maintain the structural interpretation of the shock and to avoid the proliferation of parameters and reparameterizations,

see Monahan (1984), needed to guarantee stationarity in vector processes.

26 We adopt the convention that sums that terminate with an index smaller than that with which they began are

empty sets. For example, if pi = 0 in (A-14) for some i;

-0 j=1

i, jzt- j

=



such

that

zi,t

in

this

case

would

be

zi,t = i,0 i,t + i,1 i,t-1 . . . + i,qi i,t-qi

36

where p~ = max (p, 1),27

While we could redefine the model (A-13) to include the entire state vector (A-18) as endoge-

nous variables to bring the model into the canonical form of, say, Sims (2001) or Klein (2000),

doing so would significantly increase the computation costs involved in the QZ decomposition for

the state transition and the Sylvester equation for the impact matrix of shocks. The solution for the

endogenous variables is, accordingly, given by

(A-19)

Xt = Xt-1 + 0Zt + 1Zt-1 . . . + p~-1Zt-(p~-1) + 0 t + 1 t-1 . . . + q-1 t-(q-1)

where

(A-20)

, 0, 1, . . . , p~-1, 0, 1, . . . , q-1

are the unknown coefficients that we solve for.

We will make the following two assumptions that correspond to the Blanchard and Kahn's

(1980) order and rank conditions to guarantee a unique stable solution. The order condition as-

sumes a full set of latent roots with half inside and half outside the unit circle

Assumption A.3. Order There exist 2nx latent roots of A2+B+C--that is, nx+rank (A) finite   R : det A2 + B + C = 0 as well as nx - rankA infinite --of which nx lie inside the unit circle and nx outside.
We then assume that a solution, or solvent, can be constructed containing these stable roots

Assumption A.4. Rank There exists an   Rnx◊nx such that A2 + B + C = 0 and |eig()| < 1.
Thus,  is the unique solution to the matrix quadratic equation A2 + B + C = 0 whose eigenvalues coincide with the stable latent roots of the quadratic  matrix A2 + B + C.28

Under the order and rank assumptions, as well as the stationarity assumption on the exoge-

nous processes, the model (A-13) has a unique, stable solution, as we summarize in the following

proposition

27This follows directly from (A-16) expressed in first order vector form

Zt Zt-1 . . . Zt-(p~-1) t t-1 . . . t-(q-1) = PP Zt-1 Zt-2 . . . Zt-p t-1 t-1 . . . t-q + QQ t
for appropriate PP and QQ matrices. The left hand side of the foregoing is then the current exogenous state vector. The case p = 0 is permitted through p~, which ensures Z t remains on the left hand side of the foregoing despite the indexing convention laid out in footnote 26.
28See Lancaster (1966), Dennis, Jr., Traub, and Weber (1976), and Higham and Kim (2000) for detailed analysis of matrix polynomials and  matrices, as well as Lan and Meyer-Gohde (2014) for an application to DSGE models.

37

Proposition A.5. Let assumptions A.3, A.4, and A.2 hold. There exists a unique, stable solution

(A-19) to (A-13). The coefficient  in (A-19) is the solvent of assumption A.4, the coefficients

0, 1, . . . , q-1 for q > 0 solve

0
nx ◊nz

=A

(0

+

0Q1

+

1)

+

B0

0
nx ◊nz

=A

(1

+

0Q2

+

2)

+

B1

...

0 =A q-2 + 0Qq-1 + q-1 + Bq-2
nx ◊nz

(A-21)

0 =A
nx ◊nz

q-1 + 0Qq

+ Bq-1

and the coefficients 0, 1, . . . , p-1 solve

0
nx ◊nz

=A

(0

+

0P1

+

1)

+

B0

+

D

0 =A (1 + 0P2 + 2) + B1
nx ◊nz
...

(A-22) for p > 0 and 0 solves
otherwise.

0 =A
nx ◊nz

p-2 + 0Pp-1 + p-1

+ Bp-2

0 =A
nx ◊nz

p-1 + 0Pp

+ Bp-1

0
nx ◊nz

=A0

+

B0

+

D

Proof. Insert the solution (A-19) for Xt once and for Xt+1 twice in (A-13), substitute (A-16) lagged forward once for the Zt+1 that arises when Xt+1 is replaced with (A-19), and then collect coefficients on the state variables (A-18). As the solution (A-19) must hold for all values of the state variables, the coefficients just collected must all be zero. The resulting equations are those stated in the proposition.

We can also calculate an infinite moving average representation for the solution, which will prove useful in the estimation exercise, allowing us to calculate the likelihood spectrally and to apply the closed form frequency domain representation of the HP filter (Hodrick and Prescott 1997) to treat the model with the filter while estimating. Taking the unique stable solution derived above

38

as given, we define the following  matrices for the exogenous processes

(A-23) (A-24)

P () Q ()

Inz - P1 - P22 . . . - Ppp I + Q1 . . . + Qqq

and for the endogenous transfer function

(A-25)

 () 0 + 1 . . . + p~-1p~-1

(A-26)

 () 0 + 1 . . . + q-1q-1

Replacing  with the lag or backshift operator L,29 we can express Xt as an infinite moving average, as we summarize in the following proposition

Proposition A.6. Let assumptions A.3, A.4, and A.2 hold. The unique, stable solution (A-19) to

(A-13) for Xt in proposition A.5 has a unique infinite moving average representation given by

(A-27)

-1

Xt =

I - L
nx ◊nx

 (L) P (L)-1 Q (L) +  (L) t

Proof. Invertibility of I - L follows from proposition A.5 and that of P (L) from lemma A.2. nx ◊nx
Uniqueness follows from the uniqueness of the homogenous representation from assumptions A.3

and A.4 and of the uniqueness of the inhomogenous representation from proposition A.5.

A.5 Solving for the Coefficients in the Recursive Solution for DSGE Models with VARMA(p,q) Processes

For the sequence of coefficients {i}ip~=-01 that measure the impact of the exogenous processes in Zt on Xt we need to solve (A-22) or (A-23) if p = 0. This set of equations can be rewritten by

recursive (A-28)

subspti-tiu=tion

as30
i j=1
i j=1

- (B + A)-1 A - (B + A)-1 A

j 0Pp-i+ j j 0Pp-i+ j - (B + A)-1 D

for i = 1, 2, . . . p - 1 for i = p

where the invertibility of B + A follows from assumptions A.3 and A.4.31 Thus, given 0 from

the i = p case we can recover the remaining matrices i.

For i = p, (A-28) is

(A-29)

p

0 =

- (B + A)-1 A j 0P j - (B + A)-1 D

j=1

29See, e.g., Sargent (1987). 30Starting with the last equation of (A-22). It is already in this form. Then proceed to the second-to-last equation
and eliminate p-1 in this equation using the last equation. Proceed thusly to the first equation. 31See Lan and Meyer-Gohde (2014).

39

or

(A-30)

p

0 +

- (B + A)-1 A j 0 -P j = - (B + A)-1 D

j=1

which is linear in 0, being a p'th generalized Sylvester equation of the form

(A-31)

x + x1 + 2x2 . . . + pxJ = 

where x 0 and  - (B + A)-1 A.32

Proposition A.7. A generalized Sylvester equation of the form

(A-32)

x + x1 + 2x2 . . . + J xJ = 

can be solved recursively for x as follows



na◊nb





(A-33)

x~i,∑  J  jUij,i = i,∑ - na-i J {U j}i,na+k x~na+k,∑ j , for i = na, na - 1, . . . , 1

j=0 k=1 j=0

where x~ Qx, QUQ =  with U upper diagonal and Q unitary is the complex Schur decompo-

sition33 of ,  indicates conjugate transposition, and c,d references the c'th row and d'th column of

a matrix.

Proof. With the Schur decomposition QUQ = , (A-31) can be rewritten as

(A-34)

x + QUQ x1 + QUQ 2 x2 . . . + QUQ J xJ = 

The matrix Q is unitary, so Q = Q-1 reducing the foregoing to

(A-35)

x + QUQ x1 + QU2Qx2 . . . + QU J Q xJ = 

multiplying through with Q and using the definition x~ Qx gives

(A-36)

x~ + U x~1 + U2 x~2 . . . + UJ x~J = Q

As U is upper diagonal, so is any power of U; thus given all rows of the matrix x~ after some i, the

i'th row of x~, x~i,∑ solves (A-37)

 J Uij,i x~i,∑ j = i,∑ - na-i J {U j}i,na+k x~na+k,∑ j

j=0 k=1 j=0

recognizing that Ui,i is a scalar gives (A-33) which can be solved by multiplying on the right by

the inverse of

J j=0



jUij,i

.

Given 0, the remaining sequence of coefficients {i}ip=-11 can be recovered recursively from

(A-22) starting with p-1 and working backwards to 1. Likewise, given 0, the sequence of

32For completeness,  j -P j, for j = 1, 2, . . . , p and  - (B + A)-1 D. 33See, e.g., Golub and Van Loan (1996).

40

coefficients {i}qi=-01 an be recovered recursively from (A-21) starting with q-1 and working backwards to 0.

A.6 DSGE Likelihood with VARMA(p,q) Processes

One difficulty in implementing likelihood methods lies in the evaluation of the likelihood func-

tion. As we will consider applying the HP filter to the model when it was applied to the data, the

Kalman filter is less desirous here due to the availability of a closed form frequency domain repre-

sentation for the HP filter, see King and Rebelo (1993). We follow an alternative approach based

on the Toeplitz structure of the covariance of stationary time series that uses the iterative method of

Meyer-Gohde (2010) for evaluating the likelihood function by treating the sample as a single draw

from a multivariate normal distribution,34 where the derivation of the sequence of autocovariances

is done spectrally to enable us to apply the HP filter to the model while evaluating the likelihood

function.

Consider now a linear combination of elements of Xt. I.e., the observables, given by

(A-38)

Yt

=

X
ny ◊nx

Xt

To evaluate the likelihood function, we will need to calculate the sequence of autocovariance ma-

trices associated with the observables, Yt,

(A-39)

0 E YtYt , 1 E YtYt-1 , . . . n E YtYt-n

Using the moving average representation of the observables

(A-40)

-1

Yt = X

I - L
nx ◊nx

 (L) P (L)-1 Q (L) +  (L) t

The autocovariances can be recovered, see, e.g., Sargent (1987), Hamilton (1994), and Uhlig

(1999), through

(A-41)


n = G()eind

-

the inverse Fourier transformation of the spectral density of Yt, G() given by

G() = X

I

-1
- e-i



e-i

P

e-i

-1
Q

e-i

+

e-i

nx ◊nx

(A-42)

◊  X

I

-1
- ei



ei

P

ei

-1
Q

ei

+

ei

nx ◊nx

As we will also consider applying the HP filter to the model as well as to the data, we can use

34Similarly to Leeper and Sims (1994) and Schmitt-Grohe¥ and Uribe (2010).

41

closed form representation of the HP filter in the frequency domain, see King and Rebelo (1993),

given as

(A-43)

HP(, )

=

1

4 (1 - cos())2 + 4 (1 - cos())2

where  is the HP smoothing parameter and  a frequency. In this case, the autocovariances of the

HP filtered observables can be recovered through

(A-44)


n = HP(, )2G()eind

-

Given the assumptions of linearity and stationarity behind proposition A.5 and that of the nor-

mality of the innovations t, T observations on Yt are normally distributed with mean zero and

non-singular (A-45)

block

Toeplitz

cov=arianTT...c01--e12

matrix 1 0
T -3 T -2

... ... ...
... ...

T -2 T -3
0 1

TT...01--12

with the autocovariance matrices, n, given by (A-41) or (A-44) depending on whether the HP

filter was used and the log-likelihood of a vector of parameters  given the data is thus

(A-46)

L(|Y) = -0.5pT ln (2) - 0.5ln (det (())) - 0.5Y ()-1Y

where X = [Y1Y2 . . . YT ] . Given (A-45), only two potentially challenging quantities need to be calculated: ln (det (()))
and X ()-1X, which we calculate using the recursive block-Levinson type algorithm of Meyer-

Gohde (2010).

42

SFB 649 Discussion Paper Series 2015

For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001 002
003
004 005
006 007 008 009 010 011 012 013 014

"Pricing Kernel Modeling" by Denis Belomestny, Shujie Ma and Wolfgang Karl H‰rdle, January 2015. "Estimating the Value of Urban Green Space: A hedonic Pricing Analysis of the Housing Market in Cologne, Germany" by Jens Kolbe and Henry W¸stemann, January 2015. "Identifying Berlin's land value map using Adaptive Weights Smoothing" by Jens Kolbe, Rainer Schulz, Martin Wersing and Axel Werwatz, January 2015. "Efficiency of Wind Power Production and its Determinants" by Simone Pieralli, Matthias Ritter and Martin Odening, January 2015. "Distillation of News Flow into Analysis of Stock Reactions" by Junni L. Zhang, Wolfgang K. H‰rdle, Cathy Y. Chen and Elisabeth Bommes, January 2015. "Cognitive Bubbles" by Ciril Bosch-Rosay, Thomas Meissnerz and Antoni Bosch-DomËnech, February 2015. "Stochastic Population Analysis: A Functional Data Approach" by Lei Fang and Wolfgang K. H‰rdle, February 2015. "Nonparametric change-point analysis of volatility" by Markus Bibinger, Moritz Jirak and Mathias Vetter, February 2015. "From Galloping Inflation to Price Stability in Steps: Israel 1985≠2013" by Rafi Melnick and till Strohsal, February 2015. "Estimation of NAIRU with Inflation Expectation Data" by Wei Cui, Wolfgang K. H‰rdle and Weining Wang, February 2015. "Competitors In Merger Control: Shall They Be Merely Heard Or Also Listened To?" by Thomas Giebe and Miyu Lee, February 2015. "The Impact of Credit Default Swap Trading on Loan Syndication" by Daniel Streitz, March 2015. "Pitfalls and Perils of Financial Innovation: The Use of CDS by Corporate Bond Funds" by Tim Adam and Andre Guettler, March 2015. "Generalized Exogenous Processes in DSGE: A Bayesian Approach" by Alexander Meyer-Gohde and Daniel Neuhoff, March 2015.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatﬂraeﬂ1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasrecahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

