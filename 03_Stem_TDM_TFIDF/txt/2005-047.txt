BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2005-047
Estimation and Testing for Varying Coefficients in Additive Models with
Marginal Integration
Lijian Yang* Byeong U. Park**
Lan Xue*** Wolfgang Härdle****
* Department of Statistics and Probability, Michigan State University, USA
** Department of Statistics, Seoul National University, Korea *** Department of Statistics, Oregon State University, USA **** Institute for Statistics and Econometrics, Humboldt-
Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Estimation and Testing for Varying Coefficients in Additive Models with Marginal Integration

Lijian Yang

Byeong U. Park Lan Xue September 6, 2005

Wolfgang Ha¨rdle

Abstract
We propose marginal integration estimation and testing methods for the coefficients of varying coefficient multivariate regression model. Asymptotic distribution theory is developed for the estimation method which enjoys the same rate of convergence as univariate function estimation. For the test statistic, asymptotic normal theory is established. These theoretical results are derived under the fairly general conditions of absolute regularity (-mixing). Application of the test procedure to the West German real GNP data reveals that a partially linear varying coefficient model is best parsimonious in fitting the data dynamics, a fact that is also confirmed with residual diagnostics.
KEY WORDS: Equivalent kernels; German real GNP; Local polynomial; Marginal integration; Rate of convergence
Lijian Yang is Associate Professor, Department of Statistics and Probability, Michigan State University, East Lansing, Michigan 48824 (E-mail: yang@stt.msu.edu). Byeong U. Park is Professor, Department of Statistics, Seoul National University, Seoul 151-747, Korea (E-mail: bupark@stats.snu.ac.kr). Lan Xue is Assistant Professor, Department of Statistics, Oregon State University, Corvallis, Oregon 97331-4501 (E-mail: xuel@stat.oregonstate.edu). Wolfgang H¨ardle is Professor, Institut fu¨r Statistik und O¨ konometrie, HumboldtUniversit¨at zu Berlin, Spandauer Str.1, D-10178 Berlin, Germany (E-mail: haerdle@wiwi.hu-berlin.de). This work was supported by the Deutsche Forschungsgemeinschaft through SFB 373 "Quantifikation und Simulation O¨ konomischer Prozesse" and SFB 649 "Economic Risk". Yang's research was also partially supported by NSF grants DMS 9971186, DMS 0405330 and SES 0127722. Park's research was also supported by the SRC/ERC program of MOST/KOSEF (grant #R11-2000-073-00000). Xue's research was also partially supported by NSF grants BCS 0308420 and DMS 0405330. The authors thank the editor, the associate editor, and two referees for their insightful comments, which led a substantial improvement of the article.

1 INTRODUCTION

Parametric regression analysis usually assumes that the response variable Y depends linearly on a vector X of predictor variables. More flexible non- and semi- parametric regression models allow the dependence to be of more general nonlinear forms. On the other hand, the appeal of simplicity and interpretation still motivates search for models that are nonparametric in nature but have special features that are appropriate for the data involved. Such are additive models (Chen and Tsay 1993a, Linton and Nielsen 1995, Masry and Tjøstheim 1995, 1997, Mammen, Linton and Nielsen 1999, Sperlich, Tjøstheim and Yang 2002), generalized additive models (Linton and Ha¨rdle 1996), partially linear models (Ha¨rdle, Liang and Gao 2000), etc.
In this paper, we consider a form of flexible nonparametric regression model proposed by Hastie and Tibshirani (1993). The following model

Yi = m(Xi, Ti) + (Xi, Ti)i, i = 1, ..., n

(1)

where {i}i1 are i.i.d. white noise, each i independent of (Xi, Ti) where Xi = (Xi1, ..., Xid)T , Ti = (Ti1, ..., Tid)T ,

(2)

is called a varying-coefficient model if

d
m(Xi, Ti) = fs(Xis)Tis.
s=1

(Model I)

In Model I, all the variables {Xs}ds=1 are different from each other. The model with all

the variables {Xs}sd=1 being the same, i.e., m(Xi, Ti) =

d s=1

fs(Xi)Tis,

is

the

functional

coefficient model of Chen and Tsay (1993b) with univariate coefficient functions. The latter

is different from Model I and was fully discussed by Cai, Fan and Li (2000) and Cai, Fan

and Yao (2000). Indeed, Hastie and Tibshirani (1993) fitted real data examples exclusively

with the functional coefficient model. Although the name varying-coefficient model was used

by Cai, Fan and Li (2000), the model they studied was the same model proposed by Chen

and Tsay (1993b), except with the additional feature of a possibly non-trivial link function.

Cai, Fan and Li (2000) used local maximum likelihood estimation for all coefficient functions {fs}ds=1, whose computing was no more than a univariate estimation, due to the fact that all these univariate functions depend on the same variable X. The estimation method proposed

for the functional coefficient model does not apply for Model I.

2

For Model I, the only existing estimation method was the backfitting method of Hastie

and Tibshirani (1993), which has not been theoretically justified. Intuitively, inference about

model (1) is no more complex than that of univariate models. In this paper, we develop a marginal integration type estimator for each varying coefficient {fs}ds=1 in the case when each varying coefficient can have a different variable. Our method achieves the optimal rate

of convergence for univariate function estimation, and has a simple asymptotic theory for

the estimators.

As an illustration of the effectiveness of Model I, we consider a real time series data {Yt}nt=1 on West German GNP in Section 5. After taking first difference and de-seasonalization, the data is considered strictly stationary, as shown by the dotted curve in Figure 4. The vary-

ing coefficient models Yt = f1 (Yt-1) Yt-2 + f2 (Yt-3) Yt-4 + (noise) and Yt = f1 (Yt-3) Yt-2 + f2 (Yt-1) Yt-4 + (noise) are fitted and the estimates of the functions f1 and f2 are plotted in Figure 2. These varying coefficient AR models have 2.81 and 2.46 times, respectively, more

prediction power than the simple linear AR model. See Table 3 to find 0.00059/0.00021 =

2.81 and 0.00059/0.00024 = 2.46. More details about the data and the modelling procedures

are found in Section 5.

Model I may be viewed as a special case of a functional coefficient model with mul-

tivariate coefficient functions m(Xi, Ti) =

d s=1

gs(Xi

)Tis,

where

gs(Xi)

=

fs(Xis)

for

s = 1, . . . , d. In this respect, it would be of interest to compare Model I with some related

FAR (functional coefficient autoregressive) models. For example, for the varying coefficient

model Yt = f1(Yt-3)Yt-1 + f2(Yt-4)Yt-2 + (noise), one may consider the following FAR model

for a comparison: Yt = f1(Yt-3, Yt-4)Yt-1 + f2(Yt-3, Yt-4)Yt-2 + (noise). In a simulation study that is presented in Section 4, we find that the mean average squared residuals and

the mean average squared prediction errors of the FAR model are larger than those of the

varying coefficient model. More details on the simulation results are found in Section 4.2.

Of another special practical interests is the model that allows some of the Xs's to be the same. For this we consider the following generalization of Model I:

d0 rs

m(Xi, Ti) =

fsu(Xis)Tisu,

s=1 u=1

(Model II)

where now the coefficient functions fs1, . . . , fsrs depends on the same variable Xs. In Model

II the dimension of X is d0 which is less than d =

d0 s=1

rs,

the

dimension

of

T,

and

all

the

variables {Xs}sd=0 1 are different from each other. An advantage of Model II is that it alleviates

the dimensionality problem that the marginal integration method may have in fitting Model

I. Furthermore, the functional coefficient model of Chen and Tsay (1993b) is a special case

3

of Model II where d0 = 1. As an example of Model II, one may have

Yt = c + a1(rt)Mt + a2(rt)Mt2 + a3(rt)Mt2I{Mt<0} + b1(t)t + b2(t)t2 + t, t = 1, ..., n

in which Yt denotes the implied volatility, rt the interest rate, Mt the moneyness, and t the

maturity at time t.

Although our models consist of additive bivariate functions, they are linear in the vari-

ables Ts(Tsu). One interesting question one may ask is: are some of the coefficient functions

fs(fsu) constant? If the answer is yes for some but not all, then the model is partially linear

in

some

variables

Ts(Tsu);

if

the

answer

is

yes

to

all,

then

the

model 

is

the

classical

linear

regression model. Any constant fs(fsu) can then be estimated at 1/ n-rate of convergence.

A formal testing procedure is proposed in Section 3 for determining the constancy of co-

efficient functions fs(fsu). For the German GNP data, it is found that f1 can be set to a

constant, while f2 can not. We organize the paper as the follows. In Section 2, we describe marginal estimation

methods for Models I and II, and derive asymptotic distribution theory of the estimators.

In Section 3, a test procedure is proposed to test the hypothesis that fs(fsu) is a constant. In Section 4 we illustrate the finite sample properties of our proposals in the estimation and

testing problems. In Section 5, we apply our estimation and testing methods to the West

German real GNP data. All technical assumptions and proofs are in the Appendix.

2 ESTIMATION OF VARYING COEFFICIENTS
2.1 Model I
In this section we formulate local polynomial integration estimators of the coefficient functions {fs}sd=1 in Model I. For general background on the local polynomial method, see Stone (1977), Katkovnik (1979), Ruppert and Wand (1994), Wand and Jones (1995) and Fan and Gijbels (1996).
We assume that each i is independent of the vectors {(Xj, Tj)}j=1,...,i for each i = 1, ..., n. This is sufficient for obtaining our main results on distribution theory as we assume {(Xj, Tj)}j=1,...,n to be strictly stationary and geometrically -mixing in assumption A2 (see appendix.), but weaker than the usual assumption that each i is independent of the vectors {(Xj, Tj )}j=1,...,n.
Note that if there exists nontrivial linear dependence among the variables Ts with corresponding functions of Xs as coefficients, then functions fs are unidentifiable. To be precise,
4

suppose that
d
rs(Xis)Tis = 0, a.s.
s=1
for some nonzero measurable functions rs, then the regression function m in Model I equals
d
{fs(Xis) + rs(Xis)} Tis
s=1
as well. Hence for identifiability, we assume that

d
rs(Xis)Tis = 0 a.s. = rs(x)  0, s = 1, ..., d.
s=1

(3)

The condition (3) may be considered as an analogue of linear independence between

covariates in linear models. It is a sufficient condition of avoiding concurvity as termed by

Hastie and Tibshirani (1990). The term concurvity in addtive models is understood as an

analogue of collinearity in linear models. The condition is closely related to the invertibility

of the matrix ZsT Ws(X-s)Zs to be defined below, see Subsection A.2 of the Appendix for more details.
Now Let x =(x1, ..., xd)T IRd be a point where we want to estimate the functions {fs}sd=1. We denote by (X, T) = (X1, ..., Xd, T1, ..., Td) a generic random vector having the same distribution as (Xi, Ti) = (Xi1, ..., Xid, Ti1, ..., Tid), and define X-s and T-s, as
obtained from X and T by removing the s-th components, by

X-s = (X1, .., Xs-1, Xs+1..., Xd)T , s = 1, ..., d, T-s = (T1, .., Ts-1, Ts+1..., Td)T , s = 1, ..., d.

For a kernel function K we write Kh(u) = K(u/h)/h. We fit p-th order local polynomials to estimate the varying coefficients. Write Y = (Yi)1in and denote p(u) = (1, u, . . . , up)T . Define Zs be the n × (p + d) matrix which has p {(Xis - xs)/h}T Tis, TTi,-s as its i-th row. Let Ws(x-s)  Ws(xs, x-s) be the n × n diagonal matrix defined by

Ws(x-s) = diag {Kh(Xjs - xs)Lg(Xj,-s - x-s)/n}1jn

where Lg(u) = (g1 · · · gs-1gs+1 · · · gd)-1L(g1-1u1, . . . , gs--11us-1, gs-+11us+1, . . . , gd-1ud), L is a
(d - 1)-variate kernel, and g1, . . . , gs-1, gs+1, . . . , gd are bandwidths that are allowed to be different from each other. Then the first component of the minimizer ^ of the weighted sum

of squares

np
Yj - sl(Xjs - xs)lTjs -

2
kTjk Kh(Xjs - xs)Lg(Xj,-s - x-s)

j=1 l=0

k=s

5

is given by

^s0  ^s0(x-s) = e0T ZTs Ws(x-s)Zs -1 ZsT Ws(x-s)Y

where el is the (p + d)-dimensional vector whose entries are zero except the (l + 1)-th element which equals 1.

The integration estimator of fs(xs) is a weighted average of s0(Xi,-s)'s, i.e.

nn

f^s(xs) =

w-s(Xi,-s)^s0(Xi,-s)/

w-s(Xi,-s),

i=1 i=1

(4)

where the weight function w-s(·) has a compact support with nonempty interior, and is introduced here to avoid some technical difficulty that may arise when the density of Xi,-s's has an unbounded support. Based on (4), one can predict Y given any realization (x, t) of

(X, T) by the predictor

d
m^ (x, t) = f^s(xs)ts.

(5)

s=1

In the estimation procedure for fs for a given s, we fit local constants for the other

varying coefficients fs , s = s. One could fit higher order local polynomials for those varying

coefficients, too. The theoretical performance of the resulting estimator would be the same

as the present one, however. The smoothing bias of the present estimator due to the local

averaging for fs , s = s can be made negligible by choosing the bandwidth vector g of smaller order than h and using a higher-order kernel L. See the conditions for the bandwidths and

the kernel L given in the Appendix. In fact, the approach of taking a smaller bandwidth

g and a higher order kernel L for the directions not of interest was also adopted by Fan,

Ha¨rdle and Mammen (1998). One may sacrifice some rate of convergence in order to use a

lower order kernel.

Let , -s and s denote the densities of X, X-s and Xs, respectively. Define

bs(xs)

=

fs(p+1)(xs)

up+1E (p

{w-s(X-s)TsKs(u; + 1)!E{w-s(X-s)}

T,

xs,

X-s)}

du

,

s2(xs)

=

E

w-2 s(X-s) 2(X)

-2 s(X-s)2(X,

T)

Ks2 (u; T, X) du Xs = xs

E

s(xs) 2{w-s(X-s)}

,

where Ks is an equivalent kernel defined at (A.7).

Theorem 1 Under the assumptions A1-A7 given in the appendix, we have, for any s =

1, ..., d, as n  ,

 nh

f^s(xs) - fs(xs) - hp+1bs(xs)

-L N

0, s2(xs)

.

(6)

6

The estimator m^ (x, t) of the prediction function m(x, t) enjoys the same rate of con-
vergence as that of a single varying coefficient, and its asymptotic parameters are easily calculated from those of the f^s(xs)'s and the value of t, as in the following theorem

Theorem 2 Under the assumptions A1-A7 given in the appendix, we have, for any s = s ,

 cov nh

f^s(xs) - fs(xs)

 , nh

f^s (xs ) - fs (xs )

- 0,

(7)

as n  , and hence

 nh

m^ (x, t) - m(x, t) - hp+1bm(x, t)

-L N

0, m2 (x, t)

where bm(x, t) =

d s=1

bs(xs)ts

and

m2 (x, t)

=

d s=1

s2 (xs )t2s .

(8)

We comment here that Theorems 1 and 2 hold only for local polynomial estimators of odd degree p, while similar results hold for p even as well. In particular, p = 0 corresponds to integrating the well-known Nadaraya-Watson type estimator. When an even p is used instead, the variance formula remains the same while the bias formula contains extra terms involving the derivatives of the design density.
For selecting the bandwidths, following the idea of Ruppert, Sheather and Wand (1995) in local least squares regression, several plug-in type bandwidth selectors may be developed based on the asymptotic formulas given in the above theorems. Also, the modified multifold cross-validation criterion considered by Cai, Fan and Yao (2000) may be adapted for the above estimation. Theoretical development for these bandwidth selectors is beyond the scope of the paper. Below we describe a simple plug-in selection procedure for h and g, which is employed in our numerical study in Sections 4 and 5.
The optimal bandwidth hopt which minimizes the asymptotic mean integrated squared error of f^s is given by

hopt =

s2 (xs) dxs 2n(p + 1) b2s (xs) dxs

1/(2p+3)
.

7

Now,

bs2 (xs) dxs and s2 (xs) dxs can be approximated respectively by

n -2
(p + 1)! n-1 w-s(Xi,-s)

fs(p+1) (xs)

n
up+1n-1

w-s (Xi,-s)

i=1 i=1 2

×TisKs (u, Ti, xs, Xi,-s) du dxs,

n -2 n
n-1 w-s(Xi,-s) n-1 w-2 s (Xi,-s) -2 (Xi) 2-s (Xi,-s) 2 (Xi, Ti)
i=1 i=1
× Ks2 (u, Ti, Xi) du.

The unknown functions fs(p+1) (xs) , 2(x, t), (x), (x-s) and Ks may be substituted with their estimators as follows.
The (p + 1)th derivative function fs(p+1) (xs) is estimated by fitting a polynomial regres-
sion model of degree (p + 2):

d p+2

m(X, T) =

as,k Xsk Ts .

s=1 k=0

This leads to an estimator f^s(p+1) (xs) = (p + 1)! a^s,p+1 + (p + 2)! a^s,p+2 xs. As a by-product, the mean squared residual is used as an estimator of 2(x, t). The density functions (x) and (x-s) are estimated by

^(x)

=

1 n

n

d

h

1 (X,

d)



Xis - xs h (X, d)

,

i=1 s=1

^-s(x-s)

=

1n d

1

n i=1 s =s h (X-s, d - 1)

Xis - xs h (X-s, d2 - 1)

with the standard normal density  and the rule-of-the-thumb bandwidth

h(X, m) = var (X) {4/(m + 2)}1/(m+4) n-1/(m+4).

According to its definition given at (A.7), the dependence of the function Ks (u, t, x) on u and t is completely known. The only unknown term E TTT |X = x contained in Ss-1 (x) is estimated by fitting a matrix polynomial regression

dp

E TTT |X = x = c +

cs,k xks

s=1 k=1

in which the coefficients c and cs,k are d × d matrices.

8

For the bandwidth vector g, we note that the choice g1 = · · · = gs-1 = gs+1 = · · · = gd = (log n)-1h(p+1)/q with h asymptotic to n-1/(2p+3) satisfies the condition (A7) for Theorem 1 if q, the order of the kernel L, is greater than (d-1)/2. Thus one may take gj  (log n)-1ho(pp+t 1)/q for j = 1, . . . , s-1, s+1, . . . , d, where hopt is the optimal bandwidth obtained from the above
procedure.

2.2 Model II

In this section we describe local polynomial integration estimators of the coefficient functions {fsu, 1  u  rs, 1  s  d0} in Model II. For the identifiability of the functions fsu, we assume that

d0 rs
rsu(Xis)Tisu = 0 a.s. = rsu(x)  0, u = 1, . . . , rs, s = 1, . . . , d0.
s=1 u=1

Define X-s and x-s as in Section 2.1. Let ^su0(x-s) be the first component of the minimizer ^ of the following weighted sum of squares:

n rs p

d0 rs

2

Yj -

sul(Xjs - xs)lTjsu -

s u Tjs u Kh(Xjs - xs)Lg(Xj,-s - x-s).

j=1 u=1 l=0

s =s u =1

The integration estimator of fsu(xs) is given by a weighted average of su0(Xi,-s)'s, i.e.

nn

f^su(xs) =

w-s(Xi,-s)^su0(Xi,-s)/

w-s(Xi,-s).

i=1 i=1

As in Model I, one may predict Y given any realization (x, t) of (X, T) by the predictor

d0 rs

m^ (x, t) =

f^su(xs)tsu.

s=1 u=1

We have the following theorem which is an analogue of Theorem 1.

(9)

Theorem 3 Under the assumptions A1-A2, A3 , A4 , A5-A6 and A7 given in the appendix,

we have, for any u = 1, . . . , rs and s = 1, . . . , d0,

 nh

f^su(xs) - fsu(xs) - hp+1bsu(xs)

L N

0, s2u(xs)

(10)

as n  , where bsu(xs) = su(xs)/s and s2u(xs) = s2u(xs)/s2. The definition of su and s2u are given at (A.19) and (A.20)

9

Each pair of the entries f^su and f^s u for 1  s, s  d0 and 1  u, u  rs has a negligible asymptotic covariance when s = s . However, it has the same magnitude as the variance of each entry when s = s . The following theorem is an analogue of Theorem 2.

Theorem 4 Under the assumptions of Theorem 3, we have as n  

(i) when s = s

 cov nh

f^su(xs) - fsu(xs)

 , nh

f^s u (xs ) - fs u (xs )

 0;

(ii) when s = s

cov

 nh

f^su(xs) - fsu(xs)

 , nh

f^su (xs) - fsu (xs)

 suu (xs)/s2

where suu is defined at (A.23). Hence

 nh

m^ (x, t) - m(x, t) - hp+1~bm(x, t)

L N

0, ~m2 (x, t)

where ~bm(x, t) =

d0 s=1

suu (xs) = suu (xs)/s2.

rs u=1

bsu

(xs)tsu

,

~m2 (x, t)

=

d0 s=1

rs u=1

rs u =1

suu

(xs)tsutsu

and

3 TESTING FOR VARYING COEFFICIENTS

Suppose we are interested in testing the hypothesis

H0 : fs(xs)  constant

(11)

for a specific s in Model I. Testing the hypothesis (11) is a very important first step in model

building procedure. If this hypothesis is true, one would get minE{fs(Xs) - }2ws(Xs) = 0 where ws is an arbitrary positive weight function with a compact support. This leads us to propose the following test statistic:

n
Vns = n-1min {f^s(Xis) - }2ws(Xis)
i=1 n nn
= n-1 f^s(Xis)2ws(Xis) - n-1{ ws(Xis)}-1{ f^s(Xis)ws(Xis)}2,
i=1 i=1 i=1

(12)

where the obvious solution of the least squares problem is given by

nn
^s = { ws(Xis)}-1{ f^s(Xis)ws(Xis)}.
i=1 i=1

(13)

The next theorem describes the asymptotic distribution of the test statistic (12) under

the null hypothesis (11).

10

Theorem 5 Under the null hypothesis (11) and the assumptions A1-A7 given in the appendix, we have, for any s = 1, ..., d,

nh1/2(Vns - n-1h-1vs) L N 0, s2

(14)

as n  , where vs and s are as given in (A.17) and (A.16).

For the practical implementation of the test, we suggest to use a bootstrap procedure

instead of the asymptotic normal distribution theory in Theorem 5. The reason is that

for a test statistic based on kernel type of smoothing, the normal approximation to the

distribution of the test statistic is very poor, as shown in Ha¨rdle and Mammen (1993) and,

more recently, confirmed by Sperlich, Tjøstheim and Yang (2002). Another reason is that

the normal approximation given in Theorem 5 involves too complicated expressions, which

makes the task of obtaining asymptotic critical values out of reach.

It is well-known that the ordinary method of resampling residuals fails to work when the

error variances are allowed to be different. See Wu (1986), Liu (1988), and Mammen (1992).

Ha¨rdle and Mammen (1993) also pointed out that it breaks down even for homoscedastic er-

rors in the case of the goodness-of-fit test statistic for testing a parametric hypothesis against

the nonparametric alternative. As an alternative, we suggest to use the wild bootstrap pro-

cedure which was first introduced by Wu (1986) and implemented in various settings by Liu

(1988), Ha¨rdle and Mammen (1993), and Sperlich, Tjøstheim and Yang (2002) among oth-

ers. Basically, this approach attempts to mimic the conditional distribution of each response

given covariate using the corresponding single residual, in such a way that the first three

moments of the bootstrap population equal to those of the single residual.

To describe the procedure in our setting, let m~ (x, t) = ^sts +

d k=s

f^k(xk)tk

be

the

regression estimator under the hypothesis (11), where ^s is an estimate of the constant f^s(xs) given by (13) while f^k(xk) (k = s) is the marginally integrated estimate of fk(xk) in

(4). The wild bootstrap procedure to estimate the sampling distribution of Vns under the

null hypothesis then consists of the following steps:

(i) Find the residuals ~i = Yi - m~ (Xi, Ti) for i = 1, . . . , n.
(ii) Generate i.i.d. random variables ZiW such that E(ZiW ) = 0, E(ZiW )2 = 1 and E(ZiW )3 = 1. Put Yi = m~ (Xi, Ti) + ~iZiW .
(iii) Compute the bootstrap test statistic Vns using the wild bootstrap sample {(Yi, Xi, Ti)}in=1.

11

(iv) Repeat the steps (ii) and (iii) M times, obtaining Vns,1, . . . , Vns,M . Estimate the null distribution of Vns by the empirical distribution of Vns,1, . . . , Vns,M .

For examples empirical example

of in

ZiW the 

satisfying the moment conditions, see Mammen (1992).

next

section,

we

used

a two-point 

distribution

:

ZiW

= (1 

For the 
- 5)/2

with probability (5 + 5)/10, and ZiW = (1 + 5)/2 with probability (5 - 5)/10, with

M = 200.

For Model II, we consider the following hypothesis:

fsu(xs)  constant.

(15)

The corresponding test statistic for the hypothesis (15) is given by
n nn
Vnsu = n-1 f^su(Xis)2ws(Xis) - n-1{ ws(Xis)}-1{ f^su(Xis)ws(Xis)}2.
i=1 i=1 i=1
The next theorem describes the asymptotic distribution of the test statistic Vnsu under the null hypothesis (15).

Theorem 6 Under the null hypothesis (15) and the assumptions of Theorem 3, we have, for any u = 1, . . . , rs and s = 1, ..., d0,
nh1/2(Vnsu - n-1h-1vsu) L N 0, s2u

as n  , where vsu and su are as given in (A.22) and (A.21).

For testing the hypothesis (15), let m¯ (x, t) = ^sutsu +

d0 s =s

rs u =1

f^s

u

(xs

)ts

u

where

nn
^su = { ws(Xis)}-1{ f^su(Xis)ws(Xis)}.
i=1 i=1

A wild bootstrap procedure may be obtained by simply replacing m~ , Vns and Vns by m¯ , Vnsu and Vnsu, respectively, in the four steps described above for testing (11).
Some related work on this testing problem includes Chen and Liu (2001), and Cai, Fan and Yao (2000). The former paper treated testing, in the FAR model, whether all the coefficient functions are constant, i.e., whether the underlying process is simply a linear AR model. The latter proposed a testing procedure for the hypothesis that all the coefficient functions have known parametric forms. We think testing for a parametric form in our models is also an interesting topic for future research.

12

4 SIMULATION STUDY
In this section we investigate the finite sample properties of the estimation and testing methods through two simulated examples. One is the case where (Xi, Ti) are independent and identically distributed (i.i.d.), and the other is the case where they are endogenous and are lagged observations of the response Y . We employed local linear smoothing (p = 1) in all cases. Both of the kernels K and L were the quartic kernel K(x) = L(x) = 0.9375 (1 - x2)2 I(-1,1)(x), while the bandwidths were chosen as described Section 2.1.

4.1 The i.i.d. case
In this case we generated the data from the following varying coefficient model:

Y = f1(X1) + f2(X2)T1 + f3(X3)T2 + (X, T),

where f1(X1) = 1 + exp (2X1 - 1), f2(X2) = cos (2X2), f3(X3) = 2. The heteroscedastic conditional standard deviation was set to be

(X,

T)

=

0.5

+

1

T12 + T22 + T12 + T22

exp

(-2

+

(X1

+

X2)/2)

.

The particular form of (X, T) was considered to ensure the variance to be bounded. The

vector X = (X1, X2, X3)T was generated from the uniform distribution over the unit cube

[0, 1]3, and T = (T1, T2)T was generated from the bivariate normal with mean zero and covari-

1 0.5

ance matrix

. The vectors X and T were generated independently. Finally, the

0.5 1

error term  was generated from the standard normal distribution independently of (X, T).

A total of 100 independent data sets with sizes n = 50, 100 and 250 were generated. The

estimated functions of fs, s = 1, 2, 3 were evaluated on a grid of 91 equally-spaced points xj, j = 1, ..., 91 with x1 = 0.05, x91 = 0.95. To assess the performance of f^s for s = 1, 2, 3, we calculated the mean integrated squared error (MISE) of f^s which is defined by

MISE(f^s)

=

1 R

R

ISE(f^r,s)

=

1 R

R

1 g

g

f^r,s(xj) - fs(xj)

2
.

r=1 r=1 j=1

Here f^r,s(xj) denotes the estimated value of fs at xj for the rth data set, R = 100 and g = 91 are the numbers of data sets and grid points, respectively. Table 1 summarizes the MISE values of the function estimators. This simulation study numerically supports our theoretical results for the estimation method as given in Section 2.

13

(Insert Table 1 about here)
To see how the marginal integration improves the three dimensional function estimators, we also computed the mean average squared errors for the case where n = 50. Consider ^s0, as defined in Section 2.1, evaluated at the observed Xi1, Xi2, Xi3. Write them ^s0(Xi1, Xi2, Xi3). These are the estimates before the marginal integration. We computed the mean average squared error

MASE1

=

1 R1 Rn

n

^r,10(Xi1, Xi2, Xi3) + ^r,20(Xi1, Xi2, Xi3)Ti1

r=1 i=1

+^r,30(Xi1, Xi2, Xi3)Ti2 - f1(Xi1) - f2(Xi2)Ti1 - f3(Xi3)Ti2

2
,

and also for the marginal integration estimate

MASE2

=

1 R1 Rn

n

f^r,1(Xi1) + f^r,2(Xi2)Ti1 + f^r,3(Xi3)Ti2

r=1 i=1

-f1(Xi1) - f2(Xi2)Ti1 - f3(Xi3)Ti2}2 ,

where ^r,s0(Xi1, Xi2, Xi3) and f^r,s(Xis) are the estimates for the rth dataset. We found MASE1 = 0.3164 and MASE2 = 0.2761.
Next, we give some numerical results for the testing method. For each of the simulated data sets above, we applied the proposed wild bootstrap method with M = 500 to test the null hypothesis Hs0 : fs = cs for some constants cs. Table 2 provides for each s the proportion of the cases where the null hypothesis Hs0 was rejected at the significance level  = 0.05 among the 100 replications.
(Insert Table 2 about here)

4.2 The time series case
In this simulation, R = 200 time series were generated. Each time, 1000 observations were generated from the following varying coefficient AR (VCAR) model, among which only the last 250 observations were used:

Yt = f1(Yt-3)Yt-1 + f2(Yt-4)Yt-2 + 0.2t

(16)

where f1(Yt-3) = 0.4 + (0.1 + Yt-3) exp(-3Yt2-3), f2(Yt-4) = -0.2 - (0.6 + Yt-4) exp(-3Yt2-4),
and t are i.i.d. standard normal random variates. Again, the performance of the estimators of f1 and f2 were assessed by MISE. We obtained MISE(f^1) = 0.0137 and MISE(f^2) = 0.0151.

14

We found that the Monte Carlo average over 200 time series of

250 t=1

(Yt

-

Y¯

)2/250

equals

0.6374 with the standard error 0.0026, where Y¯ =

250 t=1

Yt/250.

The obtained values of

MISEs are much smaller than the variation of Y , which means that the fitted model with f^1

and f^2 is useful to explain the variation of Y .

Similarly as in the i.i.d. case, we report here a numerical result for testing H10 : f1 =

constant and H20 : f2 = constant. For each of the simulated time series we applied the wild bootstrap method with M = 500 and used the significant level 0.05. We found that the

proportion of the cases where the null hypothesis was rejected among the 200 replications

was 0.57 for H10 and 0.943 for H20. It is also of interest to examine the effectiveness of the varying coefficient model (16) in

comparison with some related FAR models, discussed in Cai, Fan and Li (2000) and Cai,

Fan and Yao (2000), where all the coefficient functions depend on the same variable(s). For

this purpose, we considered the following three FAR models:

Yt = g1(Yt-3)Yt-1 + g2(Yt-3)Yt-2 + 0.2t, Yt = g1(Yt-4)Yt-1 + g2(Yt-4)Yt-2 + 0.2t, Yt = g1(Yt-3, Yt-4)Yt-1 + g2(Yt-3, Yt-4)Yt-2 + 0.2t.

(17) (18) (19)

We fitted the three FAR models with the same series generated by (16). For comparison we computed the mean average squared residuals (MASR) defined by

200 250

MASR =

(yr,t - yr,t)2 /(200 × 250),

r=1 t=1

where yr,t denotes the tth observation in the rth replication, and yr,t is the corresponding fitted value based on the underlying model. We note that average squared residuals (ASR), as a statistic that can be computed from any data, real or simulated, is a very useful measure of goodness-of-fit. This is illustrated in the next section where ASR is used to select an optimal forecasting model. Thus, MASR is a sensible criterion to compare different models. Although it varies with the bandwidth, an incorrect model would have an MASR asymptotically greater than that of a correct model by a positive constant, which is of larger magnitude than any variation caused by bandwidth tuning. The three FAR models (17), (18) and (19) gave the MASR values 1.020, 0.343 and 0.081, respectively, whereas the VCAR model (16) gave a much smaller 0.075.
We also compared the mean average squared prediction errors (MASPE) of these models. For this, we generated additional 50 observations for each of the 200 times series of size 250

15

and computed

200 300

MASPE =

(yr,t - yr,t)2 /(200 × 50),

r=1 t=251

where yr,t is the predicted value of yr,t based on the estimated model from the first 250 observations. The three FAR models (17), (18) and (19) gave the MASPE values 0.075,

0.071 and 0.062, respectively, while the VCAR model (16) gave 0.059.

5 AN EMPIRICAL EXAMPLE
We illustrate our estimation and testing methods with an analysis of the quarterly (seasonally non-adjusted) West German real GNP data collected from 1960:1 to 1990:4. The data Gt, 1  t  n = 124, which was compiled by Wolters (1992, p. 424, note 4), is plotted in Figure 1(a). One sees clearly a linear trend and a seasonal pattern. Based on the seasonal unit root test of Franses (1996), we took the first differences of the logs, and obtained a time series data, Dt, 1  t  n = 124, which is plotted in Figure 1(b). This time series no longer reveals any linear or higher order trends, but is obviously seasonal. Following the de-seasonalization procedure of Yang and Tschernig (2002), the sample means of the four seasons -0.065116, 0.038595, 0.051829 and 0.008944, respectively, were calculated and subtracted from Dt so that the de-seasonalized Yt, 1  t  n = 124, became the growth rates with respect to the spring season. As such, it is reasonable to assume that the Yt's satisfy our strict stationarity and mixing conditions. In Figure 4, the data Yt, 1  t  n = 124, is plotted as the dotted curve.
(Insert Figure 1 and Table 3 about here)
According to the semiparametric lag selection performed in Yang and Tschernig (2002), the significant variables for the prediction of Yt are Yt-4 and Yt-2. Calculation of the autocorrelation functions indicated that Yt is more correlated with Yt-1 and Yt-3 than other lagged values. Hence we fitted all the twelve VCAR models of Model I type, consisting of the lagged variables Yt-1, Yt-2, Yt-3 and Yt-4. According to the definition (4) of the marginal integration estimator, we estimated all VCAR models using the first 114 observations and made out-of-sample predictions for the last 10 observations. Their average squared residuals (ASR) and average squared prediction errors (ASPE) are reported in Table 3. One may expect the ASRs should be smaller than the ASPEs. But we found in the residual plots that there were some very large residual terms that made all the ASRs larger than their

16

corresponding ASPEs. The model with the smallest ASR is

Yt = f1 (Yt-1) Yt-2 + f2 (Yt-3) Yt-4 + (noise), while the model with the smallest ASPE is

(20)

Yt = f1 (Yt-3) Yt-2 + f2 (Yt-1) Yt-4 + (noise).

(21)

Both of the above models include as special case the following linear AR(2) model:

Yt = c1Yt-2 + c2Yt-4 + (noise).

(22)

In Table 3, the ASR and ASPE of the linear AR model (22) are also included. Both the optimal VCAR models (20) and (21) have much smaller ASR and ASPE than the linear AR model. These two VCAR models have similar values of ASR and ASPE. Figure 2 depicts the estimates of the functions f1 and f2 for each model. To test if these functions are significantly different from a constant, we carried out the wild bootstrap procedures. For the model (20), the p-values were 0.80 for f1 and 0.01 for f2, while for the model (21) they were 0.22 and 0.48, respectively. This means that for the model (20) the function f1 is not significantly different from a constant but there is a strong evidence in the data for that f2 is not a constant. Thus one may conclude that a parsimonious model is the partially linear model:

Yt = f1Yt-2 + f2 (Yt-3) Yt-4 + (noise).
We further computed the ASR and ASPE of this semiparametric partially linear model, which are 0.00032 and 0.00024 respectively, as seen in Table 3. In terms of these estimation and forecasting errors, the semiparametric model is much inferior to its nonparametric parent model (20). Thus the simpler semiparametric model is preferred only for its parsimony while the nonparametric model (20) should be used if optimal forecasting is the goal. The testing for coefficient functions, therefore, works in a similar fashion as BIC works for linear AR time series where ASR is similar to AIC. For linear AR time series, it is well known that AIC is optimal for forecasting while BIC is consistent in identifying a correct AR model. It should be noted also that ASR can be compared across models not necessarily nested within each other, while the testing procedure selects the most parsimonious model from a nested hierarchy of models.
To further verify the validity of the models (20) and (21), we examined the residuals ^t to check the independence of the error terms as it is another way of assessing goodness-of-fit for the models. At a practical level, such independence can be checked using the autocorrelation

17

functions (ACF) of powers of |^t|. Figure 3 shows the ACFs of both |^t| and ^2t for the models (20) and (21). As can be seen from the plots, within the confidence levels of ±2 × n-1/2 lie more than 95% of all the sample ACFs, and hence we can conclude that both |^t| and ^2t have no autocorrelation. The ACF plots for |^t|3, ^4t , etc., led to the same conclusion. Thus, the models (20) and (21) fit well the structure of the data Yt. As a further evidence, Figure 4 shows the overlay of Yt together with the predicted series Y^t obtained from fitting the models (20) and (21). The predicted series follows the actual series very closely.
(Insert Figures 2, 3 and 4 about here)
APPENDIX: PROOFS
A longer version of the paper with proofs of greater detail may be found at http: ace.snu.ac.kr/ theostat/papers/jasa-ypxh.pdf.
A.1 Preliminaries
We shall need the following technical assumptions on the kernels.
A1: The kernels K and L are symmetric, Lipschitz continuous with K (u) du = L (u) du = 1, and have compact supports with nonempty interiors. While K is nonnegative, the kernel L is of order q.
When estimating the function fs for a particular s, a multiplicative kernel is used consisting of K for the s-th variable and L for all other variables. To accommodate dependent data, such as those from varying-coefficient autoregression models, we assume that
A2: The vector process {(Xi, Ti)}in=1 is strictly stationary and -mixing with mixing coefficients (k)  C2k, 0 <  < 1. Here
(n) = sup E sup P (A|F-k) - P (A) : A  Fn+k
k
where Ftt is the -algebra generated by (Xt, Tt) , (Xt+1, Tt+1) , ..., (Xt , Tt ) for t < t .
The following assumptions are on the smoothness of the functions involved in the estimation and testing, and on the moments of the process for the proofs of Theorems 1, 2 and 5.
18

A3: The functions fs's have bounded continuous (p + 1)-th derivatives for all 1  s  d, and p  q - 1
A4: The distribution of (X, T) has a density  and X has a marginal density . On the supports of weight functions w-s and ws, the densities -s of X-s and s of Xs, respectively, are uniformly bounded away from zero and infinity. The marginal density  and E (TsTs |X = ·) for 1  s, s  d are Lipschitz continuous. Also, 2(·, t) and (·, t) are equicontinuous.
A5: The weight functions w-s and ws are nonnegative, have compact supports with nonempty interiors, and are continuous on their supports.
A6: The error term t satisfies E|t|4+ <  for some  > 0. For j < k < l < m there exists a joint probability density function j,k,l,m of (Xj, Tj; Xk, Tk; Xl, Tl; Xm, Tm). Let X = {x : xs  supp(ws), x-s  supp(w-s)}, and for > 0 define X = {x : there exists z  X such that z - x  }. There exist > 0, ~(t) and ~j,k,l,m(tj, tk, tl, tm) such that (x, t)  ~(t) for all x  X , j,k,l,m(xj, tj; xk, tk; xl, tl; xm, tm)  ~j,k,l,m(tj, tk, tl, tm) for all xj, xk, xl, xm in X , and ( tj tk tl tm )2+c|~(tj)~(tk)~(tl)~(tm)|2+c~j,k,l,m (tj, tk, tl, tm) dtjdtkdtldtm  C <  for some c > 0 and C > 0.
Also, we assume that the bandwidths, g for the kernel L and h for the kernel K, satisfy
A7: (ln n) (nhgprod)-1/2 = O (n-a) for some a > 0 and (nh ln n)1/2gmq ax  0 as n   where gprod = g1 · · · gs-1gs+1 · · · gd and gmax = max(g1, . . . , gs-1, gs+1, . . . , gd), and h is asymptotic to n-1/(2p+3).
For Theorems 3, 4 and 6, we need to modify the assumptions A3, A4 and A7 slightly as follows:
A3 : The functions fsu's have bounded continuous (p + 1)-th derivatives for all 1  s  d0, 1  u  rs and p  q - 1.
A4 : It is the same as A4 except that now we require E (TsuTs u |X = ·) for 1  s, s  d0 and 1  u, u  rs are Lipschitz continuous.
A7 : It is also the same as A7 except that d is replaced by d0.
19

One should note here that for existence of the bandwidth vector g satisfying the assumption A7 and A7 it is necessary that q, the order of the kernel L, should be larger than (d - 1)/2 and (d0 - 1)/2, respectively.
To prove many of our results, we make use of some inequalities about U -statistic and von Mises statistic of dependent variables derived from Yoshihara (1976). Let i, 1  i  n be a strictly stationary sequence of random variables with values in Rd and -mixing coefficients (k), k = 1, 2, ..., and r a fixed positive integer. Let {n (F )} denote the functionals of the distribution function F of i
n (F ) = gn (x1, ..., xm) dF (x1) · · · dF (xm)
where {gn} are measurable functions symmetric in their m arguments such that
|gn (x1, ..., xm)|2+ dF (x1) · · · dF (xm)  Mn < +,

sup
(i1 ,....,im )Sc

|gn (x1, ..., xm)|2+ dFi1,...,im (x1, ..., xm)  Mn,c < +, c = 0, ..., m - 1

for some  > 0, where Sc = {(i1, ...., im)|#r(i1, ...., im) = c} , c = 0, ..., m - 1 and for every

(i1, ...., im), 1  i1  · · ·  im  n, #r(i1, ...., im) = the number of j = 1, ..., m - 1 satisfying ij+1 - ij  r. Clearly, the cardinality of each set Sc is less than nm-c.

The von Mises' differentiable statistic and the U -statistic

n (Fn) = gn (x1, ..., xm) dFn(x1) · · · dFn(xm)

=

1 nm

n
···

n

gn (i1, ..., im)

i1=1 im=1

Un =

1
n

gn (i1, ..., im) ,

m 1i1<···<imn

allow decompositions as

m
n (Fn) = n (F ) +

m c

Vn(c),

c=1

m
Un = n (F ) +

m c

Un(c).

c=1

Here, gn,c are the projections of gn defined by

gn,c (x1, ..., xc) = gn (x1, ..., xm) dF (xc+1) · · · dF (xm), c = 0, 1, ..., m

20

so that gn,0 = n(F ), gn = gn,m, and

c

Vn(c) = gn,c (x1, ..., xc) [dFn(xj) - dF (xj)] ,

j=1

Un(c)

=

(n - c)! n!

1i1<···<icn

c

gn,c (xi1, ..., xic )

dIR+d (xj - ij ) - dF (xj)

j=1

where IR+d is the indicator function of R+d = (y1, ..., yd)  Rd | yj  0, j = 1, ..., d .

Lemma A.1 If (k)  C1k-(2+ )/ for  >  > 0, then

EVn(c)2 + EUn(c)2

(A.1)

n

m-1

r

 C (m, , r) n-c Mn2/(2+)

k/(2+)(k) +

n-c Mn2,c/(2+)

k  /(2+) (k )

k=r+1

c =0

k=1

for some constant C (m, , r) > 0. In particular, if one has (k)  C2k for 0 <  < 1, then

EVn(c)2 + EUn(c)2  C (m, , r) C2C()n-c

m-1

Mn2/(2+) +

n-c Mn2,c/(2+)

c =0

.

(A.2)

Proof. The proof essentially is the same as Lemma 2 in Yoshihara (1976), which dealt
with the special case of gn  g, r = 1, Mn = Mn and yielded (A.1). The inequalities in the proof of this lemma do not require all gn's to be the same for n = 1, 2, ..., and terms in Un(c) where exactly c pairs of neighboring indices differ by at most r form a subset of terms with cardinality of order nc-c . Elementary arguments then establish (A.2) under geometric
mixing conditions.

A.2 Proofs of Theorems 1, 2 and 5

Define the following square matrix of dimension (p + d)

Ss(x) =

p(u)pT (u)K(u)du E(Ts2|X = x) p(u)K(u)du E(TsTT-s|X = x) E(TsT-s|X = x) pT (u)K(u)du E(T-sT-T s|X = x)

.

The identifiability condition given at (3) is closely related to the invertibility of the matrix Ss(X). To see this, we note that for vectors 1 and 2 of dimensions p + 1 and d - 1, respectively,

(1T , T2 )Ss(x)(1T , T2 )T = E T1 p(u)Ts + T2 T-s 2 X = x K(u) du.

21

Thus, if 1(Xs)T , 2(X-s)T Ss(X) 1(Xs)T , 2(X-s)T T = 0, a.s., then 1(Xs)T p(u)Ts + 2(X-s)T T-s = 0, a.s. (X, T) and u  supp(K). Since K has a nonempty interior, the identifiability condition (3) implies 1  0 and 2  0 by the uniqueness of polynomial expansion.
The next lemma shows that the matrix Ss(x) is proportional to the limiting dispersion matrix

Lemma A.2 As n   sup ZsT Ws(x-s)Zs - (xs, x-s)S(xs, x-s) = o(b) a.s.
xssupp(ws),x-ssupp(w-s)
where b = ln n h + gmq ax + 1/ nhgprod . Proof. The conclusion follows by directly using the covering technique and exponential
inequalities for -mixing processes, as in the proof of Theorem 2.2 of Bosq (1998).

Now let c be an integer such that bc+1 = o (hp+2), the next lemma decomposes the dispersion matrix.

Lemma A.3 For any integer k,

ZsT Ws(x-s)Zs

-1 -

Ss-1(xs, x-s) (xs, x-s)

=

Ss-1(xs, x-s) c (xs, x-s) =1

Ip+d

-

ZsT Ws(x-s)ZsSs-1(xs, (xs, x-s)

x-s)

as n  , where the matrix Rs (xs, x-s) satisfies

+ Rs (xs, x-s)

sup |Rs (xs, x-s)| = o hp+2 a.s.
xssupp(ws),x-ssupp(w-s)
Proof. By a Taylor expansion for the matrix inversion operation, Lemma A.2 immedi-

ately yields the result.

Lemma A.4 Define

Ds1 (xs)

=

1 n

n
w-s(Xi,-s)Rs (xs, Xi,-s) ZsT WisE,

i=1

Ds2 (xs)

=

1 n

n

w-s(Xi,-s)Rs (xs, Xi,-s) ZsT Wis

i=1

{fs(Xjs)}nj=1

-

p =0

fs()(xs)h !

Zse

Ds3 (xs)

=

1 n

n
w-s(Xi,-s)Rs (xs, Xi,-s) ZsT Wis

i=1

n



×  fs (Xjs ) - fs (Xis )Zsep+s  .

s =s

j=1 s =s

,

22

Then, as n  

sup {|Ds1 (xs)| + |Ds2 (xs)| + |Ds3 (xs)|} = o(hp+2) a.s.
xssupp(ws)
Proof. The lemma follows directly from Lemmas A.3.

Lemma A.5 Write Wis = Ws(Xi,-s) and E = {(X1, T1)1, ..., (Xn, Tn)n}T . For = 1, 2, ..., define

R 1(xs)

=

1 n

n i=1

w-s(Xi,-s) (xs, Xi,-s)

Ss-1

(xs,

Xi,-s)

Ip+d

-

ZTs WisZsSs-1(xs, Xi,-s) (xs, Xi,-s)

×ZTs WisE

R 2(xs)

=

1 n

n i=1

w-s(Xi,-s) (xs, Xi,-s)

Ss-1

(xs,

Xi,-s)

Ip+d

-

ZsT WisZsSs-1(xs, Xi,-s) (xs, Xi,-s)

×ZTs Wisis

{fs(Xjs)}nj=1 -

p

fs()(xs !

)h

Zs

e

=0

(A.3) (A.4)

R 3(xs)

=

1 n

n i=1

w-s(Xi,-s) (xs, Xi,-s)

Ss-1

(xs,

Xi,-s)



Ip+d

-

ZsT WisZsSs-1(xs, Xi,-s) (xs, Xi,-s)



n

×ZTs Wis 

fs (Xjs )

- fs (Xis )Zsep+s  .

s =s

j=1 s =s

Then, as n  ,

 |R 1(xs)| + |R 2(xs)| + |R 3(xs)| = op b / nh .

(A.5) (A.6)

Proof. For simplicity of notations, consider the case of R 1(xs) and only = 1. The term R 1(xs) equals P1 - P2 in which

P1

=

1 n

n

w-s(Xi,-s)Ss-1(xs, Xi,-s)

i=1

S(xs, Xi,-s) (xs, Xi,-s)

-

E

ZsT WisZs|xs, Xi,-s 2(xs, Xi,-s)

×Ss-1(xs, Xi,-s)ZTs WisE,

P2 =

1 n

n

w-s(Xi,-s)Ss-1(xs, Xi,-s)

i=1

ZTs WisZs (xs, Xi,-s)

-

E

ZsT WisZs|xs, Xi,-s 2(xs, Xi,-s)

×Ss-1(xs, Xi,-s)ZTs WisE.

23

Denote i = (Xi, Ti, Yi), The term P1 can be written as the von Mises' differentiable statistic

Vn = (2n2)-1

n i,j=1

gn

(i,

j )

where

gn

(i,

j )

equals

w-s(Xi,-s)Ss-1(xs, Xi,-s) 

S(xs, (xs,

Xi,-s) Xi,-s)

-

E

ZTs WisZs|xs, Xi,-s 2(xs, Xi,-s)



×Ss-1(xs, Xi,-s) 

TjsKh (Xjs - xs) Lg(Xj,-s - Xi,-s)(Xj, Tj)j p {(Xjs - xs)/h} TjsKh (Xjs - xs) Lg(Xj,-s - Xi,-s)(Xj, Tj)j



Tj,-sKh (Xjs - xs) Lg(Xj,-s - Xi,-s)(Xj, Tj)j

+ w-s(Xj,-s)Ss-1(xs, Xj,-s) 

S(xs, (xs,

Xj,-s) Xj,-s)

-

E

ZsT WjsZs|xs, Xj,-s 2(xs, Xj,-s)



×Ss-1(xs, Xj,-s) 

TisKh (Xis - xs) Lg(Xi,-s - Xj,-s)(Xi, Ti)i p {(Xis - xs)/h} TjsKh (Xis - xs) Lg(Xi,-s - Xj,-s)(Xi, Ti)i

 .

Ti,-sKh (Xis - xs) Lg(Xi,-s - Xj,-s)(Xi, Ti)i

First, one calculates that gn,0 = 0 and gn,1 (j) equals

Ss-1(xs, z-s)w-s(z-s)Ss-1(xs, z-s) 

S(xs, (xs,

z-s) z-s)

-

E

ZsT WisZs|xs, z-s 2(xs, z-s)



×Ss-1(xs, z-s) 

TjsKh (Xjs - xs) Lg(Xj,-s - z-s)(Xj, Tj)j p {(Xjs - xs)/h} TjsKh (Xjs - xs) Lg(Xj,-s - z-s)(Xj, Tj)j



Tj,-sKh (Xjs - xs) Lg(Xj,-s - z-s)(Xj, Tj)j

×-s(z-s)dz-s

which has mean zero and variance of order b2/nh. So Vn(1) = n-1

n j=1

gn,1

(j )

=

op

 b/ nh .

Next, take a small constant  > 0. Then, the (2 + )-th moment of gn (i, j) , i < j, is not

greater than



C

b2+

C

()



1 gp1+ro2d

E

TjsKh (Xjs - xs) (Xj, Tj)j p {(Xjs - xs)/h} TjsKh (Xjs - xs) (Xj, Tj)j Tj,-sKh (Xjs - xs) (Xj, Tj)j

2+2 (2+)/(2+2) 

 Cb2+C()

1 h1+2 gp1+ro2d

(2+)/(2+2)

by Lemma 1 of Yoshihara (1976).
Hence, one can take Mn = Mn,0 = Cb2+ h1+2gp1+ro2d -(2+)/(2+2) in the context of Lemma A.1 with m = c = 2 and r = 1. Similarly, one can show that Mn,1 = Cb2+h-(1+)gp-r(o2d+).

24

Now applying Lemma A.1 with m = c = 2 and r = 1, (A.2) gives
EP12  Cn-2b2 (hgprod)-2(1+2)/(2+2) + Cn-3b2h-(1+)2/(2+)gp-r(o2d+)2/(2+) + Cb2/nh  Cn-1h-1b2

by

making



sufficiently 

small.

Similar

arguments

establish

that

E P22



C n-1 h-1 b2 .

Hence

P1 - P2 = op(b/ nh). We have thus concluded the proof of the lemma.

Now write qs(u; t) for the (p + d)-dimensional vector given by qs(u; t)T = p(u)T ts, t-T s = (ts, uts, . . . , upts, t-T s),
and define an equivalent kernel

Ks(u; t, x) = eT0 Ss-1(x)qs(u; t)K(u). Write Ks,h(u; t, x) = (1/h)Ks(u/h; t, x) , i.e.
Ks,h(u; t, x) = (1/h)eT0 Ss-1(x)qs(u/h; t)K(u/h).

(A.7) (A.8)

This kernel satisfies the moment conditions as are given in the following lemma, which follows directly from the definition of Ss(x) and Ss-1(x).

Lemma A.6 Let jk equal 1 if j = k and 0 otherwise. Then
E uqTsKs(u; T, X)du|X = x = 0q, 0  q  p ; E Ts Ks(u; T, X)du|X = x = 0, s = 1, ..., d, s = s. In order to prove Theorem 1, we begin by observing

(A.9)

eT0 ZTs WisZs -1 ZsT WisZsel = 0l, l = 0, ..., p + d - 1.

Define Q1n =

n i=1

w-s(Xi,-s)/n

and

n

Q2n(xs) = n-1

w-s(Xi,-s)e0T ZsT WisZs -1 ZsT Wis

i=1

d

- fs (Xis )Zsep+s .

s =s

Y-

p

fs(

)(xs)h !

Zs

e

=0

Then, we obtain Q1n f^s(xs) - fs(xs) = Q2n(xs). By Lemmas A.5, A.4 and A.3 and by the definition of Ks,h(u, t; x) in (A.8), we now write

3
Q2n(xs) =
a=1

c
Pan(xs) + Rla(xs) + Dsa (xs)
l=1

(A.10)

25

where for a = 1, 2, 3

Pan(xs)

=

n-2

n i,j=1

w-s(Xi,-s) (xs, Xi,-s)

Ks,h

(Xjs

-

xs;

Tj ,

xs,

Xi,-s)

Lg(Xj,-s

-

Xi,-s)Hjs

(A.11)

with Hjs being (Xj, Tj)j for a = 1, {fs(Xjs) -

and

d s

=1,s

=s{fs

(Xjs

)

-

fs

(Xis

)}Tjs

.

p =0

fs()(xs)(Xjs

-

xs) / !}Tjs

for

a

=

2

In the following three lemmas, we derive the asymptotics for P1n, P2n and P3n.

Lemma A.7 As n  ,
n
P1n(xs) = n-1 pjs(xs)j + op{(nh log n)-1/2}
j=1
where pjs(xs) = w-s(Xj,-s)Ks,h (Xjs - xs; Tj, xs, Xj,-s) -s(Xj,-s)(Xj, Tj)/(xs, Xj,-s).

Proof. By the definition (A.11) and using Lemma A.1 for geometrically -mixing processes,

n
P1n(xs) = n-1
j=1

w-s(x-s) (xs, x-s)

Ks,h

(Xjs

-

xs;

Tj ,

xs,

x-s)

×Lg(Xj,-s - x-s)-s(x-s)dx-s(Xj, Tj)j + op{(nh log n)-1/2}.

By the change of variable x-s = Xj,-s - gv-s and the fact that L is of order q, it equals

n-1

n j=1

w-s(Xj,-s) (xs, Xj,-s)

Ks,h

(Xjs

- xs; Tj, xs, Xj,-s) -s(Xj,-s)(Xj, Tj)j

+op{(nh log n)-1/2}.

This completes the proof of the lemma.

Lemma A.8 As n  , P2n(xs) = s(xs)hp+1 + op(hp+1) where

s(xs) = (p + 1)!-1fs(p+1)(xs) up+1E {w-s(X-s)TsKs(u; T, xs, X-s)} du.

Proof. By definition (A.11) and again using Lemma A.1, we derive

P2n(xs) =

w-s(x-s) (xs, x-s)

Ks,h

(zs

-

xs;

t,

xs,

x-s)

Lg(z-s

-

x-s)

p
× fs(zs) - fs()(xs)(zs - xs)/! ts(z, t)-s(x-s)dzdtdx-s {1 + op(1)} .
=0

26

By the changes of variables zs = xs + hu and z-s = x-s + gv-s, we obtain

P2n(xs) = hp+1(p + 1)!-1

w-s(x-s) (xs, x-s)

Ks

(u;

t,

xs,

x-s)

fs(p+1)

(xs)up+1ts

×-s(x-s)(xs, x-s, t)dudx-sdt {1 + op(1)}

= hp+1(p + 1)!-1fs(p+1)(xs)E w-s(X-s) Ks (u; t, xs, X-s) up+1ts

(t|xs, X-s)dudt + op(hp+1).

This completes the proof of the lemma.

Lemma A.9 As n  , P3n(xs) = Op (gmq ax).

Proof. By definition (A.11) and applying Lemma A.1, one has

P3n(xs) =

w-s(x-s) (xs, x-s)

Ks,h

(zs

-

xs;

t,

xs,

x-s)

Lg(z-s

-

x-s)

× {fs (zs ) - fs (xs )} ts (z, t)-s(x-s)dzdtdx-s {1 + op(1)} .
s =s
After the changes of variables z-s = x-s + gv-s and zs = xs + hu, we obtain

P3n(xs) =

w-s(x-s) (xs, x-s)

Ks

(u;

t,

xs,

x-s)

L(v-s)

{fs (xs + gs vs ) - fs (xs )} ts
s =s

×(xs + hu, x-s + gv-s, t)-s(x-s)dudv-sdtdx-s {1 + op(1)}

= Op(gmq ax)

since L is of order q by the assumption A1. Thus, we have proved the lemma.

Proof of Theorem 1. By Lemma A.7 and the martingale central limit theorem of Liptser 
and Shirjaev (1980), nhP1n(xs) for each xs  supp(ws) is asymptotically normal with mean 0 and variance

h

w-2 s(z-s) 2(xs, z-s)

Ks,2h

(zs

-

xs;

t,

xs,

z-s)

2-s(z-s)2(z,

t)(z,

t)dzdt

{1

+

o(1)}

.

By the change of variable zs = xs + hu, the leading term of this equals

s2(xs) =

w-2 s(z-s) 2(xs, z-s)

Ks2

(u;

t,

xs

,

z-s)

2-s

(z-s

)2(xs,

z-s

,

t)

(xs,

z-s

,

t)dudz-sdt.

The theorem now follows immediately from Lemmas A.7, A.8, the conditions on the bandwidths as given in A7, and the fact that Q1n = w-s(z-s)-s(z-s)dz-s + Op(n-1/2).

27

Proof of Theorem 2. One first notes that (8) follows directly from (7), so we will only

show the latter. Now, from Lemmas A.7, A.8, A.9 and the conditions on the bandwidths,

we obtain

n
f^s(xs) - fs(xs) = bs(xs)hp+1 + n-1s-1 pjs(xs)j + op(hp+1).
j=1

(A.12)

Applying (A.12), one only needs to show that the two stochastic terms n-1

n j=1

pjs(xs)j

and n-1

n j=1

pjs

(xs

)j

for s = s

have covariance of order o(n-1h-1).

Noting that the j's

are i.i.d. white noise and each i is independent of the vectors (Xj, Tj), j = 1, ..., i for each

i = 1, ..., n, we need only to show that

E {pjs(xs)pjs (xs )} = o(h-1).

(A.13)

By change of variables technique for Xs and Xs which are contained in pjs(xs) and pjs (xs ) respectively, one may show that the left hand side of (A.13) is actually O(1), which proves the theorem.

Proof of Theorem 5. For this proof, we use (A.10) again. Under the hypothesis (11), P2n(xs) = Rl2(xs) = Ds2 (xs) = 0 and thus

cc
Q1n f^s(xs) -  = P1n(xs) + Rl1(xs) + Ds1 (xs) + P3n(xs) + Rl3(xs) + Ds3 (xs) .
l=1 l=1

Hence to study

n k=1

f^s(Xks)2ws(Xks)/n,

we

derive

the

asymptotics

of

such

as

P12n(Xks)/n. Let i = (Xi, Ti, Yi) and define

n k=1

ws(Xks)

gn (i, j, k, l, m)

=

ws(Xks)

w-s(Xi,-s) (Xks, Xi,-s

)

Ks,h

(Xjs

-

Xks;

Tj ,

Xks,

Xi,-s)

×Lg(Xj,-s - Xi,-s)(Xj, Tj)j

×

w-s(Xl,-s) (Xks, Xl,-s

)

Ks,h

(Xms

-

Xks;

Tm,

Xks,

Xl,-s)

×Lg(Xm,-s - Xl,-s)(Xm, Tm)m.

Then, by the definition (A.11)

nn

ws(Xks)P12n(Xks)/n = n-5

gn (i, j, k, l, m) .

k=1 i,j,k,l,m=1

Next, we define gn (i, j, k, l, m) = gn (i , j , k , l , m ) /5!, where the sum is over

all possible permutations (i , j , k , l , m ) of (i, j, k, l, m). Then

n k=1

ws(Xks)P12n

(Xks)/n

is

28

expressed as a V statistic n-5

n i,j,k,l,m=1

gn

(i,

j ,

k,

l,

m).

It

is

easy

to

see

that

gn,0

=

0,

gn,1 = 0, and by changes of variables gn,2 (j, m) equals

(Xj, Tj)(Xm, Tm)jm

ws(Xjs - huks)w-s(Xj,-s - gui,-s)w-s(Xm,-s - gul,-s) (Xjs - huks, Xj,-s - gui,-s)(Xjs - huks, Xm,-s - gul,-s)

×Ks uks; Tj, Xjs - huks, Xj,-s - gui,-s L(ui,-s)L(ul,-s)

×Ks,h Xms - Xjs + huks; Tm, Xjs - huks, Xm,-s - gul,-s

×(xis,Xj,-s - gui,-s, ti)(xls, Xm,-s - gul,-s, tl)(Xjs - huks, xk,-s, tk)

×dxis dui,-s dxls dul,-s duks dxk,-s dti dtl dtk .

To establish the asymptotic normality of the off-diagonal sum 2 n-2 1j<mn gn,2 (j, m), we use Lemma 3.2 of Hjellvik, Yao and Tjøstheim (1998). Let n2 be their n2, i.e., n2 =
1j<mn var {2 n-2gn,2 (j, m)}. Define nk in the same way as their Mnk for k = 1, . . . , 6 with 2 n-2gn,2 (j, m) taking the role of their jm. If we prove that for some  > 0

n2n-2 n1/1(+1) + n1/5(+1) + 1n/62 - 0 n3/2n-2 n1/2{2(+1)} + 1n/32 + n1/4{2(+1)} - 0,

(A.14) (A.15)

then we establish that 2 n-2 1j<mn gn,2 (j, m) is asymptotically normal with mean 0 and variance n2.
We compute n2 first. Note that

n2

=

2 n2

(xj, tj)(xm, tm)

ws(xjs)w-s(xj,-s)w-s(xm,-s) (xjs, xj,-s)(xjs, xm,-s)

×Ks (uks; tj, xjs, xj,-s) Ks,h (xms - xjs + huks; tm, xjs, xm,-s)

×L(ui,-s)L(ul,-s)(xis,xj,-s, ti)(xls, xm,-s, tl)(xjs, xk,-s, tk)
2
×dxisdui,-sdxlsdul,-sduksdxk,-sdtidtldtk (xj, tj)(xm, tm)

×dxjdxmdtjdtm 1 + O hp+1 + gq .

By the change of variable xms = xjs + hvs and further approximations of the functions, we obtain n2 = {1 + O(hp+1 + gq)}n-2h-1s4s2, where

s2

=

2 s4

w-2 s(x-s)w-2 s(z-s)ws2(xs) 2(xs, x-s)2(xs, z-s)

Ks(c) (u; t1, t2, xs, x-s, z-s) 2

×-2 s(x-s)2-s(z-s)2(xs, x-s, t1)2(xs, z-s, t2)s2(xs)

(A.16)

×(xs, x-s, t1)(xs, z-s, t2)dudxsdx-sdz-sdt1dt2

and Ks(c) (w; t1, t2, xs, x-s, z-s) = Ks (u; t1, xs, x-s) Ks (w + u; t2, xs, z-s) du.

29

Next, we approximate nj. We only illustrate the calculation of n4. For j < k and l < m with all j, k, l, m different, we obtain

E |gn,2(j, k)gn,2(l, m)|2(1+)  const.h-4(1+)+2 E|1|2(1+) 4

(xj, tj)(xjs + hv, xk,-s, tk)(xl, tl)

×(xls + hv , xm,-s, tm) 2(1+) Ks(c) (v; tj, tk, xjs, xj,-s, xk,-s) ×Ks(c) (v ; tl, tm, xls, xl,-s, xm,-s) 2(1+)j,k,l,m(xj, tj; xjs + hv, xk,-s, tk; xl, tl;
xls + hv , xm,-s, tm) dxjdtjdvdxk,-sdtkdxldtldv dxm,-sdtm,

where the integrations with respect to xj, dv, dxk,-s, xl, dv , dxm,-s are over compact sets. By the assumption A6 the right hand side of the above inequality is bounded by

const.h-4(1+)+2 ( tj tk tl tm )2(1+)|~(tj)~(tk)~(tl)~(tm)|2(1+) ×~j,k,l,m(tj, tk, tl, tm) dtjdtkdtldtm
 const.h-4(1+)+2.

This shows

n3/2n-2n1/4{2(+1)}

n3/2 × n2h × n-4h-(1+2)/(1+) = n-(2p+2p+3+)/{2(1+)(2p+3)}.

Similarly, we can establish

n2n-2n1/1(+1) n3/2n-2n1/2{2(+1)}
n3/2n-21n/32 n2n-2n1/5{2(+1)}
n2n-21n/62

n2 × n2h × n-4h-2/(+1) = h(1-)/(1+), n3/2 × n2h × n-4h-(1+2)/(1+) = n-(2p+2p+3+)/{2(1+)(2p+3)}, n3/2 × n2h × n-4h-3/2 = (nh)-1/2, n2 × n2h × n-4h-(1+2)/{2(+1)} = h1/{2(1+)}, n2 × n2h × n-4h-1/2 = h1/2.

Thus, if we take  such that 0 <  < 1, the convergences (A.14) and (A.15) hold.

By the martingale central limit theorem again, the diagonal sum n-2

n j=1

gn,2

(j ,

j )

is

also asymptotically normal with mean s2vsn-1h-1{1 + O(hp+1)}, where vs is given by

vs =

w-2 s(x-s)ws(xs) s22(x)

Ks2

(u;

t,

x)

2-s(x-s)2(x,

t)(x,

t)s

(xs

)dudxdt.

(A.17)

The asymptotic variance of n-2

n j=1

gn,2

(j ,

j )

is

likewise

calculated,

and

may

be

shown

to

be of order n-3h-2.

30

Therefore, we establish

nh1/2

n
n-2

n

gn,2

(j ,

m)

-

s2 nh

vs

j=1 m=1

L N 0, s4s2 .

(A.18)

Application of Lemma A.1 reveals that n-c

n j1,...,jc=1

gn,c

(j1

,

...,

jc

)

=

o

n-1h-1/2

for

c = 3, 4, 5. Using Lemma A.1 again, now to terms such as

n k=1

ws(Xks)P32n(Xks)/n,

n k=1

ws(Xks)R21(Xks)/n

and

n k=1

ws(Xks)R23(Xks)/n,

one

may

show

that

they

are

all

of

order o n-1h-1/2 as well. Using Lemma A.4 one may also prove

n k=1

ws(Xks)Ds21(Xks)/n

and

n k=1

ws(Xks)Ds23(Xks)/n

are

both

of

order

o

(h2p+4)

=

o

n-1h-1/2

. Similar arguments

establish that {

n i=1

f^s(Xis)ws(Xis)}2

=

o

n-1h-1/2

. Hence,

n
Vns = Q1-n2 P12n(Xks)ws(Xks)/n + o n-1h-1/2 .
k=1

This completes the proof of Theorem 5.

A.3 Proofs of Theorems 3, 4 and 6

Define qsu(v; t)T = p(v)T tsu, tT-(su) and Ssu(x) in the same way as Ss(x) with Ts and T-(s) being replaced by Tsu and T-(su), respectively. Define an equivalent kernel K(su)(v; t, x) = e0T Ss-u1(x)qsu(v; t)K(v). Let K(s(uc)) denote the two-folded convolution of K(su). Theorems 3 and 6 can be proved in the same way as in the proofs of Theorems 1 and 5 with the following
definitions of su, s2u, su and vsu:

su(xs)

=

fs(up+1)(xs) (p + 1)!

up+1E w-s(X-s)TsuK(su)(v; T, xs, X-s) dv,

s2u(xs) =

w-2 s(z-s) 2(xs, z-s)

K(s2u)

(v

;

t,

xs

,

z-s

)

2-s(z-s)2(xs,

z-s,

t)

×(xs, z-s, t)dvdz-sdt,

s2u

=

2 s4

w-2 s(x-s)w-2 s(z-s)ws2(xs) 2(xs, x-s)2(xs, z-s)

K(s(uc)) (v; t1, t2, xs, x-s, z-s) 2

×2-s(x-s)2-s(z-s)2(xs, x-s, t1)2(xs, z-s, t2)s2(xs)

vsu =

×(xs, x-s, t1)(xs, z-s, t2)dvdxsdx-sdz-sdt1dt2,

w-2 s(x-s)ws(xs) s22(x)

K(s2u)

(v;

t,

x)

-2 s(x-s)2(x,

t)(x,

t)

×s(xs)dvdxdt.

(A.19) (A.20) (A.21)
(A.22)

The proof of Theorem 4(i) is the same as that of the first part of Theorem 2. For the

31

proof of Theorem 4(ii), define

pjsu(xs)

=

w-s(Xj,-s) (xs, Xj,-s)

K(su),h

(Xjs

- xs; Tj, xs, Xj,-s) -s(Xj,-s)(Xj, Tj)

where K(su),h (v; t, x) = (1/h)K(su) (v/h; t, x). We observe

n
f^su(xs) - fsu(xs) = bsu(xs)hp+1 + n-1s-1 pjsu(xs)j + op(hp+1).
j=1

Thus for the case s = s , we obtain cov f^su(xs), f^su (xs) = s-2suu (xs)n-1h-1{1 + o(1)} where

suu (xs) =

w-2 s(z-s) 2(xs, z-s)

K(su)

(v;

t,

xs

,

z-s)

K(su

)

(v;

t,

xs,

z-s)

-2 s(z-s)

×2(xs, z-s, t)(xs, z-s, t)dvdz-sdt,

(A.23)

We note that s2u(xs) = suu(xs). This completes the proof of Theorem 4.

REFERENCES
Bosq, D. (1998), Nonparametric Statistics for Stochastic Processes, New York: SpringerVerlag.
Cai, Z., Fan, J. and Li, R. Z. (2000), "Efficient Estimation and Inferences for VaryingCoefficient Models", Journal of the American Statistical Association, 95, 888-902.
Cai, Z., Fan, J. and Yao, Q. (2000), "Functional-Coefficient Regression Models for Nonlinear Times Series", Journal of the American Statistical Association, 95, 941-956.
Chen, R. and Liu, L. M. (1993a), "Functional Coefficient Autoregressive Models: Estimation and Tests of Hypotheses", Journal of Time Series Analysis, 22, 151-173.
Chen, R. and Tsay, R. S. (1993a), "Nonlinear Additive ARX Models", Journal of the American Statistical Association, 88, 955-967.
Chen, R. and Tsay, R. S. (1993b), "Functional-Coefficient Autoregressive Models", Journal of the American Statistical Association, 88, 298-308.
Fan, J. and Gijbels, I. (1996), Local Polynomial Modelling and Its Applications, London: Chapman and Hall.
32

Fan, J. Ha¨rdle, W. and Mammen, E. (1998), "Direct Estimation of Low-Dimensional Components in Additive Models", The Annals of Statistics, 26, 943-971.
Franses, H. F. (1996), Periodicity and Stochastic Trends in Economic Time Series, Oxford: Oxford University Press.
Ha¨rdle, W., Hlavka, Z. and Klinke, S. (2000), XploRe Application Guide, Heidelberg: Springer Verlag.
Ha¨rdle, W., Liang, H. and Gao, J. T. (2000), Partially Linear Models, Heidelberg: SpringerVerlag.
Ha¨rdle, W. and Mammen, E. (1993), "Comparing Nonparametric versus Parametric Regression Fits", The Annals of Statistics, 21, 1926-1947.
Hastie, T. J. and Tibshirani, R. J. (1990), Generalized Additive Models, London: Chapman and Hall.
Hastie, T. J. and Tibshirani, R. J. (1993), "Varying-Coefficient Models", Journal of the Royal Statistical Society Series B, 55, 757-796.
Hjellvik, V., Yao, Q. and Tjøstheim, D. (1998), "Linearity testing using local polynomial approximation", Journal of Statistical Planning and Inference, 68, 295-321
Linton, O. B. (1997), "Efficient Estimation of Additive Nonparametric Regression Models", Biometrika, 84, 469-473.
Linton, O. B. and Ha¨rdle, W. (1996), "Estimation of Additive Regression Models with Known Links", Biometrika, 83, 529-540.
Linton, O. B. and Nielsen, J. P. (1995), "A Kernel Method of Estimating Structured Nonparametric Regression Based on Marginal Integration", Biometrika, 82, 93-100.
Liptser, R. Sh. and Shirjaev, A. N. (1980), "A Functional Central Limit Theorem for Martingales", Theory of Probability and Applications, 25, 667-688.
Liu, R. (1988), "Bootstrap Procedures Under Some Non I.I.D. Models", The Annals of Statistics, 16, 1696-1708.
Mammen, E. (1992), When Does Bootstrap Work: Asymptotic Results and Simulations, Lecture Notes in Statistics 77, Berlin: Springer-Verlag.
33

Mammen, E. Linton, O. and Nielsen, J. (1999), "The Existence and Asymptotic Properties of a Backfitting Projection Algorithm under Weak Conditions", The Annals of Statistics, 5, 1443-1490.
Masry, E. and Tjøstheim, D. (1995), "Non-parametric Estimation and Identification of ARCH Nonlinear Time Series: Strong Convergence and Asymptotic Normality", Econometric Theory, 11, 258-289.
Masry, E. and Tjøstheim, D. (1997), "Additive Nonlinear ARX Time Series and Projection Estimates", Econometric Theory, 13, 214-252.
Ruppert, D., Sheather, S. J. and Wand, M. P. (1995), "An Effective Bandwidth Selection for Local Least Squares Regression", Journal of the American Statistical Association, 90, 1257-1270.
Sperlich, S., Tjøstheim, D. and Yang, L. (2002), "Nonparametric Estimation and Testing of Interaction in Additive Models", Econometric Theory, 18, 197-251.
Stone, C. J. (1977), "Consistent Nonparametric Regression", The Annals of Statistics, 5, 595 - 645.
Tjøstheim, D. and Auestad, B. (1994), "Nonparametric Identification of Nonlinear Time Series: Projections", Journal of the American Statistical Association, 89, 1398-1409.
Wand, M. P. and Jones, M. C. (1995), Kernel Smoothing, London, Chapman and Hall.
Wolters, J. (1992), "Persistence and Seasonality in Output and Employment of the Federal Republic of Germany", Rechereches Economiques de Louvain, 58, 421­439.
Wu, C. F. J. (1986), "Jackknife, Bootstrap and Other Resampling Methods in Regression Analysis (with discussion)", Annals of Statistics, 14, 1261-1350.
Yang, L. and Tschernig, R. (2002), "Non- and Semiparametric Identification of Seasonal Nonlinear Autoregression Models", Econometric Theory, 18, 1408-1448.
Yoshihara (1976), "Limiting Behavior of U-statistics for Stationary, Absolutely Regular Processes", Zeitschrift fu¨r Wahrscheinlichkeitstheorie und verwandte Gebiete, 35, 237252.
34

Table 1: MISEs of the estimators f^1, f^2 and f^3 for the i.i.d. case.

n=50 n=100 n=250

f1 0.0559 0.0300 0.0108

f2 0.1144 0.0515 0.0223

f3 0.1336 0.0617 0.0225

Table 2: Proportions among the 100 replications of rejecting the null hypotheses Hs0, s = 1, 2, 3, at the significant level 0.05 for the i.i.d. case.
H10 H20 H30 n=50 0.94 0.85 0.02 n=100 1 1 0.04 n=250 1 1 0.08

35

Table 3: Average squared residuals (ASR) and average squared prediction errors (ASPE) obtained from fitting twelve VCAR models with the German real GNP data. Each model is identified by the four digits which indicate the order in which the lagged variables Yt-1, Yt-2, Yt-2 and Yt-4 enter the VCAR model. For example, the model `1234' means Yt = f1(Yt-1)Yt-2 + f2(Yt-3)Yt-4 + (noise). The partially linear VCAR model at the bottom is Yt = f1Yt-2 + f2(Yt-3)Yt-4 + (noise).

Model 1234 1243 1324 1342 1423 1432 2134 2143 2341 2431 3142 3241 Linear AR Partially Linear VCAR

ASR 0.00021 0.00040 0.00025 0.00039 0.00026 0.00024 0.00023 0.00051 0.00049 0.00024 0.00041 0.00038 0.00059 0.00032

ASPE 0.00011 0.00019 0.00013 0.00016 0.00014 0.00009 0.00017 0.00037 0.00032 0.00015 0.00023 0.00017 0.00041 0.00024

36

Log GNP 5.2 5.4 5.6 5.8 6 6.2

Log GNP, First Difference*E-2 -10 -5 0 5 10

Log(German Real GNP)
0 50 100 Time
(a) First Difference of Log(German Real GNP)
0 50 100 Time
(b) Figure 1: Plots of the West German real GNP quarterly data from 1960:1 to 1990:4. Panel (a) shows log(GNP) over time, and (b) depicts the first difference of log(GNP).
37

0.2 0.4 0.6 0.8

-0.4 -0.2 0.0 0.2

-0.04

-0.02

0.0

0.02

0.04

a

-0.04

-0.02

0.0

0.02

0.04

b

0.2 0.4 0.6 0.8

-0.4 -0.2 0.0 0.2

-0.04

-0.02

0.0

0.02

0.04

c

-0.04

-0.02

0.0

0.02

0.04

d

Figure 2: Estimated functions under the models (20) and (21). Panels (a) and (b) depict f^1 and f^2, respectively, for the model (20), while (c) and (d) are for the model (21).

38

ACF -0.2 -0.0 0.2 0.4 0.6 0.8 1.0

ACF -0.2 -0.0 0.2 0.4 0.6 0.8 1.0

0 5 10 15 20
Lag a

0 5 10 15 20
Lag b

ACF -0.2 -0.0 0.2 0.4 0.6 0.8 1.0

ACF -0.2 -0.0 0.2 0.4 0.6 0.8 1.0

0 5 10 15 20
Lag c

0 5 10 15 20
Lag d

Figure 3: Autocorrelations of standardized residuals ^t. Panels (a) and (b) are for the model (20) and depict the autocorrelations of |^t| and ^2t , respectively, while (c) and (d) are for the model (21). The dotted horizontal lines at levels ±2 × n-1/2 represent the 95.44% confidence
bands of the autocorrelation functions.

39

-0.04 -0.02 0.0 0.02 0.04

-0.04 -0.02 0.0 0.02 0.04

20 40 60 80 100 120
(a)
20 40 60 80 100 120
(b) Figure 4: Prediction for the West German real GNP quarterly data based on the marginal integration fits of the varying coefficient models (20) and (21). Panel (a) is for the model (20), and (b) is for (21). Solid lines represent the predicted values Y^t, while the dotted are for the observed values Yt.
40

SFB 649 Discussion Paper Series
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Nonparametric Risk Management with Generalized Hyperbolic Distributions" by Ying Chen, Wolfgang Härdle and Seok-Oh Jeong, January 2005.
002 "Selecting Comparables for the Valuation of the European Firms" by Ingolf Dittmann and Christian Weiner, February 2005.
003 "Competitive Risk Sharing Contracts with One-sided Commitment" by Dirk Krueger and Harald Uhlig, February 2005.
004 "Value-at-Risk Calculations with Time Varying Copulae" by Enzo Giacomini and Wolfgang Härdle, February 2005.
005 "An Optimal Stopping Problem in a Diffusion-type Model with Delay" by Pavel V. Gapeev and Markus Reiß, February 2005.
006 "Conditional and Dynamic Convex Risk Measures" by Kai Detlefsen and Giacomo Scandolo, February 2005.
007 "Implied Trinomial Trees" by Pavel Cízek and Karel Komorád, February 2005.
008 "Stable Distributions" by Szymon Borak, Wolfgang Härdle and Rafal Weron, February 2005.
009 "Predicting Bankruptcy with Support Vector Machines" by Wolfgang Härdle, Rouslan A. Moro and Dorothea Schäfer, February 2005.
010 "Working with the XQC" by Wolfgang Härdle and Heiko Lehmann, February 2005.
011 "FFT Based Option Pricing" by Szymon Borak, Kai Detlefsen and Wolfgang Härdle, February 2005.
012 "Common Functional Implied Volatility Analysis" by Michal Benko and Wolfgang Härdle, February 2005.
013 "Nonparametric Productivity Analysis" by Wolfgang Härdle and Seok-Oh Jeong, March 2005.
014 "Are Eastern European Countries Catching Up? Time Series Evidence for Czech Republic, Hungary, and Poland" by Ralf Brüggemann and Carsten Trenkler, March 2005.
015 "Robust Estimation of Dimension Reduction Space" by Pavel Cízek and Wolfgang Härdle, March 2005.
016 "Common Functional Component Modelling" by Alois Kneip and Michal Benko, March 2005.
017 "A Two State Model for Noise-induced Resonance in Bistable Systems with Delay" by Markus Fischer and Peter Imkeller, March 2005.
018 "Yxilon ­ a Modular Open-source Statistical Programming Language" by Sigbert Klinke, Uwe Ziegenhagen and Yuval Guri, March 2005.
019 "Arbitrage-free Smoothing of the Implied Volatility Surface" by Matthias R. Fengler, March 2005.
020 "A Dynamic Semiparametric Factor Model for Implied Volatility String Dynamics" by Matthias R. Fengler, Wolfgang Härdle and Enno Mammen, March 2005.
021 "Dynamics of State Price Densities" by Wolfgang Härdle and Zdenk Hlávka, March 2005.
022 "DSFM fitting of Implied Volatility Surfaces" by Szymon Borak, Matthias R. Fengler and Wolfgang Härdle, March 2005.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "Towards a Monthly Business Cycle Chronology for the Euro Area" by Emanuel Mönch and Harald Uhlig, April 2005.
024 "Modeling the FIBOR/EURIBOR Swap Term Structure: An Empirical Approach" by Oliver Blaskowitz, Helmut Herwartz and Gonzalo de Cadenas Santiago, April 2005.
025 "Duality Theory for Optimal Investments under Model Uncertainty" by Alexander Schied and Ching-Tang Wu, April 2005.
026 "Projection Pursuit For Exploratory Supervised Classification" by EunKyung Lee, Dianne Cook, Sigbert Klinke and Thomas Lumley, May 2005.
027 "Money Demand and Macroeconomic Stability Revisited" by Andreas Schabert and Christian Stoltenberg, May 2005.
028 "A Market Basket Analysis Conducted with a Multivariate Logit Model" by Yasemin Boztu and Lutz Hildebrandt, May 2005.
029 "Utility Duality under Additional Information: Conditional Measures versus Filtration Enlargements" by Stefan Ankirchner, May 2005.
030 "The Shannon Information of Filtrations and the Additional Logarithmic Utility of Insiders" by Stefan Ankirchner, Steffen Dereich and Peter Imkeller, May 2005.
031 "Does Temporary Agency Work Provide a Stepping Stone to Regular Employment?" by Michael Kvasnicka, May 2005.
032 "Working Time as an Investment? ­ The Effects of Unpaid Overtime on Wages, Promotions and Layoffs" by Silke Anger, June 2005.
033 "Notes on an Endogenous Growth Model with two Capital Stocks II: The Stochastic Case" by Dirk Bethmann, June 2005.
034 "Skill Mismatch in Equilibrium Unemployment" by Ronald Bachmann, June 2005.
035 "Uncovered Interest Rate Parity and the Expectations Hypothesis of the Term Structure: Empirical Results for the U.S. and Europe" by Ralf Brüggemann and Helmut Lütkepohl, April 2005.
036 "Getting Used to Risks: Reference Dependence and Risk Inclusion" by Astrid Matthey, May 2005.
037 "New Evidence on the Puzzles. Results from Agnostic Identification on Monetary Policy and Exchange Rates." by Almuth Scholl and Harald Uhlig, July 2005.
038 "Discretisation of Stochastic Control Problems for Continuous Time Dynamics with Delay" by Markus Fischer and Markus Reiss, August 2005.
039 "What are the Effects of Fiscal Policy Shocks?" by Andrew Mountford and Harald Uhlig, July 2005.
040 "Optimal Sticky Prices under Rational Inattention" by Bartosz Makowiak and Mirko Wiederholt, July 2005.
041 "Fixed-Prize Tournaments versus First-Price Auctions in Innovation Contests" by Anja Schöttner, August 2005.
042 "Bank finance versus bond finance: what explains the differences between US and Europe?" by Fiorella De Fiore and Harald Uhlig, August 2005.
043 "On Local Times of Ranked Continuous Semimartingales; Application to Portfolio Generating Functions" by Raouf Ghomrasni, June 2005.
044 "A Software Framework for Data Based Analysis" by Markus Krätzig, August 2005.
045 "Labour Market Dynamics in Germany: Hirings, Separations, and Job-toJob Transitions over the Business Cycle" by Ronald Bachmann, September 2005.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

046 "Paternal Uncertainty and the Economics of Mating, Marriage, and Parental Investment in Children" by Dirk Bethmann and Michael Kvasnicka, September 2005.
047 "Estimation and Testing for Varying Coeffcients in Additive Models with Marginal Integration " by Lijian Yang, Byeong U. Park, Lan Xue and Wolfgang Härdle, September 2005.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

