BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2005-015
Robust Estimation of Dimension Reduction
Space
Pavel Cízek* Wolfgang Härdle**
* Department of Econometrics and Operations Research, Universiteit van Tilburg, The Netherlands
** Department of Economics, Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Robust estimation of dimension reduction space
P. C´izek a and W. H¨ardle b,1
aDepartment of Econometrics & OR, Tilburg University, P.O.Box 90153, 5000 LE, Tilburg, The Netherlands
bInstitut fu¨r Statistik und O¨konometrie, Humboldt-Universit¨at zu Berlin, Spandauer Str. 1, D-10178 Berlin, Germany
Abstract
Most dimension reduction methods based on nonparametric smoothing are highly sensitive to outliers and to data coming from heavy-tailed distributions. We show that the recently proposed methods by Xia et al. (2002) can be made robust in such a way that preserves all advantages of the original approach. Their extension based on the local one-step M-estimators is sufficiently robust to outliers and data from heavy tailed distributions, it is relatively easy to implement, and surprisingly, it performs as well as the original methods when applied to normally distributed data.
Key words: Dimension reduction, Nonparametric regression, M-estimation
1 Introduction
In regression, we aim to estimate the regression function, which describes the relationship between a dependent variable y  R and explanatory variables X  Rp. This relationship can be, without prior knowledge and with full generality, modelled nonparametrically, but an increasing number of explanatory variables makes nonparametric estimation suffer from the curse of dimensionality. There are two main approaches to deal with a large number of explanatory
Email addresses: P.Cizek@uvt.nl (P. C´izek), haerdle@wiwi.hu-berlin.de (W. H¨ardle). 1 This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

variables: to assume a simpler form of the regression function (e.g., its additivity) or to reduce the dimension of the space of explanatory variables. The latter, more general approach received a lot of attention recently; see Li (1991) and Xia et al. (2002), for instance. Our first aim is to study the latter approach and examine its sensitivity to heavy-tailed and contaminated data, which can adversely influence both parametric and nonparametric estimation methods (see C´izek, 2004, and Sakata and White, 1998, for evidence in financial data). Further, we propose robust and computationally feasible modifications of Xia et al. (2002)'s methods and study their behavior by means of Monte Carlo experiments.

A dimension-reduction (DR) regression model can be written as

y = g(B0 X) + ,

(1)

where g is an unknown smooth link function, B0 represents a p × D orthogonal matrix, D  p, and E(|X) = 0 almost surely. Hence, to explain the dependent variable y, the space of p explanatory variables X can be reduced to a D-dimensional space given by B0 (for D = p, the standard nonparametric regression model results). The vectors of B0 are called directions in this context. The dimension reduction methods aim to find the dimension D of the DR space and the matrix B0 defining this space.
Recently, Xia et al. (2002) proposed the minimum average variance estimator (MAVE), which improves in several aspects over other existing estimators, such as sliced inverse regression (SIR) by Li (1991). First, MAVE does not need undersmoothing when estimating the link function g to achieve a faster rate of convergence. Second, MAVE can be applied to many models including time series data and readily extended to other related problems such as classification (Antoniadis et al., 2003) and functional data analysis (Amato et al., 2005). Finally, the MAVE approach renders generalizations of some other nonparametric methods; for example, Xia's outer product of gradients (OPG) estimator extends the average derivative estimator (ADE) of H¨ardle and Stoker (1989) to multi-index models.
Despite many features, MAVE does not seem to be robust to outliers in the dependent variable y since it is based on local least-squares estimation (for evidence, see Rousseeuw and Leroy, 2003, in parametric and C´izek, 2004, in nonparametric regression). Similar sensitivity to outliers in the space of explanatory variables X (so-called leverage points), was observed and remedied in the case of the sliced inverse regression (SIR) by Gather et al. (2001). At the same time, the robustness of DR methods is crucial since analyzed data are typically highly dimensional, and as such, are difficult to check and clean. Therefore, because of many advantages that MAVE possess, we address its low robustness to outlying observations and propose ways to improve it

2

without affecting main strengths of MAVE. Additionally, we also employ and generalize OPG because, despite being inferior to MAVE, it provides an easyto-implement and fast-to-compute method that could prove preferable in some applications (especially if dimension D is expected to be very small).
The rest of the paper is organised as follows. In Section 2, we describe both the MAVE and OPG methods and discuss their sensitivity to outliers. Robust enhancements of these methods are proposed in Section 3. Finally, we compare all methods by means of simulations in Section 4.

2 Estimation of dimension reduction space

In this section, we first present MAVE and OPG (Sections 2.1 and 2.2) as well as a procedure for determining the effective DR dimension by means of cross validation (Section 2.3). At the end of the section, we will motivate our concerns about robustness of these methods (Section 2.4).

2.1 The MAVE method

Let d represent the working dimension, 1  d  p. For a given number d of directions B in model (1), Xia et al. (2002) proposed to estimate B by minimizing the unexplained variance E{y - E(y|X)}2 = E{y - g(B X)}2, where the unknown function g is locally approximated by a linear function; that is, g(B X0)  a0 + b0 B (X - X0) around some X0. The novel feature of MAVE is that one minimizes simultaneously with respect to directions B and coefficients a0 and b0 of the local linear approximation. Hence, given a sample (Xi, yi)in=1 from (X, y), MAVE estimates B by minimizing

nn
min [yi - {aj + bj B (Xi - Xj)}]2wij,
B:B B=Ip i=1 j=1
aj ,bj ,j=1,...,n

(2)

where wij are weights describing the local character of linear approximation.

Initially, weights at any point X0 are given by a multidimensional kernel func-

tion Kh, X0)}-1; i

where = 1, .

h ..

refers to a bandwidth: , n. Additionally, once

wi0 we

= Kh(Xi - X0){ have an estimate

B^in=o1fKthh(eXDi -R

space, it is possible to iterate using weights based on distances in the reduced

space: wi0 = Kh{B^ (Xi - X0)}[

n i=1

Kh{B^

(Xi - X0)}]-1. Iterating until

convergence results in a refined MAVE, which is the estimator we understand

in the rest of the paper under MAVE.

3

Xia et al. (2002) also proposed a non-trivial iterative estimation procedure
based on repeating two simpler optimizations of (2): one with respect to aj, bj given an estimate B^ and another with respect to B given estimates a^j, ^bj. This computational approach greatly simplifies and speeds up estimation.

2.2 The OPG method

Based on the MAVE approach, Xia et al. (2002) also generalised ADE of H¨ardle and Stoker (1989) to multi-index models. Instead of using a moment condition for the gradient of the regression function g in model (1), E{g(X)} = 0, the average outer product of gradients (OPG) is used:  = E{g(X) g(X)}. It can be shown that the DR matrix B consists of the d eigenvectors corresponding to the d largest eigenvalues of . Thus, recalling that local linear regression solves at point Xj

n

min
aj ,bj

[yi
i=1

-

{aj

+

bj

(Xi

-

Xj )}]2 wij ,

(3)

we

can

estimate



by

^

=

1 n

n i=1

^bj

^bj ,

where

^bj

are

estimates

of

bj

from

(3).

Hence, OPG consists in estimating ^ and determining its d eigenvectors with

largest eigenvalues. The choice of weights wij can be done in the same way as for MAVE.

Although OPG does not exhibit the convergence rate of MAVE, it is easy to implement, fast to compute, and can be flexibly combined with robust estimation methods as shown in Section 3. Moreover, our simulations show that it can perform as well as MAVE if just one or two directions, d  2, are of interest.

2.3 Dimension of effective reduction space

The described methods can estimate the DR space for a pre-specified dimension d. To determine d, Xia et al. (2002) extend the cross-validation (CV) approach of Yao and Tong (1994) and estimate d by d^ = argmin0dpCV (d), where


n

n

CV (d) = yj -

j=1 i=1,i=j

yiKh{B^ (Xi - Xj)}

2 

n i=1,i=j

Kh{B^

(Xi - Xj)}

(4)

4

Table 1 Optimal CV bandwidth for dimension d and a data set with one additive outlier c.
Simulated data with an additive outlier

Dimension d

1 2 4 6 8 10

c=0

0.61 1.28 1.65 1.28 1.28 1.65

c = 50

0.37 1.00 1.65 1.28 1.65 2.12

c = 250

0.47 1.28 2.12 2.12 0.47 0.61

c = 500

0.47 0.78 0.22 0.29 0.37 0.61

c = 1000

0.47 0.61 0.22 0.37 0.47 0.47

for

d

>

0

and

CV

(0)

=

1 n

in=1(yi - y¯)2 to incorporate the possibility of y and

X being independent. Note that this CV criterion can be also used to select

bandwidth h, which results in two-dimensional CV over d and h. Although we

use this time-demanding strategy in our simulations, in practice it is possible

to consistently select bandwidth h in the DR space for the largest dimension

d = p and employ this bandwidth for all other dimensions.

2.4 Robustness of dimension reduction

The complexity of highly dimensional data processed by means of dimension reduction methods requires estimation methodology robust to data contamination, which can arise from miscoding or heterogeneity not captured or presumed in a model. In nonparametric regression, even data coming from a heavy-tailed distribution can exhibit effects similar to data contamination. Since both MAVE, OPG, and CV are based on least-squares criteria, their sensitivity to outlying observations can be rather high. Here we discuss possible effects of a single outlying observation on the estimation results, that is on a^j, ^bj, and B^, and on the CV selection of bandwidth h and dimension d. At the end of the section, we demonstrate the effects of an outlier on a simple example.

Considering OPG and local parametric regression (3), the estimates a^j and ^bj

are just a linear combination of values yi. Since the weights wij are independent

of yi for a given bandwidth h, even a single outlying value yi, |yi|  , can arbitrarily change the estimated coefficients a^j and ^bj around Xj if h is

sufficiently

large.

This

effect

then

influences

matrix

^

=

1 n

n i=1

^bj

^bj .

In

the

case of MAVE defined by (2), the situation is more complicated since the

local linear approximation of the link function given by aj and bj can adjust simultaneously with directions B. In general, it is not possible to explicitly

state, which parameters will be affected and how, but it is likely that the effect

of an outlier will vary with working dimension d.

5

Table 2 Optimal CV dimension d for a data set with one additive outlier c.
Simulated data with an additive outlier

Method c = 0 c = 25 c = 50 c = 75 c = 100 c = 125 c = 150

OPG

12 2 3

5

4

1

MAVE 2 2 2 1

3

2

2

In addition, nonparametric estimation depends on an auxiliary parameter, bandwidth h, and its choice ­ done here by cross validation ­ is crucial for the performance of a method. As argued in Ronchetti et al. (1997), an outlier can significantly bias results of the least-squares-based CV. For OPG (3), bandwidth h is chosen generally too small: the CV criterion (4) is minimized when the outlier affects as a small number of observations in its neighborhoud as possible, that is, when the bandwidth h is small. For MAVE (2), the situation is again complicated by the fact that the outlier can be "isolated" not only by a small bandwidth, but possibly by a specific choice of directions B as well. Furthermore, since CV is also used to determine the dimension of the DR space, an outlier can adversely affect the estimation of dimension D as well.
To exemplify the influence of a single outlying observation on the bandwidth and dimension selection, we generated a random sample of 100 observation from the following nonlinear model:
yi = (Xi b1)2 - (0.5 + Xi b2)2 + 15 cos(Xi b3) + 0.5i,
where random vector Xi has the standard normal distribution in R10 (see Section 4 for detailed model description and Monte Carlo simulations). Additionally, we included one observation that has value yi increased by c ranging from 0 to 1000. We first applied OPG since it allows to determine the local linear approximation (3) and the corresponding bandwidth h separately from directions B. For various values of outlier c and dimension d, the optimal bandwidth hopt was chosen by CV, see Table 1 (line c = 0 corresponds to data without the outlier). Although it does not change monotonically with c, there is a general trend of hopt being smaller with increasing outlier value c. Further, we also estimated dimension d as a function of outlier value c. The results of CV based on MAVE and OPG estimates are summarized in Table 2. OPG seems rather sensitive to this outliers because the estimated dimension varies between 1 and 5 for c = 0, . . . , 150. MAVE results in more stable estimates, which are however still influenced by the outlier position.
6

3 Robust dimension reduction

Both MAVE and OPG seem to be sensitive to data contamination. Our aim is thus to propose robust enhancements of MAVE and OPG that should preserve their present qualities, increase their robustness, and be computationally feasible. We discuss first general ways of making MAVE and OPG more robust (Section 3.1). Next, we address computational issues and propose robust MAVE and OPG that are computationally feasible (Section 3.2). Finally, we adapt the CV procedure mentioned in Section 2.3 for robust estimation (Section 3.3).

3.1 Robust MAVE and OPG

There are many robust alternatives to least squares in linear regression. Using them methods in nonparametric regression adds a requirement on easy and fast implementation, which excludes many so-called high-breakdown point methods (Rousseeuw and Leroy, 2003), and on the other hand, eliminates need for robustness against leverage points to some extent. During last decades, local L- and M-estimators have become particularly popular and well studied, see Boente and Fraiman (1994), C´izek (2004), and Fan and Jiang (1997), H¨ardle and Tsybakov (1988), respectively.

The application of local L- or M-estimation to MAVE and OPG theoretically
reduces to replacing the squared residuals in (2) and (3) by a general convex
function  of residuals, where  represents the variance of residuals. Whereas L-estimators do not require the knowledge of , in the case of M-estimators,
robust choices of  depend on an estimate ^ of residual variance. In parametric estimation (Hampel et al., 1986),  is typically treated as a nuisance
parameter or is set proportional to the median absolute deviation (MAD). To
reflect local character of estimation given by wij in (2) and (3) and to minimize computational cost, we propose to estimate the variance of residuals by
weighted MAD with weights wij. Specifically, we define

n

^(X0)

=

1.4826

·

min
k=1,...,n

r(k)
i=1

Kh(Xi -

n i=1

Kh(Xi

X0) - X0)

·

I

ri  r(k)

 0.5

,

where r(k) is the kth order statistics of ri = |yi - µ~y(X0)|.

In the case of OPG, this means that one applies a local polynomial L- or M-estimator with a given function  and a variance estimate ^,

n

min
aj ,bj

i=1

^ [yi

-

{aj

+

bj

(Xi

-

Xj )}]2 wij ,

(5)

7

and

then

obtains

the

DR

space

B

as

d

eigenvectors

of

^

=

1 n

n i=1

^bj

^bj

with

largest eigenvalues.

In the case of MAVE, having a general objective function ^ leads to
nn
min ^ yi - {aj + bj B (Xi - Xj)} wij.
B:B B=Ip i=1 j=1
aj ,bj ,j=1,...,n

(6)

Although (6) cannot be minimized using the algorithm proposed by Xia et
al. (2002), the optimization can be still carried out by repeated estimation of (6) with respect to aj, bj given an estimate B^ and with respect to B having estimates a^j, ^bj. The first step is just the already mentioned local L- or Mestimation analogous to (5). To facilite the second step ­ estimation of B, let
us observe that (6) can be reformulated as follows. For B = (1, . . . , d) and given (aj, bj) = (aj, bj1, . . . , bjd), (6) is equivalent to

nn

d

min ^ yi -
B:B B=Ip i=1 j=1

aj + k bjk(Xi - Xj)
k=1

wij .

(7)

This represents a standard regression problem with n2 observations and pd variables, which can be estimated by usual parametric estimator. Although simulations show that estimating MAVE this way leads to slightly better estimates than the original algorithm, the size of regression (7) will be enormous as the sample size increases, which will hinder computation. For example, there are very fast algorithms available for computing least squares and L1 regression in large data sets, see Koenker and Portnoy (1997), and even in these two special cases computation becomes 10 to 20 times slower than the original algorithm for samples of just 100 observations! This disqualifies such an algorithm from practical use.

3.2 One-step estimation

To be able to employ the fast original MAVE algorithm, robustness has to be achieved only by modifying weights wij in (2). To achieve this, we propose to use one-step M-estimators as discussed in Fan and Jiang (1999) and Welsh and Ronchetti (2002): the (iteratively) reweighted least squares approach. First using an initial highly robust estimate ^0 = {B^0, a^0j, ^b0j}, we construct weights wij such that the objective function (2) is equivalent to (6) at ^0: wij = wij^(r0i)/r02i where r0i = yi - {a^0j + ^b0jB^0 (Xi - Xj)}. Next, we perform the original least-squares-based algorithm using the constructed weights wij.
8

Contrary to Fan and Jiang (1999), we use L1 regression as the initial robust estimator, which guarantees robustness against outliers and fast computation
(one does not have to protect against leverage points since estimation is done
in a local window given by the bandwidth and kernel function).

3.3 Robust cross-validation

The robust estimation of the DR matrix B0 is not sufficient if dimension d is not known. As indicated by Ronchetti et al. (1997) and the example in Sec-
tion 2.4, using the CV criterion (4) can lead to a bias in dimension estimation
(and bandwidth selection) even if a robust estimator of B0 is used. Now, due to the local nature of nonparametric regression, we have to "protect" CV pri-
marily against outliers in the y-direction. In this context, the L1 estimator is highly robust and the same should apply to CV based on L1 rather than L2 norm. Thus, we propose to use instead of (4) the L1 cross validation of Wang and Scott (1994),

nn
CV (d) = yj -
j=1 i=1,i=j

yiKh{B^ (Xi - Xj)}

n i=1,i=j

Kh{B^

(Xi - Xj)}

,

(8)

to determine both the optimal bandwidth and dimension D. This procedure is
further referred to as CVA instead of CV, which is used only for the L2 cross validation.

4 Simulations

To study finite sample performance of MAVE, OPG and their modifications proposed in Section 3, we perform a set of Monte Carlo simulations under various distributional assumption. In this section, the used data-generating model is introduced first (Section 4.1). Next, we compare the performance of all methods in estimating the directions of the DR space (Section 4.2). Finally, we examine the performance of the CV and CVA dimension estimation (Section 4.3).

4.1 Simulation models

Throughout this section, we consider the data-generating model yi = (Xi 1)2 - (0.5 + Xi 2)2 + 15 cos(Xi 3) + 0.5i,
9

(9)

Table 3 Average computational times for each method at sample size n = 100 and dimension d = 3 relative to standard OPG.
Computation times

LS L1 HUBER HAMPEL

OPG

1.0 4.6 10.4 12.3

MAVE

7.5 298.7 13.8

14.5

where the vector Xi of explanatory variables has the standard normal distributionin R10 and 1 = (1, 2, 3, 0, 0, 0, 0, 0, 0,0)/ 14, 2 = (-2, 1, 0, 1, 0, 0, 0, 0, 0, 0)/ 6, and 3 = (0, 0, 0, 0, 0, 0, 0, 1, 1, 1)/ 3. The effective DR space given by B0 = (1, 2, 3) has thus dimension D = 3. To compare the robust properties of all estimators, we use three error distributions.

(1) The standard normal errors, i  N (0, 1), generate Gaussian data without any outlying observations.
(2) The Student distributed errors, i  t1, with one degree of freedom simulate data from a heavy-tailed distribution.
(3) The contaminated errors, i  0.95N (0, 1) + 0.05U (-600, 600), represent (normal) data containing 5% of outlying observations.

For the sake of brevity, we refer to these three cases as NORMAL, STUDENT, and OUTLIERS, respectively. For all simulations from (9), we use sample size n = 100 and 100 repetitions (we observed that the results for larger samples sizes, such as n = 200, are qualitatively the same as for n = 100). Nonparametric smoothing employs the Gaussian kernel in all cases. Bandwidth is cross-validated using CVA for the proposed robust methods and using both CV and CVA for the original methods.

Let us note that we compare the methods using the same distance measure of the estimated space B^ and the true space B0 = (1, 2, 3) as Xia et al. (2002): m(B^, B0) = (I - B0B0T )B^ for d  D = 3 and m(B^, B0) = (I - B^B^T )B0 for d  D = 3 and (D = 3 is the true dimension of the reduced space used in our simulations, whereas d denotes the dimension used for estimation).

4.2 Estimation of dimension reduction space

MAVE, OPG, and their modifications are now compared by means of simulations. The estimators include MAVE defined in (6) and OPG defined in (5) using the following functions ^:
(1) ^(x) = x2 (least squares) (2) ^(x) = |x| (least absolute deviation),
10

(3) ^(x) = sgn(x) min(|x|, ^)dx (M-estimate with the Huber function) (4) ^(x) = sgn(x) max{0, min(|x|, ^) - max(0, |x| - 2^)}dx (M-estimate
with the Hampel function)
We refer to these functions as LS, L1, HUBER, and HAMPEL. The first choice corresponds to standard MAVE and OPG. The second one relies on the local L-estimation that, in the case of MAVE, has to be performed by slow algorithm based on alternating formulations (6) and (7), see Section 3.1. The last two choices represent MAVE and OPG relying on the local one-step M-estimation as described in Section 3.2.
Before discussing the estimation results, let us recall our concerns about computational speed of each method that motivated use of sub-optimal, but fast OPG and precludes practical use of MAVE based on the L1 estimator. Given model (9), Table 3 summarized computational times for all methods relative to OPG­LS. The results are to some extent implementation-specific, which does not allow to directly compare optimized least-square methods and unoptimized robust variants. Nevertheless, it is clear that OPG can be computed much faster than MAVE (if the algorithm is properly optimized) and that the general algorithm from Section 3.1 used for MAVE­L1 is too slow for real applications.
Let us first deal with the results concerning OPG and its modifications presented in Table 4. For data NORMAL, all modifications outperform the original method (OPG­LS­CV). It is interesting to note that, in the case of OPG­ LS, the CVA criterion performed better than CV. This might be due to a relatively small number of observations relative to the dimension of the data, but our feeling is that this is typical rather than exceptional for many dimensionreduction applications. Nevertheless, even the comparison of OPG­LS­CVA, OPG­HUBER, and OPG­HAMPEL does not reveal significant differences in the performance of these methods. For data STUDENT, all robust versions of OPG provide similar results, whereas the estimates by original OPG­LS exhibit rather large errors, especially in the first direction 1. For data OUTLIERS, there is a large difference between non-robust OPG­LS and the robust modifications of OPG, which documents sensitivity of the original OPG estimator to outliers. Although the performance of all robust estimators is relatively similar, OPG­HAMPEL is best due to its full-rejection feature (observations with too large residuals get zero weight).
The simulation results for MAVE are summarized in Table 5. For data NORMAL, we again observe the positive influence of CVA on the original MAVE. All estimates except for MAVE­HAMPEL perform almost equally well. MAVE­ HAMPEL provides worst estimates since it fully rejects some data points, which surprisingly did not matter in the case of OPG. For data STUDENT and OUTLIERS, all robust versions of MAVE by far outperform the original
11

Table 4 Median errors of OPG estimates for dimension d = 3.

Simulated data NORMAL, STUDENT, and OUTLIERS

Data

Method

m0(^1) m0(^2) m0(^3)

m0 (B^ )

NORMAL OPG LS CV

0.005 0.147 0.264 0.352

OPG LS CVA 0.004 0.132 0.208 0.301

OPG L1

0.004 0.117 0.215 0.279

OPG HUBER 0.004 0.127 0.227 0.310

OPG HAMPEL 0.005 0.125 0.201 0.307

STUDENT

OPG LS CV OPG LS CVA OPG L1 OPG HUBER OPG HAMPEL

0.103 0.096 0.021 0.016 0.013

0.521 0.545 0.454 0.386 0.287

0.663 0.599 0.523 0.530 0.467

0.953 0.953 0.883 0.821 0.743

OUTLIERS OPG LS CV OPG LS CVA OPG L1 OPG HUBER OPG HAMPEL

0.641 0.648 0.012 0.008 0.007

0.607 0.616 0.267 0.192 0.167

0.549 0.587 0.473 0.369 0.324

0.969 0.973 0.722 0.467 0.368

MAVE, which exhibit rather large errors in all direction. Similarly to OPG, MAVE­HAMPEL is best due to the full rejection of extreme observations; this is effect is rather pronounced in data OUTLIERS.
All presented results clearly document the need for and advantages of the proposed robust modifications of MAVE and OPG. Comparing the results for both methods, they have rather similar structure, but MAVE always outperforms OPG when considering the estimation of the whole DR space. On the other hand, OPG seems to be very good in identifying the first and to some extent also the second direction. Let us note that results can be further improved by adjusting function ^, the choice of which was typical rather than optimal.
12

Table 5 Median errors of MAVE estimates for dimension d = 3.

Simulated data NORMAL, STUDENT, and OUTLIERS

Data

Method

m0(^1) m0(^2) m0(^3)

m0 (B^ )

NORMAL MAVE LS CV

0.007 0.092 0.095 0.181

MAVE LS CVA 0.004 0.089 0.094 0.147

MAVE L1

0.005 0.060 0.090 0.148

MAVE HUBER 0.005 0.100 0.094 0.164

MAVE HAMPEL 0.006 0.116 0.153 0.250

STUDENT

MAVE LS MAVE LS CVA MAVE L1 MAVE HUBER MAVE HAMPEL

0.316 0.252 0.020 0.035 0.039

0.385 0.397 0.335 0.284 0.289

0.572 0.510 0.388 0.451 0.428

0.910 0.910 0.683 0.685 0.633

OUTLIERS MAVE LS MAVE LS MAVE L1 MAVE HUBER MAVE HAMPEL

0.747 0.752 0.029 0.014 0.010

0.732 0.682 0.165 0.228 0.151

0.664 0.680 0.221 0.202 0.176

0.976 0.976 0.470 0.416 0.312

4.3 Cross-validation simulations

The estimation of the DR space considered in the previous section is now

complemented by a study on the estimation of the effective dimension D. Simulating again from model (9), we estimated the DR dimension d^ (D = 3)

using MAVE and OPG with ^-functions LS and HAMPEL and the CV and CVA criteria (4) and (8), respectively. Note that the design of model (9) makes

the size

identification of the n = 100. Therefore,

twheiradccdeirpetcteisotnima3tedsiffid^c=ul2t

given rather small sample and d^ = 3 as appropriate.

Results for all models are summarized in Tables 6 and 7 for MAVE and OPG, respectively. Judging all methods by the number of simulated data sets for which estimated dimension equals two or three, MAVE can be always preferred to OPG. For the original methods, the CVA criterion is preferable to CV as in Section 4.2. A more interesting results is that MAVE­HAMPEL and OPG­HAMPEL outperformed the original MAVE and OPG not only in data

13

Table 6 Estimates of the DR dimension by MAVE variants using L2 (CV) and L1 (CVA) cross validation. Entries represent the numbers of samples out of 100 with estimated dimension d.
Simulated data NORMAL, STUDENT, and OUTLIER

Data

Estimation

CV D = 1 D = 2 D = 3 D = 4 D  5

NORMAL MAVE LS

CV 11 38 51 0 0

NORMAL MAVE LS

CVA 4 43 53 0 0

NORMAL MAVE HAMPEL CV 7 52 41 0 0

NORMAL MAVE HAMPEL CVA 6 54 40 0 0

STUDENT MAVE LS

CV 26 33 35

5

STUDENT MAVE LS

CVA 15 29 44 12

STUDENT MAVE HAMPEL CV

18

42

27

7

STUDENT MAVE HAMPEL CVA 15 62 21

2

1 1 5 0

OUTLIERS MAVE LS

CV 0

1 14 20 65

OUTLIERS MAVE LS

CVA 5

6 20 10 59

OUTLIERS MAVE HAMPEL CV

1

2

6 10 81

OUTLIERS MAVE HAMPEL CVA 8 47 29

4 12

OUTLIERS and STUDENT, but also in the case of data NORMAL. The only case where MAVE­LS is preferable is the number of data set for which the estimated dimension d^ equals 3 in data NORMAL. This could be partially accounted to the relatively small sample size. Finally, notice that the robust DR method such as MAVE­HAMPEL does not suffice to identify the dimension of the DR space: a robust CV criterion such as CVA has to be used as well.

5 Conclusion
We proposed robust enhacements of MAVE and OPG that can perform equally well as the original methods under `normal' data, are robust to outliers and heavy-tailed distributions, and are easy to implement. Should we pick up one method for a general use, MAVE­HUBER seems to be the most suitable candidate as (i) MAVE­LS is not robust, (ii) MAVE­L1 is slow to compute, see Section 3.1, and (iii) MAVE­HAMPEL does not perform so well for normal data.
14

Table 7 Estimates of the DR dimension by OPG variants using L2 (CV) and L1 (CVA) cross validation. Entries represent the numbers of samples out of 100 with estimated dimension d.
Simulated data NORMAL, STUDENT, and OUTLIER

Data

Estimation

CV D = 1 D = 2 D = 3 D = 4 D  5

NORMAL OPG LS

CV 8 48 38 5 1

NORMAL OPG LS

CVA 8 59 31 1 1

NORMAL OPG HAMPEL CV 8 51 40 1 0

NORMAL OPG HAMPEL CVA 8 47 43 2 0

STUDENT OPG LS

CV 21 29 27 18

5

STUDENT OPG LS

CVA 11 25 41 18

5

STUDENT OPG HAMPEL CV

10

31

33

14

12

STUDENT OPG HAMPEL CVA 23 41 31 5 0

OUTLIERS OPG LS

CV 2

1 10 12 75

OUTLIERS OPG LS

CVA 14 5 8 8 65

OUTLIERS OPG HAMPEL CV

0

0

2 13 85

OUTLIERS OPG HAMPEL CVA 12 40 23 10 15

References

[1] Amato, U., Antoniadis, A., and De Feis, I., 2005. Dimension reduction in function regression with applications. Computational Statistics & Data Analysis, in press.
[2] Antoniadis, A., Lambert-Lacroix, S., and Leblanc, F., 2003. Effective dimension reduction methods for tumor classification using gene expression data. Bioinformatics 19(5), 563­570.
[3] Boente, G. and Fraiman, R., 1994. Local L-estimators for nonparametric regression under dependence. Journal of Nonparametric Statistics 4, 91­101.
[4] C´izek, P., 2004. Smoothed local L-estimation with an application. In: M. Hubert, G. Pison, A. Struyf, and S. Van Aelst (Eds.), Theory and Applications of Recent Robust Methods, Birkh¨auser, Basel, 59­70.
[5] Fan, J. and Jiang, J., 1999. Variable bandwidth and one-step local M-estimator. Science of China, Ser. A, 29, 1­15.
[6] Gather, U., Hilker, T., and Becker, C., 2001. A robustified version of sliced inverse regression. In: L. T. Fernholz, S. Morgenthaler, and W. Stahel (Eds.),

15

Statistics in Genetics and in the Environmental Sciences, Birkh¨auser, Basel, 147­157. [7] Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A., 1986. Robust statistics, the approach based on influence function. Wiley, New York. [8] H¨ardle, W. and Stoker, T. M., 1989. Investigating smooth multiple regression by method of average derivatives. Journal of American Statistical Association 84, 986­995. [9] H¨ardle, W. and Tsybakov, A. B., 1988. Robust nonparametric regression with simultaneous scale curve estimation. Annals of Statistics 16, 120­135. [10] Koenker, R. and Portnoy, S., 1997. The Gaussian hare and the Laplacian tortoise: computability of squared-error vs. absolute-error estimators. Statistical Science 12, 279­300. [11] Li, K. C., 1991. Sliced inverse regression for dimension reduction. Journal of American Statistical Association 86, 316­342. [12] Rousseeuw, P. J. and Leroy, A. M., 2003. Robust regression and outlier detection. Wiley, New York. [13] Ronchetti, E., Field, C., and Blanchard, W., 1997. Robust linear model selection by cross-validation. Journal of American Statistical Association 92, 1017­1023. [14] Sakata, S. and White, H., 1998. High breakdown point conditional dispersion estimation with application to S&P 500 daily returns volatility. Econometrica 66(3), 529­567. [15] Wang, F. T. and Scott, D. W., 1994. The L1 method for robust nonparametric regression. Journal of American Statistical Association 89, 65­76. [16] Welsh, A. H. and Ronchetti, E., 2002. A journey in single steps: robust one-step M-estimation in linear regression. Journal of Statistical Planning and Inference 103, 287­310. [17] Xia, Y., Tong, H., Li, W. K., and Zhu, L.-X., 2002. An adaptive estimation of dimension reduction space. Journal of Royal Statistical Society, Ser. B, 64, 363­410. [18] Yao, Q. and Tong, H., 1994. On subset selection in nonparametric stochastic regression. Statistica Sinica 4, 51­70.
16

SFB 649 Discussion Paper Series
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Nonparametric Risk Management with Generalized Hyperbolic Distributions" by Ying Chen, Wolfgang Härdle and Seok-Oh Jeong, January 2005.
002 "Selecting Comparables for the Valuation of the European Firms" by Ingolf Dittmann and Christian Weiner, February 2005.
003 "Competitive Risk Sharing Contracts with One-sided Commitment" by Dirk Krueger and Harald Uhlig, February 2005.
004 "Value-at-Risk Calculations with Time Varying Copulae" by Enzo Giacomini and Wolfgang Härdle, February 2005.
005 "An Optimal Stopping Problem in a Diffusion-type Model with Delay" by Pavel V. Gapeev and Markus Reiß, February 2005.
006 "Conditional and Dynamic Convex Risk Measures" by Kai Detlefsen and Giacomo Scandolo, February 2005.
007 "Implied Trinomial Trees" by Pavel Cízek and Karel Komorád, February 2005.
008 "Stable Distributions" by Szymon Borak, Wolfgang Härdle and Rafal Weron, February 2005.
009 "Predicting Bankruptcy with Support Vector Machines" by Wolfgang Härdle, Rouslan A. Moro and Dorothea Schäfer, February 2005.
010 "Working with the XQC" by Wolfgang Härdle and Heiko Lehmann, February 2005.
011 "FFT Based Option Pricing" by Szymon Borak, Kai Detlefsen and Wolfgang Härdle, February 2005.
012 "Common Functional Implied Volatility Analysis" by Michal Benko and Wolfgang Härdle, February 2005.
013 "Nonparametric Productivity Analysis" by Wolfgang Härdle and Seok-Oh Jeong, March 2005.
014 "Are Eastern European Countries Catching Up? Time Series Evidence for Czech Republic, Hungary, and Poland" by Ralf Brüggemann and Carsten Trenkler, March 2005.
015 "Robust Estimation of Dimension Reduction Space" by Pavel Cízek and Wolfgang Härdle, March 2005.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

