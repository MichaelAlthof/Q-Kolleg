BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2006-012
Bootstrapping Systems Cointegration Tests with
a Prior Adjustment for Deterministic Terms
Carsten Trenkler*
* Institute of Statistics and Econometrics, School of Business and Economics,
Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

February 10, 2006
Bootstrapping Systems Cointegration Tests With a Prior Adjustment for Deterministic Terms
Carsten Trenkler Humboldt-Universita¨t zu Berlin School of Business and Economics Institute of Statistics and Econometrics
Spandauer Str. 1 D-10178 berlin
Germany Tel.: +49-30-2093-5711 Fax.: +49-30-2093-5712 email: trenkler@wiwi.hu-berlin.de
Abstract
In this paper we analyse bootstrap procedures for systems cointegration tests with a prior adjustment for deterministic terms suggested by Saikkonen & Lu¨tkepohl (2000b) and Saikkonen, Lu¨tkepohl & Trenkler (2006). The asymptotic properties of the bootstrap test procedures are derived and their small sample properties are studied. The simulation study also considers the standard asymptotic test versions and the Johansen cointegration test for comparison.
Keywords: Bootstrap, Systems cointegration tests, VEC models JEL classification: C12, C13, C15, C32
This research was supported by the Deutsche Forschungsgemeinschaft (DFG) through the SFB 649 "Economic Risk". Moreover, I am grateful to Anders Rygh Swensen, Helmut Lu¨tkepohl, and participants of the "Unit Root and Cointegration Testing Conference" at the University of Algarve for many helpful comments and suggestions.

1 Introduction
It is known that systems cointegration tests suffer quite often from very poor small sample properties (compare e.g. Hubrich, Lu¨tkepohl & Saikkonen 2001). In other words, the asymptotic test distributions may provide a bad approximation for small sample sizes prevailing in applied work. Hence, the inference on the number of cointegrating relations among a system of variables may be distorted. This applies both to cointegration tests based on Johansen (1988) but also to test procedures with a prior adjustment for deterministic terms as suggested by Saikkonen & Lu¨tkepohl (2000b) and Saikkonen et al. (2006).
In the literature, the use of bootstrap methods has been suggested as a possible way to improve the small sample properties of a test procedure. This usually refers to a test's size, since it is not clear what effect bootstrap methods may have on the test's power (compare e.g. Davidson & MacKinnon 2006). Improved approximation to the small sample distribution can be expected if the test statistic is asymptotical pivotal (compare e.g. Horowitz 2001). This is the case for the cointegration test statistics considerded in this paper. However, doubts have been raised about the usefulness of bootstrap methods in relation to time series data (compare e.g. Horowitz 2003, MacKinnon 2002). Obviously, the applicability and properties of bootstrap procedures are not as favourable as in the case of i.i.d. data due to the dependence structure in time series. As a response to that one may focus on time series models with i.i.d. error terms.
Nevertheless, different types of bootstrap procedures have been suggested and successfully applied in the time series framework in order to test for a unit root (see e.g. Basawa, Mallik, McCormic, Reeves & Taylor 1991, Paparoditis & Politis 2003, Park 2003). In addition, Park (2003) shows that bootstraps offer asymptotic refinements in finite samples for the ADF unit root test. The bootstrap algorithm he considers can be regarded as an univariate counterpart of the algorithms analysed later on. With respect to systems cointegration tests much less is known about bootstrapping. Based on simulation studies, van Giersbergen (1996), Harris & Judge (1998), and Mantalos & Shukur (2001) analyse different variants of so-called recursive and block bootstraps for Johansen cointegration tests. The results of these studies, although partly mixed, do not show that an application of bootstrap techniques generally improves the tests' small sample properties. However, a theoretical justification for the use of bootstrap methods to test for the cointegrating rank has been recently given by Swensen (2005a). He shows that recursive bootstrap procedures for the Johansen test have the same limiting distribution as the asymptotic test versions. Furthermore, his simulation findings demonstrate that bootstrap procedures can lead to an improvement in the tests' sizes in small samples.
1

We will follow Swensen (2005a) and analyse different variants of recursive bootstrap algorithms for systems cointegration tests with a prior adjustment for deterministic terms which have been suggested by Saikkonen & Lu¨tkepohl (2000b) and Saikkonen et al. (2006). Both tests assume that the data generating process can be decomposed into an unobservable zero-mean stochastic part, which follows a vector autoregressive process, and a deterministic component, which consists of a level term and a linear trend. Their common idea is to estimate the deterministic terms in a first step and to adjust the original time series by these estimated terms. Then, a likelihood ratio type test in the spirit of Johansen (1988) is applied to the adjusted data. This model setup requires to introduce bootstrap equivalents of both the deterministic and the stochastic components. We define these equivalents such that the sum of them produces the same bootstrap data as are obtained by applying the recursive bootstrap scheme directly to the observable data. It turns out that the bootstrap tests have the same limiting distributions under the cointegrating rank null hypothesis as the corresponding asymptotic test procedures by Saikkonen & Lu¨tkepohl (2000b) and Saikkonen et al. (2006). Furthermore, the small sample properties of the bootstrap procedures are analysed and compared with the ones of the asymptotic tests. The comparison also includes the Johansen cointegration test. Thereby we can compare our findings with the ones of Swensen (2005a, 2005b).
The rest of the paper is structured as follows. The next section describes the system cointegration tests and Section 3 introduces the bootstrap procedures. Their asymptotic properties are discussed in Section 4. Some simulation results are presented in Section 5 and Section 6 summarizes and concludes. The theoretical derivations and explanations on the response surfaces are deferred to two Appendices.
The following notation is used throughout. If A is an (n × m) matrix of full column rank (n > m), its orthogonal complement A is an (n × (n - m)) matrix of full column rank and such that A A = 0. Furthermore, let A¯ = A(A A)-1. A (n × n) identity matrix is denoted by In. LS and GLS are used to abbreviate least squares and generalized least squares respectively, LR relates to likelihood ratio, RR is short for reduced rank, and DGP refers to data generating process. The abbreviations r.h.s. and l.h.s. stand for right- and left-hand-side, respectively.

2 Test Procedures

Let us consider a n-dimensional times series yt = (y1t, . . . , ynt) (t = 1, . . . , T ), which is generated by

yt = µ0 + µ1t + xt, t = 1, 2, . . . ,

(2.1)

2

where µ0 and µ1 are unknown (n × 1) parameter vectors. Hence, the deterministic part consists of a constant and a linear trend. The term xt is an unobservable stochastic error process, for which we make the following assumption.

Assumption 1. The process xt is integrated of order at most I(1) with cointegrating rank r and follows a vector autoregressive process of order p, VAR(p),

xt = A1xt-1 + · · · + Apxt-p + t, t = 1, 2, . . . ,

(2.2)

where Aj are (n × n) coefficient matrices and the initial values are such that xt = 0, t  0. For the error terms we assume t  i.i.d.(0, ) with positive definite covariance matrix  and E(4t ) < .

The initial value condition xt = 0, t  0, is imposed for convenience to construct the test by Saikkonen & Lu¨tkepohl (2000b) and the bootstrap versions of this procedure. However, the asymptotic tests remain valid if the initial values have a fixed probability distribution, which does not depend on the sample size. Under Assumption 1, the process xt has the usual vector error correction model (VECM) form

p-1
xt = xt-1 + jxt-j + t,
j=1

t = 1, 2, . . . ,

where  and j (j = 1, . . . , p - 1) are (n×n) unknown parameter matrices. Because the cointegrating rank is r, the matrix  can be written as  =  , where  and  are (n × r)

matrices of full column rank. Obviously, the cointegrating rank is equal to the rank of the matrix

. As is well-known,  xt and xt are zero mean I(0) processes. Defining  = In - 1 -

· · · - p-1 = In +

p-1 j=1

jAj+1

and

C

=

()-1,

we

have

t
xt = C j + t, t = 1, 2, . . . ,
j=1

where t is a zero mean I(0) process. Finally, we derive the VECM representation for yt. Multiplying (2.1) by A(L) = In -
A1L - · · · - ApLp = In - L - 1L - · · · - p-1Lp-1 and rearranging yields

p-1
yt =  + ( yt-1 - (t - 1)) + jyt-j + t, t = p + 1, p + 2, . . . ,
j=1

(2.3)

where  = -µ0 + µ1 = - + µ1,  =  µ0, and  =  µ1.

3

In the following we consider the so-called trace test version, i.e. we aim to test the pair of

hypotheses

H0(r0) : rk() = r0 vs. H1(r0) : rk() > r0.

(2.4)

The idea of the test proposal of Saikkonen & Lu¨tkepohl (2000b) is to estimate the determin-

istic terms in a first step by a feasible GLS procedure. Then, yt is adjusted by these estimated terms. In a second step, a Johansen LR-type test is performed on the adjusted time series. More

precisely, defining   1 for t  1
a0t =  0 for t  0

  t for t  1 and a1t =  0 for t  0

and multiplying (2.1) by A(L) we obtain

A(L)yt = H0tµ0 + H1tµ1 + t, t = 1, . . . , T,

(2.5)

where Hit = A(L)ait (i = 0, 1), and t = A(L)xt (see (2.2)). Furthermore, define Q such that QQ = -1 and multiply (2.5) by Q , then we have

Q A(L)yt = Q H0tµ0 + Q H1tµ1 + t, t = 1, . . . , T,

(2.6)

where t = Q t. The matrix Q can be chosen as Q = [-1( -1)-1/2 : (-1)-1/2]. Hence, the error term t has a zero mean and a unit covariance matrix as it is required for a GLS transformation. To make the GLS estimation feasible, Saikkonen & Lu¨tkepohl (2000b) propose to use the RR estimators ~, ~, ~j (j = 1, . . . , p - 1) and  which are obtained from (2.3) by applying r0, the rank under the null hypothesis of the cointegration test. Based on the RR estimators, one can compute the estimators Q~ and H~it (i = 0, 1). Thus, feasible GLS estimators of µ0 and µ1, say µ~0 and µ~1, are obtained by a multivariate LS estimation of the model

Q~ A~(L)yt = Q~ H~0tµ0 + Q~ H~1tµ1 + ~t,

t = 1, . . . , T.

(2.7)

The estimated deterministic terms are used to adjust yt, what gives the sample analogue x~t = yt - µ~0 - µ~1t of xt. Then, an LR-type test is performed with respect to

p-1
x~t = x~t-1 + jx~t-j + et,
j=1

t = p + 1, . . . , T,

(2.8)

where et is an error term. Since x~t is adjusted by the deterministic terms, a test version with-
out deterministic terms like in Johansen (1988) is applied. Thus, x~t and x~t-1 are regressed on (x~t-1, . . . , x~t-p-1) to obtain the residuals R~0t and R~1t, respectively. Next, consider

4

S~ij = T -1 T R~itR~jt (i, j = 0, 1). Solving det(S~11 - S~10S~0-01S~01) = 0 we obtain the ordered
t=1
generalized eigenvalues ~1  · · ·  ~n. Finally, the LR test statistic for the pair of hypotheses

in (2.4) is1

n

GLS(r0) = -T

log(1 - ~j).

j=r0+1

As stated in Theorem 1 of Saikkonen & Lu¨tkepohl (2000b), the level parameter µ0 is not

consistently estimated in the direction of  because µ0 is not identified in that direction in

(2.3). Therefore, Saikkonen et al. (2006) suggest to avoid the estimation of the level parameter

in the first stage. Instead, only µ1 is estimated and the effect of the level parameter is taken
into account when the test is performed. Defining  = µ1 and noting that  = C( - ( )-1), Saikkonen et al. (2006) propose to estimate µ1 by

µ^1 = ~(~ ~)-1~ + ~(~~)-1~,

(2.9)

where ~, ~, ~, and ~ = ~C~(~ - ~~(~ ~)-1~) are obtained from a RR estimation of (2.3). Using µ^1 we adjust yt for a linear trend and obtain

y^tc = yt - µ^1t.

Then, we set up an auxiliary VECM for y^tc

p-1
y^tc = ( ytc-1 - ) + jy^tc-j + etc,
j=1

(2.10)

where ect is an error term. Finally, an LR-type test, now including a constant term, can be performed with respect to (2.10). Denoting the corresponding ordered generalized eigenvalues as ^1  · · ·  ^n, we obtain the LR test statistic for the pair of hypotheses in (2.4)2

n

SLT (r0) = -T

log(1 - ^j).

j=r0+1

Saikkonen & Lu¨tkepohl (2000b) and Saikkonen et al. (2006) derive the limiting distributions

for GLS(r0) and SLT (r0) given in Theorem 1.

1Note that the generalized eigenvalue problem described here is slightly different from the one in Saikkonen & Lu¨tkepohl (2000b). However, the eigenvalue problems can be transformed into each other by an appropriate redefinition of the respective eigenvalues. Hence, the LR statistics based on the two different sets of eigenvalues are identical apart from minor numerical differences.
2Saikkonen et al. (2006) allow in addition for a level shift at unknown time. The test statistic considered here can be obtained by deleting all terms associated with the level shift. This has no effect on the limiting distribution.
5

Theorem 1. Under Assumption 1 and if H0(r0) is true, then Pr0,T (GLS(r0)  x) - P (tr(DGLS)  x) and Pr0,T (SLT (r0)  x) - P (tr(DSLT )  x) for all x as T  , where

1 1 -1 1

DGLS =

B(s)dB(s)

B(s)B(s) ds

B(s)dB(s) ,

00

0

1 1 -1 1

DSLT =

B+(s)dB(s)

B+(s)B+(s) ds

B+(s)dB(s) ,

00

0

B(s) is a (n - r0)-dimensional standard Brownian motion, B(s) = B(s) - sB(1) is a (n - r0)-

dimensional Brownian bridge, B+(s) = [B(s) , 1] and dB(s) = dB(s) - dsB(1).

Several remarks are worth making regarding this theorem. The probability measures Pr0,T refer to the the joint distributions of the sample y1, . . . , yT for situations where rk() = r0 (compare Swensen 2005a). With respect to the bootstrap framework we will introduce a further probability measure conditional on the observations. The limiting distributions for GLS(r0) and SLT (r0) differ in that the process B+(s) appears in place of the Brownian bridge B(s). Of course, the reason is that an intercept term is included in the auxiliary model on which LST (r0) is based. On the other hand, the limiting distribution of LST (r0) is formally similar to its counterpart in Theorem 6.3 of Johansen (1995) where a standard Brownian motion appears instead of the Brownian bridge in Theorem 1.

3 Bootstrap Algorithms
Swensen (2005a) proposes two variants of a recursive bootstrap related to the VECM (2.3). The first algorithm to generate the so-called pseudo or bootstrapped observations y1, . . . , yT works as follows.

Algorithm 1.
(1) Estimate model (2.3) by a RR regression under the rank hypothesis H0(r0) : rk() = r0 in order to obtain estimates ~, ~, ~, ~, and ~j (j = 1, . . . , p - 1).
(2) Check whether the roots of the equation det[A~(z)] = 0, where A~(z) = (1 - z)In - ~~ z - ~1(1 - z)z - · · · - ~p-1(1 - z)zp-1,
are equal to 1 or outside the unit circle and whether ~~~ is nonsingular.
6

(3) If so, compute the residuals ~p+1, . . . , ~T from the estimated VECM (2.3).

(4) Compute yt, t = p + 1, . . . , T, recursively by

p-1
yt = ~ + ~(~ yt-1 - ~(t - 1)) + ~jyt-j + t ,
j=1

(3.1)

using the estimates obtained under (1) and sampled residuals t drawn with replacement from the estimated residuals ~p+1, . . . , ~T . The starting values of the recursion, y1, . . . , yp,

are set equal to y1, . . . , yp.

As usual in the literature, the asterisk refers to quantities related to the bootstrap procedure. Accordingly, the probability measure Pr0,T refers to the conditional distribution of y1, . . . , yT given the observations y1, . . . , yT . Now, the subscript r0 indicates that the pseudo observations are generated by imposing the rank r0. Moreover, note that the pseudo observations y1, . . . , yT depend on the sample size T . Requirement (ii) assures that the generated pseudo observations
are indeed I(1). If this condition is not satisfied, one may refer to a more appropriate resampling
scheme as pointed out by Swensen (2005a).
The bootstrap test version is then computed by running the same test procedures as described above with respect to the pseudo series y1, . . . , yT . Let GLS1(r0) and SLT1(r0) be the respective bootstrap test statistics and denote the cumulative distribution functions of GLS1(r0) and SLT1(r0) by Fr0;GLS,1 and Fr0;SLT,1 respectively. The latter are conditional distributions given the observations y1, . . . , yT . Then, we reject the null hypothesis of r0 cointegrating relations at some chosen significance level  if

1 - Fr0;GLS,1(GLS(r0))   in case of the test by Saikkonen & Lu¨tkepohl (2000b) and if

(3.2)

1 - Fr0;SLT,1(SLT (r0))  

(3.3)

for the test by Saikkonen et al. (2006). The bootstrap distributions are usually very complicated functions of the observations. Therefore, they are approximated by repeating the bootstrap algorithm a large number of times, say M .3 Then, we count the number of instances, for which GLS1(r0) > GLS(r0), say M RGLS,1, and for which SLT1(r0) > SLT (r0), say M RSLT,1. Finally, we reject H0(r0) for the bootstrap version of Saikkonen & Lu¨tkepohl (2000b) if

M RGLS,1/M  

(3.4)

3Davidson & MacKinnon (2000) suggest and analyse procedures to determine M in order to assure an appropriate approximation.

7

and for the bootstrap test of Saikkonen & Lu¨tkepohl (2000b) if

M RSLT,1/M  ,

(3.5)

respectively. For M  , the expressions (3.4) and (3.5) converge to (3.2) and (3.3) respectively.
The systems cointegration tests are sequentially applied in order to determine the cointegrating rank. Usually, one starts with a cointegration rank of zero under the null hypothesis. The rank chosen is the one for which H0(r0) cannot be rejected the first time given a certain significance level . Although Algorithm 1 can shown to be consistent with respect to a specific rank null hypothesis, Swensen (2005a) was not able to prove that Algorithm 1 consistently estimates the cointegrating rank if it is sequentially applied in connection with the Johansen test. Therefore, he considers a modification of the algorithm. The problem to be dealt with is that the bootstrap samples have to be generated for other ranks than the true one if a sequence of tests is performed. We do not explore the issue of sequential testing in the current framework. Nevertheless, we also consider the second algorithm proposed by Swensen (2005a), since it is also valid for a specific rank hypothesis.

Algorithm 2.
(1) Estimate model (2.3) by a RR regression under the rank hypothesis H0(r0) : rk() = r0 in order to obtain the estimates ~, ~, and ~. The remaining parameters are estimated via OLS from an unrestricted version of (2.3) setting r = n. Let ^ and ^j (j = 1, . . . , p - 1) be the corresponding estimators.
(2) Check whether the roots of the equation det[A~(z)] = 0, where
A^(z) = (1 - z)In - ~~ z - ^1(1 - z)z - · · · - ^p-1(1 - z)zp-1,
are equal to 1 or outside the unit circle and whether ~~~ is nonsingular.
(3) If so, compute the unrestricted residuals ^p+1, . . . , ^T based on the OLS estimation in (1).
(4) Compute yt, t = p + 1, . . . , T, recursively from
p-1
yt = ^ + ~(~ yt-1 - ~(t - 1)) + ^jyt-j + t,
j=1
with sampled residuals t drawn with replacement from the estimated residuals ^p+1, . . . , ^T . The starting values of the recursion, y1, . . . , yp, are set equal to y1, . . . , yp.
8

Note, that the pseudo observations and sampled residuals have the same notational form as in Algorithm 1 in order to avoid the introduction of additional symbols. Keep in mind that ~^~ is nonsingular is true with probability 1, since the estimator ^ is based on the unrestricted OLS estimators ^j (j = 1, . . . , p - 1) (see Swensen 2005a). The bootstrap tests based on Algorithm 2 are abbreviated as GLS2(r0) and SLT2(r0)
Instead of considering Algorithms 1 and 2 one may also think of applying a bootstrap directly to (2.1). However, we have to remember that the unobserved error term xt follows a VAR process. If this is ignored such that no finite-dimensional parametric model is used in order to reduce the DGP to independent random sampling, bootstrapping time-series data involves a number of partly unsolved problem (see e.g. Ha¨rdle, Horowitz & Kreiß 2003, van Giersbergen 1996). Most of the problems arise because bootstraps are not able to exactly replicate the dependence structure in the data. This is even relevant for stationary time series. Nevertheless, we may also employ a different bootstrap scheme specific to the test by Saikkonen & Lu¨tkepohl (2000b). For example, one can base a bootstrap on the residuals from an estimation of model (2.7). The pseudo observation could then be generated via equations (2.1) and (2.2) using the relevant parameter estimates and exploiting the relationship t = Q t and the zero initial value assumption for xt. However, the simulation results for this algorithm are rather similar to the results for Algorithm 1. Moreover, the theoretical properties will be much more difficult to derive. Therefore, we do not consider this algorithm in more detail.

4 Asymptotic Distributions of Bootstrap Tests

In this section we discuss the asymptotic properties of the bootstrap tests SLT1(r0), SLT2(r0),

GLS1(r0) and GLS2(r0) by referring to weak convergence in Pr0,T probability. As described

by Swensen (2005a), this means that the conditional distributions converge in Pr0,T probability

given

the

observations.

That

is,

E
Pr0

,T

[f

(X



)]



EPr0,T [f (X)]

in

probability

for

all

bounded

continuous

functions

f,

where

E
Pr0 ,T

is

the

expectation

conditional

on

the

observations

with

respect

to

the

measure

Pr0,T

.

In

the

following,

we

use

E

as

an

abbreviation

for

E
Pr0 ,T

.

In the following, we describe how to obtain the asymptotic distributions of the SLT1(r0)

and GLS1(r0) test statistics making use of the results in Swensen (2005a), Saikkonen et al.

(2006), and Saikkonen & Lu¨tkepohl (2000b). To this end, it is helpful to briefly review the

proof for the asymptotic null distributions of SLT (r0) and GLS(r0). The proofs have mainly three steps.

First, it is shown that the estimators of the deterministic parameters µ^1, µ~0, and µ~1 have the

9

following properties.

 (µ^1 - µ1) = Op(T -3/2) (T - p)1/2(µ^1 - µ1) d N (0, CC )
 (µ~0 - µ0) = Op(T -1/2)

(µ~0 - µ0) = Op(1)  (µ~1 - µ1) = Op(T -3/2) (T - p)1/2(µ~1 - µ1) d N (0, CC ), where C = ()-1 as before. The precondition for this to hold is that the RR estimators based on the VECM (2.3) are consistently estimated with certain rates. Second, the convergence properties of µ^1, µ~0, and µ~1 assure that the adjusted series y^tc = yt - µ^1t and x~t = yt - µ~0 - µ~1t can appropriately be used instead of ytc and xt taking account of some adjustments. To be precise, an estimation error regarding the trend components remains in terms of the standardized sample moments involving y^tc and x^t. This is the reason why Brownian bridges instead of standard Brownian motions enter the limiting distributions in The-

orem 1. Furthermore, note that it suffices that the estimator µ~0 is only bounded in probability in the direction of  for proving the asymptotic properties of GLS(r0) (see Saikkonen & Lu¨tkepohl 2000b). Thirdly, the asymptotic distributions are proven as in the case of the stan-

dard Johansen test (compare Johansen 1995, Chs. 10, 11), but now with respect to (2.8) and

(2.10)respectively. Of course, one has to adjust the respective asymptotic expressions.

Saikkonen & Lu¨tkepohl (2000b) do not follow exactly the proof of Johansen (1995) regard-

ing the GLS test. Instead, they base their considerations on an auxiliary model related to an LM

cointegration test. However, the framework of Swensen (2005a) is the direct bootstrap coun-

terpart of Johansen (1995). Therefore, we follow Johansen (1995) in order to easily apply the

results of Swensen (2005a).

Lemma 1 in Swensen (2005a) shows that a Granger representation theorem exists for the

pseudo data generated by Algorithms 1 or 2 and that the random process ST (s) = (T -

p)-1/2

[sT ] i=k+1

i,

0



s



1,

converges

in

Pr0,T

distributions

toward

a

n-dimensional

Brown-

ian motion W with covariance matrix . Furthermore, according to Lemmas 2 and 3 in Swensen

(2005a), the relevant sample moments based on the bootstrapped series converge weakly in

Pr0,T probability to the same limit expressions as the sample moments defined in terms of the observations. Thus, the limiting distribution results for the asymptotic test also hold for the

bootstrapped Johansen test. Note that Lemmas 1-3 are actually proven in conjunction with Al-

gorithm 2. However, they are also valid in case of Algorithm 1 as described in Remark 9 of

10

Swensen (2005a).
The first important step in combining the results of Swensen (2005a) on the one side and
Saikkonen et al. (2006) and Saikkonen & Lu¨tkepohl (2000b) on the other side is to express the
bootstrap framework also in terms of the process xt and the deterministic components. This is necessary because the test procedures of Saikkonen et al. (2006) and Saikkonen & Lu¨tkepohl (2000b) involve xt and ytc and their sample analogs x~t and y^tc. Accordingly, the asymptotic derivations refer to these quantities. The goal is to obtain pseudo series xt and ytc such that yt = xt + µ0 + µ1t = ytc + µ1t. This allows one to apply the SLT and GLS test procedures to the pseudo data yt while having appropriate bootstrap versions of ytc and xt. The quantities µ0 and µ1 are estimators of µ0 and µ1 based on the original observations. They have to follow certain restrictions as described below. Conditional on the observations we may regard µ0 and µ1 as parameters.
In a second step, we have to derive Granger representations for xt and ytc. Based on these representations and the properties of the bootstrap estimators of µ0 and µ1, we can derive the limiting distributions of SLT1(r0) and GLS1(r0). The proofs have the same structure as in the case of the asymptotic test procedures and we will, in fact, obtain the same limiting distributions
as in Theorem 1. Let us now consider the bootstrap algorithm related to xt. Define the starting values xt =
yt - µ0 - µ1t for (t = 1, . . . , p). Then, based on x1, . . . , xp, the pseudo series xt is recursively generated according to

p-1
xt = ~~ xt-1 + ~jxt-j + t ,
j=1

t = p + 1, . . . , T,

(4.1)

where the same RR estimators ~, ~, ~i (i = 1, . . . , p), and sampled residuals t are used as for

producing yt. It can be shown that xt + µ0 + µ1t = yt (t = 1, . . . , T ) if µ0 and µ1 satisfy the

following conditions

~ = -~~ µ0 + ~µ1 = -~ + ~µ1 ~ = ~ µ1.

(4.2)

These conditions exactly correspond to the restrictions underlying the parameters  and 
of the VECM (2.3). Similarly, we can uniquely recover  = ~ µ0 and µ1 from the parameter estimates of (2.3). We obtain  = ~ = (~ ~)-1~ (~µ^1 - ~) and µ1 = µ^1 as defined in (2.9). However, µ0 is not identified in the direction of ~ by the conditions (4.2). This corresponds to
the fact that µ0 is not identified in (2.3). The reason is that  does not change if µ0 changes

11

(compare Saikkonen & Lu¨tkepohl 2000b). Accordingly, the bootstrap level parameter µ0 is not identified since the bootstrap algorithm 1 is based on the estimated VECM for yt. To obtain a unique µ0 and, thus, a unique version of xt we have to suggest an estimator for  = µ0, say ~. The asymptotic analysis presented later on shows that it is sufficient to consider an estimator
that is bounded in Pr0,T probability. The bootstrap parameter  can be estimated from the first p observations based on (2.7),
but only from the first p observations since H~0t~(~~)-1 = 0 for t  p + 1. Therefore, the estimator cannot be consistent but is bounded in Pr0,T probability (see further below). To be precise, (2.7) is rewritten as

Q~ A~(L)yt = Q~ H~0t~(~~)-1~µ0 + Q~ H~0t~(~ ~)-1~ µ0 + Q~ H~1tµ1 + ~t.
Following Saikkonen & Lu¨tkepohl (2000b, Proof of Theorem 1), the LS estimator ~ = ~µ~0 of  = ~µ0 is derived from this regression model. We do not give an explicit expression for ~ because it will look rather complicated. Since  is consistently estimated by ~, ~ can also be regarded as an estimator of . Using the decomposition (compare Lu¨tkepohl & Saikkonen 2000, Equation (4.6))

µ^0 = ~(~~)-1~ + ~(~ ~)-1~

(4.3)

a version of µ0 is obtained that satisfies (4.2). Furthermore, we define  = ~ = ~µ0 in line with  = ~ = ~ µ0. Hence, we have the following bootstrap model framework summarized in
Assumption 2.

Assumption 2. Let the bootstrap series yt be generated by yt = xt + µ0 + µ1t, t = 1, . . . , T , where xt is determined by (4.1). Moreover, µ0 = µ^0 and µ1 = µ^1 are explicit parameter vectors conditional on the observations which satisfy the conditions (4.2).

One may also generate yt by yt = ytc + µ1t using the pseudo series ytc. The latter one can be obtained via a corresponding VECM. This approach, however, is only useful with respect to the SLT  tests since the GLS-framework requires a bootstrap equivalent of µ0. Moreover, note that the deterministic parameters chosen for the initialization of the bootstrap series xt and used for yt = xt + µ0 + µ1t (t = 1, . . . , T ) are exactly the same. Initializing the bootstrap with the true parameters µ0 and µ1, i.e. using x1, . . . , xp as starting values, and adding the estimated deterministics µ0 + µ1t to xt does not produce yt (t = 1, . . . , T ) of Algorithm 1. Instead, some other bootstrap version of yt is obtained, which the test procedures would have been applied to.
12

However, this approach is not possible since we do not know x1, . . . , xp. Therefore, we refer to the algorithm described above, which is a feasible framework to generate bootstrap observations xt and ytc that have the following Granger representations stated in the next Lemma.

Lemma 1. Under Assumptions 1 and 2, the bootstrap elements xt and ytc have the following

representations

xt = C~

t

 i + T Rt0,T , t = p + 1, . . . , T,

i=p+1

(4.4)

and

ytc = C~

t

 i + 0 + T Rtc,T , t = p + 1, . . . , T,

(4.5)

i=p+1

where for all  > 0, P (maxp+1tT |Rtc,T | > )  0 and P (maxp+1tT |Rt0,T | > )  0 in Pr0,T probability as T  , | · | is the Euclidean norm, C~ = ~(~~~)-1~, and 0 = ¯~. Moreover, E[t t ] = ~ T   in Pr0,T probability as T  .

The proof of Lemma 1 in Appendix A shows that (4.4) is obtained from Swensen (2005a,

Lemma1) by setting the parameter estimates related to the deterministic terms to zero. This

is

the

case

although

~xp

enters

the

term

 T Rt0,T .

Note

that

~(xp

-

xp)

is

only

bounded

in

Pr0,T probability since xp = xp - (µ0 - µ0) - (µ1 - µ1)p and ~µ0 =  =  + Op(1) (see

Saikkonen & Lu¨tkepohl 2000b, Eq. (A2)). As explained in the proof, however, ~xp needs

only to be bounded in Pr0,T probability to assure that the result for Rt0,T holds. Finally, the

representation for ytc is derived by adding µ0 to xt.

Given Lemma 1, we can now discuss the derivation of the limiting distribution of the boot-

strap cointegration tests. Let us start with the SLT1 test. Applying this test procedure to the bootstrap data yt delivers the estimator µ^1 = ~(~ ~)-1~ + ~ (~ ~ )-1~ for the 'bootstrap trend parameter' µ1, where ~, ~ , ~, and ~ are RR estimators based on the VECM (3.1). Accordingly, the series ytc = yt - µ1t and y^tc = yt - µ^1t have their respective bootstrap counterparts defined by ytc = yt - µ1t and y^tc = yt - µ^1t. Consequently, ytc and y^tc replace ytc and y^tc in the proof of the limiting distribution. From these relations it is obvious that µ^1 is the relevant estimator of µ1 and that the asymptotic properties of µ^1 in relation to µ1 have to be
determined. To this end, we first derive the necessary asymptotic results for the RR estimators

of the parameters in the VECM (3.1) conditional on the observations, i.e. with respect to the

measure Pr0,T . Given that, we obtain the conditional convergence and distributional properties of the estimator µ^1 in relation to µ1. These properties corresponds exactly to the ones of µ^1 re-

13

garding µ1. Hence, we can appropriately work with the adjusted bootstrap series y^tc and apply

an LR-type test to

p-1
y^tc = ~(~ y^tc- 1 - ) + ~jy^tc- j + etc,
j=1

(4.6)

where etc is an error term defined after (A.40) in Appendix A. Then, using Lemma 1 and the

results of Swensen (2005a) we derive the asymptotic properties of the relevant quantities re-

garding the measure Pr0,T . To this end, we follow again the setup of Johansen (1995, Chs. 10, 11) and obtain the limiting distribution given in Proposition 1. The details of the proof can be

found in Appendix A.

Proposition 1. Under Assumptions 1 and 2 and if H0(r0) is true, then
Pr0,T (SLT1(r0)  x) - P (tr(DSLT )  x),
in Pr0,T probability for all x as T  , where DGLS and DSLT are defined as in Theorem 1.
Let us turn to the GLS1 test. First, we have to discuss whether the inconsistency of ~xp with respect to ~xp and the inconsistent estimation of the level of yt have an impact on the limiting distribution of GLS1(r0). Given that ~xp is bounded in Pr0,T probability it should be clear that it is bounded conditionally on the observations even if T  . In addition, we will not be able to estimate the level of yt consistently in the direction of ~ conditional on the observations, i.e. ~ = ~ µ~0 is only a bounded estimator of  in Pr0,T probability. Thus, with respect to the estimated version of xt , say x~t, an error that is bounded conditional on the observation and a bounded error in Pr0,T probability are added up: the estimation errors regarding ~(xp - xp) and ~ (x~t - xt). However, this does not cause additional problems since the joint error remains bounded in Pr0,T probability. In this case, the applied normalizations of the sample moments involving ~x~t assure that the joint error has no asymptotic effect conditional on the observations. Accordingly, the limit expressions of the respective sample moments regarding the measure Pr0,T are not either affected. Hence, it suffices if the estimation errors regarding the levels of yt and ~yt are bounded in Pr0,T and Pr0,T probability respectively.

14

Applying the GLS test procedure to the bootstrap data results in the following framework

yt = µ0 + µ1t + xt , A~(L)yt = A~(L)µ0 + A~(L)µ1t + t , Q~ A~(L)yt = Q~ H~0tµ0 + Q~ H~1tµ1 + t, Q~ A~(L)yt = Q~ H~0tµ0 + Q~ H~1tµ1 + ~t,

t = Q~ t,

(4.7) (4.8) (4.9) (4.10)

which comprises the bootstrap analogues to (2.1), (2.5), (2.6) and (2.7) respectively. The error
terms ~t are defined in Appendix A after equation (A.50). Note, that we apply the estimators Q~, A~(L), H~it (i = 1, 2) in (4.10), which are computed using the generated pseudo data. Thus, we estimate µ0 and µ1 from (4.10) and denote the respective estimators by µ~0 and µ~1.
The time index for (4.7)-(4.10) covers t = 1, . . . , T . Thus, (4.8) is another representation
of the pseudo observations with respect to the bootstrap error terms t (t = p + 1, . . . , T ) of Algorithm 1 and the implicit bootstrap error terms 1, . . . p. The latter are obtained by assuming xt = 0 for t = 0, . . . , -p + 1. In fact, we require bootstrap error terms for the first p data points and need to know their properties in order to derive the properties of the estimator ~ conditional on the observations. Corresponding to the setup of the asymptotic test, the bootstrap level parameter can be estimated in the direction of ~ only from the first p
(transformed) bootstrap observations. Given that

xt = xt - (µ0 - µ0) - (µ1 - µ1)t, t = 1, . . . , p,

(4.11)

and the zero initial value assumption for both xt and xt , we have

A~(L)xt = t = A~(L)xt - H~0t(µ0 - µ0) - H~1t(µ1 - µ1), = t - (A(L) - A~(L))xt - H~0t(µ0 - µ0) - H~1t(µ1 - µ1).

t = 1, . . . , p

(4.12)

Obviously, the implicit bootstrap error terms 1, . . . , p are not random conditional on the observations although their true values are not known. Hence, E(t ) = t = 0 in general for t = 1, . . . , p. However, the non-randomness should not cause any problems for two reasons. First, we only require ~ to be bounded in Pr0,T probability. Note that ~ depends on 1, . . . , p but is still random conditional on the observations because the transformation in (4.10) involves estimators that are random conditional on the observations. Second, ~ has no effect on the asymptotic properties of the other estimators ~ µ~0 and µ~1 (compare Saikkonen & Lu¨tkepohl 2000b, Proof of Theorem 1). Therefore, we can ignore the first p observations regarding these
estimators.

15

Now, the proof regarding GLS1(r0) runs as follows. Based on the necessary asymptotic results for the RR estimators of the parameters in the VECM (3.1) we obtain the conditional con-
vergence and distributional properties of the estimators µ~0 and µ~1. These properties correspond exactly to the ones of µ~0 and µ~1 in relation to µ0 and µ1, e.g. ~(µ~0 - µ0) = Op(1) is obtained. Hence, we can appropriately work with the adjusted bootstrap series x~t = yt - µ~0 - µ~1t and apply an LR-type test to

p-1
x~t = ~~ x~t-1 + ~jx~t-j + et,
j=1

(4.13)

where et is an error term defined in the Appendix. Then, as for SLT1(r0), we refer to Lemma 1 and the results of Swensen (2005a) and obtain the asymptotic properties of the relevant quantities regarding the measure Pr0,T leading to

Proposition 2. Under Assumptions 1 and 2, assuming xt = 0 for t = 0, . . . , -p + 1, and if H0(r0) is true, then
Pr0,T (GLS1(r0)  x) - P (tr(DGLS)  x),
in Pr0,T probability for all x as T  , where DGLS and DSLT are defined as in Theorem 1.

The details of the proof of Proposition 2 are given in Appendix A. When applying Algorithm 2 we have to use the unrestricted estimators ^ and ^i (i =
1, . . . , p - 1) and the residuals ^t in order to generate xt . Furthermore, the conditions for the estimators  and µ1 have to be adjusted. The first equation of (4.2) now reads as ^ = -~ + ^µ1 such that ^ = (~ ~)-1~ (^µ^1 - ^) can be used for  and to obtain an appropriate version of
µ0. Accordingly, we adjust Assumption 2 and Lemma 1. Otherwise, we go through to the same steps as above to show that SLT2(r0) and GLS2(r0) have the same limiting distribution as SLT1(r0) and GLS1(r0) respectively.

5 Monte Carlo Study
In this section we analyse and compare the small sample properties of the bootstrap and asymptotic test versions. To this end, we first use a VAR(1) process suggested by Toda (1994, 1995), which was also applied by a couple of other authors in order to study the properties of cointegration tests (e.g. Hubrich et al. 2001, Trenkler 2002). The process has the following structure

16



xt = A1xt-1

+ t

=

A 0

   

0 In-r

xt-1 + t,

t



iid

N 

0 0

 ,

Ir 

   , In-r

(5.1)

where A = diag(a1, . . . , ar) is a (r×r) diagonal matrix and  is a (r×(n-r)) matrix describing the correlation between the stationary and nonstationary components. Other VAR(1) processes of interest can be obtained from (5.1) by linear transformations which leave the analysed LR tests invariant (see Toda 1994, 1995). Specifically, I will consider bivariate processes. In this case, if r = 0, A and  vanish and the process consists of two nonstationary components. If the cointegrating rank is 1, A = a1 with |a1| < 1 and  =  is a scalar which represents the instantaneous correlation between the two components.
In the simulations the parameter values of the constant and the linear trend are zero, i.e., µi = 0 (i = 0, 1) throughout. This is done because the results of the tests, with the exception of the bootstrap tests based on Algorithm 2, are invariant to the specific parameter values of the deterministic components they allow for. The bootstrap tests based on Algorithm 2 are not invariant to the value of µ0 since the constant is estimated using an unrestricted VAR model. In other words, the constant is not estimated under the appropriate rank specification so that changes in µ0 are not perfectly captured by the corresponding estimator. Nevertheless, the differences in the results in response to changes in µ0 are very small. Therefore, we keep yt = xt for all tests in the simulations.
The process (5.1) is a rather simple one. On the one hand, this allows to control the parameters of interest and, thereby, the properties of the DGP rather easily. On the other hand, (5.1) may be a quite unrealistic setup compared to empirical applications where richer autoregressive dynamics and error term cross-correlation structures are often present. Therefore, we use a databased DGP in order to study the test properties for more complex processes. Our DGP refers to an empirical study by King, Plosser, Stock & Watson (1991), in which they analyse a small macroeconomic model for the U.S.. It consists of per-capita private real GNP (total GNP less real total government purchases of goods and services), per-capita real consumption, and percapita gross private domestic fixed investment. Based on their recommendations we estimate a VECM with one lag and two cointegrating relationships using quarterly data in logarithms for the period 1949:1-1988:4. In other words, we consider a three-dimensional VAR(2) model. The two cointegrating relations describe the differences between GNP on the one side and consumption and investment on the other side. Hence, we specify restricted cointegrating relationships as implied by the real-business-cycle model to which King et al. (1991) refer. In order to re-

17

duce the impact of estimates of possibly insignificant parameters, a subset VECM with zero restrictions has been estimated. The subset restrictions have been searched for by a Top-Down strategy employing the Akaike Information Criterion (AIC) (see e.g. Bru¨ggemann 2004, Chapter 3). Using the AIC avoids the occurrence of too many zero restrictions. All computations have been done with the econometric software program JMulTi (Lu¨tkepohl & Kra¨tzig 2004). We obtain the following estimated process, which is used for the simulations4

 

-0.038

0

 -0.026





 0

 0 0.154

yt = -0.186+0.217

-0.150·01

0 1

--11yt-1 +

0

0.282 0.660yt-1

0.032

0.126 0

0.272 0.162 0

+ t,

 



0 0.588 t  iid N 0 , 10-4 0.821

0 0.465

0.821 4.870 1.688

 01..466858 . 1.376

(5.2)

The computations for the simulation study are performed by using programs written in GAUSS V6 for Windows. The RNDNS function with a fixed seed has been used to generate standard normally distributed random numbers. The number of replications is R = 1000, i.e. we generate 1000 different sets of multivariate random numbers. Note that we use the same random numbers for each set of parameter values. Accordingly, the test statistics are applied to the same time series for a given set of parameter values. Thus, the rejection frequencies reported later on are not independent, but they can be compared directly. We consider sample sizes of T = 50, T = 100, and T = 200. In addition to these sample sizes we use a zero starting value for the two components of the Toda processes (5.1). With respect to the data-dependent KPSW-DGP (5.2) two starting values for each of the three series are taken from the original data.
The simulations are performed with respect to 1%, 5%, and 10% significance levels. However, the presentation focuses on the 5% level, since the relative results are very similar for the other significance levels. The rejection frequencies of the asymptotic GLS and JOH tests are based on asymptotic critical values computed from response surfaces given in Trenkler (2004) and Doornik (1998) respectively. These response surfaces make use of the fact that the asymptotic null distributions can be quite well approximated by Gamma distributions. The coefficients of a corresponding response surface for the SLT test are presented in Appendix B.
4For the simulation program the parameters are stored as 8-bit, double precision. This assures that the characteristic roots obey the restrictions imposed on the DGP.

18

We use the response surface critical values, since they deliver a more accurate approximation of

the tests' asymptotic properties than the standard tabulated critical values (see Doornik 1998).

With respect to the bootstrap tests, we use the upper %-quantile ( = 1, 5, 10) of the empirical

bootstrap distribution based on M = 1000 bootstrap replications. We hope that this number of

replications is large enough for a reasonable approximation of the relevant bootstrap distribu-

tions Fr0;GLS,1, Fr0;SLT,1, etc.. Note, that the tests were not performed sequentially in the simulations. Thus, the results for

testing H0(1) : rk() = 1 are not conditioned on the outcome of the test of H0(0) : rk() = 0 etc.. Finally, we do not report size adjusted rejection frequencies with respect to the power of

the tests since such an adjustment is not possible in applied work.

The results for different versions of the bivariate Toda-DGP (5.1) are collected in Tables 1

and 2 and for the data-dependent KPSW-DGP (5.2) in Table 3. Tables 1 and 3 (Panel A) contain

the rejection frequencies of the tests for a correct null hypothesis. We will use the term size with

respect to these frequencies, although this use does not coincide with the exact definition of

the size of a test. The definition would require to maximize the power function over the whole

parameter space associated with the null hypothesis. This parameter space contains the nuisance

parameters in the test statistics' small sample distributions. However, such a maximization is

not done in our simulation framework.

In order to evaluate whether the tests keep the nominal size and whether the rejection fre-

quencies of two tests are significantly different we may refer to an approach suggested by

Paruolo (2001). He notes that for an increasing number of replications R, the Monte Carlo

rejection frequencies of two tests (p^1, p^2) converge to a bivariate normal distribution

   

  



 R

p^1

-

p1

d

N

0

,

p1(1

-

p1)

  ,

p^2 p2

0  p2(1 - p2)

(5.3)

where p1 and p2 are the true rejection probabilities of the tests with respect to the DGP considered. These are, in fact, the probabilities we aim to approximate by the Monte Carlo experiment. For the covariance we have  =  - p1p2, where  is the probability of joint rejections of both tests.
Based on (5.3) we test whether the tests keep the nominal size of 5%. It follows that the standard error of the estimator p^1 for the true rejection probability p1 is sp1 = p1(1 - p1)/R. Thus, if a Monte Carlo rejection frequency for a true null rank hypothesis is outside the band [0.0365, 0.0635], it is not in line with the nominal size level of 5%.

19

We also use (5.3) for a comparison of the rejection frequencies of the corresponding asymp
totic and bootstrap tests. First note that R(p^1 - p^2) has an asymptotic variance equal to p1(1 - p1) + p2(1 - p2) - 2 . Using the Monte Carlo rejection frequencies p^1, p^2, and ^ as (consistent) estimators we test whether p1 and p2 are equal by considering the statistic

z=

R p^1(1 - p^1) + p^2(1 - p^2) - 2^

1
2 (p^1 - p^2) d N (0, 1),

(5.4)

(see Paruolo 2001). We can apply (5.4) because all tests are based on the same generated time series for a given set of parameter values. However, (5.3) and (5.4) ignore the uncertainty regarding the computation of the critical values based on the bootstrap and response surface simulations. Therefore, we should regard (5.4) only as a test approximation, since we are not able to incorporate the mentioned simulation uncertainty into the statistic z.
To decide whether a bootstrap test has a preferable small sample power, we just have to test whether its power is significantly higher than the small sample power of the corresponding asymptotic test version. Of course, a significant deterioration is tested for in an analogous way. In order to study the relative size properties we have to take account of the nominal significance level as the benchmark. Referring to a 5% nominal level, bootstrapping significantly reduces the difference between the true and nominal probabilities of rejecting a correct null hypothesis in the following three cases.
a) Both size values are not larger than 0.05. The size of the bootstrap test is significantly higher than the size of the asymptotic test.
b) Both size values are not smaller than 0.05. The size of the bootstrap test is significantly lower than the size of the asymptotic test.
c) One size value is above and the other one is below 0.05. The size values differ significantly and the size of the asymptotic test is significantly different from the nominal level while the size of the bootstrap test is not.
In c) one may skip the requirement that the size values differ significantly. Of course, one can setup a corresponding set of conditions for a significant deterioration in the tests' sizes. We apply a 5% level to test for significant differences between two rejection frequencies.
With respect to the Toda-DGPs we see in Table 1 that significant improvements in the tests' sizes are possible if the bootstrap test versions are applied. This is especially apparent for processes with a true cointegrating rank of r = 1 for which the sizes of the asymptotic tests quite
20

Table 1. Rejection Frequencies of Tests for Bivariate Toda-DGP (5.1), VAR Order p = 1, Nominal Significance Level 0.05

21

GLS GLS1 GLS2
SLT SLT1 SLT2
JOH J OH1 J OH2

Panel A: a1 = 1 (r = 0), H0 : r0 = 0

T = 50 0.049 0.047 0.050
0.058 0.052+ 0.052+
0.060 0.052+ 0.056

T = 100 0.041 0.039 0.039
0.064 0.056+ 0.058+
0.052 0.048 0.053

T = 200 0.048 0.052 0.050
0.059 0.056 0.055
0.067 0.062+ 0.062+

Panel B: a1 = 0.9 (r = 1), H0 : r0 = 1

T = 50 0.019 0.028+ 0.042+
0.031 0.028 0.026-
0.021 0.020 0.018

T = 100 0.023 0.040+ 0.060+
0.049 0.042- 0.042-
0.034 0.032 0.030

T = 200 0.025 0.040+ 0.072
0.066 0.059+ 0.058+
0.069 0.064+ 0.062+

Panel C: a1 = 0.7 (r = 1), H0 : r0 = 1

T = 50 0.047 0.056 0.067-
0.067 0.061+ 0.057+
0.066 0.051+ 0.055+

T = 100 0.034 0.042+ 0.043+
0.046 0.042 0.044
0.063 0.056+ 0.055+

T = 200 0.047 0.057 0.063
0.062 0.058+ 0.060
0.070 0.067 0.065+

Note: Bold entries indicate significant size differences with respect to a nominal level of 5%. Significant improvements of the bootstrap tests' rejection rates with respect to the corresponding asymptotic tests' rejections are indicated by + and significant deteriorations by -. Significance refers to a 5% level. The tests are based on an approximation ignoring the uncertainty involved in the bootstrap simulations and the computation of the critical values.

Table 2. Rejection Frequencies of Tests for Bivariate Toda-DGP (5.1) with True Cointegrating Rank r = 1, Rank under H0 is r0 = 0, VAR Order p = 1,  = 0.8, Nominal Significance Level 0.05

Panel A: a1 = 0.9

Panel B: a1 = 0.7

GLS GLS1 GLS2
SLT SLT1 SLT2
JOH J OH1 J OH2

T = 50
0.153 0.152 0.155
0.175 0.161- 0.155-
0.138 0.123- 0.126-

T = 100 T = 200

0.474 0.480 0.471

0.928 0.929 0.927

0.518 0.503- 0.501-

0.963 0.961 0.959

0.395 0.387 0.375-

0.929 0.928 0.928

T = 50
0.722 0.722 0.723
0.801 0.783- 0.778-
0.738 0.715- 0.720-

T = 100 T = 200

0.964 0.963 0.963

1.000 1.000 1.000

0.993 0.992 0.993

1.000 1.000 1.000

0.998 0.997 0.998

1.000 1.000 1.000

Note: Significant improvements at the 5% level of the bootstrap tests' rejection rates with respect to the corresponding asymptotic tests' rejections are indicated by + and significant deteriorations by -. The tests are based on an approximation ignoring uncertainty involved in the bootstrap simulations and the computation of the critical values.

often deviate significantly from the 5% level (see Panels B and C in Table 1). By contrast, for DGPs with r = 0 the sizes of the asymptotic test are much more in line with the nominal level. Nevertheless, we find that the bootstrap tests reject sometimes too often such that significant deteriorations may occur as in case of GLS2 (see Panels B and C in Table 1).
We have also performed simulations with a1 equal to 0.8, 0.6 and 0.5 in order to evaluate whether the parameter a1 has a systematic impact on whether and how the application of bootstrap tests may improve the tests' sizes. Our overall conclusion is that the values of a1 seem to be less important in these respects than e.g. the sample size T . Interestingly, the most pronounced improvements occur for T = 50 in case of SLT1, SLT2, JOH1, and JOH1, although we also observe a number of significant improvements in case of T = 200. Regarding GLS1 and GLS2 most of the improvements are observed for T = 100, while GLS2 tend to overreject for larger values of a1.
Table 2 displays the small sample power for the case of a true cointegrating rank of r = 1 (a1 = 0.9 and a1 = 0.7) if the null hypothesis H0(r0) : r0 = 0 is tested. It seems to be

22

obvious that no gain in small sample power should be expected when applying the bootstrap tests. In case of SLT1, SLT2, JOH1, and JOH1 even significant reductions occur. Of course, a size adjustment would slightly improve the relative performance of the bootstrap test versions, since their sizes for the corresponding DGPs are in general a bit lower than the ones of the asymptotic tests (compare Table 1, Panel A). Interestingly, the instances of significant size reductions regarding SLT1, SLT2, JOH1, and JOH1 correspond quite well to the cases where significant power losses occur.
The relative performance of the bootstrap tests becomes somewhat worse if we consider the results for the data-dependent KPSW-DGP (5.2), which are summarized in Table 3. In particular, bootstrap Algorithm 2 produces inferior results both for the size and the small sample power. This is especially apparent for GLS2, which suffers from excessive size distortions and a very low power. A closer inspection of the respective bootstrap iterations points to problems with the estimation of the deterministic terms. The adjusted series have a mean clearly different from zero. Compared to the simpler Toda-DGPs and the asymptotic GLS test these deviations are much more pronounced. That may explain the poor performance of GLS2. Nevertheless, the size for GLS1 are preferable compared to GLS. This also applies to the small sample power if the case r0 = 1 (Panel B) is considered.
The respective sizes of the SLT and SLT1 and the JOH and JOH1 tests are approximately the same. In general, we observe a lower power level for the bootstrap tests. In case of SLT2 and JOH2 the lower power might be attributable to their lower sizes. However, taken into account the results for SLT1 and JOH1 the size differences seem only to explain some portion of the drop in the powers of the tests based on Algorithm 2.
Comparing the different test proposals we find that no test outperforms the other ones. The only notable exception might be that SLT and JOH are generally preferable to GLS in terms of the small sample power. This also applies to the corresponding bootstrap test versions.
Finally, we study the correlation between the asymptotic and bootstrap tests' decisions. This should help to answer the question whether applied researchers have to expect mixed signals when using both the bootstrap and the asymptotic test versions. To asses the relationship between the rejections of two tests we first compute the proportion of joint rejections (^) with respect to the lower rejection frequency, p^l = min(p^1, p^2). Hence, we obtain the ratio r^ = ^/p^l, which measures the degree to which the higher rejection frequency of one test is due to additional rejections. Thus, a low value of r^ means that the test with the lower number of rejections rejects the null hypothesis in many simulation iterations in which the other test does not.
23

Table 3. Rejection Frequencies of Tests for KPSW-DGP (5.2) with True Cointegrating Rank r = 2, VAR Order p = 2, Nominal Significance Level 0.05

24

GLS GLS1 GLS2
SLT SLT1 SLT2
JOH J OH1 J OH2

T = 50 0.001 0.023+ 0.067
0.011 0.009 0.001-
0.004 0.003 0.000-

Panel A: H0 : r0 = 2
T = 100 0.006 0.027+ 0.113
0.025 0.027 0.009-
0.010 0.011 0.006-

T = 200 0.017 0.043+ 0.083
0.051 0.046 0.027-
0.029 0.028 0.013-

T = 50 0.022 0.033+ 0.000
0.058 0.026- 0.013-
0.039 0.014- 0.007-

Panel B: H0 : r0 = 1
T = 100 0.040 0.062+ 0.000-
0.122 0.096- 0.059-
0.097 0.077- 0.044-

T = 200 0.074 0.118+ 0.010-
0.357 0.338- 0.314-
0.366 0.332- 0.299-

T = 50 0.172 0.100- 0.000-
0.276 0.126- 0.109-
0.285 0.109- 0.099-

Panel C: H0 : r0 = 0
T = 100 0.487 0.423- 0.008-
0.630 0.498- 0.497-
0.596 0.485- 0.461-

T = 200 0.939 0.930- 0.362-
0.990 0.982- 0.981-
0.994 0.991 0.992

Note: Bold entries indicate significant size differences with respect to nominal level of 5% (Panel A). Significant improvements of the bootstrap tests' rejection rates with respect to the corresponding asymptotic test rejections are indicated by + and significant deteriorations by -. Significance refers to a 5% level. The tests are based on an approximation ignoring uncertainty involved in the bootstrap simulations and the computation of the critical values.

Table 4. Relationship Between Test Rejections for Bivariate Toda-DGP with True Cointegrating Rank r = 1 (a1 = 0.9), VAR Order p = 1,  = 0.8, Nominal Significance Level 0.05

H0 : r0 = 1

H0 : r0 = 0

r^/^ T = 50 T = 100 T = 200

GLS/GLS1 GLS/GLS2 GLS1/GLS2
SLT /SLT1 SLT /SLT2 SLT1/SLT2
J OH /J OH1 J OH /J OH2 J OH1/J OH2

1.00/0.82 1.00/0.67 1.00/0.81
0.96/0.91 0.96/0.88 0.89/0.85
0.95/0.93 0.94/0.87 0.89/0.84

1.00/0.75 1.00/0.61 0.95/0.77
1.00/0.92 1.00/0.92 0.98/0.98
0.97/0.94 0.90/0.84 0.87/0.83

1.00/0.78 1.00/0.56 0.93/0.67
0.97/0.91 0.97/0.90 0.97/0.96
1.00/0.96 1.00/0.94 0.97/0.95

T = 50 T = 100 T = 200
0.95/0.93 0.99/0.96 1.00/0.96 0.95/0.94 0.98/0.96 1.00/0.96 1.00/0.97 0.99/0.96 0.95/0.93
1.00/0.94 0.98/0.94 1.00/0.95 1.00/0.93 0.99/0.94 1.00/0.90 0.96/0.93 0.98/0.96 1.00/0.92
0.96/0.93 0.98/0.96 1.00/0.93 0.98/0.92 0.99/0.94 1.00/0.95 1.00/0.94 0.99/0.95 0.98/0.96

Note: The first entry measures the relative portion of joint rejections with respect to the test with the lower rejection rate (r^). The second entry describes the correlation between the rejections (^).

The second measure we consider is the correlation of the rejections of two tests given by
^ = ^ . p^1(1 - p^1) · p^2(1 - p^2)
The ratio ^ can also be regarded as an estimator of the limit correlation of the rejection frequencies p^1 and p^2 (compare (5.3)). The correlation measure ^ automatically takes account of differing rejection frequencies. To be precise, ^ < 1 if p^1 = p^2. Note that ^ might get very small if p^1 and p^2 are both close to 1 and ^ is just slightly smaller than min(p^1, p^2).
In Tables 4 and 5 we report figures for r^ and ^ for some representative cases. We obtain the following main results. The correlations of the rejections of GLS1, SLT1, SLT2, JOH1, and JOH2 with the rejections of the corresponding asymptotic tests are generally quite high, i.e. above 0.8. In case of the KPSW-DGP (5.2) values below 0.60 can occur for the correlation measure ^ owing to the differing rejection frequencies of asymptotic and bootstrap tests. The rejections of GLS2 are strongly correlated with the rejections of GLS for the Toda-DGPs. However, the correlations dramatically fall when using the KPSW-DGP.
The correlations between the rejections of the bootstrap tests based on Algorithms 1 and 2 are usually quite large as long as the rejection frequencies are close to each other. If they deviate, it is ^ that falls more than r^. Hence, higher rejections of one test are due to additional
25

Table 5. Relationship Between Test Rejections for KPSW-DGP with True Cointegrating Rank r = 2, Rank under H0 is r0 = 1, VAR Order p = 2, Nominal Significance Level 0.05

r^/^
GLS/GLS1 GLS/GLS2 GLS1/GLS2
SLT /SLT1 SLT /SLT2 SLT1/SLT2
J OH /J OH1 J OH /J OH2 J OH1/J OH2

T = 50
0.96/0.77 0.00/ -- 0.00/ --
1.00/0.66 1.00/0.46 0.92/0.65
1.00/0.59 1.00/0.42 1.00/0.71

T = 100
1.00/0.79 0.00/ -- 0.00/ --
0.99/0.86 1.00/0.67 1.00/0.77
0.99/0.87 1.00/0.66 1.00/0.74

T = 200
0.99/0.76 0.60/0.20 0.70/0.18
1.00/0.96 1.00/0.91 1.00/0.94
0.99/0.92 1.00/0.86 0.99/0.91

Note: The first entry measures the relative portion of joint rejections with respect to the test with the lower rejection rate (r^). The second entry describes the correlation between the rejections (^).

rejections. However, the correlations of GLS1 and GLS2 are very low for the KPSW-DGP in any case. To sum up, corresponding asymptotic and bootstrap tests do not differ systematically in their test decisions. However, the applied researcher has to be aware that there are situations where disagreements may occur.
Finally, we relate our findings to the results of the simulation studies performed by Swensen (2005a, 2005b) in terms of the Johansen test. Swensen (2005a) considers a five-dimensional VAR(1) process with one cointegrating relation as DGP, which is similar to a five-dimensional version of the Toda process (5.1). This process has also been employed by Johansen (2002). In addition, Swensen (2005b) also employs the Toda process and another DGP that has been used in the literature before. There are a couple of issues worth mentioning.
First, the results of Swensen (2005a, 2005b) on the tests's sizes are slightly more encouraging for the bootstrap tests and, second, he found the differences between the Algorithms 1 and 2 to be rather small. These findings may be DGP-specific. Nevertheless, our results are in line with these findings if we focus our attention on the simple Toda-DGP and the JOH- and SLT -tests. In fact, most of the deteriorations with respect to the bootstrap tests and most of the pronounced differences between the algorithms are confined to the more complex KPSWDGP (5.2). Probably, it is rather difficult to replicate the autoregressive structure of this DGP by appropriate parameter estimates. As a consequence of this, we may obtain autocorrelated

26

residuals. It is known from other studies that this negatively affects the bootstrap tests (compare e.g. Ha¨rdle et al. 2003, MacKinnon 2002).
However, our results regarding the loss in small sample power and the rather high correlations between the asymptotic and bootstrap test decisions are confirmed by the findings of Swensen (2005a, 2005b). Performing a sequential test application, Swensen (2005a, 2005b) figured out that the asymptotic tests may found more often the true cointegrating rank than the bootstrap tests, although the latter have superior size properties. In other words, the bootstrap test's sequence terminates too early due to a lower small sample power.
6 Summary
In this paper we have analysed two recursive bootstrap procedures for systems cointegration tests with a prior adjustment for deterministic terms suggested by Saikkonen & Lu¨tkepohl (2000b) and Saikkonen et al. (2006). Their small sample properties are studied and compared to the properties of the corresponding asymptotic tests within a simulation study. This simulation study also includes the Johansen cointegration test.
Our overall conclusions are that bootstrap methods can help to improve the size properties of systems cointegration tests in small samples if simple processes are considered. However, in case of more complex DGPs the situation is less favourable. With respect to the tests' small sample power no increase should be expected. In fact, we often observe significant reductions in the small sample power if the bootstrap test versions are applied. Our mixed findings correspond quite well to the outcomes of different simulation studies on the Johansen cointegration test. While Swensen (2005a, 2005b) also detects size improvements for the bootstrap test versions in case of DGPs with a simple autoregressive structure, van Giersbergen (1996) and Harris & Judge (1998) report deteriorations in terms of small sample size and power for more complex DGPs. Moreover, some of our findings seem to support a remark of Johansen (2002, p. 1930). He does not necessarily expect an improved approximation of the tests' distributions by the use of bootstrap methods. The reason is the nonuniformity in the convergence with respect to the nuisance parameters in the null distributions of the test statistics. By contrast, Johansen (2002) proposes a small sample correction following Bartlett (1937). However, Swensen (2005a) has shown that bootstrap approximations work better than the correction suggested by Johansen (2002) in a number of situations.
When using bootstrap tests, we have a clear preference for bootstrap Algorithm 1, in which all parameters are estimated imposing the cointegrating rank under the null hypothesis. In case
27

of the more complex data-dependent DGP, the tests based on Algorithm 1 clearly outperform the corresponding versions related to Algorithm 2 with respect to the small sample properties. The latter algorithm uses residuals and estimators for the short-run parameters from an unrestricted VAR. Furthermore, we have found that the rejections of corresponding bootstrap and asymptotic tests are usually strongly correlated.
It should be rather straightforward to adjust our asymptotic results such that different assumptions about the presence of deterministic terms can be made. Hence, one may apply valid bootstrap versions of cointegration tests considered by Saikkonen & Lu¨tkepohl (2000a), which allow for level shifts at known time, or of a test procedure excluding a linear trend as e.g. suggested by Saikkonen et al. (2006). Finally, an extension of the bootstrap algorithms in order to consistently determine the cointegration rank may be conducted in future research. The asymptotic results of Swensen (2005a) regarding the Johansen test indicate that such a consistent sequential procedure can also be found for tests with a prior adjustment for deterministic terms. However, a sequential bootstrap procedure would probably not work better than the asymptotic version due to the inferior small sample power of the bootstrap tests.
van Giersbergen (1996) pointed out the difficulties of both bootstrap and asymptotic tests to deal with autocorrelated error terms in case of an inappropriate VAR order choice. Given that, a more promising line of research may be to design and evaluate an overall bootstrap procedure for choosing the lag order and the cointegrating rank. The hope is to improve the model specification resulting in a more reliable inference on the cointegrating rank.

Appendix A: Proofs

Proof of Lemma 1

According to Lemma 1 and Remark 9 of Swensen (2005a) there exists a Granger representation for yt, which is given by

yt = C~

t

 i + ~(t - p) + T Rt,T , t = p + 1, . . . , T,

i=p+1

(A.1)

where

C~

=

~(~~~)-1~,

~

=

C~(~

+

~~) - (C~~

-

I )~¯~,

and

 T Rt,T

is

defined

in

(A.4).

Note that xt and yt are generated using the same bootstrap residuals and parameter estima-

tors. Therefore it is sufficient to show that the deterministic part µ0 + µ1t can be extracted from

28

(A.1) in order to prove (4.4) of Lemma 1. Hence, we have to show

xt + µ0 + µ1t = C~

t

 i + T Rt0,T + µ0 + µ1t, t = p + 1, . . . , T,

i=p+1

(A.2)

 where yt = xt + µ0 + µ1t and T Rtc,T as defined in (A.9). Then, we can subtract µ0 + µ1t

from both sides of (A.2) and obtain the Granger representation for xt as proposed in Lemma 1.

In the following, we first repeat some steps of the proof in Swensen (2005a) in order to

introduce a number of expressions in line with the notation used in this paper. Let Y~t = (yt~, yt ~) . Then, according to Johansen (1995) and Swensen (2005a), the recursion de-

fined in Algorithm 1 can be stated as

A~~(L)Y~t = (~¯, ¯~) (t + ~ + ~~ - ~~t)

for some suitable lag polynomial A~~(L). Further, following the arguments in Swensen (2005a),

we can define C~~(z) = A~~(z)-1 and obtain



Y~t

=

 ~ yt  ~yt

=

A~~(L)-1(~¯,

¯~)

(t

+

~

+

~~

-

~~t)

= C~~(L)(¯~, ~¯) (t + ~ + ~~ - ~~t) = C~~(1)(¯~, ~¯) (t + ~ + ~~ - ~~t) + C~~#(L)(¯~, ¯~) (t + ~ + ~~ - ~~t).

(A.3)

The derivation of the Granger representation for yt is based on the decomposition

yt = ~¯~ t yi + ~¯~ yt + ~¯~yp.
i=p+1

Using this equation in connection with (A.3), Swensen (2005a) derives the expressions for ~

and the remainder term. In our notation we have for the latter one

 T Rt,T

=

(0, ¯~)C~~#(L)(~¯, ~¯) (t

- p) + (¯~, 0)C~~(1)(¯~, ~¯) t

+ (~¯, 0)C~~#(L)(¯~, ~¯) t

+ (¯~, 0)C~~(1)(~¯, ¯~) (~ + ~~) - (~¯, 0)C~~#(1)(~¯, ¯~) ~~

+ p~¯~ + ~¯~yp.

(A.4)

Thus, in order to derive (4.5) from (A.1) we have to analyse the last four terms of (A.4) and
~(t - p), which all involve deterministic components. Since we assume that µ0 and µ1 satisfy the conditions (4.2) the estimators ~ and ~ are replaced accordingly.

29

Let us start with the term ~(t - p). Given the definition of ~ and the conditions (4.2) we

obtain

~ = C~(~ + ~µ1 + ~~ µ1) - (C~~ - I)~¯~ µ1 = C~~µ1 - (C~~ - I)¯~~ µ1 = C~~(I - ~¯~ )µ1 + ~¯~ µ1 = C~~~¯~µ1 + ¯~~ µ1
= [~(~~)-1~ + ~(~ ~)-1~ ]µ1
= µ1.

(A.5)

Furthermore,

p~¯~ + ¯~~yp = p~¯~ µ1 + ~¯~(xp + µ0 + pµ1) = ~¯~xp + ~¯~µ0 + p(¯~~ + ¯~~)µ1 = ~¯~xp + ¯~~µ0 + pµ1.

(A.6)

Then,

(¯~, 0)C~~(1)(~¯, ~¯) (~ + ~~) = (¯~, 0)C~~(1)(¯~, ~¯) (-~ + ~µ1 + ~~ µ1) = ¯~~¯ (~C~ - I)(-~~ µ0 + ~µ1 + ~~ µ1) = ¯~~¯ (~C~ - I)~µ1 + ~¯~ µ0 - ~¯~ µ1 = ~¯¯~ ~(C~~ - I)µ1 + ¯~~ µ0 - ¯~~ µ1,

(A.7)

where the second equality follows from the proof of Lemma 1 in Swensen (2005a). Finally, we

have

-(¯~, 0)C~~#(1)(¯~, ~¯) ~~ = -(¯~, 0)C~~#(1)(~ µ1, 0) = -¯~ -I - ~¯ ~~¯ + ~¯ ~~¯(~¯~~¯)-1¯~~¯~ ~ µ1
= ~¯~ µ1 + ~¯¯~ ~¯~~ µ1 - ~¯~¯ ~C~~~¯~ µ1 = ¯~~ µ1 - ¯~¯~ ~(C~~ - I)¯~~ µ1,

(A.8)

where the second equality follows from the results in Rahbek & Mosconi (1999, p. 89). Note in this respect that the expression -D (1) in Rahbek & Mosconi (1999) corresponds to C~~#(1).

30

Collecting the results of (A.5) - (A.8) yields

µ1(t - p) + [pµ1 + ~¯~µ0] + [¯~~¯~(C~~ - I)µ1 + ~¯~ µ0 - ~¯~ µ1] + [~¯~ µ1 - ¯~¯~ ~(C~~ - I)~¯~ µ1] = (¯~~ + ¯~~ )µ0 + µ1t + [~¯¯~~(C~~ - I)](I - ¯~~ )µ1 = µ0 + µ1t + ~¯¯~~(C~~ - I)~¯~µ1 = µ0 + µ1t + ¯~~¯~(¯~~ - ¯~~)µ1
= µ0 + µ1t

for the deterministic components on the r.h.s. of (A.1). Note that the third equality is obtained

by plugging in the definition of C~. Hence, as mentioned above, we can subtract µ0 + µ1t from

both sides of (4.5) and obtain the Granger representation for xt as given in Lemma 1 with

 T Rt0,T

=

(0, ~¯)C~~#(L)(¯~, ¯~) (t

- p) + (¯~, 0)C~~(1)(¯~, ¯~) t

+ (~¯, 0)C~~#(L)(¯~, ~¯) t + ¯~~xp.

(A.9)

As mentioned in Section 4, ~(xp-) does not converge to zero. However, it is bounded in Pr0,T probability and bounded with respect to the measure Pr0,T . To see this, we write ~xp = ~[xp - (µ0 - µ0) - (µ1 - µ1)p] in line with (4.11). It is obvious, that this expression is

bounded conditional on the observations since all terms are nonrandom in this case. Regarding

the measure Pr0,T we have ~(µ1 - µ1) = Op(T -1/2) if µ1 = µ^1 and, from Saikkonen & Lu¨tkepohl (2000b, Proof of Theorem 1), ~(µ0 - µ0) = Op(1) if µ0 = µ^0 from (4.3) is applied. Thus, ¯~~xp = ~¯~xp + Op(1) is bounded in Pr0,T probability due to the consistency of ~. Thus, the result Pr0,T (maxp+1tT |Rt0,T | > )  0 for all  > 0 in Pr0,T probability

as T   follows from the proof of Lemma 1 in Swensen (2005a) because for all  > 0,

Pr0,T (|¯~~xp|/T

>

)  0 in Pr0,T

probability as T

 .

In fact, for proving the 

corresponding result for Rt,T , Swensen (2005a) only considers our remainder term T Rtc,T

with yp instead of xp since the estimators of the deterministic parameters of the VECM (2.3)

are bounded in probability due to their consistency.

E[tt ] = ~ T   in Pr0,T probability as T   follows directly from Lemma 1 and

Remark 9 of Swensen (2005a) since we use the same bootstrap residuals for generating xt as

for yt.

The representation (4.5) given for ytc in Lemma 1 follows from the foregoing derivations. First, add µ0 on both sides of (4.4). Second, decompose µ0 into µ0 = (¯~~ + ¯~~)µ0 = 0 + ¯~~µ0. Then, we have ¯~~xp + ~¯~µ0 = ¯~~ypc. Hence, (4.5), i.e. ytc = xt + µ0 =

31

C~

t i=p+1

t

+

0

+

 T

Rtc,T

,

is

obtained

with

 T Rtc,T

=

(0, ¯~)C~~#(L)(¯~, ¯~) (t

- p) + (~¯, 0)C~~(1)(~¯, ¯~) t

+ (¯~, 0)C~~#(L)(¯~, ¯~) t + ¯~~ypc.

(A.10)

Note that ¯~~ypc = ~¯~(ypc - (µ1 - µ1)p) = ~¯~ypc + Op(T -1/2) such that ¯~~ypc is bounded in Pr0,T probability. The result Pr0,T (maxp+1tT |Rtc,T | > )  0 for all  > 0 in Pr0,T probability as T   follows from the argumentation with respect to the remainder term Rt0,T above. This completes the proof of Lemma 1.

Proof of Proposition 1
We now present the results needed to prove Proposition 1. They are collected in different Lemmas. The idea is to mimic the proof of Theorem 4.1 for SLT (r0) given in Saikkonen et al. (2006).5 To this end, we first show a number of results conditional on the observations, i.e. with respect to the measure Pr0,T . Then, we obtain convergence and distribution results regarding the measure Pr0,T . In order to accomplish that we have to make us of Lemma 1 and of Lemmas 1-3 in Swensen (2005a) several times. Note again that Lemmas 1-3 are actually proven in conjunction with Algorithm 2 but are also valid in case of Algorithm 1 (see Remark 9 of Swensen 2005a).
In the following we will use the bootstrap stochastic order symbols which have been introduced by Park (2003) and Chang & Park (2003). Similar to Chang & Park (2003), they are defined here as follows. Let (Xn) be a sequence of bootstrap statistics. Then, Xn = Op(T ) if for every > 0 there exists a finite K and N (K, )  N, such that Pr0,T {|T -Xn| > K} < for all T  N (K, ). Furthermore, Xn = op(T ) if for every Pr0,T {|T -Xn| > } converges to zero in Pr0,T probability as T  . These definitions can be adopted for sequences of random vectors. Furthermore, d signifies convergence in Pr0,T distributions (compare Swensen 2005a) and, p and p refer to convergence in Pr0,T and Pr0,T probability, respectively.
The framework, on which the derivation of the asymptotic distribution is based, is described in Section 4. As mentioned there, the bootstrap estimator of µ1 has to satisfy certain convergence and distributional properties. In deriving them we need convergence results for the RR estimators of the parameters of the VECM (3.1), which is based on the pseudo data yt. They will be stated conditional on the observations y1, . . . , yT in Lemma A1. The asterisk indicates
5Note again that we do not consider a level shift at unknown time in our framework. Therefore, the model setup and the corresponding proofs in Saikkonen et al. (2006) simplify accordingly. Nevertheless, the asymptotic test distribution is not affected by excluding the level shift.
32

the estimators that are based on the bootstrap data. For the derivations, we have to assume that ~ has been made unique by the normalization ~[(~ ~)-1~ ~]-1 = ~(~¯ ~)-1. A corresponding normalization follows for ~. Note that all other relevant quantities are invariant to
these normalizations (compare Saikkonen & Lu¨tkepohl 2000b). Therefore, we do not explicitly
indicate the normalized version of the estimators in the following.

Lemma A1. Under Assumption 1, we have
~ - ~ = Op(T -1/2) ~ - ~ = Op(T -1) ~ - ~ = Op(T -3/2) ~ - ~ = Op(T -1/2) ~i - ~i = Op(T -1/2) (i = 1, . . . , p - 1) ~  - ~ = Op(T -1/2).

(A.11) (A.12) (A.13) (A.14) (A.15) (A.16)

Proof To proof Lemma A1 we follow Johansen (1995, Ch. 13). Note, however, that he consid-

ers a linear trend orthogonal to the space spanned by the cointegration vectors. Thus, we have

to adjust the treatment of the deterministic terms. This basically results in the use of different

norming matrices. Furthermore, keep in mind that estimators based on the observations are not

random if we condition on the observations.

Define Z0t = yt, Z1t = (yt-1, t - 1) , and Z2t = (yt-1, . . . , yt-p+1, 1) . Then, regress

Z0t and Z1t on Z2t to obtain the residuals R0t and R1t, respectively. Using these residuals we

define

T

Sij = (T - p)-1

RitRjt

t=p+1

i, j = 0, 1.

In the same way we can obtain the moment matrices Sij (i, j = 0, 1) from Z0t, Z1t, Z2t, R0t,

and R1t, which are defined in terms of the original observations. We first show the consistency of the estimators ~, ~, ~, and ~ . Define ~# = (~ , -~) .

The estimator ~# is based on the eigenvectors corresponding to the r largest eigenvalues of

|S11 - S10S00-1S01| = 0,

(A.17)

(see Swensen 2005a). Given ~#, we can obtain the estimators ~ = S01~#(~# S11~#)-1 and ~  = S00-1 - S01~#(~# S11~#)-1~# S10 corresponding to Johansen (1995). Similar to

33

Johansen (1995, Ch. 13) we transform equation (A.17) to

|A~T S11A~T - A~T S10S00-1S01A~T | = 0,

(A.18)

where



A~T

=

 ~ -

T -1/2¯~ T -1/2~ ~¯

0 = T -1

~#

T -1/2C~T

with

 C~T = ~¯~¯~


0 T -1/2

as in Swensen (2005a). The eigenvalues of (A.17) and (A.18) are identical and the eigenvectors

of (A.18) can be obtained from those of (A.17) by premultiplying with

 ~¯ A~T-1 = T 1/2~



0 0



.

T ~ T

Next, we study the weak limit of (A.18) conditional on the observations. Given the definition

of A~T we can write (A.18) as 





 T

~# S11~# -1/2C~T S11~#

T

-1/2~# S11C~T C~T S11C~T

-C~~#T

S10S00-1S01~# S10S00-1S01~#



T -1/2~# S10S00-1S01C~T  T -1S10S00-1S01C~T

= 0.

Hence, we need a number of conditional convergence results to determine the weak limit of

(A.18). They are summarized in the next lemma, which is the counterpart of Lemma 10.3 in

Johansen (1995).

Lemma A2. Under Assumption 1, we have

S00 p S00

S01~# p S01~#

~# S11~# p ~# S11~#

(T - p)-1C~T S11C~T d C~T (S10 - S11~#~) d

1
G(s)G(s) ds
0 1
G(s)(dW (s))
0

C~T S11~# = Op(1),

(A.19) (A.20) (A.21) (A.22)
(A.23) (A.24)

where G(s) = [(W (s) - W¯ (s)) C~¯~, s - 1/2] , W¯ (s) =

1 0

W

(s)ds,

and

W (s)

is

a

n-dimensional Brownian motion with covariance matrix ~ .

34

Proof (A.19) - (A.21) follow from the proof of Lemma 2 in Swensen (2005a). Consider for

example (A.19). Note that S00 = M00 - M02M22-1M20 where Mij = (T - p)-1

T t=p+1

ZitZjt

(i, j = 0, 2). Similarly, we can write S00 by the help of Mij (i, j = 0, 2) defined with respect to

the original observations. Swensen (2005a) shows that the first differences of the observations

and the bootstrap data have invertible moving average representations given by yt-j = ~j +

 i=0

~i,j ~t-j

and

yt-j

=

~j +

 i=0

~i,j t-j

respectively,

where

~j

and

~i,j

are

a

(n × 1)

vector

and a (n × n) matrix of parameter estimates described in Swensen (2005a). Note that yt-j

is expressed using the true error terms t in Swensen (2005a). However, we can equally use

the residuals ~t by replacing the true parameter vector j by the estimated vector ~j. This is

possible because we condition on the observations.

Given the moving average representations it is straightforward to see that E[(yt-j -

~j)(yt-k - ~k) ] =

 i=0

~i,j~ ~i,j

=

E  [(yt-j

- ~j)(yt-k

- ~k) ]

(j, k

=

0, . . . , p

- 1),

because E[tt ] = (T - p)-1

T i=p+1

~t~t

=

~ .

Based

on

this

result

we

obtain

E  (Mij )

=

E(Mij). Furthermore, we conclude from the argumentation of Swensen (2005a) related to

equation (22) in the proof of Lemma 2 that the conditional variance of Mij (i, j = 0, 2) con-

verges to zero in Pr0,T probability. Thus, (A.19) will follow. (A.20) and (A.21) can be obtained in a similar way by noting that ~# [yt-1, (t-1)] is a stationary series with respect to the measure

Pr0,T . Note that ST (s) d W (s) (see Swensen 2005a), where ST (s) = (T - p)-1

[sT ] i=k+1

i,

0



s  1. Here, we consider the Brownian motion W  with the estimated covariance matrix ~ .

We may also consider a Brownian motion W with covariance matrix , since ~ converges to

 for T  . Nevertheless, we use W  to stress that we refer to convergence conditional on the observations. Now, we replace the unconditional distribution result ST (s) d W (s) used in Lemma 3 of Swensen (2005a) by the conditional result mentioned before. Then, (A.22) and

(A.23) follow from Lemma 3 and its proof in Swensen (2005a).

Finally, we turn to (A.24). Define Z¯1 = (T -p)-1

T t=p+1

Z1t,

Z2lt

=

(yt-1, . . . , yt-p+1)

,

Z¯2 = (T - p)-1

T t=p+1

Z2lt ,

Syy

=

(T

- p)-1

tT=p+1(Z1t - Z¯1)(Z1t - Z¯1) , Syz = (T -

p)-1 Tt=p+1(Z1t-Z¯1)(Z2lt -Z¯2) , and Szz = (T -p)-1 tT=p+1(Z2lt -Z¯2)(Z2lt -Z¯2) . Then, we

have S11 = Syy - SyzSzz-1Szy with Szy = Syz corresponding to Johansen (1995, pp. 146-147).

Hence,

C~T S11~# = C~T Syy~# - C~T SyzSzz-1Szy~#.

(A.25)

From Lemma 1 of Swensen (2005a) we conclude that ~# (Z1t - Z¯1) and (Z2lt - Z¯2) are zeromean I(0) series and that C~T (Z1t - Z¯1) is a zero-mean I(1) series with respect to the measure

35

Pr0,T . Now, we can apply Theorem B.13 of Johansen (1995) to determine the conditional convergence orders of the product moment matrices involved in (A.25). We obtain that C~T Syy~#, C~T Syz, Szz-1, and Szy~# are all Op(1). Thus, C~T S11~# is also Op(1) confirming (A.24). This
completes the proof of Lemma A2.

Using Lemma A.2, the weak limit of (A.18) can be derived. Following Johansen (1995) we

see that the ordered eigenvalues of (A.18) converge, conditional on the observations, to those of

  ~# S11~#
0



1 0

0 G(u)G(u)

 du

-

~#

S10

S0-01S01~# 0

 0 = 0 0

1
|~# S11~# - ~# S10S0-01S01~#|| G(u)G(u) du| = 0,
0

and the space spanned by the r first eigenvectors of (A.18), given by sp(A~-T 1~#), converges to

the space spanned by vectors with zeros in the last (n - r + 1) coordinates. Thus, we have

A~T-1~#

=





Ir T 1/2~~



T (~ ~ - ~)

=


 Ir  T 1/2UT

=

 opI(r1)
op(1)

,

with UT = (~ ~, T 1/2(~ ~ - ~ )) . Hence, ~ is a consistent estimator of ~ conditional on the observations. Using this result, it is straightforward to show that ~ ~ p ~ ~ = ~. Thus, ~
is consistent as well and we have (~ - ~) = Op(T -1) and (~ - ~) = Op(T -1/2). Further, note that ~# S11~# = (~# + C~T UT ) S11(~# + C~T UT ) and ~# S10 = (~# +
C~T UT ) S10 corresponding to Johansen (1995, p. 181). Using (A.19) - (A.21) and the fact that UT = op(T -1/2) we obtain ~# S11~# p ~# S11~# and ~# S10 p ~# S10. Given the definition of ~ and ~ , the latter two results in conjunction with (A.19) imply that ~ and ~ 
converge towards ~ and ~ in Pr0,T probability respectively. Having established the consistency of ~, ~, ~ and ~ , we now proceed to prove (A.11)-

(A.13) following the proof of Johansen (1995, Lemma 13.2). In our case the first order conditions for ~# and ~ read as

~ ~ -1(S01 - ~~# S11) = 0 (S01 - ~~# S11)~# = 0.

(A.26) (A.27)

Further, define

T

S

 1

=

(T

-

p)-1

~t R~1t = S01 - ~~# S11,

t=p+1

(A.28)

36

where ~t = R~0t - ~~# R~1t (compare Johansen 1995, p. 91). Insert (A.28) and C~T UT = ~# - ~# into (A.26) and multiply by C~T from the right. Then,
using Lemma A.2 and the consistency of ~ and ~  we find that T UT = Op(1) in line with Johansen (1995, p. 182-183). Recalling the definition of UT , we conclude that (~ - ~) = Op(T -1) and (~ - ~) = Op(T -3/2). Now, insert (A.28) into (A.27) and multiply by (T - p)1/2.
Then, using (A.12) and (A.21) gives ~ - ~ = Op(T -1/2) corresponding to Johansen (1995,
p. 183), which shows (A.11).

(A.14) and (A.15) can be obtained from an analog of Theorem 13.5 in Johansen (1995). We do not show this in detail but consider instead the convergence result for ~  in (A.16), which

is not treated by Johansen (1995). The proof of (A.16) closely follows the idea of the proof

of Theorem 13.5 in Johansen (1995). Thus, (A.14) and (A.15) are derived accordingly. Write

(3.1) as

yt = ~Zt(~#) + t ,

(A.29)

where Zt(~#) = (1, yt-#1~#, yt-1, . . . , yt-p+1) and ~ = (~, ~, ~1, . . . , ~p-1). Note that

Zt(~#) is a I(0) series conditional on the observations. Let  = (T - p)-1

T t=p+1

tt

and

~  = (T - p)-1

T t=p+1

~t~t

,

where

~t

are

the

residuals

obtained

from

estimating

(A.29)

given

~#. Tedious but straightforward derivations show that

T

(T - p)1/2(~  - ) = (T - p)-1/2

t (Zt(~#) - Z¯T (~#))

t=p+1

T

× (T - p)-1

(ZT (~#) - Z¯T (~#))(Zt(~#) - Z¯T (~#))

t=p+1

-1

T

× (T - p)-1

(ZT (~#) - Z¯T (~#))t ,

t=p+1

(A.30)

where Z¯T (~#) = (T - p)-1

T t=p+1

Zt(~#).

From Theorem B.13 in Johansen (1995) we

conclude that the first and second term on the right hand side of (A.30) are Op(1) and the third term is op(1). Thus, ~  -  = op(T -1/2), which means that ~  and  have the same

probability limit conditional on the observations. Given (A.12) and (A.13), it is clear that this

result also holds if ~# is replaced by ~# (compare Johansen 1995, p. 188).

Given that the bootstrap residuals t are randomly drawn with replacement, they can be
regarded as i.i.d. samples from the empirical distribution given by the estimated residuals ~t. In other words, the bootstrap samples t become i.i.d. with covariance matrix ~ = (T -

37

p)-1

T t=p+1

~t~t

under

the

probability

measure

Pr0,T

(compare

Park

2002,

2003

for

the

uni-

variate case). Because we assume that the fourth moments of the error terms exists, a central

limit theorem shows that  - ~ = Op(T -1/2) (compare Hamilton (1994, p. 340-343) for the standard asymptotic situation). Hence, we finally get ~  - ~ = Op(T -1/2), which proves

(A.16). Hence, we have shown Lemma A.2.

Saikkonen et al. (2006) consider a transformed version of the relevant VECM in order to introduce certain parameters needed for the derivations later on. In our case we have to use the decomposition yt  µ0 + µ1t + xt with respect to (3.1) assuming that the conditions (4.2) hold. This yields the transformed VECM

p-1
xt = ~0 + ~(~ xt-1 - ~(0)(t - 1)) + ~jxt-j + t ,
j=1

t = p + 1, p + 2, . . . ,

(A.31)

where ~(0) = ~ - ~µ1 + ~~ µ0 and ~(0) = ~ - ~ µ1. Note that the true values of ~(0) and ~(0) are zero due to the conditions (4.2).
Define the estimators ~(0) = ~ - ~ µ1 and ~(0) = ~ - ~µ1 + ~~ µ0, which are obtained by transforming the RR estimators of the VECM (3.1) corresponding to Saikkonen et al.
(2006). Their convergence properties are summarized in the next lemma.

Lemma A3. Under Assumptions 1 and 2, we have
~(0) = Op(T -3/2) ~(0) = Op(T -1/2).

(A.32) (A.33)

Proof The estimators ~(0) and ~(0) can also be obtained from the transformed VECM (A.31). This is a model with a restricted linear trend and unrestricted mean term as model (3.1). Thus, we can apply (A.13) of Lemma A.1 for the estimators of the trend parameter ~(0) and the mean term ~(0). Since they have true values of zero, (A.32) follows.
We are now in the position to state the properties of the estimator of µ1 based on the bootstrap data.

38

Lemma A4. Under Assumptions 1 and 2, we have
~ (µ^1 - µ1) = Op(T -3/2) (T - p)-1/2~(µ^1 - µ1) d N (0, ~C~~ C~ ~).

(A.34) (A.35)

Proof The proof of Lemma A4 is analogous to the corresponding part of the proof of Lemma
A.14 in Saikkonen et al. (2006). There, the properties of the estimator µ^1 based on the original observations are shown. Given the definition of ~(0) we have ~C~~(0) = ~C~~ - ~C~~µ1. From Saikkonen et al. (2006, Section 4) ~C~~ = ~µ^1 + ~C~~¯~~ is obtained. Using the latter equality and the definition of ~(0) results in ~C~~(0) = ~µ^1 - ~C~~µ1 + ~C~~~¯~ µ1 + ~C~~¯~~(0) such that ~(µ^1 - µ1) = ~C~(~(0) - ~~¯~(0)). Since the same relation applies to the estimators based on the pseudo observations one has

~ (µ^1 - µ1) = ~ C~(~(0) - ~~¯~(0)).

Then, by Lemma A.2,

~ (µ^1 - µ1) = ~ C~~(0) + op(T -1/2)

follows. Corresponding to Saikkonen et al. (2006) the estimator ~(0) can be viewed as the LS estimator of ~(0) in the auxiliary model

p-1
xt = ~(0) + ~(~ xt-1 - ~(0)(t - 1)) + ~jxt-j + ut ,
j=1

(A.36)

which is obtained from (A.31) by replacing ~ and ~(0) by their bootstrap analogs ~ and ~(0) and by noting that ut = t - ~[(~ - ~) xt-1 + ~(0)(t - 1)], ~ ytc- 1 = ~ xt-1 + , ~ xt-1 = ~ ytc- 1 - ~, and that ytc- i = xt-i (i = 0, . . . , p - 1). Keep in mind, that the estimator ~
can be recovered from the RR estimators of (3.1). Equation (A.36) implies that the estimator ~ C~~(0) can be obtained by LS from

~ C~xt = ~ pt + u~t ,

(A.37)

where pt = [1, xt-1, . . . , xt-p+1], ~ is a conformable coefficient matrix and u~t = ~ C~t - ~ C~~[(~ - ~) xt-1 + ~(0)(t - 1)]. By the definition of C~ and Lemma A.2, ~ C~~ =
Op(T -1/2). Using this fact, Lemma A.2 and the assumptions, it is straightforward to show that conditional on the observations the asymptotic properties of the LS estimator of ~ in the aux-
iliary regression model (A.37) can be obtained by assuming that the error term equals ~C~t .

39

Following Saikkonen et al. (2006), it is also easy to show that the estimation of the intercept

term in (A.37) is asymptotically independent of the estimation of the other regression coeffi-

cients. Thus, we asymptotically obtain ~ C~~(0) = (T - p)-1~C~

T t=p+1

t

since

the

true

value of ~(0) is zero. Therefore, we can conclude that

T

(T - p)1/2~ (µ^1 - µ1) = ~C~(T - p)-1/2

t + op(1).

t=p+1

This, Lemma 1 in Swensen (2005a), and a standard central limit theorem yield

(T - p)1/2~ (µ^1 - µ1) d N (0, ~C~~ C~ ~).

(A.38)

In order to obtain (A.35) we need to show that ~ on the l.h.s. of (A.38) can be replaced by ~. First note that

(~ -~) (µ^1 -µ1) = (~ -~) ~(~ ~)-1~ (µ^1-µ1)+(~ -~) ~ (~ ~ )-1~ (µ^1 -µ1)
(A.39) Due to the consistency of the estimator ~ with respect to ~ and the result (A.38) the latter term on the r.h.s. is of order op(T -1/2). The same is true for the former term because ~ (µ^1 - µ1) = ~(0) = Op(T -3/2) given the definition of ~(0), the conditions (4.2), and Lemma A.3. Thus, (A.35) follows and (A.34) is obtained if ~ in the latter equality can be replaced by ~. To this
end, write

(~ - ~) (µ^1 - µ1) = (~ - ~) ~(~ ~)-1~ (µ^1 - µ1) + (~ - ~) ~ (~ ~ )-1~ (µ^1 - µ1).

From the arguments made in relation to (A.39) we can deduce that both terms on the r.h.s. are op(T -1/2) such that (A.34) is shown. This completes the proof of Lemma A.4.

As described in Section 4, the SLT1 test is based on the VECM for the adjusted bootstrap series y^tc = yt - µ^1t given by

p-1
y^tc = ~(~ y^tc- 1 - ) + ~jy^tc- j + etc,
j=1

(A.40)

where etc = t - ~~ (y^tc- 1 - ytc- 1)(t - 1) + y^tc - ytc -

p-1 j=1

~j

(y^tc- j

- ytc- j)

=

t + ~~ (µ^1 - µ1)(t - 1) - ~(µ^1 - µ1). Define Z~0ct = y^tc, Z~1ct = (y^tc- 1, 1) , and Z~2ct =

(y^tc- 1, . . . , y^tc- p+1, 1) . Then, regress Z~0ct and Z~1ct on Z2ct to obtain the residuals R~0ct and

R~1ct , respectively. Using these residuals we write

T

S~icj = (T - p)-1

R~ictR~jct

t=p+1

i, j = 0, 1.

40

In the same way we can obtain the moment matrices Sicj (i, j = 0, 1) from Z0ct, Z1ct, Z2ct, R0ct , and R1ct , which are defined in terms of ytc = yt - µ1t. Furthermore, let Sicj (i, j = 0, 1) be the moment matrices based on the original observations adjusted by the trend component, i.e ytc = yt - µ1t. Finally, define ~+ = (~ , -) .
In order to proof Proposition 1 we follow Johansen (1995, Ch. 11). Thus, we need corre-
sponding results to Lemma 2 and 3 of Swensen (2005a) with respect to model (A.40). To this
end, we consider first the following lemma.

Lemma A5. Under Assumptions 1 and 2, we have
S~0c0 p c00 S~0c1~+ p c0 ~+ S~1c1~+ p c ,
where 0c0, 0c, and c are the respective probability limits of S0c0, S0c1+, and + S1c1+ with respect to the original observations as T  .

Proof We first prove that S~0c0 p S0c0, S~0c1~+ p S0c1~+, and ~+ S~1c1~+ p ~+ S1c1~+. Hence, conditional on the observations, the use of the estimator µ^1 instead of µ1 for the bootstrap data adjustment has no asymptotic impact on the properties of the product moment matrices

of interest. Then, Lemma A5 follows from Lemma 2 of Swensen (2005a) in connection with

Lemma 1 above.

Using the definitions stated before Lemma A5, we can write in accordance with Johansen

(1995, Ch. 6)



TT

S~0c0 = (T - p)-1 

Z~0ctZ~0ct -

Z~0ctZ~2ct

t=p+1

t=p+1

TT

S~0c1~+ = (T - p)-1 

Z~0ctZ~1ct -

Z~0ctZ~2ct

t=p+1
T
~+ S~1c1~+ = (T - p)-1~+ 

t=p+1

T

Z~1ctZ~1ct -

Z~1ctZ~2ct

t=p+1

t=p+1

T
Z~2ctZ~2ct
t=p+1
T
Z~2ctZ~2ct
t=p+1
T
Z~2ctZ~2ct
t=p+1

-1 T



Z~2ctZ~0ct 

t=p+1 -1 T



Z~2ctZ~1ct  ~+

t=p+1 -1 T



Z~2ct Z~1ct ~+ .

t=p+1

Note that

T t=p+1

Z~0ctZ~0ct

=

T t=p+1

[Z0ct

-

(µ^1

-

µ1)][Z0ct

-

(µ^1

-

µ1)]

,

where

Z0ct

=

ytc

is

a zero-mean stationary process with respect to the measure Pr0,T according to Lemma 1. Using

Lemma A4 and Johansen (1995, Theorem B.13), we have

T t=p+1

Z~0ct

Z~0ct

=

T t=p+1

Z0ctZ0ct

+

41

Op(1). Similarly,

T t=p+1

Z~0ct

Z~2ct

=

T t=p+1

Z0ct

Z2ct

+ Op(1) and

T t=p+1

Z~2ct

Z~2ct

=

T t=p+1

Z2ct

Z2ct

+ Op(1)

are obtained.

Combining

these

results, yields


T T T -1 T

S~0c0 = (T - p)-1

Z0ctZ0ct -

Z0ctZ2ct

Z2ctZ2ct

 Z2ctZ0ct +op(1)

t=p+1

t=p+1

t=p+1

t=p+1

= S0c0 + op(1).

Moreover,

T t=p+1

Z~0ctZ~1ct

~+

=

tT=p+1[Z0ct - (µ^1 - µ1)][Z1ct - {(µ^1 - µ1) (t - 1) : 0} ] ~+.

Note that ~+ Z1ct = ~ ytc- 1 -  is also a zero-mean stationary process with respect to the

measure Pr0,T . Again, we refer to Lemma A4 and Johansen (1995, Theorem B.13) to con-

clude that

T t=p+1

Z~0ctZ~1ct

~+

=

T t=p+1

Z0ctZ1ct

~+

+

Op(1).

In a similar fashion, we ob-

tain

T t=p+1

Z~2ctZ~1ct

~+

=

T t=p+1

Z2ctZ1ct

~+

+

Op(1)

such

that,

together

with

the

results

found above, S~0c1~+ = S0c1~+ + op(1) can be deduced. By an analogous argumentation,

T t=p+1

~+

Z~1ctZ~1ct

~+

=

T t=p+1

~+

Z1ctZ1ct

~+

+

Op(1)

can

be

shown.

Using this, we fi-

nally find ~+ S~1c1~+ = ~+ S1c1~+ + op(1), what completes the proof of Lemma A5.

As an intermediate step in deriving the distributional properties of the relevant quantities we first show

Lemma A6. Under Assumptions 1 and 2, we have



where

B~T

=

~ 0

T

(T - p)-1B~T S~1c1B~T p (T - p)-2B~T

Z~1ctZ~1ct B~T

t=p+1

T

B~T S~1c0~ p (T - p)-1B~T

Z~1ct¯tc ~

t=p+1



0 T 1/2

and

¯tc

=

t

-

~~~¯(µ^1

-

µ1).

(A.41) (A.42)

Proof The estimation error with respect to µ^1 does not vanish on the r.h.s. of (A.41) because ~(µ^1 - µ1) = Op(T -1/2) in contrast to ~ (µ^1 - µ1) = Op(T -3/2) according to Lemma A4.
Thus, we only have to show that

T

(T - p)-2

B~T Z~1ctZ~2ct

t=p+1

T
Z~2ctZ~2ct
t=p+1

-1 T
Z~2ctZ~1ct B~T p 0.
t=p+1

(A.43)

Note

that

~ytc- 1

equals

a

zero-mean

process

plus

a

bounded

term 

with

respect

to

the

measure

Pr0,T . This follows from the definition of 0 in Lemma 1 and T Rtc,T in (A.10). Then, using

42

the results of Lemmas 1 and A4 in connection with Theorem B.13 of Johansen (1995) we find

that

T t=p+1

Z~2ctZ~2ct

= Op(T ),

T t=p+1

~

y^tc- 1Z~2ct

= Op(T ), and

T t=p+1

Z~2ct

= Op(T 1/2).

Combining the latter two results yields

T t=p+1

B~T

Z~1ctZ~2ct

= Op(T ) and (A.43) follows, which

implies (A.41).

In line with Johansen (1995, p. 91) we can write S~1c0~ = (T -p)-1

T t=p+1

R~1ct (~~+

R~1ct

+

~ct) ~ = (T - p)-1

T t=p+1

R~1ct ~tc

~,

where

~tc

=

R~0ct - ~~+ R~1ct .

Given

the derivations

used in Johansen (1995, pp. 90-91) and our notation with respect to the VEC model (A.40)

it is straightforward to show that ~tc = ect I - Z~2ct

T t=p+1

Z~2ct

Z~2ct

-1 Z~2ct

and R~1ct =

Z~1ct -

T t=p+1

Z~1ct

Z~2ct

T t=p+1

Z~2ctZ~2ct

-1 Z~2ct. Using these expressions, we have



TT

S~1c0~ = (T - p)-1 

Z~1ctect -

Z~1ctZ~2ct

t=p+1

t=p+1

T
Z~2ctZ~2ct
t=p+1

-1 T



Z~2ctetc  ~

t=p+1

Note again that ect = t + ~~ (µ^1 - µ1)(t - 1) - ~(µ^1 - µ1). Then, by Lemmas 1 and 4

together with Johansen (1995, Theorem B.13),

T t=p+1

Z~2ctect

~

=

Op(T 1/2).

Combining

this with

T t=p+1

B~T

Z~1ctZ~2ct

= Op(T ) and

T t=p+1

Z~2ctZ~2ct

= Op(T ) shows that B~T S~1c0~ =

(T - p)-1

T t=p+1

B~T

Z~1ctect

~

+

op(1).

As in case of (A.41), the estimation error regarding µ^1 does not vanish in B~T S~1c0~ =

(T - p)-1

T t=p+1

B~T

Z~1ctetc

~.

However,

we

can

write

this

expression

more

explicitly

by

not-

ing that ect ~ = {t - ~[~(~ ~)-1~ + ~(~~)-1~](µ^1 - µ1)} ~. Since ~ (µ^1 - µ1) =

Op(T -3/2), we obtain B~T S~1c0~ = (T - p)-1

T t=p+1

B~T

Z~1ct

[t

-

~~(~~)-1~(µ^1

-

µ1)] ~ + op(1), what shows (A.42) and completes the proof of Lemma A6.

We can now state Lemma A7. Under Assumptions 1 and 2,

(T - p)-1B~T S~1c1B~T  B~T S~1c0~ 

1
G+(s)G+(s) ds
0 1
G+(s)dG(s) ,
0

(A.44) (A.45)

where G+(s) = [B(s) 1/2C , 1] , dG(s) = 1/2dB(s), and convergence is weakly in Pr0,T probability.

Proof We can exactly argue as in Swensen (2005a, Proof of Lemma 3). Our proof also relies on the continuous mapping theorem involving the same functionals as in Saikkonen et al. (2006)

43

and related work by Lu¨tkepohl & Saikkonen (2000) and Hansen & Johansen (1998, pp. 118-

121). Thus, analogous to Swensen (2005a), we only have to show that the process {(T -

p)-1/2y[csT ], 0  s  1} converges weakly in Pr0,T probability as element in D[0, 1]n. From Lemma 1 we see, that the remainder term Rtc,T vanishes asymptotically. The constant
term is treated as in Saikkonen et al. (2006). A more detailed description of this treatment in

relation to the Johansen test can be found in Hansen & Johansen (1998, pp. 118-121). Therefore,

we only have to analyse ST (s) = (T - p)-1/2

[sT ] i=p+1

i .

This

is

the

same

partial

sum

analysed

by Swensen (2005a) except that the our bootstrap residuals are based on Algorithm 1 and not

on Algorithm 2. However, according to Swensen (2005a, Remark 9) this does not change the

asymptotic results.

Hence, following the arguments of Swensen (2005a), convergence in Pr0,T probability is obtained by using the approach of Pollard (1984, Theorem V.19). It can be shown that

E[f (ST )]  E[f (W )] in Pr0,T probability for all bounded continuous functions f defined on D[0, 1]n, the space of functions that are right continuous and have left limits. Thus, ST converges weakly in Pr0,T probability to a Brownian motion W with covariance matrix . Furthermore, note that the estimators ~, ~, and ~ involved in the l.h.s. of (A.44) and (A.45)
are consistent in terms of the measure Pr0,T (compare e.g. Johansen 1995). Moreover, we have B~T Z~1ct = {[~ytc- 1 - ~(µ^1 - µ1)(t - 1)] , T 1/2} . Then, given the consideration of the (restricted) constant term as in Saikkonen et al. (2006) or Hansen & Johansen (1998, pp. 118-121),

Lemma A7 follows from the relationship of the functionals on the r.h.s. of (A.41) and (A.42)

with those in equations (A.10)/(A.11) and (A.14)/(A.15) of Lu¨tkepohl & Saikkonen (2000).

Note in this respect that the estimator µ~1 used in Lu¨tkepohl & Saikkonen (2000) and µ^1 have the same asymptotic properties. Thus, the asymptotic properties of µ^1 also correspond to those of µ~1 in Lu¨tkepohl & Saikkonen (2000).

Finally, Proposition 1 follows from Lemmas A5 and A7 in the same way as in Johansen (1995, Ch. 11) and Saikkonen et al. (2006) with obvious modifications (compare Swensen 2005a).

Proof of Proposition 2
We now prove Proposition 2. First, we need

44

Lemma A8. Under Assumptions 1 and 3, we have
~ (µ~0 - µ0) = Op(T -1/2) ~(µ~0 - µ0) = Op(1) ~ (µ~1 - µ1) = Op(T -3/2) (T - p)-1/2~(µ~1 - µ1) d N (0, ~C~~ C~ ~)

(A.46) (A.47) (A.48) (A.49)

Proof First, it is shown that Lemma A8 holds if ~ and ~ are replaced by ~ and ~ . Then, the

lemma follows from (A.12). Thus, we consider the following two sets of quantities:  = ~ µ0,



=

~ µ0,



=

~

µ1,



 

=

~ µ1

and



=

~

µ0,



=

~µ0,



=

~ µ1,



=

~µ1.

The

second set can be regarded as the real parameters conditional on the observations and the first

set comprises the analogs of the real parameters that are considered first. The estimators ~, ~, ~, and ~ are obtained by LS estimation of

Q~ A~(L)yt = Q~ H~0t~(~ ~)-1 + Q~ H~0t~ (~ ~ )-1

+

Q~

H~ 1t ~ (~

~)-1 

+

Q~

H~1t~ (~

~ )-1

 

+

~t,

(A.50) t = 1, . . . , T,

which is a rewritten version of (4.10). For the error terms we have ~t = Q~ nt = Q~ (t - (~~ - ~~ )xt-1 - jp=-11(~j - ~j)xt-j) corresponding to (A.4) in Saikkonen & Lu¨tkepohl (1997), which is the discussion paper version of Saikkonen & Lu¨tkepohl (2000b).
The definition of H~0t leads to H~0t~ (~ ~ )-1 = 0 for t  p + 1. Corresponding to Lu¨tkepohl & Saikkonen (1997, Proof of Lemma 2), which is the discussion paper version of
Lu¨tkepohl & Saikkonen (2000), this implies that the appropriately standardized moment matrix related to the LS estimator ~ is asymptotically block diagonal. For the upper left block we have
pp
(~ ~ )-1~ H~0t~ -1H~0t~ (~ ~ )-1 = (~~)-1~H~0t~ -1H~0t~(~~)-1 + op(1)
t=1 t=1

owing to Lemma A1. Following the argumentation in Lu¨tkepohl & Saikkonen (1997, Proof of Lemma 2), we see that this block is positive definite given the definition of H~0t and the requirement within bootstrap Algorithm 1 that ~~~ is nonsingular. The lower right block in the standardized moment matrix related to the LS estimator ~ is asymptotically nonsingular as
will become evident from the explanations below in connection with Saikkonen & Lu¨tkepohl (1997, Proof of Theorem 1). Furthermore, tp=1(~ ~ )-1~ H~0t~ -1nt is bounded in Pr0,T probability. This follows from the definition of nt above and the definition of 1, . . . , p in

45

(4.12). Finally, the standardized sample moments of the other regressors and the error terms are also bounded in Pr0,T probability. Again, this will be seen from the descriptions below and Saikkonen & Lu¨tkepohl (1997, Proof of Theorem 1). Hence, it follows that

~ = ~ µ~0 = ~ µ0 + Op(1) =  + Op(1),

which implies (A.47).

Following the above arguments it is straightforward to see that ~ has no effect on the asymptotic properties of ~, ~, and ~ conditional on the observations. Therefore, we can focus on

the

estimation

of

,

 ,



 

and

ignore

the

first

p

(bootstrap)

observations

in

(A.50).

Then,

the

proof of (A.46), (A.48), and (A.49) follows exactly the structure of the proof of Theorem 1 in

Saikkonen & Lu¨tkepohl (2000b) and Saikkonen & Lu¨tkepohl (1997), which deal with the cor-

responding properties of the estimators of the deterministic parameters in the asymptotic test

set-up. To be precise, by conditioning on the observation we are able to replicate the proof

of Saikkonen & Lu¨tkepohl (2000b) for the bootstrap quantities since they take the role of the

observations. Thus, we have to replace a number of quantities with their bootstrap counter-

parts in the proof by Saikkonen & Lu¨tkepohl (2000b) and consider now convergence in Pr0,T probability. This is analogous to the proof of Lemma A4. The following rules apply. The boot-

strap series yt, xt and the adjusted series x~t are used instead of yt, xt, and x~t. Moreover, the true parameters are replaced by their estimators regarding the observations and the latter ones

are interchanged with the estimators based on the pseudo data. Finally, the considered error

terms have to be redefined accordingly in terms of the quantities which refer to the bootstrap

framework, i.e. ~t defined above is used instead of ~t, which occur in (2.7). In addition, we

conclude from Lemma 2 that ~ xt and xt are zero the measure Pr0,T and that the random process ST (s)

mean stationary processes

 = 1/ T

[sT ] i=k+1

i,

0



s

with 1

respect to converges

in Pr0,T distributions toward a n-dimensional Brownian motion with covariance matrix ~ .

Adjusting the proof of Theorem 1 in Saikkonen & Lu¨tkepohl (2000b) and Saikkonen &

Lu¨tkepohl (1997) in the aforementioned way gives first

~ =  + Op(T -1/2)

~ =   + Op(T -3/2)

T

(T - p)-1/2(~ -  ) = ~ C~(T - p)-1/2

nt + op(1)

t=p+1

T

= ~C~(T - p)-1/2

t + op(1) d N (0, ~C~~ C~ ~).

t=p+1

(A.51)

46

The latter result is obtained by Lemma 2, Lemma A.1, and the definition of nt. Note again that ~xt does not have a zero mean asymptotically because xp is not a consistent estimator of xp in the direction of ~. However, the estimation error is bounded. Inspecting the deriva-
tions in Saikkonen & Lu¨tkepohl (2000b) and Saikkonen & Lu¨tkepohl (1997), it is seen that
this is sufficient to assure that the required standardized sample moments are all bounded in
Pr0,T probability. Accordingly, we obtain (A.51). Then, (A.46), (A.48), and (A.49) follow from (A.51) by applying Lemma A.1 as mentioned above. This completes the proof of Lemma A8.

Given Lemma A8 we can base the GLS1 test on the VECM (4.13) for the adjusted bootstrap

series x~t = yt-µ~0-µ~1t with et = t -~~ (x~t-1-xt-1)(t-1)+x~t-xt -

p-1 j=1

~j

(x~t-j

-

xt-j) = t + ~~ (µ~0 - µ0) + ~~ (µ~1 - µ1)(t - 1) - ~(µ~1 - µ1). Thus, we need analogous

versions of Lemmas A5-A7 in relation to (4.13). To this end, we define the following quantities.

Let Z~0xt = x~t , Z~1xt = x~t-1, and Z~2xt = (x~t-1, . . . , x~t-p+1) . Then, regress Z~0xt and Z~1xt on Z2xt to obtain the residuals R~0xt and R~1xt, respectively. Using these residuals we have

T

S~ixj = (T - p)-1

R~ixtR~jxt

t=p+1

i, j = 0, 1.

Similarly, we obtain the moment matrices Sixj (i, j = 0, 1) from Z0xt, Z1xt, Z2xt, R0xt, and R1xt, which are defined in terms of xt = yt - µ0 - µ1t. Furthermore, let Sixj (i, j = 0, 1) be the moment matrices based on the original observations adjusted by the level term and trend com-

ponent, i.e based on xt = yt - µ0 - µ1t.

Lemma A9. Under Assumptions 1 and 3, we have
S~0x0 p x00 S~0x1~+ p x0 ~ S~1x1~ p x,
where x00, x0, and x are the respective probability limits of S0x0, S0x1, and  S1x1 with respect to the original observations as T  .
Proof The proof is analog to the one of Lemma A5. First, one has to show that S~0x0 p S0x0, S~0x1~ p S0x1~, and ~ S~1x1~ p ~ S1x1~. These results are again proven by writing S~0x0, S~0x1~, and ~ S~1x1~ in terms of Z~0xt, Z~1xt, and Z~2xt. Then, use Lemma 2, Lemma A8 and Johansen (1995, Theorem B.13). Note in this respect that Z0xt, Z2xt, and ~ Z1xt are zero-mean stationary
47

 processes with respect to the measure Pr0,T given Lemma 2 and the definition of T Rt0,T in (A.9). Finally, it follows from of Swensen (2005a, Lemma 2) in connection with Lemma 2 that S0x0, S0x1~, and ~ S1x1~ converge to x00, x0, and x in Pr0,T probability.

Lemma A10. Under Assumptions 1 and 3, we have

T

(T - p)-1~S~1x1~ p (T - p)-2~

wt-1wt-1~

t=p+1

T

~S~1x0~ p (T - p)-1~

wt-1¯t ~

t=p+1

where wt-1 = xt-1 - (µ~1 - µ1)(t - 1) and ¯t = t - ~~~¯(µ~1 - µ1).

(A.52) (A.53)

Proof Note that ~xt-1 is a zero-mean process plus a bounded term with respect to the mea-

sure Pr0,T in line with Lemma 2 because ~xp = O(1). Then, using Lemma 2, Lemma A8, and

Theorem B.13 of Johansen (1995) we get (T -p)-1~S~1x1~ p (T -p)-2

T t=p+1

~Z~1xtZ~1xt

~

corresponding to (A.41). Finally, (A.52) follows by applying (A.47) and (A.49) to (T -

p)-2

T t=p+1

~Z~1xtZ~1xt

~

since

~Z~1xt

=

~x~t-1

=

~xt-1-~(µ~0-µ0)-~(µ~1-µ1)(t-1).

Analogously to (A.42), we first have ~S~1x0~ p (T - p)-1~

T t=p+1

Z~1xt¯t

~

by

taking

into account the obvious modifications due to the fact that now the VECM (4.13) is consid-

ered instead of (A.40). Furthermore, we make use of (T - p)-1

T t=p+1

~(µ~0

-

µ0)¯t

~

=

(T - p)-1~(µ~0 - µ0)

T t=p+1

t ~ + ~(µ~0 - µ0)(µ~1 - µ1) ~~¯~ ~

= op(1) obtained

from Lemmas 2, A1, and A8. Connecting this result with the conditional limit expression for

~S~1x0~ and the definition of Z~1xt gives (A.53). This completes the proof of Lemma A10.

Finally, we turn to the analog of Lemma A7 for the GLS1 test.

Lemma A11. Under Assumptions 1 and 3,

(T - p)-1~S~1x1~  ~S~1x0~ 

1
G(s)G(s) ds
0 1
G(s)dG(s) ,
0

where G(s) = C1/2B(s), dG(s) = 1/2dB(s), and convergence is weakly in Pr0,T probability.

48

Proof Lemma A12 can be proven following the argumentation related to the proof of Lemma A7. Now, we consider the process {(T - p)-1/2x[sT ], 0  s  1} and refer to Lemma 2. From the proof of the latter we see that also the remainder term Rt0,T vanishes asymptotically since the estimation error related to ~xp is bounded in Pr0,T probability. Moreover, the functionals on the r.h.s. of (A.52) and (A.53) correspond exactly to the ones in equations (A.10)/(A.11) and (A.14)/(A.15) of Lu¨tkepohl & Saikkonen (2000) since we do not have to consider a constant term in the VECM (4.13). Thus, Lemma A11 will hold.
Finally, Proposition 2 follows given Lemmas A9 and A11. As for showing Proposition 1, we refer to the proof approach in Johansen (1995, Ch. 11) performing the necessary modifications due to the bootstrap framework (compare Swensen 2005a) and due to specific characteristics of the GLS testing framework (see Saikkonen & Lu¨tkepohl 2000b and the derivations above).
In line with Swensen (2005a) the limiting results of Propositions 1 and 2 also hold if the bootstrap Algorithm 2 is used. As mentioned in Section 4, the residuals and estimators for the short-run parameters are taken from an estimated unrestricted VAR model in first differences. Accordingly, the conditions (4.2), Assumptions 2 and 3, and Lemmas 1 and 2 are modified. A careful inspection of Lemmas A1-A11 and their proofs shows that the derivations are also valid in terms of SLT2(r0) and GLS2(r0) with obvious adjustments. Hence, it can be shown that SLT2(r0) and GLS2(r0) converge to the same limiting distributions in Pr0,T probability as SLT1(r0) and GLS1(r0) respectively.
Appendix B: Response Surface for the SLT-Test
The response surface for the SLT test by Saikkonen et al. (2006) is computed exactly in the same way as the one for the GLS test, which is presented in Trenkler (2004). The idea of the response surface is to approximate the limiting distribution given in Theorem 1 by a Gamma distribution with two parameters. These parameters can be related to the mean and variance of the distribution of interest. Therefore, the aim of the response surface simulations is to compute the mean and variance for discrete realizations of the limit distribution of SLT . To this end, we consider different sample sizes T and dimensions d = n - r0. These simulated realizations are based on random walks, which are the discrete counterparts of Brownian motions. The dependency of the mean and variance on the sample size T is used to estimate the asymptotic moments for T  . Thus, we only need to present response surface coefficients with re-
49

Table 6. Response Surface for Mean and Variance of Asymptotic Distribution of the Trace Test Statistics SLT (r0)

d2 d
d 1 (constant)

Mean 2.0046 1.7392 1.0027 -0.5442

Variance 3.0125 1.9664
-- 1.4214

spect to the dimension d in order to estimate the asymptotic mean and variance of the relevant distributions. These coefficients are given in Table 6 and refer to polynomials of d. The calculated moments are then used to fit an approximating Gamma distribution, which can be used to compute p-values or arbitrary quantiles. Further details can be found in Trenkler (2004).
References
Bartlett, M. S. (1937). Properties of sufficiency and statistical tests, Proceedings of the Royal Society of London, Series A 160: 268­282.
Basawa, I. V., Mallik, A. K., McCormic, W. P., Reeves, J. H. & Taylor, R. L. (1991). Bootstrap test of significance and sequential bootstrap estimation for unstable first order autoregressive processes, Communications in Statistcs A 20: 1015­1026.
Bru¨ggemann, R. (2004). Model Reduction Methods for Vector Autoregressive Processes, Springer-Verlag, Berlin.
Chang, Y. & Park, J. Y. (2003). A sieve bootstrap for the test of a unit root, Journal of Time Series Analysis 24: 379­400.
Davidson, R. & MacKinnon, J. G. (2000). Bootstrap tests: How many bootstraps?, Econometric Reviews 19: 55­69.
Davidson, R. & MacKinnon, J. G. (2006). The power of bootstrap and asymptotic tests, Journal of Econometrics, forthcoming.
Doornik, J. A. (1998). Approximations to the asymptotic distributions of cointegration tests, Journal of Economic Surveys 12: 573­593.
50

Hamilton, J. D. (1994). Time Series Analysis, Princeton University Press, Princeton.
Hansen, P. R. & Johansen, S. (1998). Workbook on Cointegration, Oxford University Press, Oxford.
Ha¨rdle, W., Horowitz, J. & Kreiß, J.-P. (2003). Bootstrap methods for time series, International Statistical Review 71: 435­459.
Harris, R. I. D. & Judge, G. G. (1998). Small sample testing for cointegration using the bootstrap approach, Economics Letters 58: 31­37.
Horowitz, J. L. (2001). The bootstrap, in J. J. Heckmann & E. E. Leamer (eds), Handbook of Econometrics, Vol. 5, North-Holland, Amsterdam.
Horowitz, J. L. (2003). The bootstrap in econometrics, Statistical Science 18: 211­218.
Hubrich, K., Lu¨tkepohl, H. & Saikkonen, P. (2001). A review of systems cointegration tests, Econometric Reviews 20: 247­318.
Johansen, S. (1988). Statistical analysis of cointegration vectors, Journal of Economic Dynamics and Control 12: 231­254.
Johansen, S. (1995). Likelihood-Based Inference in Cointegrated Vector Autoregressive Models, Oxford University Press, Oxford.
Johansen, S. (2002). A small sample correction for the test of cointegrating rank in the vector autoregressive model, Econometrica 70: 1929­1961.
King, R. G., Plosser, C. I., Stock, J. H. & Watson, M. W. (1991). Stochastic trends and economic fluctuations, American Economic Review 81: 819­840.
Lu¨tkepohl, H. & Kra¨tzig, M. (2004). Applied Time Series Econometrics, Cambridge University Press, Cambridge.
Lu¨tkepohl, H. & Saikkonen, P. (1997). Testing for the cointegration rank of a VAR process with a time trend, Discussion Paper 79, SFB 373, Humboldt-Universita¨t zu Berlin.
Lu¨tkepohl, H. & Saikkonen, P. (2000). Testing for the cointegration rank of a VAR process with a time trend, Journal of Econometrics 95: 177­198.
51

MacKinnon, J. G. (2002). Bootstrap inference in econometrics, Canadian Journal of Economics 35: 615­645.
Mantalos, P. & Shukur, G. (2001). Bootstrapped johansen tests for cointegration relationships: A graphical analysis, Journal of Statistical Computations and Simulation 68: 351­371.
Paparoditis, E. & Politis, D. N. (2003). Residual-based block bootstrap for unit root testing, Econometrica 71: 813­855.
Park, J. Y. (2002). An invariance principle for sieve bootstrap in time series, Econometric Theory 18: 469­490.
Park, J. Y. (2003). Bootstrap unit root tests, Econometrica 71: 1845­1895.
Paruolo, P. (2001). The power of lambda max, Oxford Bulletin of Economics and Statistics 63: 395­403.
Pollard, D. (1984). Convergence of Stochastic Processes, Spinger, New York.
Rahbek, A. & Mosconi, R. (1999). Cointegration rank inference with stationary regressors in VAR models, Econometrics Journal 2: 76­91.
Saikkonen, P. & Lu¨tkepohl, H. (1997). Trend adjustment prior to testing for the cointegration rank of a VAR process, Discussion paper 84, SFB 373, Humboldt-Universita¨t zu Berlin.
Saikkonen, P. & Lu¨tkepohl, H. (2000a). Testing for the cointegration rank of a VAR process with structural shifts, Journal of Business & Economic Statistics 18: 451­464.
Saikkonen, P. & Lu¨tkepohl, H. (2000b). Trend adjustment prior to testing for the cointegration rank of a VAR process, Journal of Time Series Analysis 21: 435­456.
Saikkonen, P., Lu¨tkepohl, H. & Trenkler, C. (2006). Break date estimation for VAR processes with level shift with an application to cointegration testing, Econometric Theory, 22: 15­ 68.
Swensen, A. R. (2005a). Bootstrap algorithms for testing and determining the cointegration rank in VAR models, mimeo, revised version of the Statistical Research Report, No. 5, 2004, with the same titel, Department of Mathematics, University of Oslo.
52

Swensen, A. R. (2005b). Finite sample properties of a bootstrap procedure for estimation of rank in reduced rank VAR-models, mimeo, Department of Mathematics, University of Oslo.
Toda, H. Y. (1994). Finite sample properties of likelihood ratio tests for cointegrating ranks when linear trends are present, The Review of Econometrics and Statistics 76: 66­79.
Toda, H. Y. (1995). Finite sample performance of likelihood ratio tests for cointegrating ranks in vector autoregressions, Econometric Theory 11: 1015­1032.
Trenkler, C. (2002). Testing for the Cointegrating Rank in the Presence of Level Shifts, Shaker Verlag, Aachen.
Trenkler, C. (2004). Determining p-values for systems cointegration tests with a prior adjustment for deterministic terms, CASE Discussion Paper 37, Humboldt-Universita¨t zu Berlin.
van Giersbergen, N. P. A. (1996). Bootstrapping the trace statistic in VAR models: Monte Carlo results and applications, Oxford Bulletin of Economics and Statistics 58: 391­408.
53

SFB 649 Discussion Paper Series 2006
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Calibration Risk for Exotic Options" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
002 "Calibration Design of Implied Volatility Surfaces" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
003 "On the Appropriateness of Inappropriate VaR Models" by Wolfgang Härdle, Zdenk Hlávka and Gerhard Stahl, January 2006.
004 "Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification" by Harald Uhlig, January/February 2006.
005 "British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis" by Enzo Weber, January 2006.
006 "A Combined Approach for Segment-Specific Analysis of Market Basket Data" by Yasemin Boztu and Thomas Reutterer, January 2006.
007 "Robust utility maximization in a stochastic factor model" by Daniel Hernández­Hernández and Alexander Schied, January 2006.
008 "Economic Growth of Agglomerations and Geographic Concentration of Industries - Evidence for Germany" by Kurt Geppert, Martin Gornig and Axel Werwatz, January 2006.
009 "Institutions, Bargaining Power and Labor Shares" by Benjamin Bental and Dominique Demougin, January 2006.
010 "Common Functional Principal Components" by Michal Benko, Wolfgang Härdle and Alois Kneip, Jauary 2006.
011 "VAR Modeling for Dynamic Semiparametric Factors of Volatility Strings" by Ralf Brüggemann, Wolfgang Härdle, Julius Mungo and Carsten Trenkler, February 2006.
012 "Bootstrapping Systems Cointegration Tests with a Prior Adjustment for Deterministic Terms" by Carsten Trenkler, February 2006.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

