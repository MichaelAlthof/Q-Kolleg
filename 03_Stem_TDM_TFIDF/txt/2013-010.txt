BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2013-010
Composite Quantile Regression for the SingleIndex Model
Yan Fan* Wolfgang Karl Härdle**
Weining Wang** Lixing Zhu***
* Renmin University of China, Beijing, China ** Humboldt-Universität zu Berlin, Germany
*** Hong Kong Baptist University This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Submitted to the Annals of Statistics
COMPOSITE QUANTILE REGRESSION FOR THE SINGLE-INDEX MODEL
By Yan Fan,Wolfgang Karl Ha¨rdle,Weining Wang and Lixing Zhu§
Renmin University of China, Beijing, China, Humboldt-Universita¨t zu Berlin, Germany and Hong Kong Baptist University, Hong Kong, China§
Abstract Quantile regression is in the focus of many estimation techniques and is an important tool in data analysis. When it comes to nonparametric specifications of the conditional quantile (or more generally tail) curve one faces, as in mean regression, a dimensionality problem. We propose a projection based single index model specification. For very high dimensional regressors X one faces yet another dimensionality problem and needs to balance precision vs. dimension. Such a balance may be achieved by combining semiparametric ideas with variable selection techniques.
1. Introduction. Regression between response Y and covariates X is a standard element of statistical data analysis. When the regression function is supposed to be estimated in a nonparametric context, the dimensionality of X plays a crucial role. Among the many dimension reduction techniques the single index approach has a unique feature: the index that yields interpretability and low dimension simultaneously. In the case of ultra high dimensional regressors X though it suffers, as any regression method, from singularity issues. Efficient variable selection is here the strategy to employ. Specifically we consider a composite regression with general weighted loss and possibly ultra high dimensional variables. Our setup is general, and includes quantile, expectile (and therefore mean) regression. We offer theoretical properties and demonstrate our method with applications to firm risk analysis in a CoVaR context.
Quantile regression(QR) is one of the major statistical tools and is "gradually developing into a comprehensive strategy for completing the regression prediction" [13]. In many fields of applications like quantitative finance, econometrics, marketing and also in medical and biological sciences, QR is a fundamental element for data analysis, modeling and
The financial support from the Deutsche Forschungsgemeinschaft via CRC 649 "O¨ konomisches Risiko", Humboldt-Universita¨t zu Berlin, and the University Grants Council of Hong Kong is gratefully acknowledged. We also gratefully acknowledge the funding from DAAD ID 50746311.
Keywords and phrases: Quantile Single-index Regression, Minimum Average Contrast Estimation, CoVaR estimation, Composite quasi-maximum likelihood estimation, Lasso, Model selection, JEL Classification: C00, C14, C50, C58, AMS codes: 62G08; 62G20; 62P20
1 -aos ver. 2012/08/31 file: 20130205*Hae*Wan*Zhu*Composite*Quantile*Regression*SIM.tex date: February 6, 2013

inference. An application in finance is the analysis of conditional Value-at-Risk (VaR). [5] proposed the CaViaR framework to model VaR dynamically. [12] used their QR techniques to test heteroscedasticity in the field of labor market discrimination. Like expectile analysis it models the conditional tail behavior.
The QR estimation implicitly assumes an asymmetric ALD (asymmetric Laplace distribution) likelihood, and may not be efficient in the QMLE case. Therefore, different types of flexible loss functions are considered in the literature to improve the estimation efficiency, such as, composite quantile regression, [29], [9] and [10]. Moreover, [3] proposed a general loss function framework for linear models, with a weighted sum of different kinds of loss functions, and the weights are selected to be data driven. Another special type of loss considered in [17] corresponds to expectile regression (ER) that is in spirit similar to QR but contains mean regression as its special case. Nonparametric expectile smoothing work with application to demography could be found in [19]. The ER curves are alternatives to the QR curves and give us an alternative picture of regression of Y on X.
The difficulty of characterizing an entire distribution partly arises from the high dimensionality of covariates, which asks for striking a balance between model flexibility and statistical precision. To crack this tough nut, dimension reduction techniques of semiparametric type such as the single index model came into the focus of statistical modeling. [23] considered quantile regression via a single index model. However, to our knowledge there are no further literatures on generalized QR for the single-index model.
In addition to the dimension reduction, there is however the problem of choosing the right variables for projection. This motivates our second goal of this research: variable selection. [14], [22] and [27] focused on variable selection in mean regression for the single index model. Considering the uncertainty on the multi-index model structure, we restrict ourselves to the single-index model at the moment. An application of our research is presented in the relevant financial risk area: to investigate how the revenue distribution of companies depends on financial ratios describing risk factors for possible failure. Such kind of research has important consequences for rating and credit scoring.
When the dimension of X is high, severe nonlinear dependencies between X and the expectile (quantile) curves are expected. This triggers the nonparametric approach, but in its full gear, it runs into the "curse of dimensionality" trap, meaning that the convergence rate of the smoothing techniques is so slow that it is actually impractical to use in such situations. A balanced dimension reduction space for quantile regression is therefore needed. The MAVE technique, [24] provides us 1) with a dimension reduction and 2) good numerical properties for semiparametric function estimation. The set of ideas presented there, however, have never been applied to composite quantile framework or an even more general composite quasi-likelihood framework. The semiparametric multi-index approach that we consider herein will provide practitioners with a tool that combines flexibility in modeling with applicability for even very high dimensional data. Consequently the curse of
2

dimensionality is circumvented. The Lasso idea in combination with the minimum average contrast estimate (MACE) technique will provide a set of relevant practical techniques for a wide range of disciplines. The algorithms used in this project are published on the quantlet database www.quantlet.org.
This article is organized as follows: In Section 2, we introduce the basic setup and the estimation algorithm. In Section 3, we build up asymptotic theorems for our model. In Section 4, simulations are carried out. In Section 5, we illustrate our estimation with an application in financial market. All the technical details are delegated to appendix.

2. MACE for Single Index Model. Let X and Y be p dimensional and univariate random elements respectively, (p can be very large, namely of the rate exp(n), where (
is a constant). The single index model is defined to be:

(2.1)

Y = g(X ) + ,

where g(·) : R1 - R1 is an unknown smooth link function,  is the vector of index parameters,  is a continuous variable with mean zero. The interest here is to simultaneously estimate  and g(·). Different assumptions on the error structure will give us quantile,
expectile or mean regression scenarios.

2.1. Quasi-Likelihood for The Single Index Model. There exist several estimation technique for (2.1), among these the ADE method as one of the oldest ones [7]. The semiparametric SIM (2.1) also permits a one-step projection pursuit interpretation, therefore estimation tools from this stream of literature might also be employed [8]. The MAVE technique aiming at simultaneous estimation of (, g(·)) has been proposed by [24]. Here we will apply a minimum contrast approach, called MACE. The MACE technique uses similar to MAVE a double integration but is different since the squared loss function is replaced by a convex contrast. Here we generalize MAVE in 3 ways. First, we generalize the setting to weighted loss functions that allow us to identify and estimate conditional quantiles, expectiles and other tail specific objects. Second, we consider the situation where p   might be very large and therefore will add penalty terms that yield automatic model selection of e.g. Lasso or SCAD type. Third, we implement a composite estimation technique for estimating  that involves possibly many estimates.
In a quasi maximum likelihood (or equivalently a minimum contrast) framework the direction  (for known g(·)) is the solution of

(2.2)

min E w{Y - g(X )},

3

K
with the general quasi-likelihood loss function w(·) = wkk(·), where 1(·) , . . . , K(·)
k=1
are convex loss functions and w1, ..., wK are positive weights. This weighted loss function form includes many situations such as ordinary least square, quantile regression(QR),
expectile regression(ER), composite quantile regression(CQR) and so on. For model iden-
tification, we assume that the L2-norm of ,  2 = 1 and the first component of  is positive.
For example when K = 1, the QR setting means to take the loss function as:

(2.3)

w(u) = 1(u) =  u1(u > 0) + (1 -  )u1(u < 0),

Moreover, for ER with K = 1, we have:

(2.4)

w(u) = 1(u) =  u21(u > 0) + (1 -  )u21(u < 0).

More general, the CQR setting employs K different quantiles 1, 2, . . . , K, with wk = 1/K, k = 1, . . . , K and

(2.5)

k(u) =  (u - bk)1(u - bk > 0) + (1 -  )(u - bk)1(u - bk < 0),

where bk is the k quantile of the error distribution, see [3]. Let us now turn to the idea of MACE. First, we approximate g(Xi ) for x near Xi:

(2.6)

g(Xi )  g(x ) + g (x )(Xi - x) ,

In the context of local linear smoothing, a first order proxi of  (given x) can therefore be constructed by minimizing:

(2.7)

Lx() d=ef E w{Y - g(x ) - g (x )(Xi - x) },

The empirical version of (2.7) requires minimizing, with respect to :

n
(2.8) Ln,x() d=ef n-1 w{Yi - g(x ) - g (x )(Xi - x) }Kh{(Xi - x) }
i=1

Employing now the double integration idea of MAVE, i.e. integrating with respect to the edf of the X variable yields as average contrast:

(2.9)

nn

Ln() d=ef n-2

w Yi - g(Xj ) - g (Xj )(Xi - Xj) 

j=1 i=1

Kh{(Xi - Xj) }

where Kh(·) is the kernel function, Kh(u) = h-1K(u/h), h a bandwidth.

For simplicity, from now on we write g(Xj ) and g (Xj ) as a(Xj) and b(Xj) or aj and bj respectively. The calculation of the above minimization problem can be decomposed into two minimization problems, motivated by the proposal in [15],
4

· Given , the estimation of a(·) and b(·) are obtained through local linear minimization.
· Given a(·) and b(·), the minimization with respect to  is carried out by the interior point method.

2.2. Variable Selection for Single Index Model. The dimension p of covariates is large, even p = O{exp(n)}, therefore selecting important covariates is a necessary step. Without loss of generality assume that the first q components of , the true value of , are
non-zero. To point this out write  = ((1) , (0) ) with (1) d=ef (1, . . . , q) = 0 and
(0) d=ef (q+1, . . . , p) = 0 element-wise. Accordingly we denote X(1) and X(0) as the first q and the last p - q elements of X, respectively.

Suppose {(Xi, Yi)}ni=1 be n i.i.d. copies of (X, Y ). Consider now estimating the SIM coefficient  by solving the optimization problem

(2.10)

nn

p

min n-1

w

(aj ,bj ) s,

j=1 i=1

Yi - aj - bjXij 

ij() + (|^l(0)|)|l|,
l=1

where Xij d=ef Xi - Xj, ij() d=ef Kh(Xij )/ n Kh(Xij ). And (t) is some non-negative
i=1
function, and ^(0) is an initial estimator of  (eg. linear QR with variable selection). The
penalty term in (2.10) is quite general and it covers the most popular variable selection
criteria as special cases: the Lasso [21] with (x) =  , the SCAD [6] with

(x) = {1(x  ) +

(a - x)+ (a - 1)

1(x

>

)},

(a > 2)

and (x) = |x|-a for some a > 0 corresponding to the adaptive Lasso [28].

We propose to estimate  in (2.10) with an iterative procedure described below. Denote ^w the final estimate of . Specifically, for t = 1, 2, · · · , iterate the following two steps. Denote ^(t) as the estimate at step t.

· Given ^(t), standardize ^(t) so that ^(t) has length one and positive first component. Then compute

(2.11)

n
(a^(jt), ^b(jt)) d=ef arg min w Yi - aj - bj Xij ^(t) ij (^(t)).
(aj ,bj ) s i=1

· Given (a^j(t), ^bj(t)), solve

nn

p

(2.12) ^(t+1) = arg min


w Yi - a^(jt) - ^b(jt)Xij  ij (^(t)) + d^(lt)|l|,

j=1 i=1

l=1

5

where d^l(t) d=ef (|^l(t)|). Please note here that the kernel weight ij(·) use the ^(t) from the step before.
When choosing the penalty parameter , we adopt a Cp-type criterion as in [26] instead of the computationally involved cross validation method. We choose the optimal weights of the convex loss functions w by minimizing the asymptotic variance of the resulting estimator of , and the bandwidth by criteria minimizing the integrated mean squared errors of the estimator of g(·).

3. Main Theorems. Define ^w d=ef (^w(1), ^w(2)) as the estimator for  attained by the procedure in (2.11) and (2.12). Let ^w(1) and ^w(2) be the first q components and the remaining p - q components of ^w respectively. If in the iterations, we have the initial estimator ^((10)) as a n/q consistency one for (1) (2.12), we will obtain with a very high probability, an oracle estimator of the following type, say ~w = (~w(1), 0 ) , since the oracle knows the true model M d=ef {l : l = 0}. The following theorem shows that the penalized estimator enjoys the oracle property. Define ^0 (note that it is different from the initial estimator ^((10))) as the minimizer with the same loss in (2.2) but within subspace {  Rp : Mc = 0}.
Theorem 1. Under conditions 1-5, the estimators ^0 and ^w exist and coincide on a set with probability tending to 1. Moreover,

(3.1)

P(^0 = ^w)  1 - (p - q) exp(-C n)

for a positive constant C .

Theorem 2. Under conditions 1-5, we have

(3.2)

^w(1) - (1) = Op{(Dn + n-1/2)q}

For any unit vector b in Rq, we have

(3.3)

b C0-(11)n(^w(1) - (1)) -L N(0, w2 )

where C0(1) d=ef E{[g (Zi)]2[E(X(1)|Zi) - Xi(1)][E(X(1)|Zi) - Xi(1)] }, Zi d=ef Xi , w() is a choice of the subgradient of w() and w2 d=ef E[w(i)]2/[2 E w(i)]2, where

(3.4)

2 E w(·) =

2 E w(i - v)2 v2

.
v=0

6

Let us now look at the distribution of g^(·) and g^ (·), the estimator of g (·).

Theorem 3. Under conditions 1-5,

Let µj d=ef ujK(u)du and j d=ef ujK2(u)du, j = 0, 1, 2. For any interior point z = x , fZ(z) is the density of Zi, i = 1, . . . , n, if nh3   and h  0,, we have

 nh

fZ (z)/(0w2 )

g(x

) - g(x

) - 1 h2g 2

(x

)µ2 E w



-L N (0, 1) ,

Also, we have

 nh3

{fZ (z)µ22}/(2w2 ) g (x ) - g (x )

-L N (0, 1)

4. Simulation. In this section, we evaluate our technique in several settings, involv-
ing different combinations of link functions g(·), distributions of , and different choices of (n, p, q,  )s, where n is the sample size, p is the dimension of the true parameter , q is the number of non-zero components in , and  represents the quantile level. The
evaluation is first done with a simple quantile loss function, and then with the composite
L1 - L2 and the composite quantile cases.

4.1. Link functions. Consider the following nonlinear link functions g(·)s. Model 1:

(4.1)

Yi = 5 cos(D · Zi) + exp(-D · Zi2) + i,

where Zi = Xi , D = 0.01 is a scaling constant and i is the error term. Model 2:

(4.2)

Yi = sin{(A · Zi - B)} + i,

with the parameters A = 0.3, B = 3. Finally Model 3 with D = 0.1:

(4.3)

Yi = 10 sin(D · Zi) + | sin(0.5 · Zi) + i|,

4.2. Criteria. For estimation accuracy for  and g(·), we use following five criteria to measure:

1) Standardized L2 norm: 2) Sign consistency:

Dev d=ef

 -   2

2,

p
Acc d=ef |sign(l) - sign(l)|,
l=1
7

3) Least angle:

Angle d=ef

< ,  > ,

 2 ·  2

4) Relative error:

n
Error d=ef n-1

g(Zi) - g(Zi) ,

i=1 g(Zi)

5) Average squared error:

ASE(h) d=ef 1 n n

g(Zi) - g(Zi) 2.

i=1

4.3. L1-norm quantile regression. In this subsection, consider the L1-norm quantile regression described by [16]. The initial value of  can be calculated by the L1-norm quantile regression, then the two-step iterations mentioned in theoretical part are performed. Recall that X is a p × n matrix, and q is the number of non-zero components in . The jth column of X is an i.i.d. sample from N(j/2, 1). Two error distributions are considered: i  N(0, 0.1) and t(5). Note that (1) is the vector of the non-zero components in . In the simulation, we consider different (1): (1) = (5, 5, 5, 5, 5), (1) = (5, 4, 3, 2, 1) and (1) = (5, 2, 1, 0.8, 0.2). Here the indices Zi's are re-scaled to [0, 1] for nonparametric estimation. The bandwidth is selected by as in [25]:
h = hmean  (1 -  ){-1( )}-2 0.2.
where hmean can be calculated by using the direct plug-in methodology of a local linear regression described by [18]. To see the performance of the bandwidth selection, we compare the estimated link functions with different bandwidths. Figure 1 is an example showing the true link function (black) and the estimated link function (red). The left plot in Figure 1 is with the bandwidth (h = 0.68) selected by applying the aforementioned bandwidth selection. We can see that the estimated link function curve is relatively smooth. The middle plot shows the estimated link function with decreased bandwidth (h = 0.068). It can be seen that the estimated curve is very rough. The right plot shows that the estimated link function with increased bandwidth (h = 1), the deviation between the estimated link function curve and the true curve is very large. From this comparison we may consider the aforementioned bandwidth selection preforms well.

Figure 1 about here

Table 1 shows the criteria evaluated with different models and quantile levels. Here (1) = (5, 5, 5, 5, 5), the error  follows a N (0, 0.1) distribution or follows a t(5) distribution. In 100 simulations we set n = 100, p = 10, q = 5. Standard deviations are given in brackets.
We find that for quantile levels 0.95 and 0.05 the errors are usually slightly larger than
8

the median. Although the estimation for the nonlinear model 2 are not as good as model 1 and model 3, the error is still moderate. Figures 2 to Figure 4 present the plots of the true link function against the estimated ones for different quantile levels.
Table 1 and Figures 2 to 4 about here
Table 2 reports the criteria evaluated under different (1) cases. In this table two different (1) are considered: (a) (1) = (5, 4, 3, 2, 1), (b) (1) = (5, 2, 1, 0.8, 0.2), the error  follows a N (0, 0.1) distribution. In 100 simulations we set n = 100, p = 10, q = 5,  = 0.95. Standard deviations are given in brackets. We notice that for the case (b), the estimation results are not better than (a) since the smaller values of (1) in case (b) would be estimated as zeros, and the estimation of the link function would be affected as well. Figure 5 and Figure 6 are the plots of the estimated link functions in these two cases.
Table 2 and Figures 5 to 6 about here
Table 3 shows the criteria evaluated under p > n case. Here (1) = (5, 5, 5, 5, 5), the error  follows a N (0, 0.1) distribution. In 100 simulations we set n = 100, p = 200, q = 5,  = 0.05. Standard deviations are given in brackets. We find that the errors are still moderate in p > n situation compared with Table 1. Figure 7 shows the graphs in this case.
Table 3 and Figures 7 about here

4.4. Composite L1-L2 Regression. In this subsection, a combined L1 and L2 loss is

concerned and thus, the corresponding optimization is formed as:

(4.4)

nn

p

arg min w1
,g(·)

|Yi - g(Xi )|i() + w2

{Yi - g(Xi )}2i() + n

(|l|)|l| .

i=1 i=1

l=1

It can be further formulated as

(4.5)

np

arg min {w1
,g(·)

|Yi - g(Xi )|-1i() + w2}|Yi - g(Xi )|2i() + n

(|l|)|l| .

i=1 l=1

Let Resit d=ef Yi - g^t(Xi ^t) be the residual at t-th step, and the final estimate can be acquired by the iteration until convergence:

(4.6)

np

arg min
,g(·)

{w1

|Resit|-1i(t) + w2}|Yi - g(Xi
i=1

)|2i(^t) + n (|l|)|l|
l=1

.

9

Three different settings are conducted. The results are reported in Table 4. Figure 8 (the upper panel) shows the difference between the estimated and true g(·) functions. The level of estimation error is roughly the same as the previous level. Also the results would not change too much w.r.t. the error distribution and the increasing dimension of p, since only the dimension of q matters.
Table 4 and Figure 8 about here.

4.5. Composite L1 Quantile Regression. Use MM algorithm for a large scale regression problem. Table 5 shows the estimation quality. Compared with the results in Table 1, the estimation efficiency is improved, even in the case of p > n. Figure 8 presents the plots of the estimated link functions for different models using both the composite L1 regression and the L1-L2 regression.
Table 5 and Figure 8 about here

5. Application. In this section, we apply the proposed methodology to analyze risk conditional on macroprudential and other firm variables for small financial firms. More specifically, for small financial firms, we aim to detect the contagion effects and the potential risk contributions from larger firms and other market variables. As a result one identifies a risk index, which is expressed as a linear combination, composed of selected large firm returns and market prudential variables.

5.1. Data. The firm data are selected according to the ranking of NASDAQ. We take as example city national corp. CYN as our objective. The remaining 199 financial institutions together with 7 lagged macroprudential variables are chosen as covariates. The list of these firms comes from the website.1 The daily stock prices of these 200 firms are from Yahoo Finance for the period from January 6, 2006 to September 6, 2012. The descriptive statistics of the company, the description of the macroprudential variables and the list of the firms (Table 7 to Table 9) can be found in the Appendices. We consider a two step regression procedure. The first one is a quantile regression, where one regresses log returns of each covariate on all the lagged macroprudential variables:

(5.1)

Xi,t = i + i Mt-1 + i,t,

where Xi,t represents the asset return of financial institution i at time t. We apply quantile regression proposed by [11]. Then the VaR of each firm with F-i,1t ( |Mt-1) = 0 can be
obtained by:

(5.2)


V aRi,t = i + i Mt-1,

1 http://www.nasdaq.com/screening/companies-by-industry.aspx?industry=Finance.
10

Then the second regression is performed using our method, where the response variable is log returns of CYN, and the explanatory variable are the log returns of those covariates and the lagged macroprudential variables:

(5.3)

Xj,t = g(S j|S) + j,t,

where S d=ef [Mt-1, R], R is a vector of log returns for different firms. j|S is a p × 1 vector, p large. With F-j,1t( |S) = 0 the CoVaR is estimated as:

(5.4)


CoV aRj|S = g(S j|S),

where S d=ef [Mt-1, V ], where V is the estimated VaR in (5.2).

Then we proceed the backtesting. The days on which the log returns are lower than the VaR or CoVaR can be called violations. The violation sequence is defined as follows:



It =

1, 0,

Xi,t < V aRi,t; otherwise.

Generally, It should be a martingale difference sequence. Then we apply CaViaR test, see by [2]. The CaViaR test model:

It =  + 1It-1 + 2V aRt + ut.

The test procedure is to estimate 1 and 2 by logistic regression, then Wald's test is applied with null hypothesis: 1 = 2 = 0.

5.2. Results. We use a moving window size of n = 126 to calculate VaR of the log returns for the 199 firms. We also calculated the VaR of CYN. Figure 10 and Figure 11 show one example of the estimated VaR of one covariate (JPM) and the estimated VaR of CYN, respectively. It can be seen that the estimated VaR becomes more volatile when volatility of the returns is large.
Figures 10 and 11 about here.
Then the estimation of the CoVaR for CYN is conducted by applying a moving window size of n = 126. L1-norm quantile regression is applied with  = 0.05. Recall there are p = 206 covariates, the CoVaR is estimated with different variables selected in each window. Figure 12 shows the result.
Figures 12 about here. 11

Figures 13 and 14 show the estimated link functions against the indices in different window. We find some evidence on nonlinearity of the link function, although Figure 14 looks linear.
Figures 13 and 14 about here.
Figure 15 summarized the selection frequency of the firms and macroprudential variables for all the windows. The variable 187, "Radian Group Inc. (RDN)" is the most frequently selected variable with frequency 557.
Figure 15 about here.
Next we apply the backtesting. Figure 16 shows the I^t sequence of V aR of CYN, there are totally 8 violations. Since T = 1543, we get that  = 0.005. And the p value of wald test is 0.87, we can not reject the null hypotheses. From Figure 17 we get the I^t sequence of CoV aR of CYN, there are 53 violations for T = 1543, and  = 0.034. Since the p value of wald test is 0.36, the null hypotheses is not rejected. Therefore both VaR and CoVaR algorithms perform well.
Figures 16 and 17 about here.

6. Appendices.

6.1. Proof of the main theorems. We make the following assumptions for the proofs of the theorems in this paper.

Condition 1. The kernel K(·) is a continuous symmetric pdf having at least four finite moments. The link g(·) has a continuous second derivative.

Condition 2. Assume that k(x) are all strictly convex, and suppose k(x), the derivative (or a subgradient of ) of k(x), satisfies E k(i) = 0 and inf|v|c  E k(i - v) = C1 where  E k(i - v) is the partial derivative with respect to v, and C1 is a constant.

Condition 3. In the case of composite quantile, K > 1 assume that the error term i is independent of Xi. For K = 1 with a quantile and expectile loss relax to Fy-|x1( ) = 0.

Let X(1)i denote the sub-vector of Xi consisting of its first q elements. Let Zi d=ef Xi 

and Zij = Zi - Zj . Define C0(1) d=ef E{[g (Zi)]2(E(Xi(1)|Zi) - Xi(1))(E(Xi(1)|Zi - Xi(1))} ,

and the matrix C0(1) constants L1 and L2.

satisfies 0 < L1  min(C0(1))  max(C0(1))  L2 < There exists a constant c0 > 0 such that ni=1{ Xi(1)

/nfo}r2+pco0 sitiv0e,

12

with 0 < c0 < 1. vij d=ef Yi - aj - bjXij . Also
X(0)ij ij X(1)ij  E w(vij ) 2, = O(n1-1 ).
ij

Condition 4. The penalty parameter  is chosen such that Dn = O{n-1/2}, with Dn d=ef max{dl : l  M} = O(n1-2/2), dl d=ef (|l|), M = {l : l = 0} be the true model. Furthermore assume qh  0 as n goes to infinity, q = O(n2), p = O{exp(n)}, nh3   and h  0. Also, 0 <  <  < 2/2 < 1/2, 2/2 < 1 < 1.

Conditions 5. The error term i satisfies E i = 0 and Var(i) < . Assume that

(6.1)

E wm(i)/m!  s0M m

where s0 and M are constants.

Condition 1 is commonly-used and the standard normal pdf is a kernel satisfying this condition. Condition 2 is made on the weighted loss function so that it admits a quadratic approximation. Under condition 3, the matrix in the quadratic approximation is nonsingular, so that the resulting estimate of  has an non-degenerate limiting distribution. Condition 4 guarantees that the proposed variable and estimation procedure for  is modelconsistent. Condition 5 implies a certain tail behavior that we employ in all statistics argument.
Recall ^0 as the minimizer with the same loss in (2.2) but within the subspace {  Rp : Mc = 0}. The following lemma assures the consistency of ^0,

Lemma 1. Under conditions 1-5, recall dj =  |j| , we have that

(6.2)

^0 -  = Op q/n +  d(1)

where d(1) is the subvector of d = (d1, · · · , dp) which contains q elements corresponding to the nonzero (1).

Proof. Note that the last p - q elements of both ^0 and  are zero, so it is sufficient to prove ^(01) - (1) = Op q/n +  d(1) .

Write

nn

p

L~n() =

w Yi - aj - bjXij  ij() + n dl|l|.

j=1 i=1

l=1

13

We first show for n = O(1):

P

inf
u =1

L~n((1) + nu, 0) > L~n()

1

Construct n  0 so that for a sufficiently large constant B: n > B · q/n +  d(1) . We will show that by the local convexity of L~n((1), 0) near (1), there exists a unique minimizer inside the ball {(1) : (1) - (1)  n} with probability tending to 1.
Let X(1)ij denote the subvector of Xij consisting of its first q components. By Taylor expansion at n = 0:
L~n((1) + nu, 0) - L~n((1), 0) = L~n((1) + nu, 0) - L~n((1), 0) - E{L~n((1) + nu, 0) - L~n((1), 0)}
(T1n)
+ E{L~n((1) + nu, 0) - L~n((1), 0)}
(T2n)

Taking Taylor expansion for the term T1n, T2n respectively, where T1n up to 1 order, T2n up to 2 order:

L~n((1) + nu, 0) - L~n((1), 0)

nn

= - n

bj w Yi - aj - bj X(1)ij (1) ij ()X(1)ij u

i=1 j=1

nn

+ n

bj  E w Yi - aj - bj X(1)ij (1) ij ()X(1)ij u

i=1 j=1

nn

- n

bj  E w Yi - aj - bj X(1)ij (1) ij ()X(1)ij u

i=1 j=1

+

1 2

n2

n

n
bj22 E w Yi - aj - bj X(1)ij (1) - bj ¯nX(1)ij u ij ()(X(1)ij u)2

i=1 j=1

q
+ n dl |(1)l + nul| - |(1)l| + Op(n)

l=1

14

nn

= - n

bj w Yi - aj - bj X(1)ij (1) ij ()X(1)ij u

i=1 j=1

+

1 2

n2

n

n
b2j 2 E w Yi - aj - bj X(1)ij (1) - bj ¯nX(1)ij u ij ()(X(1)ij u)2

i=1 j=1

q
+ n dl |(1)l + nul| - |(1)l| + Op(n)

l=1

d=efP1 + P2 + P3 + +Op(n)

where ¯n  [0, n].

Define

ij

d=ef

ij (  ),

it

is

not

difficult

to

derive

that

ij

=

Kh(Zij ) nfZ (Zj )

{1

+

Op(1)}

where

Zi = Xi , Zij = Zi - Zj and fZ (·) is the density of Z = X .

For P1, because u = 1 and Yi = ai + i, we get

nn

|P1|  n

bj w Yi - aj - bj X(1)ij (1) ij X(1)ij {1 + Op(1)}

i=1 j=1

=

n

n
bj
j=1

1n n w
i=1

i + ai - aj - bjZij

Kh (Zij ) fZ (Zj)

X(1)ij

{1 + Op(1)}

=

n

n
bj E i,Xi
j=1

w

i + ai - aj - bjZij

Kh (Zij ) fZ (Zj)

X(1)ij

{1 + Op(1)}

=

n

n
bj E Zi
j=1

E i [w

i + ai - aj - bjZij

] Kh(Zij) fZ (Zj)

E(X(1)ij |Zi)

{1 + Op(1)}

n

= n

bj E[w j + aj - aj ]{E(X(1)j|Zj) - X(1)j} {1 + Op(1)}

j=1

where E i,Xi means taking expectation with respect to (i, Xi). Furthermore we have

n

E bj E[w j + aj - aj ]{E(X(1)j|Zj) - X(1)j}

j=1

 E w2 j + aj - aj E n bj2[E(X(1)j |Zj ) - X(1)j ] [E(X(1)j |Zj ) - X(1)j ] 1/2

j=1

=

 n{E

w2

j + aj - aj

tr(C0(1))}1/2,

15

recall C0(1) d=ef E{[g (Zj)]2(E(X(1)j|Zj) - X(1)j)[E(X(1)j|Zj) - X(1)j]} . We thus arrive at

(6.3)

P1 = Op(nnq)

because tr(C0(1)) = O(q) and E w2 j + aj - aj <  by Condition 3.

For P2, according to the property of kernel estimation, it can be seen that

P2

=

1 2

n2

n

n
b2j  E w

i=1 j=1

Yi - aj - bj Zij - bj ¯nX(1)ij u

Kh (Zij ) nfZ (Zj)

(X(1)ij

u)2{1

+

Op(1)}

=

1 2

n2

n

bj2E{w Yi - aj - bj¯nX(1)iju (X(1)iju)2}{1 + Op(1)}

j=1

Let Hi(c) = inf|v|c  E (i - v). By lemma 3.1 of Portnoy (1984), we have

(6.4)

P2



1 2

n2

n

n
b2j H(n|X(1)ij u| (X(1)ij u)2  cn2n

i=1 j=1

for some positive c.

For P3, it is clear that

(6.5)

q
|P3|  nn dl|ul|  nn d(1)
l=1

Combining (6.3), (6.4)and (6.5), the following inequality holds with probability tending to 1 that

(6.6)

L~n((1) + nu, 0) - L~n((1), 0)  nn cn - q/n -  d(1)

n = B q/n +  d(1) and B is a sufficiently large constant, so that the RHS of (6.6) is larger than 0. Owing to the local convexity of the objective function, there exists a unique minimizer ^(01) such that
^0 -  = ^(01) - (1) = Op q/n +  d(1)
Therefore, (6.2) is proved.

Recall that X = (X(1), X(2)) and M = {1, . . . , q} is the set of indices at which  are nonzero.
16

Lemma 2. Under conditions 1-5, the loss function (2.2) has a unique global minimizer ^ = (^1 , 0 ) , if and only if

(6.7) (6.8)

nn
w Yi - a^j - ^bjXij ^w ^bjX(1)ijij() + nd(1)  sign(^w) = 0
j=1 i=1
z(^w)   n,

where (6.9)

z(^w) d=ef d(-01) 

nn
bj w Yi - aj - bj Xij ^w X(0)ij ij (^w)
j=1 i=1

where  stands for multiplication element-wise.

Proof. According to the definition of ^w, it is clear that ^(1) already satisfies condition (6.7). Therefore we only need to verify condition (6.8).

To prove (6.8), a bound for

(6.10)

nn
bj w Yi - aj - bj Xij  ij X(0)ij
i=1 j=1

is needed. Define the following kernel function

hd(Xi, aj, bj, Yi, Xj, ai, bi, Yj)

n =
2

bj w Yi - aj - bj Xij  ij X(0)ij + biw Yj - ai - biXji  jiX(0)ji

,
d

where {.}d denotes the dth element of a vector, d = 1, . . . , p - q.

According to [20], the proof of theorem B in page 201, and following the Conditions 5:

(6.11)

EF [exp{s · hd(Xi, aj, bj, Yi, Xj, ai, bi, Yj)}] < , 0 < s < s0,

where s0 is a constant.

Define

Un,d

d=ef

1 n(n-1)

(6.10).

1i<jn hd(Xi, aj, bj, Yi, Xj, ai, bi, Yj) as the U - statistics for

Then, for  > 0,

exp - s · EUn,d E exp{s · hd(.)} = 1 + O(s2), s  0. 17

By taking s = /{n2+},  = n1/2+, we have

[n/2]
P(|Un,d - EUn,d| > )  2 exp (-s · ) exp (-s · EUn,d) E exp{shd(.)}

[n/2]
 2 1 + O(s2) exp -2/n2+

 2 exp -Cnn , where Cn is a constant depending on n.

Define

nn

Fn,d d=ef n-1

bj w Yi - aj - bj Xij  ij X(0)ij ,

i=1 j=1

also it is not hard to derive that Un,d = Fn,d. It then follows that

P(|Fn,d - EFn,d| > ) = P(|Un,d - EUn,d| > )  2 exp -Cnn

Define An = { Fn - EFn   }, thus

p-q
P(An)  1 - P(|Fn,d - EFn,d| > )  1 - 2(p - q) exp -Cnn .
d=1

Finally we get that on the set An,

z(^0)  

dM-1c  Fn  +

nn
bj w Yi - aj - bjXij ^0
i=1 j=1

-w Yi - aj - bj Xij  ij X(0)ij 

nn

 O(n1/2+ +

 E w(vij )bj X(1)ij (^(1) - (1))ij X(0)ij ),

i=1 j=1

where vij is between Yi - aj - bjXij  and Yi - aj - bjXij ^0. From Lemma 5.1,

^0 - (1) 2 = Op

 d(1)

 + q/ n

.

Choosing n-1/2+2/2, 0

<

i
2

j
<

X(0)ij ij 1, d(1)

X=(1O)ij(EqDwn()v=ij )O2(,n=2/2OD(nn)1-1

),

q

=

O(n2 ),



=

O(

(n)-1 z(^0) 

=

O{n-1-1(n1/2+

+

n1-1

 q/ n

+



d(1)

n1-1 )}

= O(n-2/2+ + n-1 + n-1+2/2Dn),

18

n/q) =

assume Dn = O(n1-2/2), and let 0 <  <  < 2/2 < 1/2, 2/2 < 1 < 1, with rate p = O{exp(n)}, then (n)-1 z(^0)  = O(1).

Proof of Theorem 2. By Theorem 3.1, ^w(1) = (1) almost surely. It then follows from Lemma 5.2 that
^w(1) - (1) = Op{(Dn + n-1/2)q}.

This completes the first part of the theorem.

Given aj, bj, consistent estimates of aj and bj, the estimate ^w is:

nn

p

^w = arg min


w Yi - aj - bjXij  ij + n (|~l|)|l|

j=1 i=1

l=1

where ij = ij(~) and Xij = Xi - Xj.

Define

^ d=ef

n(^w

-

)

and

Yij

=

Yi

- aj

-

bj Xij .

As

ij (~)

=

Kh (Zij nfZ (Zj

) )

{1

+ Op(1)},

it follows that ^ is the minimizer of

Hn() =

nn

w

Yij

-

n-

1 2

bj

Xij



- w Yij

i=1 j=1

p
+n (|l|)(|l + n-1/2l| - |l|)

l=1

d=ef Q1(){1 + Op(1)} + Q2(),

Kh (Zij ) nfZ (Zj)

{1

+

Op

(1)}

recall fZ(z) is the density function of Z = Xi , i = 1, . . . , n. We study Q1() and Q2() respectively.

Let

ij ( )

d=ef

w

Yij

-

n-

1 2

g

(Zj )Xij



- w Yij

-

n-

1 2

w

(Yij

)g

(Zj)Xij .

It

can

be

seen that

Q1() =

n i=1

n j=1

n-1/2w(Yij )g

(Zj

)

Kh (Zij ) nfZ (Zj)

Xij



+

n i=1

n j=1

ij

(

)

Kh (Zij ) nfZ (Zj)

{1 + Op(1)}

d=ef [A  + I()]{1 + Op(1)}

19

Recall that Yij = i + ai - aj - bjZij + op(1). Therefore we have

A

=

n i=1

n j=1

1 n

w

(i

+

ai

-

aj

-

bj Zij )g

(Zj

)

Kh (Zij ) nfZ (Zj)

Xij

{1

+

Op(1)}

=

n

1 n

w(i)g

(Zi ){E(X |Zi )

-

Xi}{1

+

Op(1)}.

i=1

Similarly we have

I ( )

=

n i=1

n j=1

ij

()

Kh (Zij ) nfZ (Zj)

nn

=

w

Yij

-

n-

1 2

g

(Zj )Xij



- w Yij

i=1 j=1

-n-

1 2

w (Yij )g

(Zj )Xij



Kh (Zij ) nfZ (Zj)

n

=

E i,Xi

w

Yij

-

n-

1 2

g

(Zj )Xij



- w Yij

j=1

-n-

1 2

w(i)g

(Zi)Xij



Kh (Zij ) fZ (Zj)

n

=

E Zi E i,Xi|Zi

w

Yij

-

n-

1 2

g

(Zj )Xij



- w Yij

j=1

-n-

1 2

w(i)g

(Zi)Xij



Kh (Zij ) fZ (Zj)

n

=

E w



-

n-

1 2

g

(Zj )(Xj

-

E(Xj |Zj ))



- E w 

j=1

-n-

1 2

E

[w()]g

(Zj )(Xj

-

E(Xj |Zj ))



n
= (2n)-1 {2 E w(j)} E[g (Zj)]2 {Xj - E(Xj|Zj)}{Xj - E(Xj|Zj)} 

j=1

=

1 2

{

2

E

w(j

)}

E{[g (Zj)]2{Xj - E(Xj|Zj)}{Xj - E(Xj|Zj)} }

d=ef

1 2

{

2

E

w(j

)}

C0[1 + O(1)]

About Q2(), we find that if l = 0, that is q < l  p, n(|l + n-1/2| - |l|) = ||, 20

Lnotahseslpro=w,1iwsee(w|hhalev|n)e(|1ln+ln(|-q1l,|/)2ln| (-|0|lf+lo|r)nw-l1il/l=2c|o0-nva|enrlgd|e)ton0so(i|gnnlly(|)ilf).lIn=t0hfeoforcrasalell

of = q

adaptive 0. Thus < l  p,

otherwise it will converge to infinity. The loss function Hn() gives nontrivial solutions only when l = 0 for all q < l  p. This implies that ^w(2) will converge to zero with

probability one.

When (2) = 0 the loss function

Dn()

=

[-(1)A(1)

+

1 {2 2

E

w(i)}(1)C0(1)(1)]{1

+

O(1)}

where A(1) is the sub-vector of A consisting of its first q components, and C0(1) is the

upn-(le^f(t1)q-×q(1s)u)b=-m[a2trEixwof(Ci)0].-F1Col0-l(o11w)Ain(g1)

quadratic + Op(1).

approximate

lemma,

we

obtain

^(1)

=

n

Recall that A(1) =

1 n

w(i)g

(Zi){E(X(1)|Zi)

-

Xi(1)}{1

+

Op(1)}.

Thus

we

have

i=1

 nb

C01(/12)(^(1) - (1))

=

b [2 E w(i)]-1C0-(11/)2

× 1 n

n

w(i)g (Zi){E(X(1)|Zi) - Xi(1)}{1 + Op(1)}

i=1

-L N(0, w2 )

where
w2 = b [2 E w(i)]-2 E{[w(i)]2}C0-(11/)2 × E{[g (Zi)]2[E(X(1)|Zi) - Xi(1)][E(X(1)|Zi) - Xi(1)] }C0-(11/)2b
= E{[w(i)]2}/[2 E w(i)]2.
Here we have used the fact C0(1) = E{[g (Zi)]2[E(X(1)|Zi) - Xi(1)][E(X(1)|Zi) - Xi(1)] }. The asymptotic normality can be proved.

Proof of Theorem 3. We note that  
nh g^(x ^) - g(x ) = nh g^(x ^) - g^(x ) + nh g^(x ) - g(x )

As w is strictly convex, then g^(·) is continuous almost surely. As qh  0, the consistency

of

^

in

Theorem

2

implies

 nh

g^(x

^) - g(x

)

= Op(1). Consequently it is sufficient

to prove

 nh3

{fZ (z)µ22}/(2w2 ) g(x ) - g(x )

-L N (0, 1)

21

We now prove equation (6.12). Let Zi = Xi  and z = x . Recall that g^(z) = a^ and g^ (z) = ^b where

(6.12)

n

(a^, ^b) d=ef arg min(nh) w Yi - a - b(Zi - u) i()

(a,b)

i=1

where i() = Kh(Zi - z)/

n k=1

Kh(Zk

-

z)

=

{1

+

Op(1)}Kh(Zi

-

zi)/{nfZ (z)}

  Denote  = [ nh{a - g(z)}, nh3{b - g (z)}] ]

and

^

=

 [ nh{g^(z)

-

g(z)},

nh3{g^ (z) - g (z)}] . Also let Zi = (1, (Zi - z)/h) and i = g(Zi) - g(z) - g(z)(Zi - z).

Then ^ is the minimizer of

(6.13)

n
L() = [w i + i - (nh)-1/2Zi 
i=1

- w

i + i

]

K

(Zi - fZ (z)

z)

[1

+

Op

(1)].

By similar Taylor expansion to Q1() in the previous proof, we have

L()

=

1 nh

n
w
i=1

i + i

Zi



Kh(Zi - fZ (z)

z

)

[1

+

Op(1)]

1 +
2nh

n
{2 E w
i=1

i + i

}(Zi

)2

K

(Zi - fZ (z)

z)

[1

+

Op

(1)]

=

An



+

1  2

Sn(1

+

op(1))

where

An

=

1 nh

n
w
i=1

i + i

Zi

K

(Zi - fZ (z)

z)

Sn

=

1 nh

n
{2 E w
i=1

i + i

}ZiZi

K(Zi - z) . fZ (z)

It is easy to see that both An and Sn are sums of independent and identically distributed random elements. Thus the leading term of Sn is

Sn

=

1 E
h

2 E w

i + i

ZiZi

K

(Zi - fZ (z)

z

)

|Zi]

=

1 E
h

2 E w

i + i

|Zi]ZiZi

K(Zi - z) fZ (z)

= 2 E w i

1 ( - u)/h ( - u)/h ( - z)2/h2

 - z d K
hh

= [2 E w i ]

10 0 µ2

{1 + O(1)} = S{1 + O(1)}

22

where µ2 = t2K (t) dt. As w is strictly convex, its second derivative E (·) is nonnegative, which implies Sn is non-negative definite with probability tending to one. And it follows that ^ = -S-1An{1 + Op(1)}.

It is clear that An is asymptotically distributed as a bivariate normal distribution N(mA, ) with mean

mA

=

1 E h

w

i + i

Zi

K

(Zi - fZ (z)

z)

=

1 E h

E

w

i + i

|Zi

Zi

K

(Zi - fZ (z)

z)

= 1 h

w  + g() - g(z) - g (z)( - z) fE()d

×

1 ( - z)/h

K(( - z)/h)d

=

1 h5/2g 2

(z)µ2[ E w()]

1 0

{1 + O(1)}.

As mA = O(1), it can be seen that the asymptotic covariance matrix  is



=

1 E
h

w2 i + i ZiZi

K(Zi - z) 2 fZ (z)

{1 + o(1)}

1 =
h

w2  + g() - g(z) - g (z)( - z) fE()d

×

1 ( - z)/h ( - z)/h ( - z)2/h2

K2(Zi - z) d{1 + o(1)} fZ (z)

=

1 fZ (z)

E[w2 ()]

0 0 0 2

{1 + O(1)}.

where k = tkK2(t)dt.

Thus we finally obtain that as n tends to infinity, ^- S-1mA converges in distribution to N(0, S-1S-1). Slight algebra gives

S-1mA

=

1 h5/2g 2

(z)µ2[ E w



]

1 0

S-1S-1 =

E[w2 ()]

1

{[2 E w  ]}2 fZ (z)

10 0 µ2-1

=

w2

1 fZ (z)

0 0 0 2/µ22

.

This completes the proof.

23

0 0 0 2

10 0 µ-2 1

Considering the algorithm in (2.11 ) and (2.12), for both step involves a plug-in estima-
tor, therefore it would be important for us to check the closeness between L~n() - Ln() and L~n,x() - Ln,x(). We have the following Lemma.

Lemma 3. Assuming Condition 1-5, for a ball with B d=ef { :  -   q/n}, (similar argument see Mammen (2012) Lemma 2)

nn

sup |n-1 w Yi - aj - bjXij  ij() - n-1 w Yi - aj - bjXij  ij()|

B

i=1

i=1

= Op(h-1 q/nn),

with n a slowly varying function. So we need to make sure h-1 q/n = O(1).

Also, Baj d=ef {a~j : aj - a~j  nh-1/2}, Bbj d=ef {~bj : bj - ~bj  nh-1/2}

nn

sup |n-1

w Yi - a~j - ~bjXij  ij()

Baj ,Bbj

j=1 i=1

nn

-n-1

w Yi - aj - bjXij  ij()|

j=1 i=1

= Op(n-1/2n + q/nn).

6.2. Application. The macroprudential variables are the same as suggested by [1] and applied and extended by [4]. The macro variables and the corresponding source are as follows:
1) VIX, which measures the implied volatility in the market. 2) The short term liquidity spread, which is calculated by the difference between the
3-month Treasury repo rate and 3-month Treasury constant maturities. 3) The daily change in the 3-month Treasury constant maturities, which can be defined
as the difference between the current day and the previous day of 3-month Treasury constant maturities. 4) The change in the slope of the yield curve, which is defined by the difference between the 10 year Treasury constant maturities and the 3-month Treasury constant maturities. 5) The change in the credit spread between 10 years BAA corporate bonds and the 10 years Treasury constant maturities. 6) The daily S&P500 index returns. 7) The daily Dow Jones U.S. Real Estate index returns.
24

The repo data can be obtained from the Bloomberg database. The Treasury constant maturities data, 10 year Treasury constant maturities and BAA corporate bonds data can be found in the website of the Federal Reserve Board H.15: http://www.federalreserve.gov/releases/h15/data.htm. Other data are available in Yahoo Finance. The data period of these macro variables is from January 5, 2006 to September 5, 2012, the data frequency is daily.
Table 6 and Figure 9 about here
Table 6 shows the descriptive statistics of this series. While the mean of CYN before crisis (i.e. before September 15, 2008) is -0.000365, the mean of it in crisis (i.e. from September 15, 2008 to September 5, 2012) is a little higher, i.e. -0.000092. The standard deviation reverses, in crisis group is higher than before crisis group. The values of skewness in both groups are larger than 0. And the kurtosis of both groups are all higher than 3, which are steeper than normal distribution.
Robust Jarque Bera Test is performed. Since the p values of this test in both groups are smaller than 0.05. H0 is rejected. It indicates that log returns of CYN are not normally distributed. Stationarity is an important point in time series. Unit root test is performed. The result is that log returns of CYN are stationary. We also performed the mentioned two tests for the 20 larger firms, and found the same result, i.e. these series are all not normally distributed, but they follow stationary process.
Figure 9 is the line and symbol graph for the log returns of CYN. It can be found that the volatility between 2008 and 2010 is very high, and there are some clusters in this series.
6.3. Tables and Figures.
25

g(·) Model 1 Model 2 Model 3



Dev

Acc

Angle

Error

ASE

0.95 1.22(0.36) 0.85(3.53) 9.874(0.079) 0.029(0.004) 0.044(0.014)

N 0.50 0.74(0.25) 0.63(1.45) 9.969(0.023) 0.007(0.002) 0.003(0.002) 0.05 1.75(0.59) 1.81(3.55) 9.829(0.123) 0.038(0.006) 0.064(0.021)

0.95 1.79(0.76) 0.95(4.23) 9.720(0.191) 0.050(0.008) 0.111(0.037)

t 0.50 0.91(0.39) 1.17(2.56) 9.951(0.039) 0.009(0.002) 0.004(0.003) 0.05 1.92(0.79) 2.45(4.48) 9.746(0.128) 0.053(0.009) 0.122(0.049)

0.95 1.68(1.88) 6.57(9.32) 9.691(0.666) 7.564(7.159) 4.769(8.771)

N 0.50 1.49(1.46) 1.01(2.82) 9.780(0.401) 5.916(4.874) 1.363(2.305) 0.05 1.50(1.73) 9.87(9.71) 9.556(0.985) 8.627(8.526) 6.145(9.168)

0.95 2.31(1.88) 9.22(9.48) 9.610(0.668) 9.158(9.561) 5.643(6.561)

t 0.50 1.77(1.59) 5.00(3.58) 9.712(0.487) 8.152(7.278) 1.785(2.814) 0.05 3.07(1.06) 7.60(9.28) 9.695(0.551) 9.750(7.464) 4.643(4.462)

0.95 0.37(0.27) 0.40(2.19) 9.989(0.016) 0.141(0.069) 0.574(0.624)

N 0.50 0.11(0.08) 0.25(0.79) 9.997(0.002) 0.051(0.029) 0.076(0.049) 0.05 0.56(0.32) 0.39(2.28) 9.978(0.025) 0.229(0.063) 0.724(0.711)

0.95 0.32(0.24) 0.50(2.11) 9.987(0.016) 0.235(0.117) 0.759(0.798)

t 0.50 0.29(0.11) 0.31(0.90) 9.994(0.008) 0.077(0.052) 0.081(0.085) 0.05 0.42(0.26) 0.58(2.26) 9.982(0.019) 0.326(0.201) 0.861(0.863)

Table 1 Criteria evaluated with different models and quantiles. (1) = (5, 5, 5, 5, 5), N means the error  follows a
N (0, 0.1) distribution, t means the error  follows a t(5) distribution. In 100 simulations we set
n = 100, p = 10, q = 5. Standard deviations are given in brackets. Dev, Acc, Angle, Error and their standard deviations are reported in 10-1. ASE and its standard deviations are reported in 10-2.

g(·) Model 1 Model 2 Model 3

(1) Dev

Acc

Angle

Error

ASE

(a) 1.51(0.36) 1.02(3.62) 9.861(0.092) 0.135(0.105) 0.152(0.124)

(b) 1.72(0.38) 1.35(3.94) 9.892(0.099) 0.166(0.119) 0.359(0.223)

(a) 1.85(1.95) 7.37(9.45) 9.541(0.752) 8.135(8.352) 5.731(8.928)

(b) 2.34(2.21) 9.54(9.88) 9.432(0.856) 8.374(8.973) 7.212(9.134)

(a) 0.41(0.26) 0.53(2.46) 9.981(0.019) 0.259(0.122) 0.786(0.812)

(b) 0.53(0.28) 0.62(2.87) 9.973(0.021) 0.352(0.229) 0.814(0.921)

Table 2 Criteria evaluated with different models. Two different (1): (a) (1) = (5, 4, 3, 2, 1), (b) (1) = (5, 2, 1, 0.8, 0.2) the error  follows a N (0, 0.1) distribution. In 100 simulations we set
n = 100, p = 10, q = 5,  = 0.95. Standard deviations are given in brackets. Dev, Acc, Angle, Error and their standard deviations are reported in 10-1, ASE and its standard deviations are reported in 10-2.

26

g(·) Model 1 Model 2 Model 3

Dev 1.86(0.84) 1.85(1.65) 0.92(0.39)

Acc 5.61(6.92) 9.72(8.51) 6.20(5.72)

Angle 9.891(0.225) 9.873(0.651) 9.952(0.041)

Error 0.046(0.009) 9.731(9.516) 3.851(0.108)

ASE 0.103(0.040) 4.971(3.121) 9.432(1.042)

Table 3 Criteria evaluated with different models under p > n case. (1) = (5, 5, 5, 5, 5), the error  follows a
N (0, 0.1) distribution. In 100 simulations we set n = 100, p = 200, q = 5,  = 0.05. Standard deviations are given in brackets. Dev, Acc, Angle, Error and their standard deviations are reported in 10-1, ASE
and its standard deviations are reported in 10-2.

model Model 1 Model 2 Model 3

settings p = 10, q = 2 p = 10, q = 7 p = 100, q = 5 p = 10, q = 2 p = 10, q = 7 p = 100, q = 5 p = 10, q = 2 p = 10, q = 7 p = 100, q = 5

 Dev

Acc Angle Error

N 1.51%(0.003) 0.12(0.067) 0.999 0.50%(0.001)

t 3.62%(0.062) 0.43(0.421) 0.989 0.49%(0.001)

N 8.27%(0.145) 0.31(0.118) 0.989 0.81%(0.002)

t 6.44%(0.163) 0.68(0.081) 0.996 0.72%(0.002)

N 8.50%(0.394) 1.19(0.886) 0.905 0.73%(0.001)

t 6.50%(0.237) 1.82(1.023) 0.920 0.84%(0.002)

N 0.25%(0.003) 0.03(0.015) 0.999 0.50%(0.001)

t 2.22%(0.200) 0.02(0.201) 0.979 0.50%(0.001)

N 6.13%(0.034) 0.07(0.271) 0.998 0.69%(0.001)

t 5.69%(0.033) 0.18(0.393) 0.998 0.70%(0.001)

N 8.10%(0.227) 2.42(0.214) 0.910 0.73%(0.003)

t 7.50%(0.225) 2.55(1.893) 0.912 0.70%(0.001)

N 1.04%(0.003) 0.02(0.003) 0.999 0.50%(0.001)

t 1.67%(0.079) 0.02(0.611) 0.994 0.49%(0.001)

N 5.28%(0.062) 0.08(0.246) 0.997 0.68%(0.003)

t 5.49%(0.136) 0.08(0.482) 0.997 0.72%(0.002)

N 4.12%(0.323) 1.67(0.724) 0.944 0.84%(0.004)

t 8.20%(0.814) 2.38(0.706) 0.910 0.80%(0.005)

Table 4 Simulation results under sparsity, non-sparsity and large p cases. N means errors follow i.i.d. N(0, 0.1), t
means t distribution with degree of 5.

27

model Model 1 Model 2 Model 3

settings p = 10, q = 2 p = 30, q = 3 p = 120, q = 5 p = 10, q = 2 p = 30, q = 3 p = 120, q = 5 p = 10, q = 2 p = 30, q = 3 p = 120, q = 5

 Dev

Acc Angle Error

N 2.68%(0.003) 0.07(0.003) 0.999 0.44%(0.001)

t 1.74%(0.004) 0.08(0.003) 0.999 0.51%(0.001)

N 2.14%(0.025) 0.87(0.378) 0.982 0.69%(0.001)

t 2.02%(0.034) 0.14(0.018) 0.997 0.77%(0.001)

N 4.35%(0.009) 1.14(0.099) 0.984 0.74%(0.001)

t 5.40%(0.016) 1.33(0.054) 0.977 0.60%(0.001)

N 2.79%(0.005) 0.00(0.000) 0.999 0.50%(0.001)

t 1.88%(0.004) 0.00(0.000) 0.999 0.50%(0.001)

N 2.76%(0.028) 0.04(0.204) 0.995 0.72%(0.001)

t 2.46%(0.023) 0.03(0.007) 0.997 0.73%(0.001)

N 2.80%(0.017) 0.67(0.707) 0.991 0.46%(0.001)

t 1.20%(0.014) 1.45(1.213) 0.978 0.88%(0.001)

N 3.64%(0.003) 0.00(0.000) 0.999 0.47%(0.001)

t 0.91%(0.004) 0.02(0.003) 0.999 0.49%(0.001)

N 5.84%(0.012) 0.16(0.081) 0.996 0.62%(0.001)

t 2.26%(0.013) 0.64(0.003) 0.997 0.72%(0.001)

N 9.89%(0.013) 0.44(0.387) 0.993 0.75%(0.001)

t 3.20%(0.034) 0.95(0.993) 0.986 0.81%(0.001)

Table 5 Simulation results for Composite L1 Quantile Regression. N means errors follow i.i.d. N(0, 0.1), t means
t distribution with degree of 5.

Before crisis In crisis

Mean
-0.0004 -9.247 × 10-5

SD 0.0209 0.0312

Skewness 0.2408 0.1326

Table 6 Descriptive statistics of CYN

Kurtosis 12.1977 8.9544

28

The financial frims

1. Wells Fargo & Co (WFC)

15. Franklin Resources Inc. (BEN)

2. JP Morgan Chase & Co (JPM)

16. The Travelers Companies, Inc. (TRV)

3. Bank of America Corp (BAC)

17. AFLAC Inc. (AFL)

4. Citigroup Inc (C)

18. Prudential Financial, Inc. (PRU)

5. American Express Company (AXP)

19. State Street Corporation (STT)

6. U.S. Bancorp (USB)

20. The Chubb Corporation (CB)

7. The Goldman Sachs Group, Inc. (GS)

21. BB&T Corporation (BBT)

8. American International Group, Inc. (AIG)

22. Marsh & McLennan Companies, Inc. (MMC)

9. MetLife, Inc. (MET)

23. The Allstate Corporation (ALL)

10. Capital One Financial Corp. (COF)

24. Aon plc (AON)

11. BlackRock, Inc. (BLK)

25. CME Group Inc. (CME)

12. Morgan Stanley (MS)

26. The Charles Schwab Corporation (SCHW)

13. PNC Financial Services Group Inc. (PNC)

27. T. Rowe Price Group, Inc. (TROW)

14. The Bank of New York Mellon Corporation (BK) 28. Loews Corporation (L)

29. SunTrust Banks, Inc. (STI)

44. Lincoln National Corporation (LNC)

30. Fifth Third Bancorp (FITB)

45. Affiliated Managers Group Inc. (AMG)

31. Progressive Corp. (PGR)

46. Cincinnati Financial Corp. (CINF)

32. M&T Bank Corporation (MTB)

47. Equifax Inc. (EFX)

33. Ameriprise Financial Inc. (AMP)

48. Alleghany Corp. (Y)

34. Northern Trust Corporation (NTRS)

49. Unum Group (UNM)

35. Invesco Ltd. (IVZ)

50. Comerica Incorporated (CMA)

36. Moody's Corp. (MCO)

51. W.R. Berkley Corporation (WRB)

37. Regions Financial Corp. (RF)

52. Fidelity National Financial, Inc. (FNF)

38. The Hartford Financial Services Group, Inc. (HIG) 53. Huntington Bancshares Incorporated (HBAN)

39. TD Ameritrade Holding Corporation (AMTD)

54. Raymond James Financial Inc. (RJF)

40. Principal Financial Group Inc. (PFG)

55. Torchmark Corp. (TMK)

41. SLM Corporation (SLM)

56. Markel Corp. (MKL)

42. KeyCorp (KEY)

57. Ocwen Financial Corp. (OCN)

43. CNA Financial Corporation (CNA)

58. Arthur J Gallagher & Co. (AJG)

Table 7 The financial firms

29

The financial frims

59. Hudson City Bancorp, Inc. (HCBK)

74. Commerce Bancshares, Inc. (CBSH)

60. People's United Financial Inc. (PBCT)

75. Signature Bank (SBNY)

61. SEI Investments Co. (SEIC)

76. Jefferies Group, Inc. (JEF)

62. Nasdaq OMX Group Inc. (NDAQ)

77. Rollins Inc. (ROL)

63. Brown & Brown Inc. (BRO)

78. Morningstar Inc. (MORN)

64. BOK Financial Corporation (BOKF)

79. East West Bancorp, Inc. (EWBC)

65. Zions Bancorp. (ZION)

80. Waddell & Reed Financial Inc. (WDR)

66. HCC Insurance Holdings Inc. (HCC)

81. Old Republic International Corporation (ORI)

67. Eaton Vance Corp. (EV)

82. ProAssurance Corporation (PRA)

68. Erie Indemnity Company (ERIE)

83. Assurant Inc. (AIZ)

69. American Financial Group Inc. (AFG)

84. Hancock Holding Company (HBHC)

70. Dun & Bradstreet Corp. (DNB)

85. First Niagara Financial Group Inc. (FNFG)

71. White Mountains Insurance Group, Ltd. (WTM) 86. SVB Financial Group (SIVB)

72. Cullen-Frost Bankers, Inc. (CFR)

87. First Horizon National Corporation (FHN)

73. Legg Mason Inc. (LM)

88. E-TRADE Financial Corporation (ETFC)

89. SunTrust Banks, Inc. (STI)

104. Valley National Bancorp (VLY)

90. Mercury General Corporation (MCY)

105. KKR Financial Holdings LLC (KFN)

91. Associated Banc-Corp (ASBC)

106. Synovus Financial Corporation (SNV)

92. Credit Acceptance Corp. (CACC)

107. Texas Capital BancShares Inc. (TCBI)

93. Protective Life Corporation (PL)

108. American National Insurance Co. (ANAT)

94. Federated Investors, Inc. (FII)

109. Washington Federal Inc. (WAFD)

95. CNO Financial Group, Inc. (CNO)

110. First Citizens Bancshares Inc. (FCNCA)

96. Popular, Inc. (BPOP)

111. Kemper Corporation (KMPR)

97. Bank of Hawaii Corporation (BOH)

112. UMB Financial Corporation (UMBF)

98. Fulton Financial Corporation (FULT)

113. Stifel Financial Corp. (SF)

99. AllianceBernstein Holding L.P. (AB)

114. CapitalSource Inc. (CSE)

100. TCF Financial Corporation (TCB)

115. Portfolio Recovery Associates Inc. (PRAA)

101. Susquehanna Bancshares, Inc. (SUSQ)

116. Janus Capital Group, Inc. (JNS)

102. Capitol Federal Financial, Inc. (CFFN)

117. MBIA Inc. (MBI)

103. Webster Financial Corp. (WBS)

118. Healthcare Services Group Inc. (HCSG)

Table 8 The financial firms

30

The financial frims

119. The Hanover Insurance Group Inc. (THG) 134. BancorpSouth, Inc. (BXS)

120. F.N.B. Corporation (FNB)

135. Privatebancorp Inc. (PVTB)

121. FirstMerit Corporation (FMER)

136. United Bankshares Inc. (UBSI)

122. FirstMerit Corporation (FMER)

137. Old National Bancorp. (ONB)

123. RLI Corp. (RLI)

138. International Bancshares Corporation (IBOC)

124. StanCorp Financial Group Inc. (SFG)

139. First Financial Bankshares Inc. (FFIN)

125. Trustmark Corporation (TRMK)

140. Westamerica Bancorp. (WABC)

126. IberiaBank Corp. (IBKC)

141. Northwest Bancshares, Inc. (NWBI)

127. Cathay General Bancorp (CATY)

142. Bank of the Ozarks, Inc. (OZRK)

128. National Penn Bancshares Inc. (NPBC)

143. Huntington Bancshares Incorporated (HBAN)

129. Nelnet, Inc. (NNI)

144. Euronet Worldwide Inc. (EEFT)

130. Wintrust Financial Corporation (WTFC)

145. Community Bank System Inc. (CBU)

131. Umpqua Holdings Corporation (UMPQ)

146. CVB Financial Corp. (CVBF)

132. GAMCO Investors, Inc. (GBL)

147. MB Financial Inc. (MBFI)

133. Sterling Financial Corp. (STSA)

148. ABM Industries Incorporated (ABM)

149. Glacier Bancorp Inc. (GBCI)

164. Citizens Republic Bancorp, Inc (CRBC)

150. Selective Insurance Group Inc. (SIGI)

165. Horace Mann Educators Corp. (HMN)

151. Park National Corp. (PRK)

166. DFC Global Corp. (DLLR)

152. Flagstar Bancorp Inc. (FBC)

167. Navigators Group Inc. (NAVG)

153. FBL Financial Group Inc. (FFG)

168. Boston Private Financial Holdings, Inc. (BPFH)

154. Astoria Financial Corporation (AF)

169. American Equity Investment Life Holding Co. (AEL)

155. World Acceptance Corp. (WRLD)

170. BlackRock Limited Duration Income Trust (BLW)

156. First Midwest Bancorp Inc. (FMBI)

171. Columbia Banking System Inc. (COLB)

157. PacWest Bancorp (PACW))

172. Safety Insurance Group Inc. (SAFT)

158. First Financial Bancorp. (FFBC)

173. National Financial Partners Corp. (NFP)

159. BBCN Bancorp, Inc. (BBCN)

174. NBT Bancorp, Inc. (NBTB)

160. Provident Financial Services, Inc. (PFS)

175. Tower Group Inc. (TWGP)

161. FBL Financial Group Inc. (FFG)

176. Encore Capital Group, Inc. (ECPG)

162. WisdomTree Investments, Inc. (WETF)

177. Pinnacle Financial Partners Inc. (PNFP)

163. Hilltop Holdings Inc. (HTH)

178. First Commonwealth Financial Corp. (FCF)

179. BancFirst Corporation (BANF)

190. Berkshire Hills Bancorp Inc. (BHLB)

180. Independent Bank Corp. (INDB)

191. Brookline Bancorp, Inc. (BRKL)

181. Infinity Property and Casualty Corp. (IPCC) 192. National Western Life Insurance Company (NWLI)

182. Central Pacific Financial Corp. (CPF)

193. Tompkins Financial Corporation (TMP)

183. Kearny Financial Corp. (KRNY)

194. BGC Partners, Inc. (BGCP)

184. Chemical Financial Corporation (CHFC)

195. Epoch Investment Partners, Inc. (EPHC)

185. Banner Corporation (BANR)

196. United Fire Group, Inc (UFCS)

186. State Auto Financial Corp. (STFC)

197. 1st Source Corporation (SRCE)

187. Radian Group Inc. (RDN)

198. Citizens Inc. (CIA)

188. SCBT Financial Corporation (SCBT)

199. S&T Bancorp Inc. (STBA)

189. WesBanco Inc. (WSBC)

Table 9 The financial firms

31

-1.0 -0.5 0.0 0.5 1.0

-1.0 -0.5 0.0 0.5 1.0

-1.0 -0.5 0.0 0.5 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 1. The true link functions (black) and the estimated link functions (red) in model 2 with (1) = (5, 5, 5, 5, 5), and   N(0, 0.1), n = 100, p = 10, q = 5,  = 0.05, where h = 0.68 (left), h = 0.068 (middle)
and h = 1 (right).

23456

-1.0 -0.5 0.0 0.5 1.0

5.7 5.8 5.9 6.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 2. The true link functions (black) and the estimated link functions (red) with (1) = (5, 5, 5, 5, 5), and   N(0, 0.1), n = 100, p = 10, q = 5,  = 0.95, model 1 (left) with h = 1.02, model 2 (middle) with
h = 0.15 and model 3 (right) with h = 0.76.

2.5 3.5 4.5 5.5

-1.0 -0.5 0.0 0.5 1.0

5.95

5.85

5.75

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 3. The true link functions (black) and the estimated link functions (red) with (1) = (5, 5, 5, 5, 5), and   N(0, 0.1), n = 100, p = 10, q = 5,  = 0.5, model 1 (left) with h = 1.76, model 2 (middle) with
h = 0.04 and model 3 (right) with h = 0.65.

32

23456

-1.0 -0.5 0.0 0.5 1.0

5.70 5.80 5.90 6.00

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 4. The true link functions (black) and the estimated link functions (red) with (1) = (5, 5, 5, 5, 5), and   N(0, 0.1), n = 100, p = 10, q = 5,  = 0.05, model 1 (left) with h = 0.78, model 2 (middle) with
h = 0.12 and model 3 (right) with h = 1.0.

012345

-1.0 -0.5 0.0 0.5 1.0

5.80 5.85 5.90 5.95 6.00

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 5. The true link functions (black) and the estimated link functions (red) with (1) = (5, 4, 3, 2, 1), and   N(0, 0.1), n = 100, p = 10, q = 5,  = 0.95, model 1 (left) with h = 0.65, model 2 (middle) with
h = 0.02 and model 3 (right) with h = 0.33.

01234

-1.0 -0.5 0.0 0.5 1.0

5.90 5.95 6.00

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 6. The true link functions (black) and the estimated link functions (red) with (1) = (5, 2, 1, 0.8, 0.2), and   N(0, 0.1), n = 100, p = 10, q = 5,  = 0.95, model 1 (left) with h = 0.21, model 2 (middle) with
h = 0.18 and model 3 (right) with h = 0.25.

33

5.65 5.75 5.85 5.95 -1.0 -0.5 0.0 0.5 1.0 123456

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 7. The true link functions (black) and the estimated link functions (red) with (1) = (5, 5, 5, 5, 5), and   N(0, 0.1), n = 100, p = 200, q = 5,  = 0.05, model 1 (left) with h = 0.81, model 2 (middle) with
h = 0.22 and model 3 (right) with h = 0.57.

23456

-1.0 -0.5 0.0 0.5 1.0

5.95

5.85

5.75

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

2345

-1.0 -0.5 0.0 0.5 1.0

5.70 5.80 5.90 6.00

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 8. Plot of the true function g(·) (black) and the estimation (red) with n = 100, p = 10, q = 5 and   N(0, 0.1) in different g(·) functions. L1-L2 regression (upper pannel), composite quantile (lower panel)

34

-0.2 -0.1 0.0 0.1 0.2

2006

2007

2008

2009

2010

2011

Figure 9. Log returns of CYN

2012

-0.2 -0.1 0.0 0.1 0.2

qq
q q
q

q q

q

q

qq q

q

q q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

qq

q q

q

qq

q

2007

2008

2009

2010

2011

2012

Figure 10. Log returns of JPM (blue) and VaR of log returns of JPM (red),  = 0.05, T = 1569, window size n = 100, refer to (5.2).

35

-0.2 -0.1 0.0 0.1 0.2

q

q qq

q

qq

q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

qqqq

q

qq q

q

2007

2008

2009

2010

2011

2012

Figure 11. Log returns of CYN (blue) and VaR of log returns of CYN (red),  = 0.05, T = 1569, window size n = 100, refer to (5.2).

-0.2 -0.1 0.0 0.1 0.2

q

q qq

q

qq

q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqq

q

qq q

q

2007

2008

2009

2010

2011

2012

2013

Figure 12. Log returns of CYN (blue) and the estimated CoVaR (black),  = 0.05, T = 1543, window size n = 126, refer to (5.4).

36

-0.2 -0.1 0.0 0.1 0.2

q qq q

q q qq q qq

q

qq qq

q qqqqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q

qq qq qq

qq q

q

q

q

q

0.0 0.2 0.4 0.6 0.8 1.0
Figure 13. The estimated link function, window size n = 126, starting date: 20080707,  = 0.05, h = 0.027, p = 206, q = 3 (where q is the number of selected variables in this window): FHN, MBI, RDN.

-0.2 -0.1 0.0 0.1 0.2

q

q

q

q

q qq
q

q

q q

q qq q qq
q

q
qq q

qqqqqqqqqqqqqqqqqqqqqqqqqq qq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqq

qq

q q

qq q

q q

q

0.0 0.2 0.4 0.6 0.8 1.0
Figure 14. The estimated link function, window size n = 126, starting date: 20100308,  = 0.05, h = 0.056, p = 206, q = 5: ZION, EWBC, CNO, SNV, RDN.

37

0 100 200 300 400 500

0 50 100 150 200
Figure 15. The frequency of the firms and macroprudential variables. The X-axis: 1 - 206 variables, and the Y-axis: the frequency of the variables selected in the moving window estimation. The variable 187, i.e. "Radian Group Inc. (RDN)" is the most frequently selected variable with frequency 557.

qq q

q qq

q

q

0.0 0.2 0.4 0.6 0.8 1.0

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

2007

2008

2009

2010

2011

2012

Figure 16. The top dots are the violations (i.e. {t : It = 1}) of V aR of CYN, totally 8 violations, T = 1543,  = 0.005.

38

0.0 0.2 0.4 0.6 0.8 1.0

q

q qq q

qqqq qq qq q qq qqq q qq q q q

q qqq

qqq

q qq

q q q qq

q

q q q q q qqq q

q

q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

2007

2008

2009

2010

2011

2012

Figure 17. The top dots are the violations (i.e. {t : It = 1}) of CoV aR of CYN, totally 53 violations, T = 1543,  = 0.034.

39

References.
[1] Adrian, T. and Brunnermeier, M. K. (2011). CoVaR. Staff Reports 348, Federal Reserve Bank of New York.
[2] Berkowitz, J., Christoffersen, P. and Pelletier, D. (2009). Evaluating value-at-risk models with desk-level data. Working Paper 010, North Carolina State University, Department of Economics.
[3] Bradic, J., Fan, J. and Wang, W. (2011). Penalized composite quasi-likelihood for ultrahigh dimensional variable selection. J. R. Statist. Soc. B. 73 (3) 325­349.
[4] Chao, S. K., Ha¨rdle, W. K. and Wang, W. (2012). Quantile regression in Risk Calibration. In Handbook for Financial Econometrics and Statistics (Cheng-Few Lee, ed.). Springer Verlag, forthcoming, SFB 649 DP 2012-006.
[5] Engle, R. F. and Manganelli, S. (2004). CaViaR: Conditional autoregressive value at risk by regression quantiles. J. Bus. Econ. Stat.. 22 367­381.
[6] Fan, J. and Li, R. (2001). Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties. J. Amer. Statist. Assoc.. 96 1348­1360.
[7] Ha¨rdle, W. and Stoker, T. M. (1989). Investigating smooth multiple regression by the method of average derivatives. J. Amer. Statist. Assoc.. 84 986­995.
[8] Huber, P. J. (1985). Projection pursuit. Ann. Statist.. 13 435­475. [9] Kai, B., Li, R. and Zou, H. (2010). Local composite quantile regression smoothing: an efficient and
safe alternative to local polynomial regression. J. R. Statist. Soc. B. 72 49­69. [10] Kai, B., Li, R. and Zou, H. (2011) New Efficient Estimation and Variable Selection Methods for
Semiparametric Varying-Coefficient Partially Linear Models. Ann. Statist.. 39 (1) 305­332. [11] Koenker, R. and Bassett, G. W. (1978). Regression quantiles. Econometrica. 46 33­50. [12] Koenker, R. and Bassett, G. W. (1982). Robust tests for heteroscedasticity based on regression
quantiles. Econometrica. 50 43­61. [13] Koenker, R. and Hallock, K. F. (2001). Quantile regression. Journal of Econometric Perspectives.
15 (4) 143­156. [14] Kong, E. and Xia, Y. (1994). Variable selection for the single-index model. Biometrika. 94 217­229. [15] Leng, C., Xia, Y. and Xu, J. (2008). An adaptive estimation method for semiparametric models
and dimension reduction. WSPC-Proceedings. [16] Li, Y. and Zhu, J. (2008). L1- norm quantile regression. J. Comput. Graph. Stat.. 17 163­185. [17] Newey, W. and Powell, J. (1987). Asymmetric least squares estimation and testing. Econometrica.
55 819­847. [18] Ruppert, D., Sheather, S. J. and Wand, M. P. (1995). An effective bandwidth selector for local
least squares regression. J. Amer. Statist. Assoc.. 90 1257­1270. [19] Schnabel, S. and Eilers, P. (2009). Optimal expectile smoothing. Comput. Stat. Data. An.. 53
(12) 4168­4177. [20] Sering, R. J. (2001). Approximation Theorems of Mathematical Statistics. Wiley, New York. [21] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B. 58 (1)
267­288. [22] Wang, Q. and Yin, X. (2008). A nonlinear multi-dimensional variable selection methods for high-
dimensional data: sparse mave. Comput. Stat. Data. An.. 52 4512­4520. [23] Wu, T. Z., Yu, K. and Yu, Y. (2010). Single-index quantile regression. J. Multivariate Anal.. 101
1607­1621. [24] Xia, Y., Tong, H., Li, W. and Zhu, L. (2002). An adaptive estimation of dimension reduction space.
J. R. Statist. Soc. B. 64 363­410. [25] Yu, K. and Jones, M. C. (1998). Local linear quantile regression. J. Amer. Statist. Assoc.. 93
228­237. [26] Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables.
J. R. Statist. Soc. B. 68 (1) 49­67. [27] Zeng, P., He, T. H. and Zhu, Y. (2012). A lasso-type approach for estimation and variable selection
40

in single-index models. J. Comput. Graph. Stat.. 21 92­109. [28] Zou, H. (2006). The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc.. 101 476. [29] Zou, H. and Yuan, M. (2008). Composite quantile regression and the oracle model selection theory.
Ann. Statist.. 36 (3) 1108­1126.
41

SFB 649 Discussion Paper Series 2013
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Functional Data Analysis of Generalized Quantile Regressions" by Mengmeng Guo, Lhan Zhou, Jianhua Z. Huang and Wolfgang Karl Härdle, January 2013.
002 "Statistical properties and stability of ratings in a subset of US firms" by Alexander B. Matthies, January 2013.
003 "Empirical Research on Corporate Credit-Ratings: A Literature Review" by Alexander B. Matthies, January 2013.
004 "Preference for Randomization: Empirical and Experimental Evidence" by Nadja Dwenger, Dorothea Kübler and Georg Weizsäcker, January 2013.
005 "Pricing Rainfall Derivatives at the CME" by Brenda López Cabrera, Martin Odening and Matthias Ritter, January 2013.
006 "Inference for Multi-Dimensional High-Frequency Data: Equivalence of Methods, Central Limit Theorems, and an Application to Conditional Independence Testing" by Markus Bibinger and Per A. Mykland, January 2013.
007 "Crossing Network versus Dealer Market: Unique Equilibrium in the Allocation of Order Flow" by Jutta Dönges, Frank Heinemann and Tijmen R. Daniëls, January 2013.
008 "Forecasting systemic impact in financial networks" by Nikolaus Hautsch, Julia Schaumburg and Melanie Schienle, January 2013.
009 "`I'll do it by myself as I knew it all along': On the failure of hindsightbiased principals to delegate optimally" by David Danz, Frank Hüber, Dorothea Kübler, Lydia Mechtenberg and Julia Schmid, January 2013.
010 "Composite Quantile Regression for the Single-Index Model" by Yan Fan, Wolfgang Karl Härdle, Weining Wang and Lixing Zhu, February 2013.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

