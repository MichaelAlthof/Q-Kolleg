BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2008-001
Testing Monotonicity of Pricing Kernels
Yuri Golubev* Wolfgang H‰rdle** Roman Timonfeev**
* CMI UniversitÈ de Provence, France ** Humboldt-Universit‰t zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Testing Monotonicity of Pricing Kernels 
Yuri Golubev
CMI, Universite de Provence 39, rue F. Joliot-Curie, 13453 Marseille, Cedex 13, France
Wolfgang K. H®ardle
CASE ≠ Center for Applied Statistics and Economics Humboldt-Universit®at zu Berlin,
Spandauer Straﬂe 1, 10178 Berlin, Germany
Roman Timofeev
CASE ≠ Center for Applied Statistics and Economics Humboldt-Universit®at zu Berlin,
Spandauer Straﬂe 1, 10178 Berlin, Germany
October 10, 2007
We gratefully acknowledge financial support by the Deutsche Forschungsgemeinschaft and the Sonderforschungsbereich 649 "O® konomisches Risiko". Roman Timofeev's research was supported by Deka Bank scholarship program.
1

Abstract The behaviour of market agents has always been extensively covered in the literature. Risk averse behaviour, described by von Neumann and Morgenstern (1944) via a concave utility function, is considered to be a cornerstone of classical economics. Agents prefer a fixed profit over uncertain choice with the same expected value, however lately there has been a lot of discussion about the reliability of this approach. Some authors have shown that there is a reference point where market utility functions are convex. In this paper we have constructed a test to verify uncertainty about the concavity of agents' utility function by testing the monotonicity of empirical pricing kernels (EPKs). A monotone decreasing EPK corresponds to a concave utility function while non-monotone decreasing EPK means non-averse pattern on one or more intervals of the utility function. We investigated the EPK for German DAX data for years 2000, 2002 and 2004 and found the evidence of non-concave utility functions: H0 hypothesis of monotone decreasing pricing kernel was rejected at 5% and 10% significance level in 2002 and at 10% significance level in 2000.
JEL classification codes: G12, C12
Keywords: Risk Aversion, Pricing kernel
2

1 Introduction
The behaviour of market agents has always been in focus in economic literature. Neumann and Morgenstern (1944) describe risk averse behaviour using concave utility functions. Agents prefer a fixed profit over uncertain choice with the same expected value, however lately there has been a lot of discussion about the reliability of this approach. Recent empirical studies by Jackwerth, J. C. (2002) showed that there is a reference point near the inital wealth where market utility functions are convex. Rosenberg, J. and Engle, R. (2002) observed a region of negative absolute risk aversion for orthogonal polynomial pricing kernel. Detlefsen, et al (2007) raised the same question by recovering utility function through empiricial pricing kernels for different time periods and observed a bump in EPK functions near zero returns.
In this paper we test the concavity of utility function by checking the monotonicity of pricing kernel. A strictly decreasing EPK corresponds to a concave utility function which is consistent with classical theory of risk averse behaviour, while rejection of monotone decreasing EPK would mean non-averse pattern of the utility function. By analysing empirical pricing kernels we can also identify on which interval or intervals monotonicity of EPK was rejected. This setup is consistent with the main goal of the paper to test for monotonicity in a particular region (e.g. near zero returns), although the results can be different on other intervals (e.g. for large positive and negative returns the behaviour of EPK can be unpredictable due to scarcity of the data).
In Figure 1.1 we compare utility functions obtained from DAX index in year 2000 and derived from Black and Scholes model. Black and Scholes model is equivalent to monotone increasing and concave utility function, see panel left of the figure. A nonparametric estimator that replicates option prices provides us with market utility function, depicted on the panel right of the figure. As can be observed, market utility function has a slight bump over the region of zero returns. The aim of this paper is to find out whether observed fluctuations are significant.
Construction of empricical pricing kernels has been well described by Ait-Sahalia and Lo (2000). In their paper they distinguish the concept of economical risk which contains
3

utility utility utility utility

-0.6 -0.8
-1 -1.2 -1.4 -1.6 -1.8
-2 -2.2 -2.4 -2.6
0.5
1.4

1 1.5 return

-0.6 -0.8
-1 -1.2 -1.4 -1.6 -1.8
-2 -2.2 -2.4 -2.6
0.5
1.4
1.2
1
0.8
0.6
0.4
0.2
0 2 0.5

1 1.5 return
1 1.5 return

2 2

1.2 Figure 1: left: Utility function in the Black Scholes model for T = 0.5 years ahead and drift µ = 0.1, volatility  = 0.2 and interest rate r = 0.03. right:
Figur1e 1.1: Classical utility function produMcaerkdet furtiolimty fuBncltaiocnkonS06c/h30o/2le00s0 fmor oTd=e0l.5(yleeafrts )aheaand.d market
0.8
utility function estimated from empirical pricing kernel on 06/30/2000 (right)
0.6

0.4
inves0t.2ors' preferences and statistical risk, which provides information on the risk of the data 5
genera00.5ting process1. Economic1r.5isk is well a2pproximated by Arrow-Debrue prices and can be
return
estimated by risk neutral density q obtained from the derivative market. Their work offers
Fahisgeueardveae1n:rdlaedfltr:ifatUcµticl=iuty0r.f1au,ntvceotiloaentisliinttyitmhe=aB0lta.o2ckraSnscdhoionlfteesqrmesuotdrseailtneforgr =T, f=0o.00r3..5eryixegaharts:mple, Black and Scholes (1973) model and also Mparrkeetsuetinlitty fnunocntiopnaorna06m/3e0/t2r0i0c0 feorsTti=m0.a5tyoearrss .aheIand. this paper risk neutral density is derived from implied

volatility models combined with Heston model, see Detlefsen, et al (2007) for details. These

kind of models provide a better fit because they incorporate sudden price jumps and explain

volatility smile. Due 5to the large number of observations in derivative option market, risk

neutral density q can be precisely estimated and considered to be known. Statistical risk is

presented by the distribution p of future prices conditional on current prices. Some attempts

to estimate p were undertaken by Rosenberg, J. and Engle, R. (2002) using GARCH model

and Ait-Sahalia and Lo (2000) using a nonparametric diffusion model. The main difficulty

in estimating p is that it depends on the underlying process of price St and can only be estimated using the historical time series of St. Therefore, estimation of historical density p is complicated by model specification and data scarcity and considered to be unknown.

Thus, we would like to test monotonicity of pricing kernel constructed as a ratio of estimated

q and unknown p.

Ait-Sahalia and Lo (2000) in their paper offer another test for risk neutrality and specific preferences. Depending on the form of preferences they define H0 hypothesis as a relationship between estimated neutral density q and subjective density p. We do not make any assump-

4

tions about the form of preferences and also consider subjective density p as unknown. In our test H0 hypothesis of monotone decreasing EPK is compared to a general class of all possible functions under H1. The test is constructed as follows: first the spacing method is used to reduce the problem to an exponential model. On the basis of this model likelihood ratio test is applied for a fixed interval and using intersection of tests for different intervals it is expanded to a test independent of intervals. Finally, test statistics, calculated on observed data, is compared to simulated critical values and a final decision about monotonicity is taken.
The paper is organised as follows. In Section 2 we introduce important notations and problem setup which is then reduced to an exponential model using the spacing method. In Section 3 we formulate the hypotheses, construct a likelihood test for a fixed interval [I, J] and then expand it to an independent test a using multiple testing technique. We also describe how to simulate critical values using the Monte-Carlo method. Section 4 contains the performance of the test for simulated data and Section 5 provides results on DAX data for 2000, 2002 and 2004.

2 Conception of the Test

2.1 Problem Setup

In this section we describe the relationships between (empirical) pricing kernel and utility

function. Suppose we have at our disposal an i.i.d. sample of asset returns X1, . . . , Xn.

Let q(x) denote risk neutral density Q(X < s) =

s -

q(x)dx

and

p(x)

denote

historical

probability density P (X < s) =

s -

p(x)dx

which

is

assumed

to

be

unknown.

Consider an investor who optimizes his strategy by maximizing his utility function U

which gives us an asset equation:

Pt = EP

(XT )

∑

U U

(XT ) (Xt)

where (XT ) is a payoff function and EP is the expectation with respect to the real/historical measure P.

5

Besides investor optimization problem, an asset can be priced under risk neutral measure Q which enables us to construct a perfect hedge on derivative market:

Pt = EQ [exp(-rT ) ∑ (XT )]

Defining

pricing

kernel

K (x)

=

q(x) p(x)

we

derive:

Pt =

T
exp(-rT )(XT ) ∑ p(x)dx =
-

T -

exp(-rT

)(XT

)

q(x) p(x)

p(x)dx

= EP [exp(-rT ) ∑ (XT ) ∑ K(XT )]

Combining both equations we obtain:

U 
U

(XT ) (Xt)

=

exp(-rT )

∑

K(XT )

(1)

Equation (1) shows the relationship between utility function U and pricing kernel K. Pricing kernel K is proportional to marginal rate of substitution (MRS) between dates t and T. Therefore, the form of utility function up to a constant is defined by a pricing kernel as follows:

1T

U (XT )

=

U 

(Xt)

∑

exp(-rT )

K (x)dx
-

(2)

We want to test concavity of U (x) by checking the monotonicity of K(x). A strictly decreasing K(x) corresponds to a concave utility function. We would like to check if there exists an interval [a, b], where K(x) is not monotone decreasing.

Denote by X(1), . . . X(n) the order statistics related to X1, . . . , Xn i.e.

X(1)  X(2), . . . ,  X(n)

With these notations we can rephrase our problem as follows: find (if possible) integers I, J

such that the sequence

Kk

=

K (X(k) )

=

q(X(k)) , p(X(k))

I kJ

is not monotone decreasing. The principal difficulty in this procedure is related to the fact

that p is considered to be unknown. To overcome this we will use three basic ingredients:

6

∑ spacing method to reduce the problem to a simple exponential model ∑ maximum likelihood test to check monotonicity of Kk for given I and J ∑ multiple-testing procedure to find I and J on the basis of the data at hand.

2.2 The Spacing Method

Our method is based on the Pyke's theorem about the distribution of order statistics, see Pyke, R. (1965). Consider U1, . . . , Un be i.i.d with a uniform distribution on [0, 1]. For the order statistics

U(1)  U(2), . . . ,  U(n)

define uniform spacings Sk as
Sk = U(k+1) - U(k) and Sn = U(n) Theorem 2.1. Let U1, . . . , Un be i.i.d. uniformly distributed on [0, 1] and e1, . . . , en be i.i.d. standard exponentially distributed random variables. Then

L {Sk, 1  k  n} = L

, 1ek

n i=1

ek



k



n

Using the fact that E[ek] = 1 we obtain the following result:

n U(k+1) - U(k) = n ∑ Sk  ek.

(3)

Let P (x) =

x -

p(u)

du

be

the

probability

distribution

function

associated

with

p(x).

Using

U(k) = P (X(k)) and first order Taylor approximation

P (X(k+1)) = P (X(k)) + P (X(k)) ∑ (X(k+1) - X(k))

7

we derive

U(k+1) - U(k) = P (X(k+1)) - P (X(k))  p(X(k)) ∑ X(k+1) - X(k)

(4)

Combining equations (4) with (3) we obtain

n

X(k+1) - X(k)

q(X(k))



q(X(k)) p(X(k))

ek

=

Kk

∑

ek.

Thus our problem is reduced to the following one: check monotonicity of K(X(k)) = Kk using

Zk = Kk ∑ ek, I  k  J

(5)

3 Construction of the Test

3.1 ML test for given I, J

Let A(I, J) be the set of all possible decreasing sequences on a given interval [I, J]: A(I, J) = ak  ak+1, I  k < J

Let us defing the following hypotheses:

Hypothesis H0: K  A(I, J) and pricing kernel K is a monotone decreasing function Hypothesis H1: K is any kind of function.

A nested model of monotone decreasing function under H0 is compared to a general class of all possible functions under H1 by calculating the maximum of likelihood function for each of the models. If function K is non-monotone in accordance with H1, maximum likelihood of two models should significantly differ from each other. On the other hand when we remove restriction on monotonicity and it does not bring significant improvement in likelihood, restricted model H0 should be accepted.

The likelihood ratio monotonicity test is defined by the function

(Z) = 1

maxKA(I,J) {p(Z, K)} maxK {p(Z, K)}

-

H (I ,

J)



0

8

In other words, if (Z) = 1 we accept the null hypothesis H0 : K  A(I, J), otherwise the alternative is accepted. This setup can be simplified with the following monotone transfor-

mation:

(Z) = 1

log

maxKA(I,J) {p(Z, K)} maxK {p(Z, K)}

-

h (I ,

J)



0

For a given probability of the first kind error , the critical value h(I, J) = log H(I, J) is defined as root of the equation:

P0

log

maxKA(I,J) p(Z, K) maxK p(Z, K)

-

h (I ,

J)



0

= ,

where P0 is the probability measure generated by the observations from (5) with Kk  1, I  k < J.

Computation of maxK log {p(Z, K)} is straightforward. Using the results from equation (5) that Zk = Kk ∑ ek we derive log-likelihood function

log {p(Z, K)}

=

-

J k=I

Zk Kk

-

J k=I

log(Kk)

(6)

which gives us analytical result for maxK log {p(Z, K)} at Kk = Zk:

maxK log {p(Z, K)} = -(J - I) -

J k=I

log(Zk

)

Computation of maxKA(I,J) log {p(Z, K)} is performed via Newton-Raphson method with the projection on decreasing sequence A(I, J). The main idea of this approach is to find the maximum likelihood over all possible monotone decreasing sequences by interative optimization via the Newton Raphson algorithm. This result is achieved through isotonic regression combined with Newton-Raphson opimization algorithm.
Isotonic regression performs the least square estimation subject to monotonicity contraint with strictly decreasing trend. For a given vectors x, y of size n the following minimization problem is fulfilled:

n
min {yi - fiso(xi)}2 s.t. fiso(xi)  fiso(xj) where i > j
fiso i=1
9

where fiso is isotonic regression. In practice isotonic regression represents a downward stepwise function, see Figure 3.1. This procedure is unfortunately very time-consuming. It can be also shown that maxKA(I,J) log {p(Z, K)} is obtained at isotonic regression over Zk parameters since Zk gives us maxK log {p(Z, K)}. Thus Newton-Raphson algorithm can be omitted, instead isotonic regression fiso(Zk) is applied to known Zk.

maxKA(I,J) log {p(Z, K)} = -

-J Zk
k=I fiso(Zk)

J k=I

log(fiso(Zk))

4.5 4
3.5 3
2.5 2
1.5 1
0.5 0 0

Initial Zs Optimal Isotonic Regression
10 20 30 40 50 60 70 80 90 100

Figure 3.1: Isotonic Regression over Zk generated as iid standard exponential

3.2 Multiple-testing
I = 1...n 1I

[I, J]

J = I +1...n
J

n

Figure 3.2: Multiple testing on intervals I, J
The principal idea in the multiple testing is to construct a test that does not depend on I and J. This problem is typically solved with the help of tests intersection, see Berger
10

(1982). The hypothesis H0 of monotone decreasing function is rejected if it is rejected at least on one of the interval [I, J], see Figure 3.2. It means that we are looking for a minimal critical surface h(I, J) such that:

P0

min
I ,J

log

maxKA(I,J) p(Z, maxK p(Z, K)

r)

-

h (I ,

J)

0

= .

Unfortunately the exact solution of this problem is extremely difficult and unknown. Therefore we use the Monte-Carlo simulations to find a reasonable critical surface. We generate "the worst"non-increasing case of the sequence K(k) as a constant:

K(1) = K(2) = . . . = K(n) = 1

Then using the result that Zk = Kk ∑ ek we generate Zk  exp(1) as an iid standard exponential random variable.

Let us define (I, J) as a test statistics over simulated Zk:

(I, J) = log maxKA(I,J) p(Z, K) = max log {p(Z, K)} - max log {p(Z, K)}

maxK p(Z, K)

K A(I ,J )

K

(7)

Here  is a matrix of dimensions I, J with non-positive values. Maximum of value 0 is reached at any monotone decreasing interval I, J.
Define mean M (I, J) and variance V 2(I, J) of test statistics (I, J):

M (I, J) = E0(I, J) V 2(I, J) = E0 {2(I, J) - E0(I, J)}2

Parameters M (I, J) and V (I, J) are calculated by Monte-Carlo simulations of Zk as specified above.

Critical value t, where  is a significance level, is calculated as a root of:

P0 min {(I, J) - M (I, J) + tV (I, J)}  0 = 
I ,J
11

(8)

Equation (8) gives us a corresponding critical surface h(I, J)

h(I, J) = M (I, J) - t ∑ V (I, J)

In Figure 3.2 the calculation algorithm of critical values t is displayed. Over all MonteCarlo simulations, Zk should violate -threshold surfaces M (I, J) - t ∑ V (I, J) in  percent cases.



(

I

,

J

)

=

log

max K  A( max K

I

.J )
p(

p Z

(
,

Z,K
K)

)

(I, J)

 M (I , J ) - t V (I , J )

0

Figure 3.3: Calculation of critical value t

3.3 Multiple testing on blocks
Suppose initial set of Zk can be divided in m blocks of size b and the remainder n - b ∑ m, see Figure 3.3.
b b b n-mb

1 I = 1...n

(I, J)

J = I +1...n

n

Figure 3.4: Multiple testing on blocks

12

The idea to introduce blocks is motivated by the variance reduction. Initially we imply that the alternative hypothesis H1 is a set of all possible functions. By introducing blocks we allow the function to be monotone decreasing on interval of size b and thus we decrease the variance of the distribution. Blocks can be considered as a trade off between the variance reduction and shift parameter. For small size block distribution is shifted, but variance is also big. For large blocks the distribution function is less shifted but at the same time associated with smaller variance.

On the left panel of Figure 3.5 distribuction functions of test statistics without block and after introduction of block are depicted. First data are generated as linear trend with slope b, constant a and iid exponential errors ei as xi = (a + b i) ∑ ei. Test statistics is obtained from equation (7) then ordered. Shift of distribution function is caused by increase of linear slope b from 0 trend to 0.05. This idea is an underlying principle of the test, non-monotone data shifts the ditribution to the left that should be determined by the test. Right panel of the figure shows the influence of block parameter on variance and shift of cdfs. At best we would like to maximize the shift and minimize the variance, with an increase of block size m both shift and variance of cdf are smaller. The idea of blocks is to test monotonicity not only on each interval I, J but also for all possible block sizes b.

cdf cdf

Initial and block tests distribution functions
1 test distribution without block test distribution after block
0.9
0.8
0.7
0.6
0.5 Shift of cdf after increase of slope
0.4
0.3
0.2
0.1
0 -50 -40 -30 -20 -10 0 10 20

Influence of block parameter b on cdf variance and shift
1 b=1 b=5
0.9 b = 50
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 30 -40 -30 -20 -10 0 10

20

Figure 3.5: Multiple testing on blocks
Test statistics (I, J, b) is obtained as a difference between maxKA(I,J) log {p(Z, K)} and maxK log {p(Z, K)} but this time we assume that under H1 function is monotone decreasing
13

on each of m blocks of size b. Instead of taking each value, average for each block is taken:

m
max log {p(Z, K)} = -m - log
K j=1

j∑b k=j∑b-b+1

Zk

b

The same procedure is performed for calculation of maxKA(I,J) log {p(Z, K)} but instead of Zk best monotone decreasing approximation is taken as an isotonic regression fiso(Zk).
Finally we can formulate hypotheses: H0 hypothesis about monotonic decreasing function is rejected when monotonicity is rejected at least on one of the intervals I, J with any block size b:

min {(I, J, b) - M (I, J, b) + t,b ∑ V (I, J, b)}  0
(I ,J,b)
Critical value t,b is different for each value of b and obtained from the equation

(9)

P0 min {(I, J, b) - M (I, J, b) + t,b ∑ V (I, J, b)}  0 = 
I ,J,b
Now we are ready to summarize the monotonicity test:

(10)

1. Compute Z(X(k)) = n ∑ q(X(k)) ∑ X(k+1) - X(k) 2. Compute test statistics

(I, J, b)

=

log

maxKA(I,J) p(Z,K) maxK p(Z,K)

=

maxKA(I,J) log {p(Z, K)}

-

maxK

log {p(Z, K)}

3. Take decision: if

min {(I, J, b) - M (I, J, b) + t,b ∑ V (I, J, b)}  0
I ,J,b
then K(∑) is a non-monotone decreasing function

14

4 Implementation
In this section the performance of monotonicity test for artifically simulated data is evaluated. We investigate the behavior of the test for different cases: monotone decreasing data, positive linear trend and sudden jumps. Simulated data are generated in accordance with one of the cases multiplied by standard exponential errors ei. By simulating different errors we can obtain distribution function and then, basing on true function, calculate error probability and evaluate the power of the test.
Before we apply the test to simulated and observed data, important parameters have to be set. The decision about monotonicity is taken basing on sequence of surfaces (I, J, b) - M (I, J, b) + t,b ∑ V (I, J, b), one surface for each block size b. If at least one surface crosses zero level H0 hypothesis about monotone decreasing funcion is rejected. If surface is located under zero level it means that calculated test statistics is to the left of threshold value M (I, J, b)-t,b∑V (I, J, b), see Figure 3.2. First we set the minimum interval of 10 observations between J and I. This parameter is introduced to approximate test statistics  with Gaussian distribution and improve the correlation betweeb statistics (I1, J1) and (I2, J2). Gaussian approximation is possible due to central limit theorem, the bigger the interval is, the better approximation. Obviously if the approximation is good, critical values t should be close to Gaussian critical values. Final goal of this parameter is to improve the power of the test.
The importance of b parameter has been discussed in Section 3.3. Large b reduces variance but at the same time decreases shift of the distribution. We start with value b = 1 which corresponds to no block until b = 0.5 ∑ n which means the dataset is divided into exactly two blocks. Values more than 50% of observations would correspond to only one block and remainder and therefore do not make sense.
Calculation of critical values is described in Section 3.2. This procedure is very time consuming that is why we use dichotomic method in order to find the root of equation (8). This is a method of iterative splitting of intervals into halfs until required precision of solution is found.
First we generate a monotone decreasing sequence and check the performance of the test on this dataset. The "worst"monotone sequence is a constant therefore we simulate x1 = x2 =
15

∑ ∑ ∑ = xi = 1. On the left panel of Figure 4.1 generated sequence x, Z and corresponding isotonic regression over Zi are displayed. Having fixed b = 3 we calculated critical values t,3 from equation (8) and corresponding testing surface M (I, J, 3) - t,3 ∑ V (I, J, 3) - (I, J, 3) which are depicted on the right side of the figure.

true function and Z on simulated data
s 3.5
True function Zk Iso-regression 3
2.5
2
1.5
1
0.5
0 0 5 10 15 20 25 30 35 40 45 50

(I,J - M(I,J) + t  V)

Testing

surface

(I,J)

-

M(I,J)+

t
0.05



V

and

Zero

surface

15
10
5
0
50 40 30 20 10
I

50 40 30 20 10
J

Figure 4.1: Simulated monotone data and resulting testing surface, b = 3
The entire surface is located above zero level and therefore H0 hypothesis of monotone decreasing function can not be rejected at 5% sifnificance level. The depicted above surface is a single result of generated errors ei and fixed parameter b and therefore can not reflect overall performance of the test. In order to demonstrate overall behavior of the test we estimate error probability by generating different errors ei. In Figure 4.2 distribution of first type error for different parameter b is plotted, i.e. probability to accept H1 although data are distributed under H0. As it can be seen b parameters does not improve the first type error.
In the next case data are generated with a positive linear trend xi = (a + 0.05 ∑ i) ∑ e, where a is a constant and i is an index from 1 to n. Simulated parameters M (I, J, b) and V (I, J, b) do no depend on data but only on parameters b and number of observations n and therefore can be taken from previously simulated example. For fixed b = 3 generated data, rejection intervals and resulting surface (I, J, 3) - M (I, J, 3) + t,3 ∑ V (I, J, 3) are given in Figure 4.3. Rejection invervals show such I and J where testing surface crossed the zero level and H0 was rejected.
16

error

0.1 0.09 0.08 0.07 0.06 0.05 0.04
0

First type error distribution by b parameter
5 10 15 20 25
b

Figure 4.2: First type error distribution for different block parameter b

true function and Z on simulated data s

80 50

True function

Zk

Iso-regression

45

70

40
60 35

50 30

Rejection intervals [I,J] for t0.05

40 25
20 30
15 20
10
10 5

0 0 5 10 15 20 25 30 35 40 45 50

5 10 15 20 25 30 35 40 45 50
J

I (I,J - M(I,J) + t  V)

Testing surface (I,J) - M(I,J)+ t0.05  V and Zero surface

15
10
5
0
-5
-10
-15 50 40 30 20 10
I

50 40 30 20 10
J

Figure 4.3: Simulated increasing data and resulting rejection intervals and testing surface
In order to calculate the error probability we calculate number of cases when test failed to identify non-monone structure of the data, i.e. second type error. On Figure 4.4 there is a distribution of second type erros for different b, starting from no block (b = 1) to exactly two intervals b = 0.5 ∑ n = 25. This figure shows that introduction of block significantly improves the performance of the test: error probability decreases from 75% for no block to almost 10% for b = 15.
In next example we simulate an artificial bump, see left panel of Figure 4.5. Ability of the test to identify jumps or bumps in pricing kernel function is especially important since observed data do not usually have an obvious positive trend. Instead EPK has various fluctuations,

17

error

Second type error distribution by b parameter
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0 5 10 15 20 25
b

Figure 4.4: Second type error distribution for different block parameter b

bumps and jumps. Significant bump would correspond to non-concave utility function and contradict to classical theory about risk-averse agents. On middle and right panels of Figure 4.5 testing surface (I, J) - M (I, J) + t ∑ V (I, J) and rejection invervals I, J are given for fixed block size b = 3.

true function and Z on simulated data s
120 True function Zk Iso-regression
100
80
60
40
20
0 0 5 10 15 20 25 30 35 40 45 50

Rejection intervals [I,J] for t 0.05
50 45 40 35 30 25 20 15 10
5
5 10 15 20 25 30 35 40 45 50
J

I (I,J - M(I,J) + t  V)

Testing surface (I,J) - M(I,J)+ t0.05  V and Zero surface

15
10
5
0
-5 50
40 30 20 10
I

50 40 30 20 10
J

Figure 4.5: Simulated data with a bump and resulting rejection intervals and testing surface
Distribution of second type errors for different b is given on Figure 4.6. We can see that there exists an optimal block size b which corresponds to a trade off between shift and variance of distribution, see Section 3.3. Optimal b is different for each dataset and therefore we consider a sequence of surfaces (I, J, b) - M (I, J, b) + t,b ∑ V (I, J, b) for each block size b. H0 hypothesis of monotonic decreasing function is rejected when at least one of these
18

surfaces crosses zero level.
1 0.95
0.9 0.85
0.8 0.75
0.7 0.65
0.6 0.55
0

error

Second type error distribution by b parameter
5 10 15 20 25
b

Figure 4.6: Second type error distribution for different block parameter b

5 Monotonicity of DAX Empirical Pricing Kernel

Final goal of this paper is to test empirical pricing kernel obtained from observed data.

For the analysis we take data used in Detlefsen, et al (2007) where the pricing kernels and

the risk aversion are analyzed in summer of years 2000, 2002 and 2004 in order to consider

different market regimes. According to our test design the decision about monotonicity of

pricing kernel is made on the basis of generated Zk = n ∑ (X(k+1) - X(k)) ∑ q(X(k)) where

X are DAX returns and q is risk neutral density. DAX returns are calculated on half year

basis Xi

=

Xi -Xi-126 Xi-126

and then ordered to X(k).

Corresponding ordered returns differences

X(k+1) - X(k) for years 2000, 2002 and 2004 are displayed in Figure 5.1.

Risk neutral density q (see Figure 5.2) is estimated using Heston model (1993) calibrated on observed implied volatility surfaces with half year maturity. Fore more details on estimation of risk neutral density refer to Detlefsen, et al (2007).

Resulting Zk values are displayed in Figure 5.3. For each set of Zk an isotonic regression was constructed which represents maxKA(I,J) log {p(Z, K)} in equation (7). Numerous simulations showed that in order to compute maximum likelihood for restricted model

19

X -X
(k+1) (k)
X -X
(k+1) (k)
X -X
(k+1) (k)

Ordered DAX half year return differences for 2000
0.06

Ordered DAX half year return differences for 2002
0.06

Ordered DAX half year return differences for 2004
0.06

0.05

0.05

0.05

0.04

0.04

0.04

0.03

0.03

0.03

0.02

0.02

0.02

0.01

0.01

0.01

000

50 100 150 200 250

50 100 150 200 250

50 100 150 200 250

kkk

Figure 5.1: Half year ordered returns differences X(k+1) - X(k) for years 2000, 2002 and 2004

pdf pdf pdf

Risk neutral density q for June 30th 2000
4.5 4
3.5
3 2.5
2
1.5 1
0.5
0 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8
Dax returns

1

Risk neutral density q for June 28th 2002
4.5 4
3.5
3 2.5
2
1.5 1
0.5
0 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8
Dax returns

1

Risk neutral density q for June 25th 2004
4.5 4
3.5
3 2.5
2
1.5 1
0.5
0 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8
Dax returns

1

Figure 5.2: Estimated risk neutral densities q for years 2000, 2002 and 2004

maxKA(I,J) log {p(Z, K)} we have to take isotonic regression over optimal parameters which

maximize log {p(Z, K)} for all possible K. maxK log {p(Z, K)} is reached at Kk = Zk and

equal to -n -

n k=1

log(Zk),

so

isotonic

regression

over

observed

Zk

maximizes

maxKA(I,J) log {p(Z, K)}.

In order to take a final decision about the motonocity sequence of surfaces M (I, I, b) and V (I, J, b) has to be computed. M and V 2 are mean and variance parameters of test statistics  obtained via Monte Carlo simulations of Zk as iid standard exponential random variable. Each value of the matrixes M and V represent correspondingly mean and standard error of  for a fixed parameter b and interval I, J and calculated as maxKA(I,J) log {p(Z(I, J), K)} - maxK log {p(Z(I, J), K)}. Matrix M has non-positive values with maximum at 0, V is nonnegative. Both matrixes exist only for J > I, see Section 3.2 for details. Since surfaces M and V do not depend on observed data but only on the number of observations n and block

20

Z
k
V(I,J) Z
k

Z
k

Calculated Z(k) values and isotonic regression

Calculated Z(k) values and isotonic regression

Calculated Z(k) values and isotonic regression

888

Zs Zs Zs

Isotonic regr.

Isotonic regr.

Isotonic regr.

777

666

555

444

333

222

111

000

50 100 150 200 250

50 100 150 200 250

50 100 150 200 250

kkk

Figure 5.3: Calculated Zk for years 2000, 2002 and 2004

size b they are computed once for all years. In Figure 5.4 corresponding surfaces M (I, J) and V (I, J) are plotted for b = 1, M is linear increasing in I, J; V is increasing in I, J at square root speed.

M(I,J) surface for n=255

V(I,J) surface for n=255

M(I,J)

0 -20 -40 -60 -80 -100 -120 -140 250
200 150 100
I

50

00

14

12

10

8

6

4

2 250

250 200 150 100 50
J

200 150 100
I

50

00

250 200 150 100 50
J

Figure 5.4: Surfaces M and V for 255 observations, b = 1
Next important step is to calculate critical values t,b which are defined as a root to equation (10). This procedure is time consuming, but at the same time does not rely on data and has to be simulated once for a fixed number of observation n and block size b. We use dichotomic method of iterative splitting intervals. In our analysis we start with intervals [0.0, 20.0] then calculate correponding  for the mean of the interval. Depending on calculated  one of two resulting intervals [0.0, 10.0] and [10.0, 20.0] is chosen. This procedure is repeated for selected
21

Crit. Value

interval until solution of required precision is found. Resulting critical values are presented in Figure 5.5. It can be seen that critical values are changing for different parameter b.
5% and 10% critival values for different b parameters
7 5% crit. values 10% crit. values
6.5
6
5.5
5
4.5
4 0 10 20 30 40 50 60 70 80 90 100
b
Figure 5.5: 5% and 10% distribution over b
Finally testing surfaces (I, J, b) - M (I, J, b) + t0.05,b ∑ V (I, J, b) for years 2000, 2002 and 2004 are produced. For fixed b = 50 corresponding surfaces are presented in figure 5.6. They show the differences between simulated 5% threshold surface M - t0.05,50 ∑ V calculated via Monte Carlo simulations and test statistics  obtained from observed data in years 2000, 2002 and 2004. Hypothesis H0 of motononic descreasing EPK is rejected at 5% significance level if test statistics  is smaller than threshold value M (I, J) - t0.05V (I, J). For each interval I, J where surface (I, J, 50)-M (I, J, 50)+t0.05,50V (I, J, 50) is negative, a corresponding rejection interval is plotted in Figure 5.7. Summary of results for three years is presented in Table 5.1. In addition to accepted hypothesis, value of minI,J,b {(I, J, b) - M (I, J, b) + t,b ∑ V (I, J, b)} is given in the table. By evaluating this values we can estimate the significance of accepted hypotheses. Test significantly rejectes monotone decreasing EPK in 2002 as well as can not reject strictly decreasing EPK in 2004 for 5% and 10% significance level. Situation in 2002 is on the verge: H0 can not be rejected with 5% critical values, but rejected at 10% signifiance level.
22

(I,J - M(I,J) + t  V) (I,J - M(I,J) + t  V) (I,J - M(I,J) + t  V)

Testing surface (I,J) - M(I,J)+ t0.05  V and Zero surface

Testing surface (I,J) - M(I,J)+ t0.05  V and Zero surface

Testing surface (I,J) - M(I,J)+ t0.05  V and Zero surface

50 40 30 20 10
0

250 200 150 100
I

50

250 200 150 100 50
J

50

0

-50

-100

-150
250 200 150 100
I

50

250 200 150 100 50
J

50 40 30 20 10
0

250 200 150 100
I

50

250 200 150 100 50
J

Figure 5.6: Surface (I, J, 50) - M (I, J, 50) - t0.05,50 ∑ V (I, J, 50) for years 2000, 2002, 2004

Rejection intervals [I,J] for t 0.05
250

Rejection intervals [I,J] for t 0.05
250

Rejection intervals [I,J] for t 0.05
250

200 200 200

150 150 150

I I I

100 100 100

50 50 50

50 100 150 200 250
J

50 100 150 200 250
J

50 100 150 200 250
J

Figure 5.7: Rejection intervals (I, J) for years 2000, 2002 and 2004, b = 50

Sign. level/Year of analysis 5% Significance level minI,J,b Accepted H0 10% Significance level minI,J,b Accepted H1

2000
0.5437 H0
-0.1840 H1

2002
-133.78 H1
-134.42 H1

2004
3.7935 H0
3.1685 H0

Table 5.1: Summary of results on monotonicity of EPK in 2000, 2002 and 2004.

23

6 Conclusion

In this paper we describe the test that checks monotonicity of pricing kernels. By testing monotonicity of pricing kernel we can determine whether utility function is concave or not. A strictly decreasing pricing kernel corresponds to a concave utility function while nondecreasing EPK means that the utility function contains some non-concave regions.

Pricing kernels are constructed as a ratio of risk neutral density q and subjective density

p. Density q is obtained from the derivative market and due to the large number of observa-

tions can be precisely estimated. p is usually estimated from historical information, but due

to scarcity of data is considered to be unknown. We therefore test the ratio of two densities

q p

,

where

q

is

given

and

p

is

unknown.

Using

Pyke's

theorem

(see

Pyke,

R.

(1965))

this

prob-

lem is reduced to a simple exponential problem. The test itself is constructed on the basis

of the likelihood ratio test for a fixed interval. By using the intersection of tests for different

intervals we can expand it to the variant which is independent of intervals. In section 3.3

we introduce a block parameter, which allows a function to be monotone on intervals of size

b. This innovation improves the power of the test by searching for the tradeoff between the

shift and variance of test distribution function.

We investigated EPK for German DAX data for years 2000, 2002 and 2004. We found the evidence of non-concave utility function: H0 hypothesis of monotone decreasing pricing kernel function was rejected at 5% and 10% significance level in 2002 and at 10% significance level monotonicity in 2000. This result is consistent with the work of Jackwerth, J. C. (2002) who observed partially negative risk aversion during the post crash period. For year 2004 a hypothesis of decreasing EPK could not be rejected at 5% as well as at 10% significance level. These findings also support the idea of Giacomini and Haerdle (2007) who wrote the the structure of pricing kernel may vary over time.

24

References
Ait-Sahalia, Y. and Lo, A. (2000). Nonparametric risk management and implied risk aversion. Journal of Econometrics 94(12), 951.
Ait-Sahalia, Y., Yubo W., and Francis Y. (2001). Do option markets correctly price the probabilities of movement of the underlying asset? Journal of Econometrics 102(1), 67110.
Berger, R. L. (1982) Multiparameter hypothesis testing and acceptance sampling. Technometrics 24: 295-300.
Black, F. and Scholes, M. (1973) The pricing of options and corporate liabilities. Journal of Political Economy, 81: 637-659.
Cox, J. C., Jonathan E. I., and Stephen A. R. (1985). An Intertemporal General Equilibrium Model of Asset Prices. Econometrica, 363-384.
Detlefsen, Ha®rdle, W. K. and Moro, R. M. (2007). Empirical Pricing Kernels and Investor Preferences, SFB 649 Discussion Paper 2007-017.
Giacomini, E. and Ha®rdle, W. (2007). Statistics of Risk Aversion, proceedings, 56th Session of the International Statistical Institute, Lisbon 2007.
Giacomini, E., Handel, M. and Ha®rdle, W. (2006). Time Dependent Relative Risk Aversion, proceedings, 9th Karlsruhe Econometrics Workshop 2006.
Jackwerth, J. C. (2002). Recovering Risk Aversion from Option Prices and Realized Returns, Review of Financial Studies, 13, 433-451.
Heston, S. (1993). A Closed-Form Solution for Options with Stochastic Volatility with Applications to Bond and Currency Options, Review of Financial Studies, 6, (2), 327-343.
Kahneman, D and Tversky, A. (1979). Prospect Theory: An Analysis of Decision under Risk, Econometrica, 47, March, 263-291.
25

Merton, R. C. (1973). An Intertemporal Capital Asset Pricing Model, Econometrica, 41, (5), 867-887.
Pyke, R. (1965 ). Spacings, J.R. Statistical Society, B 27, 395-436. Proschan, F. and Pyke, R. (1967). Tests for Monotone Failure Rate, Fifth Berkeley Sympo-
sium, 3, 293-313. Rosenberg, J. and Engle, R. (2002). Empirical Pricing Kernels, Journal of Financial Eco-
nomics, 64, June, 341-372.
26

SFB 649 Discussion Paper Series 2008
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Testing Monotonicity of Pricing Kernels" by Yuri Golubev, Wolfgang H‰rdle and Roman Timonfeev, January 2008.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

