BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2016-048
Unraveling of Cooperation in Dynamic Collaboration
Suvi Vasama*
* Humboldt-Universität zu Berlin, Germany This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Unraveling of Cooperation in Dynamic Collaboration
Suvi Vasama
November 7, 2016
Abstract We examine collaboration in a one-arm bandit problem in which the players' actions affect the distribution over future payoffs. The players need to exert costly effort both to enhance the value of a risky technology and to learn about its current state. Both product value and learning are public goods, which gives the players incentives to free-ride on each others' actions. This leads to an inefficiently low aggregate level of effort. When the players' actions affect the distribution over future payoffs, they eventually get trapped in the low action, causing an inefficient unraveling of the game. Moreover, the players' incentives to exert effort depend on the state that in turn depends on the aggregate effort. If the players start restricting effort when the belief decreases in expectation, the two effects play in the same direction. Higher effort encourages higher effort and vice versa. Unraveling leads to multiple symmetric Markov perfect equilibria.
Keywords: Stochastic games, strategic experimentation, Bayesian learning, restless bandits
JEL Classification: C73, D83, O31. Humboldt-Universität zu Berlin, Institute for Economic Theory, suvi.vasama@hu-berlin.de. I am grateful to seminar audiences at Aalto University, 2016 World Congress of the Game Theory Society in Maastricht, ESEM in Geneva, International Conference on Game Theory, Stony Brook and, in particular Eduardo Faingold, Johannes Hörner, Nikolas Klein, Sven Rady, Larry Samuelson, Roland Strausz and Juuso Välimäki for many helpful comments and suggestions. I gratefully acknowledge the financial support by the DFG (German Science Foundation) under SFB 649.
1

1 Introduction
In a market for technological innovations, firms often engage in collaboration to facilitate development of a common standard or to create a new business entity.1 In 1997, the cell phone companies Nokia, Motorola, and Ericsson started collaboration in creating a common standard for a wireless telecommunication format. The Wireless Application Protocol (WAP) soon became a widely used by mobile operators, equipment producers and software developers. However, such collaboration is related with great uncertainty and not every attempt to collaborate is successful. Starting from 1994, IBM, Apple and Hewlett-Packard spent over three years and $50 million on an unsuccessful attempt to develop a new standard for an operating system for computers that would challenge Microsoft Windows.
Constant effort is needed to both facilitate the standard and to learn if it is viable. Even if the firms manage to establish a new standard, it may lose its position and become obsolete. Indeed, in 2002, WAP Forum merged to a new Open Mobile Architecture to form a new initiative, Open Mobile Alliance (OMA), and has by now been replaced by new, more elaborate smart phone standards. Finally, the technology is a public good; the firms benefit each others efforts to facilitate the common standard.
The main result of the paper shows an inefficient unraveling of the cooperation in the dynamic collaboration relationship. The unraveling follows since the players fail to coordinate on the efficient action if they are not sufficiently confident about the viability of the technology. Due to the insufficiently high level of effort, the players quickly lose the market. Moreover, we show that the players' incentive to exert effort depend on their current assessment of the value of the technology, which again depends on the players' effort. Whenever the two effects play in the same direction, the game has multiple symmetric Markov Perfect equilibria. Otherwise, the symmetric Markov Perfect equilibrium is unique.
Specifically, the players engage in dynamic collaboration while facing uncertainty about the value of the technology that changes over time depending on the players' effort. At each point in time, each player chooses how much effort they exert in selling and developing the product. Effort is costly and the returns are uncertain and depend on the unobserved state of the world. The state changes stochastically over time and the probability distribution over future payoffs depends on the players' actions. When the players exert effort, they receive noisy signals about the state. Thus, effort has two consequences in the model: the players' receive more informative signals about the current state and they improve
1Such collaboration is legal in the US by the National Cooperative Research Act of 1984 and in the EU by Article 101(3) TFEU if the collaborative efforts do not have anticompetitive effects. See Schilling (2005) for further examples and analysis of circumstances which such a collaboration is beneficial for the firms.
2

the probability distribution over future payoffs. We analyze the noncooperative game in which each player individually decides
how much effort to exert. The state of the world is the same across the players while each player carries the cost of effort individually. The players always have the option to wait and see if the other players' efforts resulted in success. In the noncooperative game both information acquisition and the value of the product resemble a public good problem. The players have an incentive to free-ride on each others' actions.
Before solving the noncooperative game, we identify two important benchmark cases: the complete information problem and the social planner's solution when the current state is unobserved. With complete information, the planner always exerts effort if the current state is high. If the state becomes low, the planner has to decide if she should continue exerting effort. The optimal decision depends on how likely the planner is to win back the market if she lost it. If the probability of success is high enough, it is always optimal to exert full effort. Otherwise, the planner abandons the product as soon as its value becomes low. In absence of effort, the product value stays low and the planner never exerts effort again.
Next, we analyze the social planner's optimal decision if she does not observe the state, but learns about it by observing her payoffs. Again, the optimal policy depends on how likely the planner is to win back the market if she lost it. If the probability of success is not high enough, the planner eventually abandons the technology if she becomes too pessimistic about its value. The optimal strategy is a threshold policy that only conditions on the planner's belief about the state. If the planner becomes too pessimistic, she abandons the technology. Now if the planner does not exert effort, the technology is more and more likely to become obsolete. Once the technology is abandoned, it is never optimal to exert effort again.
The main part of the paper analyzes the version of the model in which the players individually decide how much effort to exert. The players observe both each others' payoffs and their effort levels, but face uncertainty about the current state. We solve for the symmetric Markov Perfect equilibria of the noncooperative game. The players choose their strategies as functions of their beliefs only. The restriction is applied for tractability and allows us to use standard dynamic programming tools to solve for the equilibrium.
As is standard in strategic learning games, we identify three qualitatively different effort regimes, depending on the players' belief about the current state. Firstly, if the players are very optimistic, effort has positive instantaneous value and the players exert every effort to extract the surplus. Secondly, if the players are sufficiently pessimistic, it is never optimal to develop the product. Thirdly, when beliefs are intermediate, the players optimally restrict their effort. The
3

third investment regime arises because of free-riding and distinguishes the noncooperative solution from the social planner's solution.
At the noncooperative optimum, the players' incentives to exert effort are driven by two important forces: free-riding and encouragement effect. Both effects are familiar from the strategic learning literature. The encouragement effect follows since the value of information is positive. The players benefit by learning from each others' signals: by exerting higher effort they encourage other players to exert higher effort. The free-riding effect arises because both the value of the technology and the information are public goods, and reduces the players' effort at the noncooperative equilibrium.
Letting the players' action affect the distribution over future payoffs adds several new features in the strategic learning game. Firstly, he players' incentives to free-ride on each others' actions depend on (a) how likely they are to lose the market if they abstain from effort; and (b) on the impact on their efforts on the state. The first effect alleviates free-riding: a sufficiently high level of effort is needed for the players not to lose the market. The second effect makes free-riding more attractive: the players have an incentive to free-ride not only on each others' information acquisition but also on their impact on the state. Secondly, restricting effort leads to an inefficiently early unraveling in the nonooperative game. The aggregate effort is not high enough to sustain the product value, but the players gradually lose the market. Thirdly, the players' incentives to exert effort depend on the current state that again depends on the aggregate effort. Thus, a higher effort encourages other players to exert higher effort. We show that this leads to multiple symmetric Markov equilibria of the noncooperative game.
Our model builds on the two-armed bandit problem that has been exhaustively studied in both economics and applied mathematics literature.23 The strategic learning game is well explored for the pure experimentation case in which exerting effort allows for the players to learn about the value of a risky arm. Our model builds on Bolton and Harris (1999) in which the payoff of the risky arm follows a Brownian motion with an unknown drift and a known volatility. We extend the framework by allowing the value of the arm to depend on the players' effort. We find that in the noncooperative equilibrium, the players fail to sustain a sufficiently high level of effort, leading to an inefficient unraveling of the game. The unraveling leads to an inefficiently short lifetime of the risky arm, and cannot occur if the value of the arm is fixed. In contrast, Bolton and Harris show that experimentation goes on indefinitely in the noncooperative game. Moreover, the symmetric Markov Perfect Equilibrium is always unique while our game has
2In particular, exerting effort corresponds to investing in the risky arm while abstaining from it corresponds to investing in the safe arm.
3Bergemann and Välimäki (2008) provide a comprehensive review of earlier literature in applied mathematics and economics.
4

multiple equilibria. In the pure experimentation problem, the players' incentives to pull the arm depend on the state, but the state is independent of the force at which the players pull. As a consequence, the noncooperative game has a unique symmetric Markov Perfect Equilibrium. Keller, Rady, and Cripps (2005); Keller and Rady (2010, 2015) analyze related models in which the risky arm generates lump sum payoffs that arrive at Poisson rate. For traditional bandits, Poisson case turns out to be much more tractable, and the players' value functions can be solved in closed form in most cases. In contrast, when the risky arm is evolving, finding closed form solutions seems unlikely.4
Bandit problems that involve evolving arms are sometimes called restless bandits in the literature. Fryer and Harms (2013) study a model in which the value of the risky arm increases deterministically in the player's investments. Following bad news, the player eventually gets trapped in investing in the safe arm instead of developing the more valuable risky arm. The results are very different from our framework in which predictions are self-fulfilling. The players eventually abandon the good risky arm which then becomes bad with probability one over a long enough time horizon. Also, good luck may encourage a player to invest in it such that the arm eventually becomes good.
Our framework borrows from related models that consider an autonomously evolving risky arm, in particular, from Keller and Rady (1999, 2003) who examine the problem of selling in the market of stochastically evolving demand. Khromenkova (2016) analyzes of the case of strategic experimentation with autonomously evolving restless bandits and Poisson distributed payoffs. Our model distinguishes from those by letting the value of the risky arm to depend on the players' action.
This paper is among the first ones to consider strategic learning in a restless bandit framework with endogenously evolving arms. Safronov (2014) develops a two-armed bandit model with competing players in which the player eventually becomes experienced in pulling an arm, which increases the payoff from that particular arm. If experienced, the player eventually gets trapped with a less valuable arm, instead of developing a potentially more valuable, but yet unexplored arm. The model exhibits no gradual learning, but players eventually learn the value of the arm after receiving an instantaneous lump-sum. The model simplifies tremendously and the players' value functions can be solved in closed form. Our model builds on a classical bandit, which, unfortunately, comes with the cost of losing on tractability.
4The classical strategic bandit problem has been extended in many important directions to answer relevant questions that remain yet unexplored in our framework. We do not attempt to review the literature here; Hörner and Skrzypacz (2016) provides an excellent review.
5

2 Setting

We examine a game with N players in continuous time. Each player can decide how much effort to put in developing a product and learning about its value. The player's profit depends on the unobserved product value and on exogenous circumstances. Moreover, the value is stochastic and may change over time. The change is unobserved and depends on the players' actions.
Each player is facing a capacity constraint; without loss of generality we normalize the maximal effort that he can exert to 1. Let t,i  [0, 1] denote the intensity at which the player i develops the product and t  (t,1, ..., t,N ) a vector summarizing all players' efforts at time t.5
The product value at time t is

µt = stµ,

(1)

where st  {0, 1} is the unobserved state of the world. In particular, if st = 1, the product value is high and if st = 0, the product value is low. The state of the world st eventually changes over time following an unobserved stochastic process. In particular, the firms need to constantly exert effort to develop the product to
facilitate its value in the market. The product value follows a continuous-time
Markov chain with the transition probabilities

Pr[ sz = i, z  [t, t + dt]| st = j] =

e-s d t

if i = j,

1 - e-sdt if i = j,

(2)

with s  {0, 1}. The product value evolves at rate s that depends on both on the development
intensities t and on an exogenous parameter   [0, ). It is formally defined as

s =

(1 -  t) if s = 1,

 t

if s = 0,

(3)

with  = (1, ..., N ), and i > 0 for all i describing the effect of the player i's effort on the state. In particular, if the product value is high at time t, the probability that it is still high at time t + t, is

Pr[st+t = 1|st = 1, t] = 1 - (1 -  t)t + o(t), and if it is low at time t, the probability that it is high at time t + t, is

Pr[st+t = 1|st = 0, t] =  tt + o(t).
5We let A denote the transpose of the matrix A.

6

The probability, that the product has a high value on the market, increases in the
players' aggregate effort t. Moreover, to keep the model as simple as possible, we assume that if all players abstain from effort, i.e. set t,i = 0 for all i, the low state becomes more and more likely over time.6 The terms of order (t)2 and higher are collected in the term o(t) and can be neglected.
If the player abstains from effort, he receives a certain flow payoff that we
normalize to 0. If the player decides to exert effort, he earns a stochastic payoff

dt,i = t,i(µt - ci)dt + 1t,/i2dZt,i,

(4)

where µt is the product value at time t as defined in (1), ci > 0, i  {1, ..., N}, is the cost of effort and  > 0 the volatility of the player's profit.

We assume that µ > ci for all i such that the effort is valuable in the high

state. Moreover,

N i=1

i

=

,

where





(0,

1]

is

a

constant

independent

of

the

total

number of players N. Notice that if  = 1, the high state st = 1 is absorbing,

conditional on the players exerting full effort.

The players only observe (4) and not the exact decomposition of the profits.

Thus, profit is a noisy signal about the state. The player faces two sources of

uncertainty: the uncertainty about the product value µt and an exogenous shock that is driven by the Brownian motion Zi. The accuracy of information increases with the aggregate effort t. Thus, higher effort both makes the high value more likely and facilitates information acquisition.

Both the players' actions and the payoffs are common knowledge. The drift µt is the same across the players, but the Brownian motions Zi are independent across them. When the players exert effort, they receive independent signals

about the state. The more the players exert effort the more accurate is the ag-

gregate information that they receive. Thus, the player learns not only from his

own payoffs, but also by observing the payoffs of the other players. Moreover,

higher effort by one player makes the high state more likely for all players.

The filtration F  keeps track of the public history. Let t denote the prior

probability that the players assign to the high state st = 1 at time t. At any point

of time t, each player chooses the optimal effort t,i to maximize the expected

payoff



v0,i = E

e-rtdt,i dt = E

e-rtt,i(µt - ci)dt .

00

(5)

The equality follows using law of iterated expectations and E[st|F ] = t.

6The assumptions are sufficient to guarantee that the efficient policy is a threshold policy that can be solved using standard dynamic programming tools. Fryer and Harms (2013) provide very general sufficient conditions for applicability of standard dynamic programming tools in a single player's experimentation problem in a related setting.

7

3 Beliefs

This section presents a heuristic derivation for the evolution of the players' beliefs

that they assign to the high state at time t; the formal derivation is delegated to

the Appendix. The effect of the players' efforts on the state as well as the fact that

the state may change due to exogenous circumstances, make the players' belief

updating nonstandard.

When the players exert effort, they both learn about the current state of the

world and affect the probability distribution over the future states. Thus, effort

has two effects: First, it increases the probability that the state is high in the fu-

ture. Second, it allows the players to learn if their past efforts resulted in success.

We derive the filtering equation that governs the players' belief that they as-

sign to the high state. The filtering equation consists of two terms. The first

term summarizes the effect of the state transition and is anticipated by the play-

ers. Even if the state is high, it may become low. At time t, the players be-

lieve that the value is high with probability t, but becomes low with probability

(1 -  t) = (1 -

N i=1

i

t,i

)

by

(2).

This effect

decreases

the

players'

belief

by

-(1 -  t)tdt.

Moreover, even if the state is low, it may become high if the players keep on

exerting effort. The players assign the probability 1 - t to the low state but a

probability  t to the transition to the high state. The latter effect increases

the players' belief by  t(1 - t)dt. By summing up the effects, we can write

the drift of the filtering equation as

( t - t)dt.

(6)

In particular, the players' belief increases in effort. If the players abstain from effort, they anticipate that they gradually lose the market. The parameter  tells how quickly this happens. i is the long term mean of the belief conditional on the player i exerting full effort.
Next, if the players exert effort, they receive informative signals about the state. Based on their observations, they update their beliefs according to Bayes rule. Since the players learn from Brownian signals, this part of the filtering equation follows a martingale. The players incorporate all available information in their belief and any deviation from their estimate comes as a surprise.
The variance of the belief depends on the model parameters. Learning is more accurate if the high value µ is high relative to the low one that we normalized to 0. Similarly, if  is higher, the profit is more volatile and the signal are noisier. Similarly, learning is faster close to the diffuse belief t = 1/2 when the players are very uncertain about the state and becomes slower as beliefs tend towards 0 or 1. Finally, the players learn more by exerting higher effort t,i. By summarizing the

8

effects, we find that learning affects the players' posterior by

µ 

t

(1

-

t)(1t /2)

d Z t

(7)

with 1t /2 = (1t,/12, ..., 1t,/N2 )

and

d Z t

=

(d

Z

 t,1

,

...,

d

Z

 t,N

)

denoting vectors summa-

rizing all players' efforts and the Brownian motions that disturb they signals.

By summing up the drift and volatility terms (6) and (7), we find the law of

motion for the players' common belief. The result is summarized in the following

proposition

Proposition 1 (Beliefs). Fix a prior belief t and a strategy t. The common pos-

terior belief, that the players assign to the high state satisfies the following filtering

equation7

dt = (

t

-

t)d

t

+

µ 

t(1

-

t

)(1t /2

)

d Z t ,

(8)

with 1t /2 = (1t,/12, ..., 1t,/N2 ) and dZt = (dZt,1, ..., dZt,N ) .

Proof. See Appendix.

If  > 0 such that the state changes over time, the players are never completely certain about the current market conditions. Indeed, (8) suggests that the beliefs are mean-reverting. They have the tendency to return to their long-term mean which depends on the players' collective efforts. The next corollary confirms the observation

Corollary 1. Fix a strategy profile t. The player's common belief for the high state at time t is

t = 0 e-t + 

t
e-(t-s)
0

sds +

t 0

e-(t-s)

µ 

s(1

-

s)(1s/2)

d Z s

Proof. Define

y(t, t) = t et

By Itô's lemma

d y(t, t) = t etdt + etdt = et

t

d

t

+

et

µ 

t(1

-

t

)(1t /2

)

dZt .

The result follows by substituting back, integrating from 0 to t and multiplying by e-t.

One can show that if  > 0 and t,i = 1 for all i and t  0, i.e. the players always exert full effort, the long term mean of the belief is i i = . The model includes the following special cases
7Cf. Liptser and Shiryayev (1977) and Keller and Rady (1999).

9

· If  = 0, the state is fixed over time. In particular, if the players keep on exerting effort, they learn the true mean as t  . This is the classical Brownian bandit problem that is comprehensively examined by Bolton and Harris (1999).
· If   , with any positive effort the state becomes i.i.d. Then there is no learning. One can show that if iµ > ci, the noncooperative game has a unique Markov perfect equilibrium in which all players exert full effort in perpetuity. Moreover, if µ < ci, effort is never optimal for the player i.

4 Social Planner's Solution
Before analyzing the noncooperative game, we solve for the social planner's problem when she decides about the optimal effort. In particular, we derive the optimal strategy in two benchmark cases: observable and unobservable state. The social planner's solution coincides with the players' optimal effort decision if they could coordinate on effort.

4.1 Complete Information Solution
Before moving to the incomplete information problem, we first solve for the social planner's problem when the state is observed. The planner's value function of the complete information solution can be solved in closed form.
With complete information, the planner always exerts full effort if the state is high. If the state is low, the optimal strategy depends on the switching rate  and the long term gains µ relative to the cost ci. If µ - ci  0, effort is has positive value in the long run. Then if  is very high, the state is very likely to recover if it becomes low and it is always optimal to exert full effort.
Otherwise, if the long term gains are not high enough compared to the cost of effort, it is optimal to bandon the technology if the state becomes low. From (2) it follows that if the state is low, and the planner does not exert effort, the state stays low. If the planner loses the market, she never exerts positive effort again.
The complete information payoff can be solved in closed form and is summarized in the following proposition

Proposition 2 (Complete Information Solution). Suppose that the state is observed by the players; i.e.   {0, 1}. Then if the following condition holds

(µ - ci)  ci,

(9)

10

the social planner exerts full effort independent of the state. The expected payoff at

time 0 is

v i ()

=

(µ - ci) . r + (1 - )

(10)

Vice versa, if (9) does not hold, the planner stops exerting effort if the state becomes

low. The expected time 0 payoff is

v

i

()

=

µ(r r(r

+ ) + )

-

ci r

.

(11)

Proof. See Appendix.

4.2 Incomplete Information Solution

We next solve for the social planner's problem when she does not observe the state,

but learns from it by observing the payoffs. The social planner's solution provides

both an appropriate benchmark for efficiency considerations and useful insights

that help to solve the noncooperative game.

Recall from the previous section that if the long-term gain of effort is very high,

it is optimal to exert full effort, regardless of the current state. When the state

is observed, this is the case if the long term gains exceed the myopic cost; i.e. if

the condition (9) holds. Otherwise, it is optimal to never exert effort if the state

becomes low. A similar logic applies for the planner's optimal decision when the

state is unobserved. However, since the planner does not observe the true state,

she conditions her strategy on her belief. We focus here on the analysis in the case

in which the planner stops exerting effort if she becomes too pessimistic.

The social planner's optimal effort only depends on the belief that she assigns

to the high state. The belief summarizes both the long-term effect of the planner's

efforts and her information about the current state. The social planner's optimal

decision is a bang-bang policy: she either exerts full effort or entirely abstains

from it. Effort is optimal if the belief is high enough, i.e. the planner is sufficiently

optimistic about the state.

Formally, the planner's optimal decision can be derived using standard dy-

namic programming tools. At any point of time, and for each i  {1, ..., N}, the

social planner chooses i(t) such that it maximizes (5). We assume that both the long term mean of the efforts and the costs are identical across the play-

ers such that i =  j and ci = c j = c for all i, j  {1, ..., N}. Then by symmetry,

i(t) = i i(t)/N.

Using symmetry together with

N i=1

i

=

,

we

can

write

the

social

planner's

Hamilton-Jacobi-Bellman equation at any point  as

rvi() = sup
i ()

 i ()(µ

-

c)

+(( i )

-

)v i ()

+

 i ()

N 2

µ2 2

2(1

-

)2 v i

()

(12)

11

for each i. From (12) we can immediately see that the social planner's objective is linear in i(). We will show below that the optimal policy is characterized by two regimes: If the belief is high, it is optimal to exert full effort; i.e. i() = 1. If the belief falls below a certain cutoff, it becomes optimal to set i() = 0. By symmetry, the same effort strategy is optimal for all i. Moreover, if the planner stops exerting effort, she anticipates that she gradually loses the market.8 Therefore, it is never optimal to exert effort again.
Now suppose that  > 0 such that the state evolves over time. Then with bad enough luck, the social planner eventually stops exerting effort even if the state is high. Then over long enough time horizon, the planner loses the market with probability 1. Thus, the model entails self-fulfilling predictions. Of course, the converse is also possible: a sequence a good outcomes may occur even if the state is low. Then the social planner keeps on exerting effort, which eventually results in success over time. The resulting predictions are in contrast with Fryer and Harms (2013) in which the players eventually get trapped in abstaining from effort even if the product value is high.
We next derive some key properties of the planner's value function vi(). Since i() = 0 is always feasible, the value function is nonnegative. Moreover, we show that the social planner's value function is convex and increasing in her belief. Convexity reflects the fact that value of information is positive. The planner can only benefit from the resolution of uncertainty since it allows her to make a more efficient decision. The result is summarized in the following lemma
Lemma 1. Social planner's value function vi() is nonnegative, nondecreasing and convex in .
Proof. We first prove that the value function is convex. Notice that for arbitrary, fixed i(), the social planner's objective (5) is linear in . Let vi () denote the planner's value from such a strategy. Next, consider  = 1 + (1 - )2 with   [0, 1]. Then
vi () =vi (1) + (1 - )vi (2)  sup vi (1) + (1 - ) sup vi (2) =vi(1) + (1 - )vi(2).
Taking the supremum on left hand side proves convexity. Next, if i() = 0, the belief drifts down according to (8). Therefore, if the
planner stops exerting effort, she stops exerting effort in perpetuity. Since i() = 0 is feasible, we must have vi()  0. Moreover, if i() = 0 in perpetuity, vi() = 0. Since vi ()  0 by the first part of the proof, we must have that vi()  0.
8Formally, as we can see from (8), the planner's belief drifts downwards.
12

We show that the social planner's optimal decision has an isomorphic interpre-
tation as an optimal stopping problem. In particular, we first show that there is a cutoff belief   0 above which the planner always exerts full effort and below which she abandons the risky technology. Formally,

Lemma 2. For all i, there exists a unique cutoff belief  such that i() = 1 for all     0 and i() = 0 otherwise.  is an absorbing boundary.

Proof. Let  denote the highest belief at which i() = 0 and recall from the proof of Lemma 5 that if the planner stops exerting effort, she never exerts positive effort again. Thus, i() = 0 for all    and  is an absorbing boundary.
Next, notice that (12) implies that i()  [0, 1] is optimal only if

c

-

µ

=

v i ()

+

N 2

µ2 2

2(1

-

)2 v i

(),

(13)

otherwise, either i() = 1 or i() = 0. Substituting (13) into (12) implies that

rvi() = -v () = 0,

where the second equality follows since vi()  0 and vi()  0 by Lemma 5. Then (13) implies that i  [0, 1] is optimal at most at the single point .
Notice that Lemmas 1 and 2 imply that the social planner always exerts effort longer than is myopically optimal. Let M  c/µ denote the cut-off belief at which a myopic player stops.

Corollary 2. The social planner keeps on exerting effort longer than myopically optimal; i.e.   M.

Proof. Since  is the optimal threshold belief, at which the planner abandons the product and vi() = 0 by Lemmas 1 and 2, we have

c - µ

=

N 2

µ2 2

2(1

-

)2

vi

()



0.

The last inequality follows by Lemma 1. The result follows by reorganizing.

The result holds for an arbitrary number of players, including N = 1. A single player always keeps on exerting effort beyond the myopic belief. We will show below that the same result holds in the noncooperative game since the aggregate effort can never cease as long as it is optimal for a single, isolated player to exert effort.
We can now write the social planner's problem as a standard optimal stopping problem. The result is summarized in the following proposition

13

Proposition 3 (Social Planner's Solution). For all   , the planner's value function is the unique solution of the ordinary differential equation

r v i ()

=

µ

-

c

+

(

-

)v i ()

+

N 2

µ2 2

2(1

-

)2 v i

()

(14)

with the boundary conditions rvi(1) = µ- c-(1-)vi(1), vi() = 0 and vi() = 0.

Proof. See Appendix.

44

v() v()

22

0 0.2 0.4 0.6 0.8

(a)  = 0.5,  = 1;  = 0.25.

1

0 0.4 0.6 0.8


1

(b)  = 0.01,  = 0.5;  = 0.47.

Figure 1: The effect of varying  and  on the social planner's optimal policy. The solid curve describes the pure experimentation problem for which  = 0.43; the dashed line describes the problem with an endogenous state. The parameter
values are µ = 5, c = 4.5,  = 5, r = 0.1 and N = 2.

Unfortunately, explicit solutions are only available for the special case  = 0 that is carefully analyzed in Bolton and Harris (1999). Figure 1 examines numerically how the changing state changes the optimal policy in comparison to the pure experimentation problem. Figure 1a describes the model in which  = 1 such that the good state is absorbing. Then increasing  increases the probability that the planner is able get to the good state by exerting effort. Consequently, the planner has a very strong incentive to exert effort also at low beliefs. As we will see below, this turns out to be a very strong prediction that holds also in the noncooperative game. Increasing  always increases the players' incentives to exert effort.
Figure 1b demonstrates the effect of lowering the low term mean  of beliefs conditional on effort. We can see that the planner's value decreases if  is lower, but that the incentives to exert effort at low beliefs are hardly affected. We will show below that this effect is very different in the noncooperative game. A lower

14

long-term mean strengthens the players' temptation to free-ride on each other impact on the state. This results in a lower level of aggregate effort which again results in an earlier unraveling of the noncooperative game.
5 Noncooperative Solution
The main section of the paper studies the noncooperative game in which each firm decides individually how much effort it wants to exert. The state is the same for all players and they benefit from each others' efforts. However, effort is eventually costly since the state may be low. While effort benefits all firms, it is only profitable if the state is high.
We find several effects that drive the players' optimal decision at equilibrium. First, the players always have the possibility to delay their effort and benefit from each others' actions. This gives incentives to free-ride. Second, value of information is positive and exerting higher effort encourages other players to exert effort. The two effects are familiar from the classical experimentation literature. The free-riding effect is affected if effort affects the distribution over future payoffs. In particular, the players are more likely to lose the market if they abstain from effort. This alleviates free-riding. However, the players have an incentive to free-ride on each others' impact on the state. The latter effect strengthens the free-riding effect.
Next, the players' incentives to exert effort depend on the state which again depends on the players' actions. Thus, if the players exert higher effort, they also encourage other players to exert higher effort. We show that if the players start restricting effort above the long term mean, the two effects play in the same direction, which leads to multiple symmetric Markov Perfect equilibria of the noncooperative game. Notice that multiplicity does not arise in the pure experimentation problem. While the players' incentives to exert effort do depend on state, the state does not depend on the actions. As a consequence, the belief uniquely pins down the symmetric Markov Perfect equilibrium.9
Finally, we show that if the state depends on the players' actions, they eventually get trapped in low effort, leading to an inefficient unraveling of the game. The unraveling is a consequence of the players' free-riding together with the fact that a sufficiently high level of effort is needed to keep the market. In the noncooperative solution, the aggregate effort is too low, which quickly drives down the state. Such unraveling is inefficient and does not occur in the social planners' optimum. Moreover, it cannot occur in Bolton and Harris (1999) in which the
9Of course, learning games typically have asymmetric equilibria that are qualitatively different. See, for example, Keller et al. (2005).
15

state does not depend on the players' action. In contrast, learning slows down but experimentation goes on indefinitely.
5.1 Markov Perfect Equilibria
We start the analysis by defining the players' strategies and the equilibrium concepts formally. In continuous time, a strategy is a stochastic process on which we impose appropriate measurability conditions
Definition 1 (Strategy). A strategy of player i is a stochastic process i = {i,t  [0, 1], 0  t < } progressively measurable with respect to the filtration Ft.
We restrict the attention on a strict subset of possible strategies, stationary Markovian strategies. Each player chooses his strategy as a function of the belief t only. Formally, we define
Definition 2 (Stationary Markovian Strategy). A strategy i is a stationary Markovian strategy if t,i = i(t) for all 0  t < .
At equilibrium, each player chooses his strategy such that it is a best response to the other players' strategies. In Markov perfect equilibria, all players play stationary Markovian strategies. At time t let
-i(t)  (1(t), ..., i-1(t), i+1(t), ..., N (t))
denote the vector summarizing a stationary Markovian strategy profile chosen by the players other than i. We formally define the equilibrium as follows
Definition 3 (Markov Perfect Equilibrium). A Markov perfect equilibrium is a Nash equilibrium in which
(i) the players update their common belief according to the filtering equation (8), starting from the initial value 0 = ;
(ii) each player i chooses a stationary Markovian strategy i() such that it is a best response to the stationary Markovian strategy profile -i() chosen by the other players.
The restriction on stationary Markovian strategies is made for tractability and allows us to use standard dynamic programming tools to derive the players' best response correspondences. Fix a stationary Markovian strategy profile -i(t) by the other players and consider the decision problem of player i. At any point,
16

the player i's optimal effort can be derived from his Hamilton-Jacobi-Bellman equation10

rvi() = sup i()(µ - ci) + (ii() + -i-i() - )vi()
i ()[0,1]

1 + 2 (i() + 1

-i

())

µ2 2

2(1

-

)2

vi

()

,

(15)

where -i = (1, ..., i-1, i+1, ..., N ) summarizes the impact of the other players' efforts on the state. Fixing the other players' strategies, we can derive the player i's best response from (15).
By comparing an individual player's decision problem (15) with the social planner's Hamilton-Jacobi-Bellman equation (12), we can immediately see that the players' efforts have an externality to the other players' value that a player does not fully internalize when choosing his strategy. We analyze the resulting freeriding problem in detail below.
Each player chooses his strategy to maximize his value in (15). Taking into account the strategies chosen by the other players, the optimal choice implies a best response by the player. The best response correspondence is summarized in the following lemma

Lemma 3 (Best Response). A stationary Markovian strategy (i(t))t0 is a best response to the stationary Markovian strategy profile (-i(t))t0 with respect to the belief process (t)t0 if and only if it solves

i()  arg sup
[0,1]

-vi() + -i()

-i

vi

()

+

1 2

1

µ2 2

2(1

-

)2

vi

()

+ i ()

µ

-

ci

+

i

v i ()

+

1 2

µ2 2

2(1

-

)2 v i

()

. (16)

Proof. See Appendix.

If  = 0, (16) reduces to the player i's best response correspondence in Bolton and Harris (1999). If  > 0, the player's best response correspondence contains additional terms that describe the effects of the evolving state on the players' optimal choice.
The term in the parenthesis on the second line of (16) describes the player i's opportunity cost of effort. The first term, µ - ci, which is eventually negative, describes the player's flow profit of effort. The second term, ivi() describes the player's benefit from the effect of his own effort on the state. The last term,
10We let 1 = (1, ..., 1) denote the vector of ones of appropriate length.

17

1/2µ2/22(1 - )2vi () is the shadow value of information from learning. We will show below that vi()  0 and vi ()  0 such that both a higher state and learning have positive value for the player.
The last term in the parenthesis of the first line of (16) describes the externality that the other players' effort has on the player i's value. The first term, -ivi(), describes the externality that results from the other players' efforts improving the state. The second term, 1/2µ2/22(1 - )2vi (), is the shadow value of information provided by the other players. Both effects are positive such that the other players' efforts have a positive externality on the player i's value.
We will show below that the player i's effort increases in his value vi(). Now the other players' effort has a positive externality on the player i's value. This increases the players' effort at equilibrium and implies that the players exert effort longer than a single player would. Bolton and Harris (1999) first discovered the encouragement effect which is also present in our model.
However, the player does not fully internalize the effect of his effort on the other players' value, but has an incentive to free-ride on the other players' actions. Also this effect was first discovered by Bolton and Harris (1999), and it is affected by the fact that the players have an incentive to free-ride on each others' impact on the state, not only on their learning. In particular, recall that when the players' action affects the state, the value deteriorates if the players' abstain from effort. The second term in (16), -vi(), describes how quickly the players lose the market if they do not exert effort. We will show below that this alleviates freeriding and increases the players' effort at the noncooperative equilibrium.
5.2 Symmetric Equilibria
We next analyze the symmetric Markov perfect equilibria in more detail. In a symmetric equilibrium, each player faces the same decision problem and the equilibrium best responses coincide. The restriction allows us to make more detailed predictions about the properties of the equilibrium.
We analyze the equilibria in the game with N  2 players. To facilitate symmetry, we again assume that each player's effort has the same effect on the state and that the players face the same cost. Thus, i =  j = /N and ci = c j = c for all i, j  {1, ..., N}.
Formally, a symmetric Markov perfect equilibrium (MPE) is defined as follows
Definition 4 (Symmetric MPE). A Markov perfect equilibrium is symmetric if i(t) =  j(t) for all t  0, and all players i and j.
In the symmetric Markov perfect equilibria, we can identify three qualitatively different effort regimes. If the players are very optimistic about the state, exerting full effort is a dominant strategy. Similarly, if the players are very pessimistic,
18

abandoning the technology is optimal. However, at intermediate beliefs, the players have an incentive to free-ride on each others' actions. At equilibrium, they do not exert full effort. Individually, each player is just indifferent between her strategies.
The player's optimal effort can be found by applying symmetry on (16) and is summarized in the following proposition

Proposition 4 (Effort Regimes). The symmetric Markov perfect equilibria are characterized by three effort regimes:

1. If c - µ < /Nvi() + µ22(1 - )2vi ()/(22), it is optimal to set i() = 1.

2. If c -µ = /Nvi()+µ22(1-)2vi ()/(22), the players choose the interior

level of effort

 i ()

=

rvi() + vi() . (N - 1)(c - µ)

(17)

3. If c - µ > /Nvi() + µ22(1 - )2vi ()/(22), it is optimal to set i() = 0.
(17) suggests that the incentives to exert effort are stronger when the exogenous switching rate  is higher. If  > 0, constant effort is needed for the players' not to lose the market. Higher  implies that abstaining effort results in the players losing the market more quickly. This makes free-riding less attractive and increases the players' effort at equilibrium.

Proof. The Hamilton-Jacobi-Bellman equation (15) if linear in i(). The optimal control is a bang-bang policy. If c - µ < /Nvi() + µ22(1 - )2vi ()/(22), it is optimal to set i() = 1. If c - µ > /Nvi() + µ22(1 - )2vi ()/(22), i() = 0 is optimal.
Finally, consider  such that /Nv () + 2(1 - )2vi ()/(22) = c - µ. Then (15) implies that i()  [0, 1] is optimal. Substituting for i() =  j(), we find
that

rvi() =i()(µ - c) + 

i() - 

v i ()

+

N 2

 i ()

µ2 2

2(1

-

)2 v i

()

=i()(µ - c) - vi() + Ni()r(c - µ)

= - vi() + i() (N - 1) (c - µ).

Rearranging yields the interior solution (17) for the equilibrium effort level.
Now, let ¯ denote the highest belief at which i() < 1 and ¯ the highest belief at which i() = 0. We will show below that the two beliefs are cutoff beliefs at which the players switch between the regimes. First if the players become

19

pessimistic enough, such that their belief falls below ¯, they start to restrict effort. Next, if they become even more pessimistic, such that their belief falls below ¯, they stop exerting effort. In absence of effort, losing the market becomes more and more likely.; the belief drifts down deterministically according to (8). The players never exert effort again.
Before writing down the main result that characterizes the equilibria, we first derive some key properties of the equilibria. Recall from Corollary 2 that the social planner keeps on exerting effort longer than is myopically optimal. In particular, the result holds if there is only one player; i.e. N = 1. We argue that also in the noncooperative game, the players exert effort longer than a myopic player would. The argument follows by contradiction. Suppose that none of the players would exert effort at the myopic cutoff. in a symmetric Markov perfect equilibrium as long as one player alone would. But then, once the players stop developing the product, exerting effort would still be profitable for each player alone. This is a profitable deviation and cannot be part of an equilibrium strategy.
The following lemma summarizes the result
Lemma 4. In the N-player noncooperative game, ¯  M.
Proof. The proof follows by contradiction. The single player's decision problem is a special case of the social planner's problem with N = 1. Let ^1 denote the corresponding threshold belief at which the single player abandons the technology.
Suppose that there exists a Markov perfect equilibrium of the noncooperative game with the strategy profile MPE() such that the players stop exerting effort at some belief ¯ > ^1. Now for   [^1, ¯] positive effort is a profitable deviation for any single player. Thus, MPE() cannot be an equilibrium. It follows that ¯  ^1. The result follows since ^1  M by Corollary 2.
The next lemma shows that at the symmetric Markov perfect equilibrium, the value of information is positive; i.e. the value function vi() is convex. The players can only benefit from the resolution of uncertainty since this helps them to find the more efficient action. Moreover, we show that the value function increasing; the expected payoff is higher at higher beliefs.
Lemma 5. At any symmetric Markov perfect equilibrium, vi()  0, vi ()  0 and /Nvi() + µ22(1 - )2vi ()/(22)  0.
Proof. Suppose first that   M. At  = 1, the boundary condition rv(1) = i(1)(µ- ci) - (1 - /Ni() - N/(N - 1) j)vi(1), i = j, implies that i(1) = 1 is optimal. At the neighborhood of  = 1, vi() > 0 and vi ()  0 by (5), and therefore i() = 1 by (17). The result follows by repeating the argument.
20

Next, suppose that   M. Again, if i() = 1, the result follows from (5). If i() < 1 is optimal, Proposition 4 implies that

i vi ()

+

1 2

µ2 2

2(1

-

)2 v i

()

=

ci

-

µ



0,

(18)

with strict inequality if  < M. Therefore, we must have that either vi()  0 or vi ()  0 or both. Notice that since vi(¯) = 0, we must have both vi()  0 and v ()  0 at the neighborhood of ¯. Moreover, (5) implies that vi()  0 and vi ()  0 at the neighborhood of ¯. This implies that we would need to have vi () < 0 on a set of positive measure. We show that this cannot be the case.
Suppose that vi () < 0 on a set of positive measure and let 1 denote the largest point at which vi ()  0. Such a point exists since vi () is continuous and vi (¯)  0. Let [1, 2], with 1 < 2 denote an interval on which vi ()  0. Then vi(1)  vi(2). (18) together with c - µ1 > c - µ2 and vi (1) implies that



i

v

i

(1)

>

i

v i (2 )

+

1 2

µ2 2

(2)2(1

-

2)2

vi

(2)



i

vi

(2)

a contradiction. Thus, we must have vi ()  0.

Lemma 5 implies that the players' value increases with other players' effort. This demonstrates the encouragement effect in the model with an endogenous state. Indeed, we can conclude that

Corollary 3. In a symmetric Markov perfect equilibrium, if 1 -i()  1 ~ -i(), then vi()  v~i().
Now it follows from Lemma 5 that the noncooperative solution is characterized by three regimes that depend on the belief and can be separated by two threshold beliefs: the threshold ¯ at which the players start restricting their effort and the threshold ¯ at which the players stop exerting effort. Between ¯ and ¯ the players gradually decrease their effort. The next lemma confirms the observation
Lemma 6. There exist two cutoff beliefs ¯ and ¯ such that (i) i() < 1 if and only if  < ¯; (ii) i() = 0 if and only if  < ¯; (iii) ¯ is an absorbing boundary.
Proof. Recall that ¯ denotes the highest  such that i() = 0. The argument that ¯ is an absorbing boundary follows along the same lines than in the social planner's problem in Lemma 2. Since the bad state becomes more and more likely over time in absence of effort, the players never start to exert effort again.
Next, notice that we can write the player i's strategy as

i() = max

min

rvi() + vi() , 1 (N - 1)(c - µ)

,0

(19)

21

by Proposition 4. Then (19) together with Lemma 5 implies that i() is nondecreasing in belief. Let ¯ denote the highest belief at which i() = 1. The result
follows.

Finally, by Corollary 3, the players' incentives to exert effort depend on the other players' effort. Now if the state is evolving, the players' incentives to exert effort depend on the state transition which again depends on the players' effort. We show that if ¯ > , such that the players start restricting effort above the long term mean,11 these two effects play in same direction. Then the game has multiple symmetric Markov Perfect equilibria.
Similarly to the social planner's problem, the solution to the noncooperative game has an isomorphic interpretation as a standard optimal stopping problem with which it is easier to deal. The symmetric Markov perfect equilibria are summarized in the following proposition

Theorem 1 (Symmetric Markov Perfect Equilibria). There exists a symmetric Markov perfect equilibrium. Starting from a prior belief 0, the player i's payoff admits the following dynamics
· As long as  > ¯, i() = 1 for all i  {1, ..., N} and the player i's payoff is the unique solution of the following ordinary differential equation

r v i ()

=

µ

-

c

+

(

-

)v i ()

+

N 2

µ2 2

2(1

-

)2 v i

()

(20)

with the boundary conditions (N - 1)(c - µ¯) = rvi(¯) + ¯ vi(¯) and rvi(1) = µ - c - (1 - )vi(1).

· When ¯ <   ¯, i()  [0, 1] for all i and the player i's payoff solves

c

-

µ

=

 N

v i ()

+

µ2 22

2(1

-

)2 v i

().

(21)

The boundary conditions are vi(¯) = 0 and vi(¯) = 0. · As soon as  reaches ¯, i() = 0 for all i.

Moreover, the game has multiple symmetric Markov perfect equilibria if ¯ > ; otherwise the symmetric Markov perfect equilibrium is unique.

Proof. See Appendix.
11Notice that this implies that unraveling starts immediately at ¯.

22

(21) is an indifference condition and describes the players' value in the region of beliefs at which they choose an interior effort level. (20) describes the players' value on the region on which they exert full effort. By comparing the value functions in the social planner's problem (12) and the noncooperative game (15), we immediately see that they coincide. Free-riding is the only source of inefficiency that restricts the players' efforts at equilibrium.
In our framework, constant effort is needed for the players not to lose the market. The assumption has several consequences. First, free-riding is alleviated. Second, if the players become too pessimistic about the state, it eventually becomes optimal to abandon the technology. If the players stop exerting effort, they gradually become more pessimistic. Then the players never exert positive effort again. Moreover, as we will show below, the players eventually get trapped in low effort, causing an inefficient unraveling of the equilibrium.
44

v i () v i ()

22

0 0.4 0.6 0.8


0

1

0.4 0.6 0.8

1



(a) Bolton and Harris (1999);  = 0.43 ¯ = (b)  = 0.1,  = 1;  = 0.40 ¯ = 0.73, ¯ =

0.82, ¯ = 0.60.

0.41.

Figure 2: Increasing  alleviates free-riding. The dashed-dotted line describes the social planner's solution. Along the solid line the players exert full effort; along the dashed line the effort is restricted. The parameter values are µ = 5, c = 4.5,  = 5 r = 0.1 and N = 2.

Figures 2 and 3 examine the player's value function in noncooperative solution relative to the corresponding efficient benchmark. Figure 2 displays the numerical solution of Bolton and Harris (1999) on the left compared to a game with an endogenous state,  > 0, but an absorbing good state,  = 1. We can see that freeriding is alleviated and the players exert effort almost as long as a social planner would, already for a small increase in . The player become very eager to win the market, which increases their effort at equilibrium.
Figure 3 illustrates numerically the equilibrium unraveling result. For the

23

44

33

v i () v()

22

11

0 0.4 0.6 0.8


1

(a)  = 0.04,  = 0.85; ¯ = 0.8, ¯ = 0.53.

0 0.4 0.6 0.8


1

(b)  = 0.01,  = 0.5; ¯ = 0.85, ¯ = 0.60.

Figure 3: Unraveling. The dashed-dotted line describes the social planner's so-
lution. Along the solid line the players exert full effort; along the dashed line the
effort is restricted. The parameter values are µ = 5, c = 4.5,  = 5 r = 0.1 and N = 2.  = 0.47 in both cases.

left panel, the long term mean is relatively high conditional on effort,  = 0.85; the exogenous switching rate is  = 0.04. For the right panel, the long term mean is relatively low  = 0.5; the switching rate is  = 0.01. For both solutions, the social planner abandons the technology at the same belief. However, we can see that
the noncooperative solution is relatively more inefficient in the right panel. In particular, ¯ = 0.80 - 0.85 = -0.05 while ¯ = 0.53 - 0.60 = -0.07. Thus, there is an almost 30% higher decrease in the threshold ¯, at which the players abandon the technology, than in the threshold ¯, at which the players start restricting effort. While higher switching rate  alleviates free-riding, a lower long term mean  leads to an earlier unraveling of the noncooperative game.
The rest of the chapter studies the unraveling result formally. Because of free-
riding, the aggregate effort is not high enough, and the value deteriorates while
learning slows down. Indeed, (8) implies that the belief drifts down if

i(t)  t

(22)

It is immediate from (22) that unraveling is more likely if , i.e. the long term mean is lower conditional on effort.
Notice that such unraveling is not possible if  = 0. Free-riding slows down the learning rate, but effort goes on indefinitely, see Bolton and Harris (1999). Moreover, the unraveling is inefficient and does not occur at the social planners' optimum. In contrast the social planner would exert every effort to improve the state.

24

Next, a natural question arises if free-riding can lead to unraveling if the state is high at time t. The next proposition derives conditions under which this is possible. In particular, the market size is not allowed to be too large. If the market is very large, the players always obtain a large number of signals, even if they exert very low effort. Then information acquisition dominates, and causes the players' belief to drift towards the true state.
The next proposition confirms the observations

Proposition 5 (Unraveling). Consider a symmetric Markov perfect equilibrium of the noncooperative game with N <  players. Then
· There exists a belief ~ such that if t < ~ t, the players' common belief is a (strict) supermartingale in the noncooperative game.
· Suppose that ¯   < ~. Then if

(rvi() + vi())

 N

+

µ2 2

(1 -

)2

N -1 < (c - µ),
N

(23)

the players' belief is a (strict) supermartingale conditional on st = 1.
Proof. Suppose that ¯   < ¯ such that the players do not exert full effort. We identify the cutoff belief ~, below which the belief (8) drifts down, and argue that ~ > ¯. Indeed, recall that the belief drifts down if (22) holds. Let ~ denote the belief such that the condition holds with equality. By substituting for (17) and reorganizing, we find that ~ is implicitly defined in

vi

(~ )

+

v

i

(~ )

=

~ 

(c

-

µ~ )(N

-

1).

Since vi(¯) = vi(¯) = 0, and 0 < ¯ < c/µ = M for N < , we find that ~ > ¯. To the second point, suppose that st = 1 such that the state is high at time t.
One can show that from the point of view of someone, who observes the true state, the players' common belief evolves according to12

dt

=

( i (t )

-

t)d

t

+

N  i (t )

µ2 2

t(1

-

t )2 d

t

+

µt(1

-

t)((t)1/2)

dZt

at the noncooperative equilibrium. Reasoning along the same lines than in the first part of the proof, one can show that the drift of the true belief process is nonpositive if and only if (23) holds.

12See Keller and Rady (1999); Liptser and Shiryayev (1977).

25

6 Discussion
We examine a new model of strategic experimentation in which the players' action affects the distribution over future payoffs. The players need to exert effort to improve the value of a product that is uncertain and unobserved by the players. Effort is needed both to enhance the product value and to learn about its viability in the market.
The main focus of the analysis is a version of the model in which each player individually decide how much effort to exert to develop and sell the product. We show that whenever the product value depends on the players' efforts, they eventually get trapped in the low action, which leads to an inefficient unraveling in the noncooperative equilibrium. For intermediate beliefs, the players have a strong incentive to free-ride on each others' actions, which results in them reducing their effort at the noncooperative equilibrium. The aggregate level of effort remains too low and the players gradually lose the market. The result is in sharp contrast with the results in pure experimentation problems, in which learning slows down, but experimentation goes on indefinitely.
Moreover, we show that if unraveling starts immediately when the players start restricting effort, the noncooperative game has multiple Markov Perfect equilibria. The players' incentives to exert effort depend on the belief that they assign for the state. However, now the state also depends on the players' efforts. Higher aggregate effort does not only lead to more accurate signals, but also to a higher value. This drives multiplicity of equilibria in our framework.
This paper only derives the equilibrium for a restricted class of processes and actions. In particular, we only consider the case in which the signals are distributed by Brownian noise and the low state becomes more and more likely if the players do not exert effort. The latter assumption implies that the players never start exerting effort again if they stop. An important line of future research seems to be allowing for a more general dependence of the state on the action, possibly in a framework with Poisson distributed profits. Also the Levy version of the bandits still remains unexplored in our framework.
26

7 Appendix

The appendix is organized as follows

1. We first derive the filtering equation governing the law of motion for the common beliefs. Since the players' actions affect the distribution over future payoffs, belief updating is nonstandard.

2. We solve for the social planner's problem when she optimizes the profits of all N players simultaneously. We analyze both the case in which she observes the state and the case in which the state is unobserved.

3. We solve for the symmetric Markov perfect equilibrium in which each player chooses his effort individually.

We start by deriving the filtering equation for the beliefs in Section 3. When the players exert effort, they both increase the probability of the good state in the future and learn from the success of past efforts. The filtering equation, that governs the evolution of beliefs, takes into account both effects.

Proof of Proposition 1. The payoffs in (4) are observationally equivalent to the sig-

nals

d~ t,i



dt,i 1t,/i2

= 1t,/i2tµdt + dZt,i

for each player i, given the strategy t,i. The players observe, not only their own signal, but also the other players' signals.

After observing the signals the players update their common belief about the

current state using Bayes rule

t-

=

t

t +1

-

, 

where the likelihood ratio t evolves according to

dt

=

µ 

t(1t /2)

dZt.

(24)

This uses Girsanov's theorem and the fact that the Brownian motions Zi are independent across the players.
By Itô's lemma, the players' posterior belief evolves according to

dt- = (1t /2)

µ 

t-

(1

-

t-

)d

Z

 t

prior to the state transition. The Brownian motions are related by dZt = dZt - 1t /2µt- dt, i.e. the ith row of the vector dZt is dZt,i = dZt,i - µt-1t,/i2dt.

27

Next, the players anticipate that the state eventually changes over time. The change depends on the players' efforts. For s  {0, 1}, let Nts = Nts - Nss-, where

Nts =

1 0

if if

st = st-, st = st-.

The process Nts follows the continuous-time Markov chain (2). At time t, the players assign a belief t- for the high state and a probability of
1 - Nt1 for the game actually remaining in the high state. Similarly, the players
belief that the state is low with probability 1 - t-, but becomes high with probability Nt0. Thus, taking into account the possibility of the state transition, the
players assign the belief



t

=

(1

-

N

1 t

)t-

+

(1

-



t-

)

Nt0

to the high state. Applying Itô's formula for jump diffusions,13 we find that

d

t

=

-

t-

d

Nt1

+

(1

-

t-

)d

N

0 t

+

(1

-

(N

1 t-

-

Nt1-

))

dt-

-

(Nt0-

-

Nt0-

)dt-

= (

t

-

t)d

t

+

µ 

t(1

-

t)(1t /2)

d Z t .

The last equality follows since t is continuous almost everywhere.

We next derive the social planner's expected payoff when she observes the current state. In particular, we derive conditions under which the social planner always exerts effort and conditions under which she abandons the technology if the state becomes low.

Proof of Proposition 2. Let vi() denote the complete information payoff from the product i. We first derive the planner's expected payoff if she stops as soon as the state becomes low. Then we derive the expected payoff when the planner always exerts effort and determine conditions under which effort is optimal in both states.
Suppose first that the planner stops exerting effort if the state becomes low. Now as long as the state remains high, full effort is optimal, i.e. t = 1. However, the state eventually becomes low, an event after which the planner stops exerting effort. The distribution of st follows (2) and is now observed. At t = 0, s0 = 1 with probability . The expected profit is

vi() = 


e-rt e-(1-)t(µ - ci)d t
0

= (µ - ci) . r + (1 - )

13See Øksendal and Sulem (2005).

28

Next, suppose that the planner always exerts effort. Conditional on st  {0, 1}, the planner's payoff solves the system of equations

rv1i =µ - ci + (1 - )v0i - (1 - )v1i , rv0i = - ci + v1i - v0i .

The solution is

v1i

(r + = r(r

)µ + )

-

ci r

,

v0i

=

µ r(r + )

-

ci r

,

(25) (26)

and the expected payoff is (11). Finally, we derive conditions under which it is optimal to stop if the state
becomes low. If the planner exerts no effort, she earns a continuation payoff 0. If she keeps on exerting effort, the payoff is (26). By comparing the payoffs, we find that it is optimal to keep on exerting effort in the low state only if (9) holds.

The next step is to analyze the social planner's problem in the case in which she does not observe the state.

Proof of Proposition 3. The proof follows in two steps: We first verify that the threshold policy suggested in Section 4 maximizes the social planner's expected payoff. Then we apply an appropriate change of variables to recast the problem in a new domain in which standard theorems imply existence and uniqueness of the solution.
Since (12) is linear in i(), the optimal policy satisfies

 =1 

if c - µ < vi() + Nµ22(1 - )2vi ()/(22),

  [0, 1] if c - µ = vi() + Nµ22(1 - )2vi ()/(22),

 = 0 if c - µ > vi() + Nµ22(1 - )2vi ()/(22).

(27)

We first verify optimality of the effort strategy (27). In particular, we show that

the process

t
St = e-rsi(s)(µs - c)ds + e-rtvi(t)
0

(28)

is a supermartingale for an arbitrary effort strategy, and a martingale if the strat-

egy (27) is chosen.

Applying Itô's lemma with (8) on (28), we find that

ertdSt = i(t)(µt - c)dt - rvi(t)dt + ( (t) - t)dt

+1

(t

)

1 2

µ2 2

2t

(1

-

t)2

vi

(t)d

t

+

µ 

t(1

-

t)((t)1/2)

d Z t .

29

From (12), we can see that St is a supermartingale and it is a martingale only if the effort policy (27) is chosen. The firm value at t = 0 satisfies

E e-rti(t)(tµ - c)dt = E[S]  S0 = vi(0),
0
with equality if and only if the optimal policy is chosen. Now it follows from Lemma 2 that i() = 1 is optimal if    and i() = 0
otherwise. Since vi() = vi() = 0 for all  < , standard arguments imply that the value function has to satisfy the value matching condition vi() = 0 and the smooth pasting condition vi() = 0. The optimal value function solves the optimal stopping problem in Proposition 3.
It remains to prove that the solution exists and is unique. We apply the change of variables x = (1 - )/(1 - ) to rewrite (12) as an ordinary differential equation with an unknown parameter  on the domain [0, 1]

r y(x; ) = µ(1 - x(1 - )) - c

( - 1 + x(1 - )) - 1 -  y

(x; ) + (1 - x(1 - ))2x2 y

(x; ),

(29)

with  = Nµ2/(22) and the boundary conditions

r

y(0;



)

-

(1 - ) 1 - 

y

(0;

)

-

(µ

-

c)

=0,

y(1; ) =0,

y (1; ) =0.

(30)
(31) (32)

and an unknown parameter . Since   M < 1, (29) satisfies the standard Lipschitz and growth conditions in x and . Then by standard arguments, the
solution of (29)-(32) exists and is unique.

The rest of the Appendix examines the noncooperative solution. We first verify that the strategies described in Lemma 3 form best responses when the players play stationary Markov strategies.

Proof of Lemma 3. We can conclude from (16) that the player i's optimal investment strategy satisfies

 =1 

if c - µ < ivi() + µ22(1 - )2vi ()/(22),

  [0, 1] if c - µ = ivi() + µ22(1 - )2vi ()/(22),

 =0

if c - µ > ivi() + µ22(1 - )2vi ()/(22).

30

Optimality follows again from a standard verification argument. We show that

the process

t
Vt = e-rt(s)(sµ - c)ds + e-rtvi(t)
0

(33)

is a supermartingale for an arbitrary control and a martingale if the optimal con-

trol is chosen.

Applying Itô's lemma with (8) on (33), we find that

ertdVt =i(t)(tµ - c)dt - rvi(t)

+ (i(i(t) + 1 -i(t)) - t)vi(t)dt

1 + 2 (i(t) + 1

-

i

(t

))

µ2 2

2

(1

-

)2

vi

(t

)d

t

+

µ 

t

(1

-

t)vi

(t

)(1/2(t

)d

Z

t,i

+

1-/i2(t

)d

Z

 t,-i

).

We can now see from (15) that Vt is a supermartingale. It is a martingale only if the optimal control is applied.
The firm value at time 0 satisfies


E e-rti(t)(tµ - c)dt = E[V]  V0 = vi(0)
0

with equality if and only if the optimal control is chosen.

We next proof existence of solution and examine the uniqueness properties of the symmetric Markov perfect equilibria.
Proof of Theorem 1. We apply the change of variables x = (1-)/(1-¯) on (¯, 1] and x = (¯ - )/(¯ - ¯) on (¯, ¯] to recast (15) and (21) as solving a system of ordinary differential equations on x  [0, 1], with unknown parameters ¯ and ¯

r

y1(x;

¯ ,

¯ )

=

µ(1

-

x(1

-

¯ ))

-

c

-

(

-

1 + x(1 (1 - ¯)

-

¯ ))

y1(x;

¯ ,

¯ )

+1(1 - x(1 - ¯))2x2 y1 (x; ¯, ¯),

-

N

 (¯ -

¯ )

y2(x;

¯ ,

¯ )

+

2(¯

-

x(¯

-

¯))2(1 - ¯ (¯ - ¯)2

+

x(¯

-

¯ ))2

y2

(x;

¯ ,

¯ )

= c - µ(¯ - x(¯ - ¯)),

(34) (35)

31

where 1 = N/2µ2/2 and 2 = µ2/(22); with the boundary conditions

r

y1(0;

¯ ,

¯ )

-

(1 - ) 1 - ¯

y1(0;

¯ ,

¯ )

-

(µ

-

c)

=

0,

r

y1(1;

¯ ,

¯ )

-

¯ 1 - ¯

y1(1;

¯ ,

¯ )

-

(N

-

1)(

c

-

µ¯ )

=

0,

y2(1; ¯, ¯) = 0, y2(1; ¯, ¯) = 0,

-

y1(1; ¯, ¯) 1 - ¯

+

y2(0; ¯, ¯) ¯ - ¯

=

0,

y1(1; ¯, ¯) - y2(0; ¯, ¯) = 0.

(36)
(37) (38) (39) (40) (41)

Since 0 < ¯ < ¯ < 1, the system (34)-(35) satisfies Lipschitz and growth conditions in x, ¯ and ¯ on x  [0, 1].
The last step is to examine the uniqueness properties of the symmetric Markov
Perfect equilibria. Recall that

y1(x, ¯, ¯) = -(1 - ¯)v (; ¯, ¯)  0 and y2(x, ¯, ¯) = -(¯ - ¯)v (; ¯, ¯)  0 (42)

and

y1 (x; ¯, ¯) = (1 - ¯)2v (; ¯, ¯)  0 and y2 (x, ¯, ¯) = (¯ - ¯)2v (; ¯, ¯)  0 (43) by Lemma 5. In particular, we show that we can find two solutions y(x; ¯1, ¯1) and y(x; ¯2, ¯2) such that

¯1 > ¯2, ¯1 > ¯2 and ¯1 - ¯1 < ¯2 - ¯2

(44)

that both satisfy the system (34)-(41). The last assumption states that the game unravels more quickly if the players start to restrict effort at a higher belief.
Notice that (38) and (39) imply that
y2(0; ¯1, ¯1) = y2(1; ¯2, ¯2) = 0 and y2(1; ¯1, ¯1) = y2(1; ¯2, ¯2) = 0.
Then (35) implies that at x  [0, 1]

 -
N

y2(x; ¯1, ¯1) ¯1 - ¯1

-

y2(x; ¯2, ¯2) ¯2 - ¯2

+ 2

¯1(1 - ¯1) ¯1 - ¯1

2
y2 (1; ¯1, ¯1) -

¯2(1 - ¯2) ¯2 - ¯2

2
y2 (1; ¯2, ¯2)

= -µ(1 - x)(¯1 - ¯2) - µx(¯1 - ¯2) < 0.

(45)

32

Now since ¯1 - ¯2 < ¯2 - ¯2 it is immediate that we can find ¯1, ¯2, ¯1, ¯2, y2(x; ¯1, ¯1)  y2(x; ¯2, ¯2) and y2 (x; ¯1, ¯1)  y2 (x; ¯2, ¯2) such that (45) is satisfied.14 Notice that since ¯1 -¯2 < ¯1 -¯2, the right hand side of (45) increases as x decreases. Also, the left hand side increases since y (x; ·, ·) increases. This demonstrates that the earlier unraveling of the first equilibrium drives the difference between the thresholds.
At x = 1, (40) and (41) imply that y1(x; ¯1, ¯1)  y1(x; ¯2, ¯2) and y1(x; ¯1, ¯1)  y1(x; ¯2, ¯2). Moreover, (37) is satisfied since ¯1/(1 - ¯1) > ¯2/(1 - ¯2), and

r( y(1; ¯1, ¯1) - y(1; ¯2, ¯2)) - 

1

¯ 1 - ¯1

y

(1;

¯ 1 ,

¯ 1 )

-

1

¯ 2 - ¯2

y

(1;

¯ 2 ,

¯ 2

)

= -(N - 1)µ(¯1 - ¯2) < 0.

(34) is satisfied at any x  [0, 1] if

r( y1(x; ¯1, ¯1) - y1(x; ¯2, ¯2)) = µx(¯1 - ¯2)

+

1

-

x

- ( - 1 - ¯1

¯ 1

x)

y1

(x;

¯ 1 ,

¯ 1 )

-

1

-

x

- ( - 1 - ¯2

¯ 2

x)

y1

(x;

¯ 2 ,

¯ 2 )

+ 1((1 - x(1 - ¯1))2x2 y1(x; ¯1, ¯1) - (1 - x(1 - ¯2))2x2 y1(x; ¯2, ¯2)).

(46)

Clearly, (46) can only be satisfied if the second term in the right hand side is negative. This is the case if ¯1 > , that is, the players start restricting effort above the long term mean. In that case, unraveling starts at ¯ and the game has multiple equilibria. If ¯1 < , the belief drifts up at the point, at which the players
start restricting effort. Now focus on the case in which ¯1 > . Then (36) implies that

r( y1(0; ¯1, ¯1) - y1(0; ¯2, ¯2)) = (1 - )

y1(0; ¯1, ¯1) 1 - ¯1

-

y1(0; ¯2, ¯2) 1 - ¯2

which is satisfied if

y(0; ¯1, ¯1) < y(0; ¯2, ¯2)

and

y1(0; ¯1, ¯1) 1 - ¯1

<

y1(0; ¯2, ¯2 1 - ¯2

)

.

Changing back the variables to recast the problem on the original domain, we

have

v(1; ¯1, ¯1) < v(1; ¯2, ¯2) and v (1; ¯1, ¯1) > v (1; ¯2, ¯2).

14Notice that y2(x; ¯1, ¯1)  y2(x; ¯2, ¯2) for all x  [0, 1].

33

r v i () µ-c µ - c - (1 - )vH(1)
µ - c - (1 - )vL(1)

¯ H

¯ L ¯ H

¯ L

1

Figure 4: Multiple equilibria in the model with  < 1 and  > 0.

The players do not only have a lower value at the upper boundary  = 1, but the value also falls more rapidly. Since the players start restricting effort earlier, the game unravels earlier, which leads to them losing value more rapidly, even at high beliefs. Figure 4 describes the situation in a phase diagram.

34

References
Dirk Bergemann and Juuso Välimäki. Bandit problems. In Steven N. Durlauf and Lawrence E. Blume, editors, The New Palgrave Dictionary of Economics. Macmillan Press, 2nd edition, 2008.
Patrick Bolton and Christopher Harris. Strategic experimentation. Econometrica, 67(2):349­374, 1999.
Roland G Fryer and Philipp Harms. Two-armed restless bandits with imperfect information: Stochastic control and indexability. Technical report, National Bureau of Economic Research, 2013.
Johannes Hörner and Andrzej Skrzypacz. Learning, experimentation and information design. Mimeo, Yale University and Stanford Graduate School of Business, 2016.
Godfrey Keller and Sven Rady. Optimal experimentation in a changing environment. The Review of Economic Studies, 66(3):475­507, 1999.
Godfrey Keller and Sven Rady. Price dispersion and learning in a dynamic differentiated-goods duopoly. RAND Journal of Economics, pages 138­165, 2003.
Godfrey Keller and Sven Rady. Strategic experimentation with poisson bandits. Theoretical Economics, 5(2):275­311, 2010.
Godfrey Keller and Sven Rady. Breakdowns. Theoretical Economics, 10(1):175­ 202, 2015.
Godfrey Keller, Sven Rady, and Martin Cripps. Strategic experimentation with exponential bandits. Econometrica, 73(1):39­68, 2005.
Daria Khromenkova. Restless strategic experimentation. Mimeo, University of Mannheim, 2016.
Robert S Liptser and AN Shiryayev. Statistics of Random Processes: Vol.: 1.: General Theory. Springer-Verlag, 1977.
Bernt Karsten Øksendal and Agnes Sulem. Applied stochastic control of jump diffusions, volume 498. Springer, 2005.
Mikhail Safronov. Obtaining experience in a multi-armed bandit. Mimeo, University of Northwestern, 2014.
35

Melissa A Schilling. Strategic management of technological innovation. Tata McGraw-Hill Education, 2005.
36

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001
002 003
004 005 006
007 008 009 010
011 012
013 014
015 016 017
018 019 020

"Downside risk and stock returns: An empirical analysis of the long-run and short-run dynamics from the G-7 Countries" by Cathy Yi-Hsuan Chen, Thomas C. Chiang and Wolfgang Karl Härdle, January 2016. "Uncertainty and Employment Dynamics in the Euro Area and the US" by Aleksei Netsunajev and Katharina Glass, January 2016. "College Admissions with Entrance Exams: Centralized versus Decentralized" by Isa E. Hafalir, Rustamdjan Hakimov, Dorothea Kübler and Morimitsu Kurino, January 2016. "Leveraged ETF options implied volatility paradox: a statistical study" by Wolfgang Karl Härdle, Sergey Nasekin and Zhiwu Hong, February 2016. "The German Labor Market Miracle, 2003 -2015: An Assessment" by Michael C. Burda, February 2016. "What Derives the Bond Portfolio Value-at-Risk: Information Roles of Macroeconomic and Financial Stress Factors" by Anthony H. Tu and Cathy Yi-Hsuan Chen, February 2016. "Budget-neutral fiscal rules targeting inflation differentials" by Maren Brede, February 2016. "Measuring the benefit from reducing income inequality in terms of GDP" by Simon Voigts, February 2016. "Solving DSGE Portfolio Choice Models with Asymmetric Countries" by Grzegorz R. Dlugoszek, February 2016. "No Role for the Hartz Reforms? Demand and Supply Factors in the German Labor Market, 1993-2014" by Michael C. Burda and Stefanie Seele, February 2016. "Cognitive Load Increases Risk Aversion" by Holger Gerhardt, Guido P. Biele, Hauke R. Heekeren, and Harald Uhlig, March 2016. "Neighborhood Effects in Wind Farm Performance: An Econometric Approach" by Matthias Ritter, Simone Pieralli and Martin Odening, March 2016. "The importance of time-varying parameters in new Keynesian models with zero lower bound" by Julien Albertini and Hong Lan, March 2016. "Aggregate Employment, Job Polarization and Inequalities: A Transatlantic Perspective" by Julien Albertini and Jean Olivier Hairault, March 2016. "The Anchoring of Inflation Expectations in the Short and in the Long Run" by Dieter Nautz, Aleksei Netsunajev and Till Strohsal, March 2016. "Irrational Exuberance and Herding in Financial Markets" by Christopher Boortz, March 2016. "Calculating Joint Confidence Bands for Impulse Response Functions using Highest Density Regions" by Helmut Lütkepohl, Anna StaszewskaBystrova and Peter Winker, March 2016. "Factorisable Sparse Tail Event Curves with Expectiles" by Wolfgang K. Härdle, Chen Huang and Shih-Kang Chao, March 2016. "International dynamics of inflation expectations" by Aleksei Netsunajev and Lars Winkelmann, May 2016. "Academic Ranking Scales in Economics: Prediction and Imdputation" by Alona Zharova, Andrija Mihoci and Wolfgang Karl Härdle, May 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

021 022
023 024
025
026 027
028 029 030
031
032 033
034 035
036
037 038
039

"CRIX or evaluating blockchain based currencies" by Simon Trimborn and Wolfgang Karl Härdle, May 2016. "Towards a national indicator for urban green space provision and environmental inequalities in Germany: Method and findings" by Henry Wüstemann, Dennis Kalisch, June 2016. "A Mortality Model for Multi-populations: A Semi-Parametric Approach" by Lei Fang, Wolfgang K. Härdle and Juhyun Park, June 2016. "Simultaneous Inference for the Partially Linear Model with a Multivariate Unknown Function when the Covariates are Measured with Errors" by Kun Ho Kim, Shih-Kang Chao and Wolfgang K. Härdle, August 2016. "Forecasting Limit Order Book Liquidity Supply-Demand Curves with Functional AutoRegressive Dynamics" by Ying Chen, Wee Song Chua and Wolfgang K. Härdle, August 2016. "VAT multipliers and pass-through dynamics" by Simon Voigts, August 2016. "Can a Bonus Overcome Moral Hazard? An Experiment on Voluntary Payments, Competition, and Reputation in Markets for Expert Services" by Vera Angelova and Tobias Regner, August 2016. "Relative Performance of Liability Rules: Experimental Evidence" by Vera Angelova, Giuseppe Attanasi, Yolande Hiriart, August 2016. "What renders financial advisors less treacherous? On commissions and reciprocity" by Vera Angelova, August 2016. "Do voluntary payments to advisors improve the quality of financial advice? An experimental sender-receiver game" by Vera Angelova and Tobias Regner, August 2016. "A first econometric analysis of the CRIX family" by Shi Chen, Cathy YiHsuan Chen, Wolfgang Karl Härdle, TM Lee and Bobby Ong, August 2016. "Specification Testing in Nonparametric Instrumental Quantile Regression" by Christoph Breunig, August 2016. "Functional Principal Component Analysis for Derivatives of Multivariate Curves" by Maria Grith, Wolfgang K. Härdle, Alois Kneip and Heiko Wagner, August 2016. "Blooming Landscapes in the West? - German reunification and the price of land." by Raphael Schoettler and Nikolaus Wolf, September 2016. "Time-Adaptive Probabilistic Forecasts of Electricity Spot Prices with Application to Risk Management." by Brenda López Cabrera , Franziska Schulz, September 2016. "Protecting Unsophisticated Applicants in School Choice through Information Disclosure" by Christian Basteck and Marco Mantovani, September 2016. "Cognitive Ability and Games of School Choice" by Christian Basteck and Marco Mantovani, Oktober 2016. "The Cross-Section of Crypto-Currencies as Financial Assets: An Overview" by Hermann Elendner, Simon Trimborn, Bobby Ong and Teik Ming Lee, Oktober 2016. "Disinflation and the Phillips Curve: Israel 1986-2015" by Rafi Melnick and Till Strohsal, Oktober 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

040 041 042 043 044 045 046 047 048

"Principal Component Analysis in an Asymmetric Norm" by Ngoc M. Tran, Petra Burdejová, Maria Osipenko and Wolfgang K. Härdle, October 2016. "Forward Guidance under Disagreement - Evidence from the Fed's Dot Projections" by Gunda-Alexandra Detmers, October 2016. "The Impact of a Negative Labor Demand Shock on Fertility - Evidence from the Fall of the Berlin Wall" by Hannah Liepmann, October 2016. "Implications of Shadow Bank Regulation for Monetary Policy at the Zero Lower Bound" by Falk Mazelis, October 2016. "Dynamic Contracting with Long-Term Consequences: Optimal CEO Compensation and Turnover" by Suvi Vasama, October 2016. "Information Acquisition and Liquidity Dry-Ups" by Philipp Koenig and David Pothier, October 2016. "Credit Rating Score Analysis" by Wolfgang Karl Härdle, Phoon Kok Fai and David Lee Kuo Chuen, November 2016. "Time Varying Quantile Lasso" by Lenka Zbonakova, Wolfgang Karl Härdle, Phoon Kok Fai and Weining Wang, November 2016. "Unraveling of Cooperation in Dynamic Collaboration" by Suvi Vasama, November 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

