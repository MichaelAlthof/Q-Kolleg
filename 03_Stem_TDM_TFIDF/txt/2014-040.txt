BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2014-040
Localising Forward Intensities for
Multiperiod Corporate Default
Dedy Dwi Prastyo* Wolfang Karl Härdle*
* Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Localising Forward Intensities for Multiperiod Corporate Default 
Dedy Dwi Prastyo1,2 and Wolfgang Karl H¨ardle1,3
1 Humboldt-Universita¨t zu Berlin, Ladislaus von Bortkiewicz Chair of Statistics Center for Applied Statistics and Economics (C.A.S.E.) Unter den Linden 6, 10099 Berlin, Germany
2 Department of Statistics, Institut Teknologi Sepuluh Nopember (ITS) Jl. Arief Rahman Hakim, Surabaya 60111, Indonesia
3 Singapore Management University 50 Stamford Road, Singapore 178899
Abstract
Using a local adaptive Forward Intensities Approach (FIA) we investigate multiperiod corporate defaults and other delisting schemes. The proposed approach is fully datadriven and is based on local adaptive estimation and the selection of optimal estimation windows. Time-dependent model parameters are derived by a sequential testing procedure that yields adapted predictions at every time point. Applying the proposed method to monthly data on 2000 U.S. public firms over a sample period from 1991 to 2011, we estimate default probabilities over various prediction horizons. The prediction performance is evaluated against the global FIA that employs all past observations. For the six months prediction horizon, the local adaptive FIA performs with the same accuracy as the benchmark. The default prediction power is improved for the longer horizon (one to three years). Our local adaptive method can be applied to any other specifications of forward intensities.
Key words: Accuracy ratio, Forward default intensity, Local adaptive, Mutiperiod prediction
JEL Classification: C41, C53, C58, G33
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 'Economic Risk', Humboldt-Universit¨at zu Berlin. Dedy Dwi Prastyo was also supported by Directorate General for Higher Education, Indonesian Ministry of Education and Culture through Department of Statistics, Institut Teknologi Sepuluh Nopember (ITS), Indonesia. We would like to thank the Risk Management Institute (RMI), the National University of Singapore (NUS) for the data used in this study and for the partial financial support under Credit Research Initiative (CRI) project. We would also like to thank the International Research Training Group (IRTG) 1792. Email: prastyod@hu-berlin.de or dedy-dp@statistika.its.ac.id (Prastyo), haerdle@wiwi.hu-berlin.de (H¨ardle).
1

1 INTRODUCTION
Credit risk analysis plays an essential role in finance in order to measure default risk that can put stakeholders on financial complication. As a consequence of Basel's proposed capital requirement on credit asset, bank and financial institution have to develope their internal credit risk system. Two key elements for the internal credit rating are (Ha¨rdle and Prastyo, 2014): (i) compute probability of default (PD) and (ii) estimate the loss given default (LGD). The PD is the probability of failing to pay debt in full over a particular time horizon. The LGD is the percentage of loss over the total exposure upon default that can be estimated by identifying its distribution on defaulters with similar attributes. This paper employs a corporate PD prediction methodology.
Many stochastic models and statistical techniques have been developed to measure the likelihood that a debtor will fail to service its obligation in full. One of the techniques for default prediction is discriminant analysis. This and also classification settings such as support vector machines (Ha¨rdle et al. (2009); Chen et al. (2011); Ha¨rdle et al. (2014)) though fall short when the interest is in a time varying context. Logit and probit regression are designed to estimate PD directly but have rarely been employed in a time series setting. Recently, the implementation of hazard rate models has received much attention, see Shumway (2001); Chava and Jarrow (2004); Campbell et al. (2008); Bharath and Shumway (2008). A general fitting problem though remains: one disregards companies delist for reason other than that of defaults. This negligence results in censoring biases. One such issue is addressed in intensity based models e.g. Duffie et al. (2007) that treats both default and other exit events of the delisted companies.
An advanced default analysis views individual defaults along with their state predictors: the default (non-default) companies are tagged with common risk factors and firm-specific attributes. Duffie et al. (2009) accommodated unobservable common factors as frailty
2

factor. Duan et al. (2012) modeled the dynamic data panel by using a forward-intensity specification. It models the default term structure by a new reduced form approach that takes into account both default and other type of exits from companies that are delisted from the market. The PD for different horizon is then computed as a function of different input variables. The advantage of this specification is that one does not need to bother about modeling the high-dimensional state variable process. It contrasts the spot intensity model (Duffie et al., 2007) which requires specification and estimation of the time series of state variables. This may be quite challenging for a high dimensional set of variables.
The forward intensity approach specifies a parametric form that is per se constant although varies over time as depicted by Figure 1. In addition, Figure 2 and 3 exhibit parameters estimates of forward intensities specification that are sensitive to the length of estimation windows. This is where a time varying approach comes into play. More precisely, in this paper a time varying parameter model is approximated by a local constant parametric form. The aim is to implement and localise such parameters for forward intensities. The technique presented selects a data-driven estimation window that allows for flexible forecasts. The key idea is to employ a sequential testing procedure to identify this time interval of constant parameters. Corporate PDs are then computed based on this data interval. By controlling the risk of false alarm, i.e. the algorithm stops earlier than an oracle interval, the algorithm selects the longest possible window for which parameter constancy cannot be rejected.
The proposed framework builds on the local parametric approach (LPA) proposed by Spokoiny (1998). LPA involves processes that are stationary only locally: (i) consider only recent data, (ii) imply sub setting of data using some localisation scheme. Methods developed in this LPA framework are local change point (LCP) (Mercurio and Spokoiny, 2004), local model selection (Katkovnik and Spokoiny, 2008), and stagewise aggregation
3

(Belomestny and Spokoiny, 2007). The studies done by Chen et al. (2008); Giacomini et al. (2009); Chen et al. (2010); and Chen and Niu (2014) showed that LCP work well in practice.
The contribution of this paper is to introduce an adaptive calibration technique for forward intensities in a multiperiod corporate default setting motivated by our preliminary analysis (Figure 1, 2 and 3). We apply LCP as in Chen and Niu (2014) to the forward intensity model of Duan et al. (2012) to detect the largest interval of homogeneity, i.e. the interval where a local constant parametric form describes the data well, and to provide an adaptive estimate as the one associated with the interval found. The proposed method tests the null hypothesis of homogeneous interval with no change points against the alternative hypothesis of at least one change point being present.
Our empirical analysis uses 2000 U.S. public firms for the period from 1991 to 2011. There are two macroeconomic factors which act as common variables and ten firm-specific variables. The empirical implementation indicates that the parameter estimate of one-year simple return of S&P500 index is sensitive to the length of estimation interval. The parameter estimate of another macroeconomic factor, i.e. 3-month US Treasury bill interest rate, varies less for different calibration window length. The volatility-adjusted leverage (measured as distance-to-default), company size, market-to-book ratio, and idiosyncratic volatility are robust to the estimation interval length whereas the remaining firm-specific attributes that represent liquidity and profitability are not. We estimate default probabilities over various prediction horizons: one month, three months, six months, one year, two years, and three years. The prediction performance is evaluated against the global forward intensities approach that employs all past observations. For the six months prediction horizon, both the global and the local forward intensities approaches perform with the same accuracy. The default prediction power is improved for the longer horizon (one to three years).
4

The remainder of the paper is structured as follows. The forward intensity approach and the LPA are introduced in Section 2 and 3, respectively. The data and empirical findings on PD prediction are provided in Section 4. Section 5 concludes this study.

2 LOCALISING FORWARD INTENSITIES

2.1 Forward Intensities Approach (FIA)

Time of default for the i-th firm is denoted by Di. We first describe FIA in the pure default case then move on to combined exits for horizon  . In the following sections we drop index i for simplicity. For known default intensity s, the survival probability in [t, t +  ] is:

t+

P (D > t +  ) = exp -

s ds .

t

(1)

It is reasonable to assume that the intensity s is driven by time dependent state variables Xs, Xs  Rp, of which their future evolution is unknown. Hence, given a model for the dynamics of Xs one may obtain s = (Xs) by forecasting the path of Xs. The state variables typically contain common factors W and firm-specific attributes U , Xt = (Wt, Ut). Consider conditioning on a filtration {Ft : t  0}, where Ft is generated by:

{(Us, Ds) : s  min(t, D)}  {Ws : s  t},

with Ds a Poisson process for default with intensity (Xs).
Given that from time t one would like to understand the default occurance until t +  one needs therefore to simulate the future path of Xs. Following the idea of Duffie et al. (2007) technique, i.e. s = (Xs; t), t is a vector of parameters obtained based on Xt,

5

the survival probability (1) reads now as:

t+s

P (D > t + s |Ft ) d=ef E exp -

(Xu; t) du |Xt .

t

(2)

Forecasting the time series of Xs is quite challenging though particularly when the dimension p is high. An alternative solution is to specify a forward default intensity:

t(s)

d=ef

lim

P (t

+

s

<

D



t

+

s

+

t |D



t

+

s) ,

t0

t

(3)

as a function of Xt alone, t(s) = (Xt, s). Duan et al. (2012) proposed t(s) = (s; Xt) that directly employs Xt instead of using an imposed dynamic of Xs. In this paper, we generalise this idea via time varying parameters, t(s) = (s,t; Xt). Table 1 summarizes the specification of intensity at time s.

Table 1: The specifications of the default intensity.
s = (Xs; t) , Duffie et al. (2007) t(s) = (s; Xt) , Duan et al. (2012) t(s) = (s,t; Xt) , Our approach

The idea behind the FIA in the pure default case is as follows. One observes firm i, i = 1, . . . , N , over an entire sample period [0, T ] and records its default time Di and state variables Xit. At any time t, 0 < t  t + s  T , the PD can be predicted for next s period using information Xt and evaluated against the true one. This predicted PD is derived by an estimate of the forward default intensity t(s) = (s; Xt). The s is calibrated by maximizing corresponding likelihood function over [0, t] that will be presented in (24). The forward default intensity specifies an explicit dependence of default intensity in the future, s, to the values of state variables at the time of prediction t.
6

Denote the (differentiable) conditional cdf of D evaluated at s:

Ft(s) = 1 - P (D > t + s |Ft ) ,

(4)

with the conditional survival probability:

t+s

P (D > t + s |Ft ) = E exp -

u du |Xt .

t

(5)

The hazard rate is the event rate at time t conditional on survival time t or later. The forward intensity is a hazard function where the survival time is evaluated at a fixed horizon of interest. Hence, the forward default intensity (3) can be rewritten as:

t(s)

=

1

Ft (s) - Ft(s)

=

t(s)

+

t(s)s,

(6)

with t(s) defined as:

t(s)

d=ef

- log {1 - Ft(s)} , s

=

log E -

exp

-

t+s t

u

du

|Xt

.

s

(7)

Thus, t(s)s =

s 0

t(u)

du

is

the

cumulative

forward

default

intensity

and

exp{-t(s)s}

is the survival probability. The proof is given in Appendix A. The forward default inten-

sity t(s) as defined in (3) is then formulated as:

E

t(s)

=

exp

{-t(s)s}

lim
t0

t+s+t t+s

exp

-

t+u t

v

dv

u du |D  t + s

.

t

(8)

7

The conditional probabilities to survive (9) and to default (10) now are, respectively:

t+s

P (D > t + s|Ft) = exp -

t(u) du = exp {-t(s)s}

t

t+s t+u

P (D  t + s|Ft) =

exp -

t(v) dv t(u) du.

tt

(9) (10)

A company traded in a stock exchange can be delisted because of a default event or other reasons, such as merger or aquisition operations. Duffie et al. (2007) modelled these two events via a doubly stochastic process driven by two independent mechanisms with intensities t and t. Denote O as the time of other exit. Recall s = (Xs; t) and specify s = (Xs; t), by law of iterated expectation and conditional on filtration Ft, the probability to survive (11) and to default (12) over [t, t +  ] are:

t+s

P (D, O > t + s, |Ft) d=ef E exp -

(u + u) du |Xt ,

(11)

t

t+s t+u

P (D, O  t + s, |Ft) d=ef E

exp -

(v + v) dv u du |Xt . (12)

tt

A default event cannot happen after a company exited from the market. Thus these two events are competing and not fully independent. The independency assumption of the two processes will blur the distinction between competing and independent risk. Duan et al. (2012) proposed a forward intensity approach that enables us to work in a more convenient way.
Time of default and other exits together, hereinafter called combined exit, is denoted by C, with C  D. Applying the same procedure as in estimating forward default intensity from the pure default process, denote the (differentiable)

Gt(s) = 1 - P (C > t + s |Ft )

(13)

8

as the conditional cdf of C evaluated at s with conditional survival probability:

t+s

P (C > t + s |Ft ) = E exp -

gu du |Xt .

t

(14)

The forward combined exit intensity gt(s):

gt(s)

d=ef

lim P (t + s < C  t + s + t |C  t + s )

t0

t

(15)

can be rewritten as:

gt(s)

=

1

Gt(s) - Gt(s)

=

t(s) +

t(s)s.

(16)

The t(s) in (7) is rewritten in term of gu. Thus, t(s)s =

s 0

gt(u)

du

and

the

conditional

survival probability (14) is given by:

P (C > t + s |Ft ) = exp {-t(s)s} .

(17)

The instantaneous default intensity at horizon t+s (forward default intensity from doubly Poisson processes) is defined as:

ft(s)

d=ef

exp

{-t(s)s}

lim
t0

P

(t

+

s

<

D

=

C  t

t

+

s

+

t

|Q ) ,

E

=

exp

{-t(s)s}

lim
t0

t+s+t t+s

exp

-

u t

gv

dv

u du |Q

,

t

(18) (19)

with Q the event that D = C  t + s. The default probability over [t, t + s] is

t+s t+u

P (C  t + s |Ft ) =

exp -

gt(v) dv ft(u) du.

tt

(20)

Duan et al. (2012) deal with fit(s) and git(s) as functions of state variables Xit for firm 9

i, with fit(s) > 0 and git(s)  fit(s). More precisely, with fit(s) = fit(s; Xit) and git(s) = git(s; Xit):

fit(s) = exp  (s)Xit , git(s) = fit(s) + exp  (s)Xit ,

(21) (22)

with Xit = (1, xit,1, xit,2, . . . , xit,p) that include macroeconomic factors (Wt) as common factors and firm-specific attributes (Uit). The survival and default probabilities are assumed to depend only upon W and U such that different firms are Ft-conditionally independent among themselves. If it is not the case, the dependency must arise from their sharing of W and/or any correlation among U . This conditional independence assumption is in essence similar to the doubly stochastic assumption. Therefore, one firm's exit neither feedback to the state variables nor influence the exit probabilities of other firms. This approach is identical to the spot intensity formulation of Duffie et al. (2007) when s = 0.

^12(12), ^12(12)
-10 -5 0 5
^12(), ^12()
-10 -5 0 5

0 5 10 15 20 25 30 35 window (6 y)

0 5 10 15 20 25 30 35 

Figure 1: The j( ) = 12(12) (solid) and j( ) = 12(12) (dashed) for idiosyncratic volatility. Left: fixed  = 12 over 35 rolling windows (length: 6 years). Right: The 35-th window with time end December 2011,  = 0, 1, . . . , 36. Solid circles represent the same estimates.

10

2.2 Local Parametric Dynamics

As can be deduced from Figure 1, the parameters in (21) and (22) may vary over time, i.e. jt( ) and jt( ), j = 1, . . . , p, are time local parameters. Time varying coefficients are typically assumed as: (i) smooth functions of time (Cai et al. (2000); Fan and Zhang (2008)) or (ii) piecewise constant functions (Bai and Perron, 1998). In contrast to these approaches that aim at establishing a time varying model for the whole sample period, our approach is local and data-driven. It is focused on an instantenous calibration of (20).

The LPA (Local Parametric Approach) aims at finding a balance between parameter variability (precision) and modelling bias by taking into account the past information which is statistically identified as being relevant. In fact one determines time localized parameters: for any particular time point t, there exists a past suitable window over which the time varying parameters in (21) and (22) are approximately constant. This is in fact the basic idea of the LPA, that is to select a window that guarantees a localised stable model. This is realised by a sequential test based on comparing the increase of the log likelihood process relative to critical values (Spokoiny, 2009).
Denote an interval I = [t - m, t] as a right-end fixed interval of m observation at time t. Suppose that our sample has period [0, T ] for each interval I. Then, the local likelihood (for the horizon  ) based on (21)(22) in interval I:

N T -1

LI, (, ) =

L,i,t (, ) ,

i=1 t=0 tI

(23)

11

where N is the number of companies at t and







0(0) 0(1) · · · 0( - 1)

0(0) 0(1) · · · 0( - 1)







  1(0)

1(1)

···

1(

- 1)

 

  1(0)

1(1)

···

1(

-

1)

 

=  

...

... . . .

...

;  =   

...

... . . .

...

.  













p(0) p(1) · · · p( - 1)

p(0) p(1) · · · p( - 1)

Let t0i be the first time that firm i appeared in the sample. If the firm does not appear in sample in t or is already delisted before t, i.e. t0i > t or Ci  t, then the likelihood is set to 1 and is transformed to 0 in log-likelihood such that

L,i,t (, ) = 1{t0it, Ci>t+}Pt(Ci > t +  ) +1{t0it, Di=Cit+ }Pt(Ci; Di = Ci  t +  ) +1{t0it,Di=Ci,Cit+ }Pt(Ci; Di = Ci&Ci  t +  ) +1{t0i>t} + 1{Cit},

(24)

with Pt(Ci) = P(Ci|Ft). For numerical application, (14),(17) are approximated by:

 -1
Pt (Ci > t +  ) = exp - git(s)t ,
s=0

(25)

with t = 1/12 to represent that prediction horizon is measured in month. Therefore,

12

the probability of exit due to default and other reasons, respectively:

Pt (Ci; Di = Ci  t +  ) 

1 

-

exp

{-fit

(0)t}

if Ci = t + 1,







=

exp -

Ci-t-2 s=0

git(s)t

× [1 - exp {-fit (Ci - t - 1) t}]









  

if t + 1 < Ci  t + ,

(26)

Pt (Ci; Di = Ci&Ci  t +  ) 

exp 

{-fit(0)t}

-

exp

{-git(0)t}







 exp -

Ci-t-2 s=0

git(s)t

×

=

if Ci = t + 1,

   

[exp {-fit (Ci - t - 1) t} - exp {-git (Ci - t - 1) t}]









  

if t + 1 < Ci  t + ,

(27)

The forward intensities in the discretized version, i.e. fit( ) and git( ), should be understood as at time t for the period [t + , t +  + 1] because horizon index s in (25)-(27) starts from zero. Forward intensity is basically spot intensity for one month ahead. Duan et al. (2012) derived the large sample properties of the likelihood (23) constructed from overlapped periods. This likelihood can be numerically maximized to obtain  and .
The log likelihood of (23) separates into a sum of terms involving  and . We can maximize its two components individually to obtain  and . In addition, the likelihood for  or  can be decomposed to terms involving ( ) or ( ) only. This property enables us to estimate  and  without performing estimation sequentially from shorter to longer

13

prediction horizon. Thus for horizon s = 0, 1, . . . ,  - 1,

with

n T -s-1

(s) = max log L{(s)} = max log

Li,t{(s)} ,

(s)

(s)

i=1 t=0

n T -s-1

(s) = max log L{(s)} = max log

Li,t{(s)} ,

(s) (s)

i=1 t=0

(28) (29)

Li,t {(s)} = 1{t0it, Ci>t+s+1} exp {-fit(s)t} +1{t0it, Di=Cit+s+1} [1 - exp {-fit(s)t}] +1{t0it,Di=Ci,Cit+s+1} exp {-fit(s)t} +1{t0i>t} + 1{Cit+s+1},
Li,t {(s)} = 1{t0it, Ci>t+s+1} exp {- [git(s) - fit(s)] t} +1{t0it, Di=Cit+s+1} +1{t0it,Di=Ci,Cit+s+1} [1 - exp {- [git(s) - fit(s)] t}] +1{t0i>t} + 1{Cit+s+1},

(30) (31)

where git(s) - fit(s) = exp {0(s) + 1(s)xit,1 + . . . + p(s)xit,p}.
All the firm-month observations are classified into following categories X0 = x01, . . . , x0N0 , X1 = x11, . . . , x1N1 , and X2 = x21, . . . , xN2 2 , where X0, X1, and X2 contain all firmmonth observations that survive, default, and exit due to other reasons, respectively. The N0, N1, and N2 are number of observations in each category. Therefore, we can express

14

the horizon-specific log-likelihood:

N0 N1
log L{(s)} = - exp(x0i )t + log 1 - exp{- exp(xi1)t}
i=1 i=1
N2
- exp(x2i )t,
i=1
N0 N1
log L{(s)} = - exp(xi0)t + log 1 - exp{- exp(x1i )t} .
i=1 i=1

(32) (33)

In LPA, the maximum likelihood estimates (MLE) of  = {, } over horizon  in the data interval I is maximizing (23):

I = arg max LI, (, ). 

(34)

The interval I controls the estimation quality and addresses the tradeoff between esti-
mation efficiency and local flexibility. The quality of I as the estimator of the true time varying parameter vector t is assessed by Kullback-Leibler (KL) divergence. Discarding the time subscript and keep an asterisk () for notational convenience, the KL divergence of approximate distribution PI from the true distribution P is KI, {I, } = E log(P/PI ) . Let NI is number of observations in interval I, the KL divergence of distributions that belong to exponential family can be represented in term of (local)
likelihood:

KI, {I , } = NI-1 LI, (I ) - LI, ()

(35)

that measures the expectation (under P) of the information lost when PI is used to approximate P. By introducing the r-th power of that likelihood difference, define a

15

loss function:

LI, (I , ) d=ef LI, (I ) - LI, ()

(36)

that obeys a parametric risk bound:
r
E LI, (I , )  Rr () ,

(37)

where Rr () denotes a constant depending on r > 0 and , see Spokoiny (2009). In the exponential family set up, the parametric risk bound is parameter invariant, i.e. Rr () = Rr. Different values of r lead to different risk bounds (37), critical values and adaptive estimates. Higher values of r lead to selection of longer intervals of homogeneity. We follow the recommendation of C´izek et al. (2009) and consider r = 0.5 and r = 1.

3 LOCAL PARAMETRIC FRAMEWORK

In practice, the interval of homogeneity is unknown and needs to be selected among a finite set of K candidates. The aim is to well approximate the time varying parameter t model by a locally constant parametric model. The approximation quality is measured by the KL divergence (35). Denote Ik, () = tIk KIk, {µt, µt()} as a measure of discrepancy between the true (unknown) data generating process µt and the parametric model µt() with intensities (21) and (22) for intervals Ik. Let for some   ,

E {Ik ()}  ,

(38)

where   0 denotes a small modelling bias (SMB) for interval Ik. Consider (K + 1) nested intervals (with fixed right-end point t) Ik = [t - mk, t] of length |Ik| = mk for
16

any particular time t, IK  · · ·  Ik  · · ·  I1  I0. The oracle, i.e. theoretically optimal, choice Ik of the interval sequence is defined as the largest interval for which the SMB condition (38) holds. In practice of course Ik is unknown and therefore the oracle choice of k cannot be implemented directly. One therefore mimics the oracle choice via sequential testing for k ideal situation, where k = 1, . . . , K (Spokoiny, 2009).
3.1 Homogeneity Interval Test for Fixed 

The interval selection algorithm chooses the (optimal) length of interval where at each Ik, it tests the null hypothesis on parameter homogeneity against the alternative of a change point within Ik. We write k instead of Ik as estimates obtained at interval Ik. The adaptive estimates k is the MLE at the interval of homogeneity, i.e. k = k. For I0, one puts 0 = 0. One iteratively extends the subsets and sequentially tests for possible change points in the next longer interval. For a fixed horizon, a likelihood ratio test (LRT) is employed at each Ik with test statistic (Chen and Niu, 2014):

r
Tk, = LIk (k) - LIk (k-1) , k = 1, . . . , K.

(39)

Assume parameter homogeneity in Ik-1 has been established at a given time point t, the hypothetical homogeneity in interval Ik is tested by measuring the difference between their corresponding estimates.
Once a set of critical values z1, , . . . , zK, is generated via a Monte Carlo simulation, the sequential testing procedure is accomplished. If Tk, > zk, , then the procedure terminates and selects interval Ik-1 such that k = k-1 = k-1. Otherwise, the interval Ik is accepted as homogeneous and one continues to update the estimate k = k. The adaptive estimation is done through comparing the test statistic (39) at every step k with the corresponding critical value zk, . One then searches for the longest interval of

17

homogeneity Ik for which the null hypothesis is not rejected:

 = k,

k = max {k : T ,  z , , kK

 k} .

(40)

The smallest interval is always considered to be homogeneous. If the null is rejected at the first step, then  = 0. Otherwise, we sequentially repeat this test until we find a change point at k, accordingly  = k, or exhaust all interval such that  = K.

3.2 Critical Values for Fixed 

Under the hypothesis of parameter homogeneity, the correct choice of interval is the largest one, IK. The critical values are chosen in a way such that the probability of selecting k < K, "false alarm", is minimized. For a fixed  , in case k is selected (instead of K) and thus  = k instead of K, the loss as defined in (36) is LIK (K, ) = LIK (K) - LIK () and stochastically bounded by

r
E LIK (K ) - LIK ()   Rr () ,

(41)

with parametric risk bound generated from (true) simulated parameter :
r
Rr () = E LIK (K ) - LIK () .

(42)

The Rr () is finite (see Appendix B) and can be numerically computed with the knowledge of .
Critical values must ensure that the loss associated with false alarm is at most a -fraction of the parametric risk bound, Rr (), of the oracle estimate K. The  can be interpreted

18

as the false alarm rate probability for r  0. Accordingly, an estimate k should satisfy

E

LIk (k)

-

LIk (k)

r



k K

Rr

() .

(43)

Comparing test statistics (39) with critical values, if Tk, > zk, one accepts k = k-1 = k-1, otherwise k = k. In general, k differs from k only if a change point is detected at the first k steps.
The critical values zk that satisfy (43) are found numerically by Monte Carlo simulation because the sampling distribution of the test statistic is unknown even asymptotically. Relatively large critical values lead to a higher probability of selecting subintervals everywhere, resulting in selecting longer intervals of homogeneity. On the other hand, small critical values lead to favor shorter intervals, discarding useful observations in the past and increasing modelling bias. The optimal critical values are the minimum values to just accept the parametric risk bound at each interval.

4 DATA AND EMPIRICAL FINDINGS
Following the notation that was introduced in section 2.1, the state variable for firm i at time t is Xi = (W, Ui), where a vector W is common to all firms in the same economy and a firm-specific vector Ui is observable from the date of firm's financial statement is firstly released until the month before the firm exits (if it does). Our data is a subset of dataset used by Duan et al. (2012) consisting of 2000 U.S. public firms over the period from 1991 to 2011 obtained from CRI database. There are two common variables and ten firm-specific variables.
Common variables (Wt) are: (i) trailing one-year simple return on S&P500 index (ii) 3-month U.S. Treasury bill rate, hence W  R2. Firm-specific variables (Uit) are: (i)
19

Table 2: State variable.

Xj X1 X2 X3, X4 X5, X6

: Index return : Interest rate : DTDlevel, DTDtrend : (CASH/TA)level, (CASH/TA)trend

X7, X8 X9, X10 X11 X12

: (NI/TA)level, (NI/TA)trend : SIZElevel, SIZEtrend : M/B
: IdV

1(=12)

2(=12)

1(=12)

2(=12)

-3 -2 -1 0 1 2 3 4 -3 -2 -1 0 1 2 3 4 -3 -2 -1 0 1 2 3 4 -3 -2 -1 0 1 2 3 4

q q q
5Y 8Y 11Y 15Y

5Y 8Y 11Y 15Y

q
5Y 8Y 11Y 15Y

qq

qq

5Y 8Y 11Y 15Y

Figure 2: Box-plots of parameters estimates corresponding to macroeconomic factors of 12 months forward default (two left) and other exit (two right) intensities over 35 windows. Each box-plot represents estimation interval length 5, 6, . . . , 15 years.

volatility-adjusted leverage; measured as distance-to-default (DTD) in a Merton-type model which are adjusted as in Duan et al. (2012), (ii) liquidity; measured as a ratio of cash and short term investment to total assets, (iii) profitability; measured as a ratio of net income to total assets, (iv) relative size; measured as the logarithm of the ratio of market capitalization to the economy's average market capitalization, (v) market-to-book asset ratio, and (vi) idiosyncratic volatility. The first four characteristics are transformed into level and trend. The level is computed as the one-year average of the measure. The trend is computed as the current value of the measure minus the one-year average of the measure. By doing this transformation, two firms with the same current value for all

20

-6 -4 -2 0 2 4

5(=12)

qq

q

q q

qqq

q
q qq qq q qq q qq qqq
q

5Y 8Y 11Y 15Y
5(=12)

-6 -4 -2 0 2 4

6(=12)
q q
q q
q q
q q q qq
q qqq
q qq
5Y 8Y 11Y 15Y
6(=12)

-6 -4 -2 0 2 4

7(=12)

qqq

q

qq q

5Y 8Y 11Y 15Y
7(=12)

-6 -4 -2 0 2 4

8(=12)
q qqqq q q q
5Y 8Y 11Y 15Y
8(=12)

q q
qq

q

qqq q

q qq
qq

q qq q

qqqq
q qq q q

-6 -4 -2 0 2 4

-6 -4 -2 0 2 4

-6 -4 -2 0 2 4

-6 -4 -2 0 2 4

5Y 8Y 11Y 15Y

5Y 8Y 11Y 15Y

5Y 8Y 11Y 15Y

5Y 8Y 11Y 15Y

Figure 3: Box-plots of parameters estimates corresponding to Xj, j = 5, 6, 7, 8, of 12 months forward default (upper) and other exit (bottom) intensities over 35 windows. Each box-plot represents estimation interval length 5, 6, . . . , 15 years.

measures may have different PD. The state variables are summarized in Table 2.
4.1 Robustness on Estimation Window Length
The estimates of parameters that determine the forward intensities,  = {, }, depend on the length of estimation intervals. Figure 2 displays box-plots for the estimates of j( ) and j( ), where horizon  = 12 months, that correspond to macroeconomic factors: (i) index return and (ii) interest rate. We explore the estimates for the different horizons, but we do not document them here, whereas the information extracted are similar to one year outlook. The estimate of one-year simple return of S&P500 index is sensitive to the length of estimation interval, particularly for default scheme. It contrasts to

21

parameter estimate of 3-months U.S. Treasury bill rate that exhibits more robustness to the calibration windows length. This finding is in line with Duan et al. (2012) that showed the standard error of 1( ) is much larger than 2( )'s as well as that for 1( ) and 2( ).
Figure 3 exhibits the estimates corresponding to liquidity and profitability, both for level and trend measures, that are sensitive to the length of estimation windows. The sensitivity increases for default events rather than other exits. The idiosyncratic volatility estimate has similar behavior as showed in Figure (1). This is also true for the intercept estimates, that empirically have negative values. This confirms the Duan et al. (2012)'s empirical result that showed the standard errors of aforestated covariates are much larger than those of the remaining covariates. Thus, DTD, company size, and market-to-book ratio are robust to the estimation interval length.
4.2 Set-up for Homogeneity Intervals Test
The complete localising analysis includes two main steps: (a) design the set-up of the procedure and (b) data-driven search for the longest interval of homogeneity, the interval where a local constant parametric form describes the data well. At the first step, the delisting process is approximated by forward intensity approach with forward intensities as in (21) and (22). Next, select the interval candidates {I0, . . . , I5} = {60, 72, 96, 120, 144, 180} months, equivalently {5, 6, 8, 10, 12, 15} years, and compute test statistic Tk, in (39). The likelihood ratio LIk, (Ik, Ik) in (43) should be divided by an effective sample size of the corresponding interval because different firms have different life time observation. Therefore, the corresponding term (k/K) can be discarded. The critical values (40) are computed using Monte Carlo simulation. The parametric risk bound (42) is computed based on the true parameter generated from the average of es-
22

timates over 35 moving windows of the longest interval (15 years). Our set up considers generating true parameter based on the grand average of estimates over interval Ik and over rolling windows at each interval. Though the risk bound is fulfilled under the simulated interval of homogeneity, it is parameter-invariant. The critical values depend on hyperparameters r and  that are counterparts of the usual significant level. We set two model risk levels r = {0.5, 1} to represent modest and conservative, respectively, and two significance levels  = {0.5, 0.75}. The conservative risk level lead to, theoretically, a selection of longer intervals of homogeneity that yield more precise estimates, but increase the modelling bias. Fortunately, this effect can be controlled according to (37).
At the second step, the smallest tested interval I0 is initially assumed to be homogeneous. If Ik-1 is negatively tested on the presence of a change point, one continues with Ik by employing (39) to detect a potential change point in Ik. If no change point is found, then Ik is accepted as time-homogeneous. We sequentially repeat these tests until we find a change point or exhaust all interval. The latest (longest) interval accepted as time-homogeneous is used for estimation. The whole search and estimation in second step is repeated at different end time point T without reiterating the first step because the critical values zk, , for fixed  , depend only on the approximating parametric model and interval length mk = |Ik|, not on the time point T .
Figure 4 and 5 depict the estimated length of interval of homogeneity over recent 35 windows at each horizon  = {1, 3, 6, 12, 24, 36} months. The 35-th window has time-end T at December 2011. The preceding windows are one month back-shifting. Therefore the timeend T of the {1-st, 2-nd, . . . , 35-th} windows are at {28.02.2009, 31.03.2009, . . . , 31.12.2011}. As expected, the selected intervals of homogeneity are shorter in the modest risk level (r = 0.5) than in the conservative risk level (r = 1). Particularly for conservative case, the test procedure apparently selects longer estimation period at recent windows. This result may happens because we employ the most recent state variable data in Monte Carlo
23

=1
q

=3
qqqqqqqqqqqqqqqqqqqqqqq

8 10 12 15

Length in Years

8 10 12 15

Length in Years

5

8 10 12 15

Length in Years

q qqqqqqqqqqqqqq

qqqqqqqqq

qqqqqqqqqqqqqqqqq

qq

qqqqqq

0 5 10 15 20 25 30 35
window
=6

qqqqq

q

qqqqqqqqqqqqqqqqqqqqqqqq

q

qqqqqqqqqqqqqqqqqqq

qqq

q qqqq

0 5 10 15 20 25 30 35
window
 = 24

qq

qqqqqqqqqq

q

Length in Years

5

8 10 12 15

5

q

qqqqqqqq

qqqqqqqqqqqqq

qqqq

qqqqqqqq

0 5 10 15 20 25 30 35
window
 = 12

qqqqqq

qqqq

qqqqqqqqqqqqq

qq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqq

0 5 10 15 20 25 30 35
window
 = 36

q qqqqqqqqqqqqqq

5

8 10 12 15

Length in Years

8 10 12 15

Length in Years

5

qqq

qqqqqqqqqqqqq

q qqqqqqqqqq

qq

q

qq q

0 5 10 15 20 25 30 35
window

5

q

qqqqqqqqqqqqqqqqq qqqqqq

qqqqqqq

0 5 10 15 20 25 30 35
window

Figure 4: Estimated length of interval of homogeneity (in years) for 35 last windows in case of a modest (r = 0.5, blue) and conservative (r = 1, red) modelling risk level, with K = 5 and  = 0.50.

simulation to obtain the critical values. This leads to select longer interval such that we can employ more observations to obtain the consistent estimators from the overlapped likelihood.

24

8 10 12 15

Length in Years

5

=1

q

qqqq

qq

qqqqqqq

qqqqqqqq

q qqqqq

qq

qqqq q
qqqq

0 5 10 15 20 25 30 35
window
=6

qqqqq

q

qqqqqqqqqqqqqqqqqqqqqqqq

qqqqq

qq

qqqqq

q qqqq

0 5 10 15 20 25 30 35
window
 = 24

qq

qqqqqqqqqqqq

Length in Years

Length in Years

5

8 10 12 15

5

8 10 12 15

qq

=3
qqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqq

0 5 10 15 20 25 30 35
window
 = 12

qqqqqq

qqqq

qqqqqqqqqqqqq

qqqqqqqqqqqqqqqqqq qqq

q qqqq

0 5 10 15 20 25 30 35
window
 = 36

qqqq

q

qqqqqqqqqqqqqq

8 10 12 15

Length in Years

5

8 10 12 15

Length in Years

8 10 12 15

Length in Years

5

qqqqqqqqqqqqqqqqqqqq

q qqqq

0 5 10 15 20 25 30 35
window

5

qqqqqqqqqqqqqqqqqqq qqqqqq

qqqqqqq

0 5 10 15 20 25 30 35
window

Figure 5: Estimated length of interval of homogeneity (in years) for 35 last windows in case of a modest (r = 0.5, blue) and conservative (r = 1, red) modelling risk level, with K = 5 and  = 0.75.

4.3 Measures of Accuracy
Figure 6 shows accuracy ratio (AR) computed from cumulative accuracy profile (CAP) (Sobehart et al., 2001), also known as power curve, over windows for a fixed horizon. The CAP evaluates the performance of a model based on default risk ranking. The higher
25

AR 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

AR 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

qqq

qqqqq q

qqqqqqqqqqqqqqqqqqqq

qqqqqq

qq
q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q =1m =3m =6m  = 12 m  = 24 m  = 36 m
0 5 10 15 20 25 30 35 window

qqq

qqqqq qq

q

q

qqqq

qq q

qqqqqqqqqqqqqq

qq

q =1m =3m =6m  = 12 m  = 24 m  = 36 m
0 5 10 15 20 25 30 35 window

qq

q

qqqq

qq

qqqqqqq

qqqqqqqq

qqqqq

qq

qqqq

AR 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

AR 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

q =1m =3m =6m  = 12 m  = 24 m  = 36 m

q =1m =3m =6m  = 12 m  = 24 m  = 36 m

0 5 10 15 20 25 30 35 window

0 5 10 15 20 25 30 35 window

Figure 6: Accuracy ratios over windows. The first row: r = 0.5,  = 0.5 (left) and r = 0.5,  = 0.75 (right). The second row: r = 1,  = 0.5 (left) and r = 1,  = 0.75 (right).

PD implies the higher risk. The model discriminates well between healthy and distressed firms if the defaulting firms are assigned among the highest PD of all firms before they default. This leads to higher values of AR. The PD are taken to be non-overlapping. The one-year AR is based on PDs computed on, for example, 31.12.2001, 31.12.2002, . . . and firms that default within one year of those dates whereas the three-years AR is based on PDs computed on 31.12.2001, 31.12.2004, . . . and firms that default within three years of those dates.

26

Table 3: Accuracy-ratio-based performance comparison for horizon 1, 3, and 6 months.

window global

 =1 local

global

 =3 local

global

 =6 local

r = 0.5,  = 0.5 r = 0.5,  = 0.75 r = 1,  = 0.5 r = 1,  = 0.75 r = 0.5,  = 0.5 r = 0.5,  = 0.75 r = 1,  = 0.5 r = 1,  = 0.75 r = 0.5,  = 0.5 r = 0.5,  = 0.75 r = 1,  = 0.5 r = 1,  = 0.75

 1  2  3 4 5 6 7 8 9 10

11

12  13

14

15  16

17

18

19

20

21

22

23  24  25  26  27

28

29

30 31 32 33 34  35

   

                                  

  
             
       


 NOTE: The check mark ( ) denotes the corresponding approach results in higher AR whereas the star ( ) implies both the global and local FIA perform with the same accuracy. For one month horizon, the global FIA does better than the local FIA in 14 out of 35 windows, whereas our the local approach yields higher AR than those of the global approach for 7 windows. In the rest windows these two methods perform equally well. For three months horizon, the global FIA shows superiorty over our local adaptive method. Both approaches perform with the same accuracy for six months horizon prediction.

The local adaptive approach results in very high AR at the first, second, and third windows particularly for one month horizon (about 95%). The ARs drop significantly
27

Table 4: Accuracy-ratio-based performance comparison for horizon 12, 24, and 36 months.

window global

 = 12 local

global

 = 24 local

global

 = 36 local

r = 0.5,  = 0.5 r = 0.5,  = 0.75 r = 1,  = 0.5 r = 1,  = 0.75 r = 0.5,  = 0.5 r = 0.5,  = 0.75 r = 1,  = 0.5 r = 1,  = 0.75 r = 0.5,  = 0.5 r = 0.5,  = 0.75 r = 1,  = 0.5 r = 1,  = 0.75

 1  2  3 4 5  6  7 8 9 10  11  12  13  14  15  16  17  18  19  20  21  22   23   24    25    26    27    28   29    30    31   32   33   34   35

  
                
  
            

    
 
             
   
 



 NOTE: The check mark ( ) denotes the corresponding approach results in higher AR whereas the star ( ) implies both the global and local FIA perform with the same accuracy. For 12 months horizon, the local FIA outperforms the global FIA in 18 out of 35 windows, whereas the local FIA yields higher AR in the rest windows. The local adaptive method shows superiorty over the global FIA for 24 months horizon (in 24 out of 35 windows). For 36 months horizon prediction, the local FIA method performs much better than the benchmark in 26 out of 35 windows.

at certain windows as exhibited in Figure 6. The proposed approach is able to generate accurate predictions, about 90%, for one month horizon. When the prediction horizon is extended to three and six months, the AR is still above 86% and 83%, respectively.

28

The one year prediction horizon drops the AR to the 75% - 80% range. For conservative modelling risk level (r = 1) the accuracies are still above 60% for both two and three years horizon. We evaluate the performance of local adaptive approach againts the global FIA that employs all past observations. This comparison is summarized in Table 3 and 4. The global FIA performs better than the localising algorithm for short outlook: one and three months horizon. Both two methods perform equally well for six months horizon prediction. Our local adaptive technique outperforms the benchmark for one year or longer horizon. This finding shows the accuracy prediction for long horizon can be increased by localising the time varying forward intensities and safely approximating them with constant.
5 CONCLUSION
In this paper we extend the idea of adaptive pointwise estimation to forward intensities calibration for multiperiod corporate default prediction. The FIA itself has simplicity substantially from the fact that no state variable forecasting model is required. Our approach addresses the inhomogeneity of parameters over time by optimally selecting the sample period over which parameters are approximately constant. The sequential LPA procedure provides an interval of homogeneity, the interval where a local constant parametric form describes the data well, that is used for modelling and prediction.
Applying the proposed method to monthly data on 2000 U.S. public firms over a sample period from 1991 to 2011, we estimate default probabilities over various prediction horizons. The default prediction performance is evaluated against the global FIA that employs all past observations. We utilize accuracy ratio from CAP curve to evaluate the performance of models based on default risk ranking. For the six months prediction horizon, the local adaptive approach performs with the same accuracy as the benchmark.
29

We show empirical evidence of increase in default prediction power for the longer horizon (one to three years). The general framework of the FIA allows adjustments on forward intensities specification for future research. Our local adaptive method is data-driven and can be applied to those other specifications with different covariates, either macroeconomic or firm-specific drivers.
APPENDIX A: CUMULATIVE FORWARD DEFAULT INTENSITY

This part shows the relationship between forward intensities and its cumulative. Denote a differentiable Ft(s), the conditional cdf of D evaluated at t + s.

Ft(s) = 1 - exp {-t(s)s} Ft (s) = - exp {-t(s)s} {-t(s)s - t(s)}
= exp {-t(s)s} t(s)s + exp {-t(s)s} t(s).

Therefore

t(s)

=

Ft (s) 1 - Ft(s)

=

exp {-t(s)s} t(s) + exp {-t(s)s} t(s)s exp {-t(s)s}

= t(s) + t(s)s.

30

This shows (6) and consequently:

s
t(u) du =
0
=

ss
t(u)du + t(u)u du
00 ss
t(u)du + t(s)s - t(u)du
00

= t(s)s.

APPENDIX B: PARAMETRIC RISK BOUND

This part proves the parametric risk bound is finite. Define E(z) d=ef  : LK(K) - LK()  z , the parametric risk bound:

r
Rr () = E LK (K , )

=-

zrdP LK (K , ) > z

z0



=r

zr-1P LK (K , ) > z dz

0 

= r zr-1P LK(K, ) > z, K  E(z) dz

0 

+ r zr-1P LK(K, ) > z, K / E(z) dz

0 

 2r

zr-1e-zdz

0

<

REFERENCES
Bai, J. and Perron, P. (1998), "Estimating and Testing Linear Models with Multiple Structural Changes," Econometrica, 66 (1), 47­78.
Belomestny, D. and Spokoiny, V. (2007), "Spatial Aggregation of Local Likelihood Esti31

mates with Applications to Classification," The Annals of Statistics, 35 (5), 2287­ 2311.
Bharath, S. T. and Shumway, T. (2008), "Forecasting Default with the Merton Distance to Default Model," The Review of Financial Studies, 21 (3), 1339­1369.
Cai, Z., Fan, J., and Yao, Q. (2000), "Functional-Coefficient Regression Models for Nonlinear Time Series," J. Am. Stat. Assoc., 95 (451), 941­956.
Campbell, J. Y., Hilscher, J., and Szilagyi, J. (2008), "In Search of Distress Risk," Journal of Finance, 63 (6), 2899­2939.
Chava, S. and Jarrow, R. A. (2004), "Bankruptcy Prediction with Industry Effects," Review of Finance, 8 (4), 537­569.
Chen, S., H¨ardle, W., and Moro, R. (2011), "Modeling Default Risk with Support Vector Machines," Quantitative Finance, 11 (1), 135­154.
Chen, Y., Ha¨rdle, W., and Jeong, S.-O. (2008), "Nonparametric Risk Management with Generalized Hyperbolic Distribution," J. Am. Stat. Assoc., 103 (483), 910­923.
Chen, Y., Ha¨rdle, W., and Pigorsch, U. (2010), "Localized Realized Volatility Modeling," J. Am. Stat. Assoc., 105 (492), 1376­1393.
Chen, Y. and Niu, L. (2014), "Adaptive Dynamic Nelson-Siegel Term Structure Model with Applications," Journal of Econometrics, 180 (1), 98­115.
C´izek, P., Ha¨rdle, W., and Spokoiny, V. (2009), "Adaptive Pointwise Estimation in TimeInhomogeneous Conditional Heterocedasticity Models," The Econometrics Journal, 12 (2), 248­271.
Duan, J.-C., Sun, J., and Wang, T. (2012), "Multiperiod Corporate Default Prediction A Forward Intensity Approach," Journal of Econometrics, 170 (1), 191­209. 32

Duffie, D., Eckner, A., Horel, G., and Saita, L. (2009), "Frailty Correlated Default," Journal of Finance, 64 (5), 2089­2123.
Duffie, D., Saita, L., and Wang, K. (2007), "Multi-period Corporate Default Prediction with Stochastic Covariates," Journal of Financial Economics, 83 (3), 635­665.
Fan, J. and Zhang, W. (2008), "Statistical Methods with Varying Coefficient Models," Stat Interface, 1 (1), 179­195.
Giacomini, E., H¨ardle, W., and Spokoiny, V. (2009), "Inhomogeneous Dependence Modeling with Time-Varying Copulae," Journal of Business & Economic Statistics, 27 (2), 224­234.
Ha¨rdle, W., Lee, Y.-J., Scha¨fer, D., and Yeh, Y.-R. (2009), "Variable Selection and Oversampling in the Use of Smooth Support Vector Machines for Predicting the Default Risk of Companies," Journal of Forecasting, 28 (6), 512­534.
Ha¨rdle, W. and Prastyo, D. D. (2014), "Embedded Predictor Selection for Default Risk Calculation: A Southeast Asian Industry Study," in Handbook of Asian Finance: Financial Markets and Sovereign Wealth Funds, eds. Chuen, D. L. K. and Gregoriou, G. N., Academic Press, vol. 1, pp. 131­148.
Ha¨rdle, W. K., Prastyo, D. D., and Hafner, C. M. (2014), "Support Vector Machines with Evolutionary Model Selection for Default Prediction," in The Oxford Handbook of Applied Nonparametric and Semiparametric Econometrics and Statistics, eds. Racine, J. S., Su, L., and Ullah, A., Oxford University Press, pp. 346­373.
Katkovnik, V. and Spokoiny, V. (2008), "Spatially Adaptive Estimation via Fitted Local Likelihood Techniques," IEEE Transactions on Signal Processing, 56 (3), 873­886.
Mercurio, D. and Spokoiny, V. (2004), "Statistical Inference for Time-Inhomogeneous Volatility Models," The Annals of Statistics, 32 (2), 577­602. 33

Shumway, T. (2001), "Forecasting Bankruptcy More Accurately: A Simple Hazard Model," The Journal of Business, 74 (1), 101­124.
Sobehart, J., Keenan, S., and Stein, R. (2001), "Benchmarking Quantitative Default Risk Models: A Validation Methodology," Algo Research Quarterly, 4 (1), 57­72.
Spokoiny, V. (1998), "Estimation of A Function with Discontinuities via Local Polynomial Fit with An Adaptive Window Choice," The Annals of Statistics, 26 (4), 1356­1378.
-- (2009), "Multiscale Local Change Point Detection with Application to Value-at-Risk," The Annals of Statistics, 37 (3), 1405­1436.
34

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Principal Component Analysis in an Asymmetric Norm" by Ngoc Mai Tran, Maria Osipenko and Wolfgang Karl Härdle, January 2014.
002 "A Simultaneous Confidence Corridor for Varying Coefficient Regression with Sparse Functional Data" by Lijie Gu, Li Wang, Wolfgang Karl Härdle and Lijian Yang, January 2014.
003 "An Extended Single Index Model with Missing Response at Random" by Qihua Wang, Tao Zhang, Wolfgang Karl Härdle, January 2014.
004 "Structural Vector Autoregressive Analysis in a Data Rich Environment: A Survey" by Helmut Lütkepohl, January 2014.
005 "Functional stable limit theorems for efficient spectral covolatility estimators" by Randolf Altmeyer and Markus Bibinger, January 2014.
006 "A consistent two-factor model for pricing temperature derivatives" by Andreas Groll, Brenda López-Cabrera and Thilo Meyer-Brandis, January 2014.
007 "Confidence Bands for Impulse Responses: Bonferroni versus Wald" by Helmut Lütkepohl, Anna Staszewska-Bystrova and Peter Winker, January 2014.
008 "Simultaneous Confidence Corridors and Variable Selection for Generalized Additive Models" by Shuzhuan Zheng, Rong Liu, Lijian Yang and Wolfgang Karl Härdle, January 2014.
009 "Structural Vector Autoregressions: Checking Identifying Long-run Restrictions via Heteroskedasticity" by Helmut Lütkepohl and Anton Velinov, January 2014.
010 "Efficient Iterative Maximum Likelihood Estimation of HighParameterized Time Series Models" by Nikolaus Hautsch, Ostap Okhrin and Alexander Ristig, January 2014.
011 "Fiscal Devaluation in a Monetary Union" by Philipp Engler, Giovanni Ganelli, Juha Tervala and Simon Voigts, January 2014.
012 "Nonparametric Estimates for Conditional Quantiles of Time Series" by Jürgen Franke, Peter Mwita and Weining Wang, January 2014.
013 "Product Market Deregulation and Employment Outcomes: Evidence from the German Retail Sector" by Charlotte Senftleben-König, January 2014.
014 "Estimation procedures for exchangeable Marshall copulas with hydrological application" by Fabrizio Durante and Ostap Okhrin, January 2014.
015 "Ladislaus von Bortkiewicz - statistician, economist, and a European intellectual" by Wolfgang Karl Härdle and Annette B. Vogt, February 2014.
016 "An Application of Principal Component Analysis on Multivariate TimeStationary Spatio-Temporal Data" by Stephan Stahlschmidt, Wolfgang Karl Härdle and Helmut Thome, February 2014.
017 "The composition of government spending and the multiplier at the Zero Lower Bound" by Julien Albertini, Arthur Poirier and Jordan RoulleauPasdeloup, February 2014.
018 "Interacting Product and Labor Market Regulation and the Impact of Immigration on Native Wages" by Susanne Prantl and Alexandra SpitzOener, February 2014.
SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasrecahrcwhaws assupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
019 "Unemployment benefits extensions at the zero lower bound on nominal interest rate" by Julien Albertini and Arthur Poirier, February 2014.
020 "Modelling spatio-temporal variability of temperature" by Xiaofeng Cao, Ostap Okhrin, Martin Odening and Matthias Ritter, February 2014.
021 "Do Maternal Health Problems Influence Child's Worrying Status? Evidence from British Cohort Study" by Xianhua Dai, Wolfgang Karl Härdle and Keming Yu, February 2014.
022 "Nonparametric Test for a Constant Beta over a Fixed Time Interval" by Markus Reiß, Viktor Todorov and George Tauchen, February 2014.
023 "Inflation Expectations Spillovers between the United States and Euro Area" by Aleksei Netsunajev and Lars Winkelmann, March 2014.
024 "Peer Effects and Students' Self-Control" by Berno Buechel, Lydia Mechtenberg and Julia Petersen, April 2014.
025 "Is there a demand for multi-year crop insurance?" by Maria Osipenko, Zhiwei Shen and Martin Odening, April 2014.
026 "Credit Risk Calibration based on CDS Spreads" by Shih-Kang Chao, Wolfgang Karl Härdle and Hien Pham-Thu, May 2014.
027 "Stale Forward Guidance" by Gunda-Alexandra Detmers and Dieter Nautz, May 2014.
028 "Confidence Corridors for Multivariate Generalized Quantile Regression" by Shih-Kang Chao, Katharina Proksch, Holger Dette and Wolfgang Härdle, May 2014.
029 "Information Risk, Market Stress and Institutional Herding in Financial Markets: New Evidence Through the Lens of a Simulated Model" by Christopher Boortz, Stephanie Kremer, Simon Jurkatis and Dieter Nautz, May 2014.
030 "Forecasting Generalized Quantiles of Electricity Demand: A Functional Data Approach" by Brenda López Cabrera and Franziska Schulz, May 2014.
031 "Structural Vector Autoregressions with Smooth Transition in Variances ­ The Interaction Between U.S. Monetary Policy and the Stock Market" by Helmut Lütkepohl and Aleksei Netsunajev, June 2014.
032 "TEDAS - Tail Event Driven ASset Allocation" by Wolfgang Karl Härdle, Sergey Nasekin, David Lee Kuo Chuen and Phoon Kok Fai, June 2014.
033 "Discount Factor Shocks and Labor Market Dynamics" by Julien Albertini and Arthur Poirier, June 2014.
034 "Risky Linear Approximations" by Alexander Meyer-Gohde, July 2014 035 "Adaptive Order Flow Forecasting with Multiplicative Error Models" by
Wolfgang Karl Härdle, Andrija Mihoci and Christopher Hian-Ann Ting, July 2014 036 "Portfolio Decisions and Brain Reactions via the CEAD method" by Piotr Majer, Peter N.C. Mohr, Hauke R. Heekeren and Wolfgang K. Härdle, July 2014 037 "Common price and volatility jumps in noisy high-frequency data" by Markus Bibinger and Lars Winkelmann, July 2014 038 "Spatial Wage Inequality and Technological Change" by Charlotte Senftleben-König and Hanna Wielandt, August 2014 039 "The integration of credit default swap markets in the pre and postsubprime crisis in common stochastic trends" by Cathy Yi-Hsuan Chen, Wolfgang Karl Härdle, Hien Pham-Thu, August 2014
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2014
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
040 "Localising Forward Intensities for Multiperiod Corporate Default" by Dedy Dwi Prastyo and Wolfgang Karl Härdle, August 2014.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

