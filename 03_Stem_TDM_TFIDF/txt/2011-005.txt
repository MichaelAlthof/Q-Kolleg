BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2011-005
Local Quantile Regression
Wolfgang Karl H‰rdle* Vladimir Spokoiny**
Weining Wang*
* Humboldt-Universit‰t zu Berlin, Germany * Weierstrass Institute (WIAS) Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Local Quantile Regression 
Wolfgang Karl Ha®rdle , Vladimir Spokoiny, Weining Wang.ß
December 13, 2010
Abstract
Conditional quantile curves provide a comprehensive picture of a response contingent on explanatory variables. Quantile regression is a technique to estimate such curves. In a flexible modeling framework, a specific form of the quantile is not a priori fixed. Indeed, the majority of applications do not per se require specific functional forms. This motivates a local parametric rather than a global fixed model fitting approach. A nonparametric smoothing estimate of the conditional quantile curve requires to consider a balance between local curvature and variance. In this paper, we analyze a method based on a local model selection technique that provides an adaptive estimate. Theoretical properties on mimicking the oracle choice are offered and applications to stock market and weather analysis are presented.
Keywords: Conditional Quantiles; Semiparametric and Nonparametric Methods; Asymmetric Laplace Distribution; Exponential Risk Bounds; Adaptive Bandwidth Selection.
JEL classification: C00, C14, J01, J31
1 Introduction
Quantile regression is gradually developing into a comprehensive approach for the statistical analysis of linear and nonlinear response models. Since the rigorous treatment of linear quantile regression by Koenker and Bassett (1978), richer models have been introduced into the literature, among them are nonparametric, semi parametric and additive
The financial support from the Deutsche Forschungsgemeinschaft via SFB 649 "O® konomisches Risiko", Humboldt-Universita®t zu Berlin is gratefully acknowledged.
Professor at Humboldt-Universita®t zu Berlin and Director of CASE - Center for Applied Statistics and Economics, Humboldt-Universit®at zu Berlin, Spandauer Straﬂe 1, 10178 Berlin, Germany and National Central University, Graduate Institute of Statistics, No. 300, Jhongda Rd., Jhongli City, Taoyuan County 32001, Taiwan (R.O.C.). Email:haerdle@wiwi.hu-berlin.de
Professor at Weierstrass Institute for Applied Analysis and Stochastics (WIAS), Mohrenstr. 39, 10117 Berlin, Germany. Email:spokoiny@wias-berlin.de
ßResearch associate at Ladislaus von Bortkiewicz Chair, the Institute for Statistics and Econometrics of Humboldt-Universit®at zu Berlin, Spandauer Straﬂe 1, 10178 Berlin, Germany. Email:wangwein@cms.huberlin.de
1

0.060 0.080

-0.8 -0.2

0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
Figure 1: The bandwidth sequence (upper panel), plot of data (x variable scaled to [0, 1]) and the estimated 90% quantile curve (lower panel)
approaches. Quantile regression or conditional quantile estimation is a crucial element of analysis in many quantitative problems. In financial risk management, the proper definition of quantile based Value at Risk impacts asset pricing, portfolio hedging and investment evaluation, see Engle and Manganelli (2004), Cai and Wang (2008), Fitzenberger and Wilke (2006). In labor market analysis of wage distributions, education effects and earning inequalities are analysed via quantile regression. Other fields of applications of conditional quantile studies include conditional data analysis of children growth and ecology, where it accounts for the unequal variation of response variables, see James, Hastie and Sugar (2010). In applications, the predominantly used linear form of the calibrated models was mainly determined by practical and numerical reasonings. More flexible functional forms have been put forward in the literature in only rare cases, Kong, Linton and Xia (2010) and Yu and Jones (1997). The aim of this research is to propose an adaptive local quantile regression algorithm that is easy to implement and works for a wide class of applications. We describe in detail the adaptation technique that is based on local parametric models. The main conclusion is that the proposed algorithm is feasible and beneficial for quantile smoothing and helps in proposing alternatives to more restricted parametric models. Figure 1 presents an example of our results for analyzing the Lidar data set, see Ruppert, Wand and Carroll (2003). The presented quantile curve switches smoothness in the middle, and it is naturally reflected by the bandwidth sequence (upper panel) selected by the proposed technique. In the
2

presence of changing to sharper slope of the curve, the bandwidth gets smaller to attain better estimation.
This article is organized as follows. In Section 2, exponential risk bounds for conditional quantiles are established using the representation of quantiles as QMLEs of the asymmetric Laplace distribution. In Section 3, we introduce the adaptation scheme and the propagation condition to control the significance level and derive theoretical bounds for critical values. In Section 4, the "Small Modeling Bias" is studied. In Section 5, "Stabiliy" and "Oracle" properties are given. In Section 6, Monte Carlo simulations are conducted to illustrate the proposed methodology. In Section 7, we apply our method on checking tail dependency among portfolio stocks; and on estimation of quantile curves for temperature risk factors.

2 Parametric Risk Bounds

Consider the MLE for the asymmetric Laplace distribution (ALD), which plays a key role in quantile estimation. An important advantage of this QMLE representation is that it and the corresponding fitted likelihood admit a closed form.

A random variable has an ALD(µ, ,  ) if its pdf is:

f (u|µ, ,  ) =  (1 -  ) exp 

-

u-µ 

, - < u < 

where  (u) = u{ 1(u  0) - (1 -  )1(u < 0)} is a loss function. The parameter 0 <  < 1 is the level of the quantile, µ a location parameter and  > 0 a scale parameter, respectively. In the following, we restrict the analysis to the standardized situation µ = 0,  = 1 and abbreviate it as ALD( ). The median, for example, is related to

f

u|0, 1, 1

1 = exp

- |u|

,

- < u < .

24

2

the pdf of a symmetric Laplace distribution.

Theorem 2.1 Let Y1, ∑ ∑ ∑ , Yn be a sample modeled by Yi = +i with i.i.d. i  ALD( ), and  a location parameter. Let Y(1)  Y(2)  ∑ ∑ ∑  Y(n) denote the order statistics of {Yi}in=1. Then the QMLE  of  is
 = Y([n ]+1)

Proof. The log-likelihood L() is

n

L() = log  (1 -  ) exp {- (Yi - )}

i=1

=

n log{ (1 -  )} - 1 2

n

{|Yi - | + (2 - 1) (Yi - )}

i=1

nn

= n log{ (1 -  )} + (1 -  ) ( - Yi)1( > Yi) -  (Yi - )1(  Yi).

i=1 i=1

3

Now one computes the derivative w.r.t. . Suppose that Y(k)   < Y(k+1), k = 0, 1, ∑ ∑ ∑ , n, where Y(0) < Y(1) and Y(n+1) > Y(n) are any two real values, then
L () = -(1 -  )k +  (n - k) = -k +  n.
We have L () = 0, for k = [n ].

Theorem 2.2 For any  the log-likelihood ratio in quantile QMLE framework is:

L(~, ) = L(~) - L()

= n(~ - )(2 - 1)/2 + 1 n 2

|Yi - | - |Yi - ~| .

i=1

Proof. For any two points ~ and , the log-likelihood ratio is:

L(~, )

=

1 2

n
{|Yi - | + (2 - 1)(Yi - )} - {|Yi - ~| + (2 - 1)(Yi - ~)}

i=1

=

1 2

n
{|Yi - | - |Yi - ~| + (2 - 1)(~ - )}

i=1

=

2 n

-

1(~ -

)

+

1

22

n
(|Yi - | - |Yi - ~|).

i=1

(1)

Next, consider the Kullback-Leibler divergence K(∑, ∑) defined for two pdf's f (y, ), f (y,  )

with parameters ,  :

f (Y, )

K(,



)

=

E



log

f (Y,



. )

Theorem 2.3 Let f (y, ) be the density of Y =  +,   ALD( ). The Kullback-Leibler divergence K(,  ) is:

K(,  ) =

  -1

+  ( -  ) +

 1-

exp {(1 -  ) (

- )} ,

 -1 

+ (1 -  )(

- ) +

1- 

exp {- (

- )} ,

, <.

(2)

Proof. The theorem may be proved by simple trivial algebraic calculations.

Remark. If  is close to , then by Taylor expansion, one sees:

K(,  ) 

 (1 -  ) (

- )2

2

(3)

Define tail functions as: +(y) = -(y/ )-1 log{ -1P(i > y)}, y  0 -(y) = -{y/(1 -  )}-1 log{(1 -  )-1P(i < y)}, y < 0
4

(4)

If we make assumptions on the error distribution (in a QMLE framework), we write (.) for +/-(y). Introduce 0(.) for 0+/-(y) in the case of true unknown tail function,
Define the rate function as M(µ, , ) d=ef - log E (.), exp{µL(, )} which we abbreviate for fixed µ as M(, ). E (.), means taking expectation under the distribution (.) of rv. Yi.
Assume +(y) and -(y)  0 as y  +/ - , and +/-(y) is bounded from below by µ+/-(z) and recall the definition of A(z, ) d=ef { : M(, )  z} and µ+(z) > C log z/z or (µ-(z) < C log z/z).
The bound for the local constant estimator is described in

Theorem 2.4 Let {Yi}in=1 be defined as Yi =  + i, with i as i.i.d. random noise, and F-1( ) = 0.

Then we have

P(.),{L(~, ) > }  C exp(-)

and for any r > 0

E (.),{|L(~, )|r}  Rr,

where Rr is defined as the moment bound for the likelihood ratio process.

(5) (6)

Proof. See appendix.

Recall that when Yi s do not follow ALD, quantile estimation may be done in a QMLE framework as in Theorem 2.1. W.L.O.G., assume that  = 0.

n
~ = arg max L() = arg min - log{ (1 -  )} +  (Yi - ),
  i=1
The general case ( = 0)can be reduced to this one by a simple change of variables. (4) gives P(Y > y) = exp{-+(y)y/ } for y  0, and for y < 0, P(Y < y) = exp{--(y)y/(1 -  )}(1 -  ), showing that the tail function identifies the cdf of Y .
The case with +(y)  0+ > 0 {-(y)  0- < 0} corresponds to light tails, while +(y)  0 {-(y)  0} as |y|   means heavy tails of the distribution P(.).
For example, a random variable Y  Pareto(, x0), x0 > 0, with density function xm /y+1, and P(Y > y) = (xm/y) has the tail function behavior:
limy+(y) = limy - (y/ )-1( log x0 -  log y - log  ) = 0,
By contrast the light tail random variable Y  exp() with density function  exp{-y}1(y  0) :
limy+(y) = limy -  y-1(-y - log  ) =  > 0.
The interesting case of course is the one where +(y) is positive and monotonously decreases to 0 in y > 0, and -(y) is negative and monotonously increases to 0 as y  . Assume further that +(y){-(y)} is sufficiently regular and its first derivative + (y){- (y)} is uniformly continuous on R.

5

2.1 Local Polynomial Estimation

Let {Xi, Yi}in=1 be independent random variables, we use the following model for construction:

Yi = l(Xi) + i,

(7)

the  th quantile curve follows with P(i > 0) =  .

We concentrate on X  R1 in this paper. It is not difficult to extend to the d-dimensional
case X  Rd d > 1. For fixed   (0, 1) and u  R, assume that the conditional quantile function FY-|1x( ) = l(x) can be approximated by a polynomial, in the vicinity of u:

l(x)  0(u) + 1(u)(x - u) + 2(u)(x - u)2 + . . . + p(u)(x - u)p.

(8)

Then we have the corresponding QMLE :

~(u) = {~0(u), ~1(u), . . . , ~p(u)}

d=ef argmaxL(W, )

n

n

= argmax{log  (1 -  ) wi -  (Yi -  i)wi}

i=1 i=1

= argmin  (Yi -  i)wi,
i=1

(9)

where i = {1, (Xi - u), (Xi - u)2, . . . , (Xi - u)p} , wi = K{(Xi - u)/h} and W = (w1, . . . , wn) , for i in 1, 2, . . . , n.

We use Theorem 2.8 in Spokoiny (2011) to prove a result similar to (6) for local polynomial
approximations (8), consider the following definitions, (from now on abbreviate E0,(.) as E):

() d=ef L(W, ) - E L(W, )

(10)

(, ) d=ef () - ()

N(µ, , ) d=ef - log E exp{µ(, )}

M(µ, , ) d=ef - log E exp{µL(W, , )} = - log E exp[µ{L(W, ) - L(W, )}]

The two conditions involved are (ED) and (L):

1.(ED)() satisfying the condition (ED):

 > 0 and a symmetric positive matrix V 2 s.t. , 0 <   ,

log E exp{ ()}  02||V 2||2/2,

(11)

for some fixed 0  1 and   Rp+1.

Set V 2() =

n i=1

ii

pi(

)wi, with pi(.) as the density of Yi.

2. (L) For R > 0, there exists constant a = a(R) > 0 such that it holds on the set 0(R) = { : ||V ( - )||2  R}:

- E L(W, , )  a||V ( - )||2/2

(12)

6

Restrict to a local central point . Define the matrix V0 = V (). Assume that the density of Yis are uniformly bounded from below in 0(R), then we have (12). The detailed proof of condition (ED) and (L) is deferred to the appendix.

Given (ED) on 0(R) with the matrix V0 and  = N 1/2()/C(, R), where

C (,

R)

d=ef

sup
Rp

sup
0(R)

||V ()|| ||V ()|| .

We may apply Theorem 2.8 of Spokoiny (2011) to obtain:

Theorem 2.5 Let V0 = V () and R be so that   qR/µ for µ = a/(20q3). We have for any 0 < r < R :
P(||V (~ - )|| > r)  exp{-a2r2/(80q3) + Q0(p, q)} + exp{-g(R)}
with Q0(p, q) = pC(q) + q2 + log(q2 - q). Moreover, if R  µ/(0q), then for any z > 0 with 2z/a  R2, it holds

P{L(~, ) > z}  exp{-za/(0q3) - Q0(p, q)} + exp{-g(R)},

(13)

where g(R) is defined as:

g(R) d=ef inf sup{µR + C(µ, , )}  µM = inf g(R, , ) 

(14) (15)

with

C(µ, , ) = q-1M(qµ, , ) - (p + 1) log+{ -1µ||V ( - )||} - Q(p, q), where log{(x)1(x  1)}.

(16)

Q(p, q) = -0q 2/2 - log(2 + 2p) - pC(q) - log(1 - a-(p+1)),

(17)

C(q) = log q log{2(1 - q-1)},
q > 1, a, 0, p, are positive real constants.
To achieve a confidence set, we want that exp{-g(R)} in (13) is negligible for moderately large sample size or not so heavy tails. Typically, the tail function is assumed to hold +() > C log . We need again to bound the rate function from below by a monotonically increasing function.
Define ~i = Yi - i  and their distributions Pi(  A) = P{(Yi -  )  A} for any Borel set A on the real line. If Yi = i  + i is the true model, then Pi coincides with distribution of each i.For simplicity of presentation, +(y){-(y)} is assumed to be sufficient regular and its first derivative is uniform continuous in R. Therefore as in the local constant case, we have |yi+(y)|  [0, 1] and |yi+(y)| < 1. Define
M(, ) d=ef sup M(µ, , )
µ
7

Theorem 2.6 We can achieve, with y > 0, and µi = i+(y)/(2 )

M(, )  M(µ, , ) 

n

log{1+i

(-)+()/ }+

n

{ (2 - 2)(i

 - i 2

)+() }.

i=1 i=1

Proof. See appendix.
The moment bounds for the likelihood ratio process can be obtained by the following theorem:

Theorem 2.7 Define the positive loss function |L(~, )|r (r > 0). Then E (.),|L(~, )|r  Rr,
where Rr > 0 is a constant.

Proof. See appendix.

3 Propagation and Bounds for Critical Values

In section 2, we derived the parametric risk bounds for local constant and local poly-
nomial quantile estimates. These bounds provide us a fundament on which to carry out a local adaptive procedure. For fixed x, a sequence of ordered weights W (k) = (w1(k), w2(k), . . . , wn(k)) is defined wi(k) = Khk (x - Xi), where (h1 < h2 < . . . < hK). The weights of a local model determine a local estimation of l(x) as well as the derivatives ^l(i)(x) of l(x), see (9). Note that ^k(x) is a p + 1 dimensional vector, with ^l(i)(x) = i!^k,i(x) and i  1, . . . , p. ^k,i(x) is the ith component of ^k(x).
The adaptation algorithm is described by:

- Start with ^1(x) = ~1(x). - For k  2, ~k(x) is accepted and ^k(x) = ~k(x), if ~k-1(x) was accepted and
L{W ( ), ~ (x), ~k(x)}  z , = 1, . . . , k - 1,

(18)

where ^k(x) is the latest accepted estimate after the first k steps. Note that (18) is equivalent to say that ~k(x) is covered in all the confidence set of ~ (x) ( < k).

The implementation of the procedure requires to fix the set of critical values of z1, . . . , zk-1.
The proposed approach suggests to tune these parameter by the so called propagation condition. For every step k, we require that the estimate at step k ^k(x) is sufficiently close to the MLE in step k : ~k, in the sense that:

E (.),|L{W (k), ~k(x), ^k(x)}|r  Rr

(19)

 and r are two hyper-parameters. The role of  is similar to the significance level of a test, while r denotes the power of the loss function. A larger r and a smaller  would lead to an increase in critical values. Note that (19) relies on the artificial parametric model P(.), not the true model P0(.),(.). A choice of critical values zl(l  1, 2, . . . , k) can be based on the following steps:

8

- Consider first only z1 and let z2 = . . . = zK-1 = , leading to the estimates ^k(x) for k = 2, . . . , K.

- The value z1 is selected as the minimal one for which

sup


E

(.),

|L{W

(k),

~k

(x), Rr

^k

(z1

,

x)}|r





K

-

, 1

k

=

2, . . . , K.

- Set zk+1 = . . . = zK-1 =  and fix zk, which leads to the set of parameters z1, . . . , zk, , . . . ,  and the estimates ^m(z1, . . . , zk) for m = k + 1, . . . , K. Select zk s.t.

sup


E

(.),

|L{W

(k)

,

~m(x),

^m(z1 Rr

,

z2

,

.

.

.

,

zk

,

x)}|r



k ,
K -1

m = k + 1, . . . , K.

The theorem below assures the existence of critical values constructed via the above technique by presenting upper bounds for the critical values.
Assume that for some constants 0 < u0 < u < 1, it holds 0 < Vk-1Vk2-1Vk-1   u < 1, define hk = C1C2k, k = 1, . . . , K.
Theorem 3.1 Suppose that r > 0,  > 0. Under the assumptions of probability bounds in (13), there are a0, a1, a2, s.t. the propagation condition is fulfilled with the choice of zk = a0r log(-1) + a1r log(hK/hk) + a2 log(nhk).

Proof.

We need first we prove,

E (.),{|L(W (k), ~k(x), ^k(x))|r}  Rr,

(20)

E (.),{|L(W (k), ~l(x), ^k(x))|r}  (3||Vk/Vl||ak/alu-0 1[zl +Q0{p}] + u-0 1[zk + Q0{p}])r,

(21)

To prove (21), we apply the following:

Theorem Spokoiny (2011) Suppose (ED) and (L) on 0(R), where R  µ0/(0q) for µ0 = a/(20q). Then for any z > 0 on a random set of probability at least 1 - e-z it holds for all   0(R)
L(, )  -a V0( - ) 2/4 + µ-0 1{zk + Q0(p)}

Similarly, if

- E (.),L(W, , )  a1||V0( - )||2/2

then it holds on the same random set

(22)

L(, )  -3a1||V0( - )||2/4 - µ0-1{zk + Q0(p)},   0(R)

9

To prove (21), we have for u > 0,
|L(W (k), ~l, )|  max{L(W (k), ~l, ), -L(W (k), ~l, )}  max{L(W (k), ~l, ), -L(W (k), ~l, )} + uL(W (l), ~l, )  max{max{L(W (k), , ), -L(W (k), , )} + uL(W (l), , )}

 max{-ak||Vk( - )||2 + µ-0 1[zk + Q0(p)], 3ak||Vk( - )||2/4 + µ0-1[zk + Q0(p)]} -ual||Vl( - )||2/4 + uµ0-1[zl + Q0(p)]
Using u > 3||Vk/Vl||ak/al to guarantee |L(W k, ~l, )| < , the proof follows.
To prove (20), define {Bk : k^ = k - 1}, by Cauchy-Schwartz inequality,
k
E (.),[|L{W (k), ~k(x), ^k(x)}|r] = E (.),{|L(W (k), ~k(x), ~l-1(x))|2r}1/2P(Bl)1/2
l=1 k
 2(2r-1)+ [E (.), |L{W (k), ~k(x), }|2r + E (.), |{W (k), ~l(x), }|2r]P(Bl)1/2
l=1 k
 C{R2r}(ak||Vk/Vl||/al)2r exp{-C1zl/2}
l=1

4 "Small Modeling Bias" Condition

Now we extend the condition (20) to the situation when the parametric assumption is not precisely fulfilled but the deviation from it is small in a modeling bias sense. We measure the deviation via the Kullback Leibler divergence between the nonparametric measure and the assumed parametric measure:

n

(.),0(.)(W (k), ) =

K{P(Xi),0(.), P,(.)}1{wi(k) > 0},

i=1

where P(Xi),0(.),P,(.) correspond to the marginal distributions of Yi with respect to (.) and . Here we tacitely assume that a misspecified 0(.) will not induce a big deviation, see Section 6 for evidence on this assumption, so assume:

(.),0(.)(W (k), )  (.),(.)(W (k), )

We define a bound by the following "small modeling bias" (SMB) condition:

(.),0(.)(W (k), )  ,

(23)

and the oracle k d=ef arg maxk{(.),0(.)(W (k), )  }.

Lemma 1 Let P , P0, be two measures s.t. E log(dP/dP0)   < , For any random variable z, with E z < , we have E log(1 + z)   + E 0z.

10

Proof: f (x) = xy - x log x + x attains maximum at the point x = ey, thus f (x)  f (ey), and we have xy  x log x - x + ey. Let x = dP/dP0 and y = log(1 + z),

E 0dP/dP0 log(1 + z) = E log(1 + z)

 E 0(dP/dP0 log dP/dP0 - dP/dP0 + 1 + z)



dP E log dP0 + E 0z

  + E 0z

From the above lemma and (23), we can derive the from the propagation condition the propagation property:

E 0(.),(.) log{1 + |L(~k, ^k)|r/Rr}   + ,

(24)

when the SMB condition is fulfilled:

(.),0(.)(W (k), )  , k < k.

5 Stability and Oracle Property

Due to the "propagation" result (20), the accuracy of the sequential test is guaranteed when the SMB assumption is fulfilled. In addition, we also need to make sure that when our final estimated step k^ overshoots the oracle k (k^ > k), the estimate does not vary too much. The stability condition shows that in the case of overshooting k^ > k, the estimate is accurate enough in the sense that,
L(W (k), ~k, ^k^)1{k^ > k}  zk.
The "stability" property naturally follows from the setup of our test.

Combination of the "propagation" and "stability" statements implies the "oracle" property, under the SMB condition,

E log{1 + L(W (k), ~k(x), )r }   + 1 Rr

(25)

E log{1 + |L(W (k), ~k(x), ~k^(x))|r }   + 

(26)

Rr

.

Proof. (25) is a trivial consequence of (24) and "stability". We now prove (26).

E

0(.),(.)

log{1

+

|L{W (k), ~k(x), ~k^(x)}|r Rr

}

=

E

0(.),(.)

log{1

+

|L{W

(k),

~k (x), Rr

~k^(x)}|r

}1(k^



k)

+ E 0(.),(.)

log{1

+

|L{W (k), ~k(x), ~k^(x)}|r Rr

}1(k^

>

k)





+

E

0(.),(.)

|L{W

(k),

~k (x), Rr

~k^

(x)}|r

+ log(1 +

|L{W (k), ~k(x), ~k^(x)}|r 1(k^ Rr

>

k)

  +  + log(1 + zk/Rr)

11

Table 1: Critical Values with different r and 

 = 0.25,  = 0.5,  = 0.6,  = 0.25,  = 0.25,

r = 0.5 r = 0.5 r = 0.5 r = 0.75 r=1

6.123 4.616 3.203 9.127 12.75

2.333 1.578 0.679 3.288 4.280

0.987 0.357 0.025 1.031 1.224

3.678e-05 2.472e-05 0.006 0.126 1.095e-04

0.000 0.000 7.278e-05 5.675e-05 0.000

Table 2: Critical Values with Different 

 = 0.05  = 0.5  = 0.75  = 0.95

6.464 7.997 9.203 8.589

2.204 3.089 3.910 5.452

0.620 0.986 1.106 1.904

3.345e-05 0.300e-05 0.123 0.334

0.000 0.000 7.254e-05 1.203e-05

6 Monte Carlo Simulation

This section aims at illustrating the local quantile estimation technique at different levels  = 0.05, 0.5, 0.75, 0.95 and for different noise distributions. Tail functions we select from a) Laplace, b) normal and c) student t(3) distribution, the errors brought by misidentification of the noise distribution are also studied. The global bandwidth selection for quantile regression follows the proposal of Yu and Jones (1998), in which they consider a rule of thumb bandwidth based on the assumption that the quantiles are parallel. We also compare with the proposals of Cai and Xu (2008), where an approach based on a nonparametric version of the Akaike information criterion (AIC) is implemented. Global bandwidth selectors are compared with the localizing technique presented here.

6.1 Critical Values
The critical values are simulated via the "propagation condition" (19).
Table 1 shows the critical values with several choices of  and r with  = 0.75, m = 10000 Monte Carlo samples, and bandwidth sequence (8, 14, 19, 25, 30, 36, 41, 52, 63)0.001. Critical values decrease when  increases, and increase when r increases. The critical values for the last 3 bandwidths is actually equal to 0.
The bandwidth sequence in Table 2 displays the values of different critical values with different  ,  = 0.25, r = 0.5, m = 10000 Monte Carlo samples, bandwidth sequence (8, 14, 19, 25, 30, 36, 41, 52, 63)  0.001, and normal noise.Critical values are roughly of the same level with respect to different  .
Table 3 displays the critical values for three alternative bandwidth sequences, i.e. (8, 16, 25, 36, 49, 63, 79, 99)  0.001, (5, 8, 14, 19, 27, 36, 46, 58, 66)  0.001, (8, 14, 19, 25, 30, 36, 41, 52, 63)  0.001, with  = 0.25, r = 0.5, and  = 0.85. We see that different bandwidth sequence would return us different level of critical values. In applications, critical values are chosen therefore in a scenario based fashion.
The critical values in Table 4 are simulated under t-distribution with three degree of
12

Table 3: Critical Values with Different Bandwidth Sequences

hseq1 11.33 1.243 6.933e-05 0.000

0.000

hseq2 18.39 6.479 2.230

0.469

8.738e-05

hseq3 6.123 2.333 0.987

3.678e-05 0.000

freedom, normal distribution and ALD(0, 1,0.5). The critical values apparently increase when the distribution tails get fatter.

Table 4: Critical Values with Different Noise Distributions

N(0,1)

11.50 4.924 2.514 1.313 2.765e-05

ALD(0,1,0.5) 14.05 6.554 3.304 1.443 5.879e-05

t(3) 15.42 8.707 2.370 0.342 3.898e-05

In Table 5, critical values are shown in the same circumstances as in Table 4 except for changing to the local linear case. Since we introduce one more variable (trend) to estimate, critical values doubled or tripled compared to the local constant case. The behavior with respect to tail functions stays the same.

Table 5: Local Linear

N(0,1)

29.97 58.64 43.21 33.41 19.43 07.40

ALD(0,1,0.5) 45.28 74.51 66.43 50.42 31.42 13.50

t(3) 51.77 84.94 59.28 44.99 29.07 11.57

6.2 Comparison of Different Bandwidth Selection Techniques
We illustrate our proposal by considering x  [0, 1], and estimation of the conditional quantile function l (x) at point x = 0.5,  = 0.75. The sample with (n = 1000) are simulated under three scenarios:
 0 if x  [0, 0.333];  f [1](x) = 8 if x  (0.333, 0666];  -1 if x  (0.666, 1]
f [2](x) = 2x(1 + x)
f [3](x) = sin(k1x) + cos(k2x)1{x  (0.333, 0.666)} + sin(k2x)
The noise distributions considered are:
- Normal distribution with 0 mean and 0.03 as variance
13

- ALD(0,2,0.5) - t-distribution with 3 degree of freedom

Figure 2 presents pictures with different noise distributions and comparison of boxplots of difference between estimations and the true curves in the local constant case. Figure 3 and 4 show in the local linear case function estimations and the first derivatives as well. The adaptive method always outperforms methods with fixed bandwidth, especially in the presence of jump. Table 6 further confirms our conclusion, by errors using four methods are evaluated with 1000 samples.

Table 6: Comparison of Monte Carlo errors at x = 0.5 with 1000 samples

f [1](x) f [2](x) f [3](x)

Fixed bandw 0.654 0.206 0.137

Local Constant 0.172 0.008 0.021

Local linear 0.169 0.008 0.019

Fixed bandw (Cai) 0.378 0.245 0.123

Table 7 offers an evaluation of misidentification errors with critical values simulated from ALD distribution while data are simulated from other distributions. Table 7 gives L1 errors between ^l and l and lets us conclude that misspecification of tail loss would not contaminate our results significantly.

Table 7: Comparison of Error Misspecification

f [1](x) f [2](x) f [3](x)

Local Constant {N(0,1)} 0.252 0.070 0.009

Local Constant {t(3)} 0.220 0.016 0.021

Local linear {N(0,1)} 0.169 0.043 0.019

7 Application
In the study of financial products, it is very important to detect and understand tail dependence among underlyings such as stocks. In particular, the tail dependence structure represents the degree of dependence in the corner of the lower-left quadrant or upper-right quadrant of a bivariate distribution. Hauksson, Michel, Thomas, Ulrich and Gennady (2001) and Embrechts and Straumann (1999) provide good access to the literature on tail dependence and Value at Risk. With the adaptive quantile technique, we provide an alternative approach to study tail dependence.
Figure 5 shows the shape of conditional quantile curves from a bivariate normal random sample. The correlation is calibrated from real data as given in Figure 6, where X is standardized returns from stock "clpholdings" from Hong Kong Hangseng Index, and Y is returns from stock "cheung kong". Figure 6 and Figure 7 show the empirical conditional quantile curves actually deviate from the one calculated from normal distributions in Figure 5, which implies non normality The motivation of doing adaptive bandwidth selection is clear to see, from Figure 6 and Figure 7, the dependency structure change is
14

-2 0 2 4 6

0.015 0.025 0.035

-5 0 5 10

0 200 400 600 800 1000

12345

-2 0 2 4

0 200 400 600 800 1000
(a) Normal

12345
(b) Normal

-2 0 2 4 6

0.015 0.025 0.035

-10 0 5

0 200 400 600 800 1000

12345

-2 0 2 4 6

0 200 400 600 800 1000
(c) ALD(0,2,0.5)

1234
(d) ALD(0,2,0.5)

5

-2 0 2 4 6

0.015 0.025 0.035

0 200 400 600 800 1000

12345

-2 0 2 4 6

-5 0 5 15

0 200 400 600 800 1000
(e) t(3)

12345
(f) t(3)

Figure 2: The bandwidth sequence (upper left panel), the data with noise (blue) , the adaptive estimation of 0.75 quantile (red), the quantile smoother with fixed optimal bandwidth = 0.06 (yellow); boxplot of block residuals fixed bandwidth (upper right), adaptive bandwidth (lower right)

15

-60 -40 -20 0 20

0.030 0.045

0 200 400 600 800 1000

12345

-50 -30 -10 10

-40 0 40

0 200 400 600 800 1000
(a)

12345
(b)

Figure 3: The bandwidth sequence (upper left panel), the data with t(3) noise (blue), the adaptive estimation of 0.75 quantile (blue), the quantile smoother with fixed optimal bandwidth = 0.06 (yellow); The blocked residual (right)

-1 0 1 2 3

0.01 0.03 0.05

0 200 400 600 800 1000

12345

01234

-30 -10 10

0 200 400 600 800 1000
(a)

12345
(b)

Figure 4: The bandwidth sequence (upper panel), the data with ALD(0,2,0.5) noise (blue), the adaptive estimation of 0.85 quantile (blue), the quantile smoother with fixed optimal bandwidth = 0.06 (yellow)

16

y -4 -2 0 2

0.0 0.2 0.4 0.6 0.8 1.0
x
Figure 5: Plot of quantile curve for two normal random variables with  = 0.3204,  = 0.75

more obvious compared with the fixed bandwidth curve. Moreover, the flexible adaptive curve is not a consequence of overfitting since it still mostly lies in the confidence bands produced by fixed bandwidth estimation, see Ha®rdle and Song (2010).
Figure 8 shows the first derivative curve for above example. The curve gets more volatile while x increases until a drastically change, then it turns flat.
We measure deviation from normality by accumulated L1 distance and examine different combination of stocks from Hong Kong Hangseng Index. The results is summarized in Table 8.

Table 8: Summary of Combination of Stocks

New world devo Sino land Swire pacific A

Chalco 0.252 0.070 0.009

Cosco pacific 0.220 0.016 0.021

Bank of China 0.169 0.043 0.019

Another application of quantile function estimation is in temperature data analysis, which is of key interest to price temperature derivatives. Quantile regression can provide a more flexible and complete approach to understand the temperature risk drivers.

Denote daily temperature as T  (t, j), with t = 1, ∑ ∑ ∑ ,  = 365 days, j = 0, ∑ ∑ ∑ , J years. The time series decomposition for Tt,j is given as:

Xt,j = Tt,j - t

L

Xt,j =

lXt-l,j + tt,j

l=1

t,j  N(0, 1),

L

^t,j = X365j+t -

^lX365j+t-l

l=1

(27) (28)

17

q
0.85 1.00 1.15

$ q[ ]
0.07

0.04

0 40 80

0.0 0.4 0.8

q
0.95 1.05

Q 1.01 1.03 1.05

0.0 0.4 0.8

0.85 1.00 1.15

Figure 6: The bandwidth sequence with smoothed bandwidth curve(upper left panel); Scatter plot of stock returns (upper right panel) , the adaptive estimation of 0.75 quantile (red), the quantile smoother with fixed optimal bandwidth = 0.06 (cyan); fixed bandwidth curve (cyan), adaptive bandwidth curve (red dashed), confidence band (magenta) (lower left panel); adaptive bandwidth with normal scale (lower right panel)

18

q
0.85 1.00 1.15

$ q[ ]
0.04

0.01

0 40 80

0.0 0.4 0.8

q
0.95 1.05

Q 0.0 1.0 2.0 3.0

0.0 0.4 0.8

0.85 1.00 1.15

Figure 7: The bandwidth sequence with smoothed bandwidth curve (upper left panel); Scatter plot of stock returns (upper right panel), the adaptive estimation of 0.75 quantile (red), the quantile smoother with fixed optimal bandwidth = 0.06 (cyan); fixed bandwidth curve (cyan), adaptive bandwidth curve (red dashed), confidence band (magenta) (lower left panel); adaptive bandwidth with normal scale (lower right panel)

19

adapt.param.tr -2.0 -1.0 0.0

0 20 40 60 80 100 Index
Figure 8: The adaptive trend curve (black), smoothed adaptive curve (red).  = 0.75
where Tt,j is the temperature at day t in year j, t denotes the seasonality effect and t the seasonal volatility.
We are interested specifically in the stochastic risk drivers t,j, Figure 9 presents a time series plot of ^t,j/^t, and the estimated 90% quantile function. By zooming in the curve, we observe a very interesting phenomena: an increase of trend of the standardized residual over years, which implies the existence of a potentially non stationarity trend possibly caused by global warming.
To further understand the risk factors, we analyze the quantile functions of ^2t,j over 12 years, and average the over every 4 years for comparison, see Figure 10 and Figure 11. The differences between Berlin and Kaoshiung are easy to see, the variance function peaks from Jan-Feb , while for Berlin the peaks come more in summer. Moreover, there is a tendency for Kaoshiung to be more volatile while this phenomenon does not appear in Berlin.
Table 9 summarizes statistics from the normality test of standardized residuals from three methods in Kaoshiung. The first method is to estimate ^ via median curve, the second method is to estimate ^ by {l,0.75 - l,0.25}/1.34 (1.34 is the inter quartile range of a standard normal distribution), the third method is to get the conditional mean. The fixed mean (fourth method) and adaptive bandwidth are compared.
A general fact from Table 9 and 10 is that Berlin has more normal residuals than Kaoshiung. Our method three mean regression is always better in getting more normal residuals, and method two from adaptive quantile is compatible with method three. It means quantiles at higher or lower levels are better to explain the extreme happened in volatility function. The fix bandwidth as usual performs poorly. Therefore we conclude that our adaptive technique is useful in modeling temperature residuals.
20

InitValues$hseq[index]
0.03 0.08

logratio
-4 0 4

l_tau
1.0 1.3

0 20 40 60 80 100
1:length(index)
0.0 0.2 0.4 0.6 0.8 1.0
rangesca
0 20 40 60 80 100
Figure 9: Plot of quantile for standardized weather residuals over 51 years at Berlin, 90% quantile .
0 100 200 300
Figure 10: Estimated 90% quantile of variance functions, Berlin, average over 1995-1998, 1999 - 2002 (red), 2003 - 2006 (green)
21

cbind(aaav, bbbv, cccv)[, 1:3] 10 20 30 40

10 15

cbind(aaav, bbbv, cccv)[, 1:3]

5

0 100 200 300
Figure 11: Estimated 90% quantile of variance functions, Kaoshiung, average over 1995 - 1998, 1999 - 2002 (red), 2003 - 2006 (green)
Table 9: P-values of Normality Tests:Berlin AD JB KS
1 0.000 0.010 0.060 2 0.062 0.000 0.020 3 0.054 0.487 0.171 4 0.009 0.000 0.002
8 Appendix
8.1 The Local Constant Case
The tail function behavior implies that {y+(y)}  [0, 1] ({y-(y)}  [-1, 0]) and hence: |y+ (y)| = {y+(y)} - +(y) < 1.
Let m() d=ef E (.), (|Y - |), q() d=ef P(Y  ) - P(Y > )
q(1)() d=ef (1 -  )P(Y  ) -  P(Y  ) and observe m () d=ef (1 -  )P(Y < ) -  P(Y  ). Next, define (y, , 0) as the likelihood ratio function for one observation,
(y, , 0) = (2 - 1) - {|Y - | - |Y |}
22

p
02

$ q[ ]
0.03 0.06 0.09

-4

0 40 80

0.0 0.4 0.8

p
02

Q -0.4 0.0 0.4

-4

0.0 0.4 0.8

-4 0 2

Figure 12: The bandwidth sequence with smoothed bandwidth curve (upper left panel); Scatter plot of temperature residuals with (upper right panel) , the adaptive estimation of 0.5 quantile (red) , the quantile smoother with fixed optimal bandwidth = 0.06 (cyan); fixed bandwidth curve (cyan), adaptive bandwidth curve (red dashed), confidence band (magenta) (lower left panel); adaptive bandwidth with normal scale (lower right panel)

23

Table 10: P-values of Normality Tests:Kaoshiung
AD JB KS 1 0.000 0.000 0.000 2 1.03e-05 0.077 0.043 3 2.37e-06 0.742 0.674 4 0.000 0.021 0.019

It is also clear that |q()|  1. For   0, it holds
(y, , 0) d=ef  (y, , 0) = 2, y  (0, ), y 0, otherwise,
and (y, , 0) = 2  - 2 for y < 0. Therefore, integration by parts yields
E (.), exp{µ (Y, , 0)} = - exp{µ (y, , 0)} dP (Y > y)

= exp{µ(2  - 2)} + 2µ (y, , 0) exp{µ (y, , 0)}P (Y > y) dy
0
= exp{µ(2  - 2)} + µ  exp{µ(2  - 2 + 2y)} exp{- 1 y+(y)} dy 0

= exp{µ(2  - 2)} + 2µ exp{µ(2  - 2)} exp[(y/ ){+() - +(y)}] dy,
0
where we fix µ() = +()/{2 }. Monotonicity of +(y) implies
E (.), exp{µ() (Y1, , 0)}  exp{µ(2  - 2)}(1 + 2µ ) = exp{+()(2  - 2)/2 }(1 + +()).

Therefore, for  > 0,

M(, 0) = - log E (.), exp{µ()

(Y, , 0)}  - log{1++()}+( -2 + 2 )+(). 2

(29)

For  < 0, take µ = -()/2(1 -  ) the monotonicity of -(y) implies

M(, 0)

=

- log E (.),

exp{µ()

(Y, , 0)}



- log{1

+

-()} +

{ (1

 -

}-() )

(29) states the identifiability of the function, now we prove the other condition to achieve the risk bound in Golubev and Spokoiny (2009).

Define

h(, ; ) d=ef log E (.), exp{2  , 0() }  v()

where

v() d=ef E 0() 0().

24

The smoothness condition is shown as:

To prove (30), define,

h(, ; )  2022,  < Ø, 0  1.

0() d=ef E (.),(-|Y - | + |Y |) - (-|Y - | + |Y |),

(30)

Then, for  > 0,

0() = 1(Y  ) - 1(Y > ) - q()

E (.), |0()|2 = 1 - q2()





Var 0 = Var 0(u)du   E (.),|0(u)|2du =  {1 - q2(u)}du

00

0

and  -2 Var 0() = -1 {1 - q2(u)}du  0,    0
because q()  1. Therefore,

1() = 1()/ = +()0() + + ()0()/.

The conditions |0()|  1, |0()/|  1, |+ ()|  1, +()  0 and Var{0()/  0 as   , easily imply h(, ; )  2022 for some fixed  < Ø, 0  1, thus the smoothness condition (30) is satisfied.

Moreover, if E (.),|Y1| <  for some  > 0, which leads to bound for the loss u~ = |~-|:

Define

(, s)(.), d=ef E (.), sup exp[{µ()L(, ) + sM(, )}]


The conditions of Theorem 3.2 in Golubev and Spokoiny (2009) are therefore satisfied, so

we have

(, s)  C |(1 - )(1 - s)|-1/2

and we can have further,

E (.), exp{2n u~+(u~) - log{1 + u~+(u~)}

C (1 - )

with some fixed constant C , C provided that n exceeds some minimal sample size n0.
To prove (5), we assume that +() is bounded from below by µ+(z) > 0 in every set A(z, ) d=ef { : M(, )  z}. Then we have:

P{L(~, ) > z}  P{L(~, ) > z, ~  A(z, )} + P(~  A(z, ))  (, 0) exp{-µ+(z)z} + (, s) exp{-sz}

Set µ+(z)z = sz, then

P{L(~, ) > z}  2(, s) exp{-µ+(z)z}

25

For the moment bound, we have:
E (.), |L(~, )|r
 r zr-1P,(.){L(~, ) > z}dz
z0
 r zr-1P,(.){  (z), ~  A(z(z), )}dz + r zr-1P[~  A{z(z), }]dz
z0 z0
 r zr-1(, 0) exp[-µ+{z(z)}z]dz + r zr-1(, s) exp{-sz(z)}dz
z0 z0
= B1 + B2
when µ+(z)z = sz, z(z) > log(z), then we have B1 < , B2 < . So we conclude that exists 0 < r <  such that E (.),|L(~, )|r < .

8.2 Proof of risk bound

8.2.1 Proof of (ED)

We know that

n
L(W, ) = -  (Yi -  i)wi
i=1 n
= i[ - 1{(Yi -  i) < 0}]wi
i=1

Thus, we have,

n
E (.),{L(W, )} - L(W, ) = i(P {(Yi -  i) < 0} - 1{(Yi -  i) < 0})
i=1

Denote i() = P{(Yi -  i) < 0} - 1{(Yi -  i) < 0}.

() = E (.),{L(W, )} - L(W, )
n
= iwii()
i=1

If i() are identically distributed, 1{(Yi -  i) < 0} is a Bernoulli random variable, and P {(Yi -  i) < 0} = p. Suppose 0 <  <  < 1, and we have,

log E (.), exp{} = log[exp{(p - 1)}p + exp(p)(1 - p)]  p(1 - p)02/2,

26

note that 0 depends on  only. Therefore it holds for any   Rp and  > 0 with |i |  1 that,
n
log E (.), exp{ ()}  log E (.), exp{  wiii()}
i=1 n
 log E (.), exp{ wiii()}
i=1 n
 2( iwi)2{pi()}2{i()}0/2
i=1
 02||V ()||2

where {pi()}2  1/4 and

n
V 2() = 2(i )ii wi2.
i=0

Denote also V 2 = (1/4)

n i=1

i

i

wi2, and V ()  V

for

all .

Then the condition (ED)

is fulfilled with the matrix V and  = N 1/2() for N () defined as:

N -1/2() d=ef max sup  iwii i Rp ||V ||

8.2.2 The (L) Condition

We know that

 E (.),L(W, ) = - 

n

i[ - P{(Yi - 

i) < 0}]wi

i=1

and

2 E (.),L(W, ) = 2

n

ii pi(i )wi d=ef H2()

i=1

Recall that - E (.),L(W, ) = 0. Now we take Taylor expansion of - E (.),L(W, , ), we conclude that, there is o  [, ] such that

n

- E (.),L(W, , ) =

|i ( - )|2pi(i o)wi

i=1

= ( - ) H2(o)( - ).

Thus the condition (L) is fulfilled in H() - aV ()  0 p.d. for   0(R).

8.2.3 Bound for the rate function Next, for   , it is also clear that     .
(y, , ) = (2 - 1)(  -  ) - {|Y -  | - |Y -  |} 27

(y, , ) d=ef  (y, , ) = 2, y  ( ,  ), y 0, otherwise.

Therefore, define Ay = [y, ), integration by parts yields

E (.), exp{µ (Y, , )} = - exp{µ (y, , )} dP (Ay)

= exp{µ(2 - 2)(i  - i )} + µ i  (y, , ) exp{µ (y, , )}P (Ay) dy i 
= exp{µ(2 - 2)(i  - i )}

+2µ

i


exp[µ{(2

- 2)(i

 - i

) + 2Y

- 2i

}]P (Ay) dy

i 

= exp{µ(2 - 2)(i  - i )}

+2µ

i


exp[µ{(2

- 2)(i

 - i

) + 2Y

- 2i

}]

exp{-+(y)y/ } dy

i 

= exp{µ(2 - 2)(i  - i )} + 2µ (exp[µ{(2 - 2)(i  - i ) - 2i }])

i 
exp{y(-i+(y)/ + 2µ)} dy,
i 
= exp{µ(2 - 2)(i  - i )} + 2µ (exp[µ{(2 - 2)(i  - i ) - 2i }])

i 
exp{y(-i+(y)/ + 2i+()/2 )} dy,
i 

where we fix µ() = +i()/{2 }. Monotonicity of +(y) implies

E (.), exp{µ() (Y1, , 0)}  {1 + 2µ i ( - ) exp(-2µi )} exp{µ(2 - 2)(i  - i )}
= {1 + 2µ i ( - )} exp{µ(2 - 2)(i  - i )},

Therefore, for   ,

M(, ) 

n

log{1 + i ( - )+()/ } +

n { (2 - 2)(i  - i )+() }. 2

i=1 i=1

The case for    can be proved in the same fashion.

28

8.2.4 Moment bounds

E (.),|L(~, )|r1(|L(~, )|r > R) = - zrdP{L(~, )  z}
0 
= r zr-1P{L(~, )  z}dz
0 
= r zr-1P{L(~, )  z, ~  0(R)}dz
0 
+r zr-1P{L(~, )  z, ~  0(R)}dz
0 
 r zr-1 exp{-[{µ1z - Q0(p)}+]dz
0 
+r zr-1 exp{-g(R)}dz
0
 Rr
References
Cai, Z. and Wang, X. (2008). Nonparametric estimation of conditional VaR and expected shortfall, Journal of Econometrics 147: 120≠130.
Cai, Z. and Xu, X. (2008). Nonparametric quantile estimations for dynamic smooth coefficient models, Journal of the American Statistical Association 103(484): 1595≠ 1608.
Embrechts, P., M. A. and Straumann, D. (1999). Correlation and dependency in risk management: Properties and pitfalls, Risk Management: Value at Risk and Beyond, Cambridge University Press pp. 176≠223.
Engle, R. F. and Manganelli, S. (2004). CAViaR: Conditional autoregressive Value at Risk by regression quantiles, Journal of Business and Economic Statistics 22: 367≠381.
Fitzenberger, B. and Wilke, R. A. (2006). Using quantile regression for duration analysis, Modern Econometric Analysis 90(1): 105≠120.
Golubev, Y. and Spokoiny, V. (2009). Exponential bounds for minimum contrast estimators, Electronic Journal of Statistics, invited resubmission .
Ha®rdle, W. K. and Song, S. (2010). Confidence bands in quantile regression, Econometric Theory 26: 1180≠1200.
Hauksson, A. H., Michel, D., Thomas, D., Ulrich, M. and Gennady, S. (2001). Multivariate extremes, aggregation and risk estimation, Quantitative Finance 1: 79≠75.
James, G. M., Hastie, T. J. and Sugar, C. A. (2010). Principal component models for sparse functional data, Biometrika 87: 587≠602.
Koenker, R. and Bassett, G. (1978). Regression quantiles, Econometrika 46(1): 33≠50.
29

Kong, E., Linton, O. and Xia, Y. (2010). Uniform Bahadur representation for local polynomial estimates of M-regression and its application to the additive model, Econometric Theory 26: 159≠166.
Ruppert, D., Wand, M. and Carroll, R. (2003). Semiparametric Regression, Cambridge University Press.
Spokoiny, V. (2011). Penalized risk bounds in parametric estimation, Manuscript . Yu, K. and Jones, M. C. (1997). A comparison of local constant and local linear regression
quantile estimation, Computational Statistics and Data Analysis 25: 159≠166. Yu, K. and Jones, M. C. (1998). Local linear quantile regression, Journal of the American
Statistical Association 93: 228≠237.
30

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Localising temperature risk" by Wolfgang Karl H‰rdle, Brenda LÛpez Cabrera, Ostap Okhrin and Weining Wang, January 2011.
002 "A Confidence Corridor for Sparse Longitudinal Data Curves" by Shuzhuan Zheng, Lijian Yang and Wolfgang Karl H‰rdle, January 2011.
003 "Mean Volatility Regressions" by Lu Lin, Feng Li, Lixing Zhu and Wolfgang Karl H‰rdle, January 2011.
004 "A Confidence Corridor for Expectile Functions" by Esra Akdeniz Duran, Mengmeng Guo and Wolfgang Karl H‰rdle, January 2011.
005 "Local Quantile Regression" by Wolfgang Karl H‰rdle, Vladimir Spokoiny and Weining Wang, January 2011.
SFB 649, Ziegelstraﬂe 13a, D-10117 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

