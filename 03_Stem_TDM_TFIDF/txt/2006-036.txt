BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2006-036
Spatial aggregation of local likelihood estimates
with applications to classification
Denis Belomestny* Vladimir Spokoiny*
* Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Spatial aggregation of local likelihood estimates with applications to classification

Belomestny, Denis
Weierstrass-Institute, Mohrenstr. 39, 10117 Berlin, Germany
belomest@wias-berlin.de

Spokoiny, Vladimir
Weierstrass-Institute, Mohrenstr. 39, 10117 Berlin, Germany
spokoiny@wias-berlin.de

Abstract
This paper presents a new method for spatially adaptive local likelihood estimation which applies to a broad class of nonparametric models, including the Gaussian, Poisson and binary response models. The main idea of the method is given a sequence of local likelihood estimates ("weak" estimates), to construct a new aggregated estimate whose pointwise risk is of order of the smallest risk among all "weak" estimates. We also propose a new approach towards selecting the parameters of the procedure by providing the prescribed behavior of the resulting estimate in the simple parametric situation. We establish a number of important theoretical results concerning the optimality of the aggregated estimate. In particular, our "oracle" results claims that its risk is up to some logarithmic multiplier equal to the smallest risk for the given family of estimates. The performance of the procedure is illustrated by application to the classification problem. A numerical study demonstrates its nice performance in simulated and real life examples.
Keywords: adaptive weights, local likelihood, exponential family, classification

AMS 2000 Subject Classification: 62G05, Secondary: 62G07, 62G08, 62H30
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 Economic Risk.
1

2 stagewise aggregation
1 Introduction
This paper presents a new method of spatially adaptive nonparametric estimation based on the aggregation of a family of local likelihood estimates. As a main application of the method we consider the problem of building a classifier on the base of the given family of k-NN or kernel classifiers.
The local likelihood approach has been intensively discussed in recent years, see e.g. Hastie and Tibshirani (1987), Staniswalis (1989), Loader (1996). We refer to Fan, Farmen and Gijbels (1998) for a nice and detailed overview of local maximum likelihood approach and related literature. Similarly to the nonparametric smoothing in regression or density framework, an important issue for the local likelihood modeling is the choice of localization (smoothing) parameters. Different types of model selection techniques based on the asymptotic expansion of the local likelihood are mentioned in Fan, Farmen and Gijbels (1998) which include global as well as variable bandwidth selection. However, the finite sample performance of estimators based on bandwidth or model selection is often rather unstable, see e.g. Breiman (1996). This point is particulary critical for the local or pointwise model selection procedures like Lepski's method (Lepski, 1990). In spite of the nice theoretical properties, see Lepski, Mammen and Spokoiny (1997), Lepski and Spokoiny (1997) or Spokoiny (1998), the resulting estimates suffer from a high variability due to a pointwise model choice, especially for a large noise level. This suggests that in some cases, the attempt to identify the true model is not necessarily the right thing to do. One approach to reduce a variability in adaptive estimation is model mixing or aggregation. Catoni (2001) and Yang (2004) among others have suggested global aggregating procedures that achieve the minimal estimation risks over the family of given "weak" estimates. In the regression setup Juditsky and Nemirovski (2000) have developed aggregation procedures which have a risk within a multiple of the smallest risk in the class of all convex combinations of "weak" estimates plus log(n)/n . Tsybakov (2003) has discussed asymptotic minimax rates for the aggregation. The aggregation for density estimation has been studied by Li and Barron (1999) and more recently by Rigollet and Tsybakov (2005). To the best of our knowledge a pointwise aggregation has not yet been considered.

belomestny, d. and spokoiny, v.

3

Our approach is based on the idea of the spatial (pointwise) aggregation of a family of local likelihood estimates ("weak" estimates) (k) . The main idea is, given the sequence {(k)} to construct in a data driven way for every point x the "optimal" aggregated estimate (x) . "Optimality" means that this estimate satisfies some kind of "oracle" inequality, that is, its pointwise risk does not exceed the smallest pointwise risk among all "weak" estimates up to a logarithmic multiple.
Our algorithm can be roughly described as follows. Let {(k)(x)} , k = 1, . . . , K , be a sequence of "weak" local likelihood estimates at a point x ordered according to their variability which decreases with k . Starting with (1)(x) = (1)(x) , an aggregated estimate (k)(x) at any step 1 < k  K is constructed by mixing the previously constructed aggregated estimate (k-1)(x) with the current "weak" estimate (k)(x) :
(k)(x) = k(k)(x) + (1 - k)(k-1)(x),
and (K)(x) is taken as a final estimate. The mixing parameter k (which may depend on the point x ) is defined using a measure of statistical difference between (k-1)(x) and (k)(x) . In particular, k is equal to zero if (k-1)(x) lies outside the confidence interval around (k)(x) . In view of the sequential and pointwise nature of the algorithm, the suggested procedure is called Spatial Stagewise Aggregation (SSA). An important features of the procedure proposed are its simplicity and applicability to a variety of problems including Gaussian, binary, Poisson regression, density estimation, classification etc. The procedure does not require any splitting of the sample as many other aggregation procedures do, cf. Yang (2004). Besides that the theoretical properties of SSA can be rigorously studied. In particular, we establish precise nonasymptotic "oracle" results which are applicable under very mild conditions in a rather general set-up. We also show that the oracle property automatically implies spatial adaptivity of the proposed estimate.
Another important feature of the procedure is that it can be easily implemented and the problem of selecting the tuning parameters can be carefully addressed.
Our simulation study confirms a nice finite sample performance of the procedure for a broad class of different models and problems. We only show the results for the classification problem as the most interesting and difficult one. Some more examples for the

4 stagewise aggregation
univariate regression and density estimation can be found in our preprint Belomestny and Spokoiny (2005). Section 4 shows how the SSA procedure can be applied to aggregating kernel and k-NN classifiers in the classification problem. Although these two nonparametric classifiers are rather popular, the problem of selecting the smoothing parameter (the bandwidth for the kernel classifier or the number of neighbors for the k-NN method) has not been yet satisfactorily addressed. Again, the SSA-based classifier demonstrates the "oracle" quality in terms of the both pointwise and global misclassification errors. This application clearly shows one more important feature of the SSA method: it can be applied to an arbitrary design and arbitrary dimension of the design space. This is illustrated by simulated and real life classification examples in dimensions up to 10.
The procedure proposed in this paper is limited to aggregating the kernel type estimates which are based on the local constant approximation. The modern statistical literature usually considers the more general local linear (polynomial) approximation of the underlying function. However, for this paper we have decided by several reasons to restrict our attention to the local constant case. The most important one is that for the examples and applications we consider in this paper, the use of the local linear methods does not improve (and even degrade) the quality of estimation. Our experience strongly confirms that for the problems like classification, the local constant smoothing combined with the aggregation technique delivers a reasonable finite sample quality.
Our theoretical study is split into two big parts. Section 2 introduces the considered local parametric set-up and extends the parametric risk bounds to the local parametric and nonparametric situation under the so called "small modelling bias" condition. The main result (Corollary 2.6) claims that the parametric risk bounds continue to apply as long as this condition is fulfilled. One possible interpretation of our adaptive procedure is the search of the largest localizing scheme for which the `small modelling bias" condition still holds. Theoretical properties of the aggregation procedure are presented in Section 5. The main result states the "oracle" property of the SSA estimate: the risk of the aggregated estimate is within a log-multiple as small as the risk of the best "weak" estimate for the function at hand. The results are established in the precise nonasymptotic way for a rather general likelihood set-up under mild regularity conditions. Moreover, our ap-

belomestny, d. and spokoiny, v.

5

proach allows to link the parametric and nonparametric theory. In particular, we show that the proposed method delivers the root-n accuracy in the parametric situation. In the nonparametric case, the quality corresponds to the best parametric approximation. Both the theoretical study and the motivation of the procedure employ some exponential bounds for the likelihood which are given in Section 2.2. An important feature of our theoretical study is that the problem of selecting the tuning parameters is also discussed in details. We offer a new approach in which the parameters of the procedure are selected to provide the desirable performance of the method in the simple parametric situation. This is similar to the hypothesis problem approach when the critical values are selected using the performance of the test statistic under the simple null hypothesis, see Section 3.3.1 for a detailed explanation.

2 Local likelihood modeling
This section presents some results on local constant likelihood estimation. We begin by describing the model under consideration. Suppose we are given independent random data Z1, . . . , Zn of the form Zi = (Xi, Yi) . Here every Xi means a vector of "features" or explanatory variables which determines the distribution of the "observation" Yi . For simplicity we assume that the Xi 's are valued in the finite dimensional Euclidean space X = IRd and the Yi 's belong to IR . The vector Xi can be viewed as a location and Yi as the "observation at Xi ". Our model assumes that the distribution of each Yi is determined by a finite dimensional parameter  which may depend on the location Xi .
More precisely, let P = (P,     IRp) be a parametric family of distribution dominated by a measure P . By p(∑, ) we denote the corresponding density. We consider the regression-like model in which every "response" Yi is, conditionally on Xi = x , distributed with the density p(∑, f (x)) for some unknown function f (x) on X with values in  . The considered model can be written as
Yi  Pf(Xi).
The aim of the data-analysis is to infer on the "regression" function f (x) . For the related models see Fan and Zhang (1999) and Cai, Fan and Li (2000).

6 stagewise aggregation

In this paper we focus on the case when P is an exponential family. This means

that

the

density

functions

p(y, ) =

dP dP

(y)

are

of

the

form

p(y, ) = p(y)eyC()-B() .

Here C() and B() are some given nondecreasing functions on  and p(y) is some

nonnegative function on Y .

A natural parametrization for this family means the equality EY = yp(y, )P (dy) =  for all    . This condition is useful because the weighted average of observations

is a natural unbiased estimate of  . In what follows we assume that P also fulfills the

following regularity conditions:

(A1) P = (P,     IR) is an exponential family with a natural parametrization, and the functions B(∑) and C(∑) are continuously differentiable.
(A2)  is compact and convex and the Fisher information I() := E| log p(Y, )/|2 fulfills for some a  1
|I( )/I( )|1/2  a,  ,   .

We illustrate this set-up with two examples relevant to the applications we consider below. Some more examples can be found in Fan, Farmen and Gijbels (1998) and Polzehl and Spokoiny (2005).
Example 2.1. (Inhomogeneous Bernoulli (Binary Response) model) Let Zi = (Xi, Yi) with Xi  IRd and Yi being a Bernoulli r.v. with parameter f (Xi) , that is, P (Yi = 1 | Xi = x) = f (x) and P (Yi = 0 | Xi = x) = 1 - f (x) . Such models arise in many econometric applications and are widely used in classification and digital imaging.
Example 2.2. (Inhomogeneous Poisson model) Suppose that every Yi is valued in the set N of nonnegative integer numbers and P (Yi = k | Xi = x) = f k(x)e-f(x)/k! , that is, Yi follows a Poisson distribution with parameter  = f (x) . This model is commonly used in the queueing theory, it occurs in positron emission tomography and also serves as an approximation for the density model obtained by a binning procedure.
In the parametric setup with f (∑)   the distribution of every "observation" Yi coincides with P for some    and the parameter  can be well estimated using the

belomestny, d. and spokoiny, v.

7

parametric maximum likelihood method:
n
 = argmax log p(Yi, ).
 i=1
In the nonparametric framework, one usually applies the local likelihood approach which
is based on the assumption that the regression function f (∑) is constant only within some
neighborhood of every point x in the "feature" space X . This leads to the local model
concentrated in some neighborhood of the point x .

2.1 Localization

We use the localization by weights as a general method to describe a local model. Let,

for a fixed x , a nonnegative weight wi = wi(x)  1 be assigned to the observation Yi at

Xi , i = 1, . . . , n . The weights wi(x) determine a local model corresponding to the point

x in the sense that, when estimating the local parameter f (x) , every observation Yi

is taken with the weight wi(x) . This leads to the local (weighted) maximum likelihood

estimate

n
f (x) = argmax wi(x) log p(Yi, ) .
 i=1

(2.1)

We mention now two possible ways of choosing the weights wi(x) . Localization by

a bandwidth is defined by weights of the form wi(x) = Kloc(li) with li = (x, Xi)/h where h is a bandwidth, (x, Xi) is the Euclidean distance between x and the design

point Xi and Kloc is a location kernel. Localization by a window simply restricts the

model to a subset (window) U = U (x) of the design space which depends on x , that

is, wi(x) = 1(Xi  U (x)) . Observations Yi with Xi outside the region U (x) are not

used for estimating f (x) . This kind of localization arises e.g. in the classification with

k -nearest neighbors method or in the regression tree approach.

We do not assume any special structure for the weights wi(x) , that is, any configu-

ration of weights is allowed. We also denote W = W (x) = {w1(x), . . . , wn(x)} and
n
L(W, ) = wi(x) log p(Yi, ).
i=1
To keep the notation short, we do not show the dependence of the weights on x explicitly

in what follows.

8 stagewise aggregation

2.2 Local likelihood estimation for an exponential family model

If P = (P) is an exponential family with the natural parametrization, the local loglikelihood and the local maximum likelihood estimates admit a simple closed form rep-

resentation. For a given set of weights W = {w1, . . . , wn} with wi  [0, 1] , denote

n
N = wi,
i=1

n
S = wiYi .
i=1

Note that the both sums depend on the location x via the weights {wi} .

Lemma 2.1 (Polzehl and Spokoiny, 2005). It holds

where R =

n
L(W, ) = wi log p(Yi, ) = SC() - N B() + R
i=1

n i=1

wi

log

p(Yi)

.

Moreover,

n
 = S/N = wiYi
i=1

n
wi
i=1

and

L(W, , ) := L(W, ) - L(W, ) = N K(, ).

(2.2)

Now we present some exponential inequality for the "fitted log-likelihood" L(W, , ) which apply in the parametric situation f (∑)   for arbitrary weighting scheme and arbitrary sample size.

Theorem 2.2 (Polzehl and Spokoiny, 2005). Let W = {wi} be a localizing scheme such that maxi wi  1 . If f (Xi)   for all Xi with wi > 0 then for any z > 0
P (L(W, , ) > z) = P  N K(, ) > z  2e-z.

Remark 2.1. Condition A2 ensures that the Kullback-Leibler divergence K fulfills K( , )  I| - |2 for any point  in a neighborhood of  , where I is the maximum of the Fisher information over this neighborhood. Therefore, the result of Theorem 2.2 guarantees that | - |  CN -1/2 with a high probability. Theorem 2.2 can be used for constructing the confidence intervals for the parameter  .

belomestny, d. and spokoiny, v.

9

Theorem 2.3. If z satisfies 2e-z   , then

E = { : N K ,   z}

is an  -confidence set for the parameter  .

Theorem 2.2 claims that the estimation loss measured by K( , ) is with high probability bounded by z/N provided that z is sufficiently large. Similarly, one can establish a risk bound for a power loss function.

Theorem 2.4. Assume A1 and A2 and let Yi be i.i.d. from P . Then for any r > 0 E Lr(, )  N rE Kr(, )  rr .
where rr = 2r z0 zr-1e-zdz = 2r (r) . Moreover, for every  < 1 E exp L(, )  E exp N K(, )  2(1 - )-1.

Proof. By Theorem 2.2

E Lr(, )  -

zrdP (L(, ) > z)

z0

r

zr-1P (L(, ) > z)dz  2r

zr-1e-zdz

z0 z0

and the first assertion is fulfilled. The last assertion is proved similarly.

2.3 Risk of estimation in nonparametric situation. "Small modeling bias" condition
This section extends the bound of Theorem 2.2 to the nonparametric situation when the function f (∑) is not any longer constant even in a vicinity of the reference point x . We, however, suppose that the function f (∑) can be well approximated by a constant  at all points Xi with positive weights wi . To measure the quality of the approximation, define for every 

(W, ) =  , f (Xi) 1(wi > 0),
i

(2.3)

10 stagewise aggregation

where with

(y,

,



)

=

log

p(y,) p(y, )

(,



)

=

log

E e-2

(Y,,

)

=

log

E

p2(Y,  ) p2(Y, )

.

One can easily check that (,  )  I| -  |2 , where I = max [, ] I( ) .

Theorem 2.5. Let FW be a  -field generated by the r.v. Yi for which wi > 0 and let (W, )   . Then it holds for any random variable  measurable w.r.t. FW
Ef(∑)  eE2 1/2.

Proof. Define ZW () = exp - i Yi, , f (Xi) 1(wi > 0) . This value is nothing but the likelihood ratio of the measure P f(∑) w.r.t. P  upon restricting to the observations Yi for which wi > 0 . Then for any   FW , it holds Ef(∑) = EZW (). Independence of the Yi 's implies

log EZW2 () = =

log Ee-2 (Yi,,f(Xi))1(wi > 0)
i
 , f (Xi) 1(wi > 0)  .
i

The result now follows from the Cauchy-Schwartz inequality EZW ()  E2EZW2 () 1/2 .

This result implies that the bound for the risk of estimation Ef(∑)Lr(, )  N rEf(∑)Kr(, ) under the parametric hypothesis can be extended to the nonparametric situation provided that the value (W, ) is sufficiently small.

Corollary 2.6. For any r > 0 and any  < 1 , it holds

N rEf(∑) K(, ) r 

e(W,) r2r ,

N Ef(∑) K(, ) r 1/r



1 

log

1

2 -



+

(W,

)

+

2(r

-

1)+

.

Proof. The first bound follows directly from Theorems 2.4 and 2.5. The proof of the

second one utilizes the fact that for r > 0 the function h(x) = logr x + cr with

cr = e(r-1)+ is concave on (0, ) because

h

(x) =

r logr-2 x + cr (x + cr)2

r - 1 - log x + cr

0

belomestny, d. and spokoiny, v.

11

for x  0 . This implies with  = L(, )/2 by monotonicity of log and Jensen's inequality that Ef(∑)r  Ef(∑)h(e )  h(Ef(∑)e ) and hence,

Ef1/(∑r)r  log Ef(∑)e + cr

 log Ef(∑)e + (r - 1)+



1 2

log

e(W,) E  e2

+ (r - 1)+

and the assertion follows.

Corollary 2.6 presents two bounds for the risk of estimation in the nonparametric situation which extend the similar parametric bounds by Theorem 2.5. The risk bound in the parametric situation can be interpreted as the bound for the variance of the estimate  while the term (W, ) controls the bias of estimation, see the next section for more details. The both bounds formally apply whatever the "modeling bias" (W, ) is. However, the results are meaningful only if this bias is not too large. The first bound could be preferable for small values of (W, ) , however, the multiplicative factor e(W,)/2 makes this bound useless for large (W, ) . The advantage of the second bound is that the "modeling bias" enters in the additive form.
In the rest of this section we briefly comment on relations between the results of Section 2.3 and the usual rate results under smoothness conditions on the function f (∑) and the regularity conditions on the design X1, . . . , Xn . More precisely, we assume that the weights wi are supported on the ball of a radius h > 0 with the center at x and the function f (∑) is smooth within this ball in the sense that for  = f (x)

1/2 , f (x + t)  Lh, |t|  h.

(2.4)

In view of the inequality (,  )  I| -  |2 this condition is equivalent to the usual Lipschitz property. Obviously, (2.4) implies with N = i 1(wi > 0)
(W, )  L2h2N .

Combined with the result of Corollary 2.6 these bounds lead to the following rate results.
Theorem 2.7. Assume (A1) and (A2) and let 1/2 , f (x + t)  Lh for and all |t|  h . Select h = c(L2n)-1/(2+d) for some c > 0 and let the localizing scheme W be such that wi = 0 for all Xi with |Xi - x| > h , N := i wi  d1nhd and

12 stagewise aggregation

N := i 1(wi > 0)  d2nhd with some constants d1 < d2 . Then Ef(∑) N K ,  r/2  exp c2+dd2 rr 1/2.
Moreover, with c2 = crd/2 exp c2+dd2/2 d1-r/2 , it holds Ef(∑) n1/(2+d)K ,  r/2  c2Lrd/(2+d)rr1/2.
This corresponds to the classical accuracy of nonparametric estimation for the Lipschitz functions, cf. Fan, Farmen and Gijbels (1998).

3 Description of the method

We start by describing the considered set-up. Let a point of interest x be fixed and the target of estimation is the value f (x) of the regression function at x . The local parametric approach described in Section 2 and based on the local constant approximation of the regression function in a vicinity of the point x strongly relies on the choice of the local neighborhood, or more generally, of the set of weights (wi) . The problem of selecting such weights and constructing an adaptive (data-driven) estimate is one of the main issues for practical applications and we focus on this problem in this section.

3.1 Local adaptive estimation. General setup

For a fixed x , we assume to be given an ordered set of localizing schemes W (k) = (wi(k)) for k = 1, ..., K . The ordering condition means that wi(k)  wi(k ) for all i and all k > k , that is, the degree of locality given by Wi(k) is weakened as k grows. See Section 3.3 for some examples. For the popular example of kernel weights wi(k) = K (Xi - x)/hk , this condition means that the bandwidth hk grows with k . Let also {(k), k = 1, ..., K} be
the corresponding set of local likelihood estimates for  = f (x) :

n

(k)(x) = argmax L(W, ) = wi(k)Yi



i=1

n
wi(k).
i=1

Due to Theorem 2.2 the value 1/Nk can be used to measure the variability of the estimate (k) . The ordering condition particularly means that Nk grows and hence, the variability of (k) decreases with k .

belomestny, d. and spokoiny, v.

13

Given the estimates (k) , we consider a larger class of their convex combinations:
 = 1(1) + . . . + K (K), 1 + .... + K = 1, k  0,
where the mixing coefficients k may depend on the point x . We aim at constructing a new estimate  in this class which performs at least as good as the best one in the original family {(k)} .

3.2 Stagewise aggregation procedure

The adaptive estimate  of  = f (x) is computed sequentially via the following algo-

rithm.

1. Initialization:

(1) = (1).

2. Stagewise aggregation: For k = 2, ..., K

(k) := k(k) + (1 - k)(k-1),

with the mixing parameter k being defined for some zk > 0 and a kernel Kag(∑) as

k = Kag m(k)/zk ,

m(k) := NkK((k), (k-1))

3. Loop: If k < K , then increase k by one and continue with step 2. Otherwise terminate and set  = (K) .
The idea behind the procedure is quite simple. We start with the first estimate (1) having the smallest degree of locality but the largest variability of order 1/N1 . Next we consider estimates with larger values Nk . Every current estimate (k) is compared with the previously constructed estimate (k-1) . If the difference is not significant then the new estimate (k) basically coincides with (k) . Otherwise the procedure essentially keeps the previous value (k-1) . For measuring the difference between the estimates (k) and (k-1) , we use m(k) := NkK((k), (k-1)) which is motivated by the results of Theorems 2.2 and 2.3. In particular, a large value of m(k) means that (k-1) does not belong to the confidence set corresponding to (k) and hence indicates a significant difference between these two estimates. To quantify this significance, the procedure utilizes the parameters (critical values) zk . Their choice is discussed in Section 3.3.1.

14 stagewise aggregation

Remark 3.1. If Kag(∑) is the uniform kernel on [0, 1] then k is either zero or one depending on the value of m(k) . This yields by induction arguments that the final estimate coincides with one of the "weak" estimates (k) . In this case our method can be considered as a pointwise model selection method.
If the kernel Kag is such that Kag(t) = 1 for t  b with some positive b , then the small values of the "test statistic" m(k) lead to the aggregated estimate (k) = (k) . This is an important feature of the procedure which will be used in our implementation and the theoretical study.

3.3 Parameter choice and implementation details
The implementation of the SSA procedure requires fixing a sequence of local likelihood estimates, the kernel Kag and the parameters zk . The next section gives some examples how the set of localizing schemes W (k) can be selected. The only important parameters of the method are "critical values" zk which normalize the "test statistics" m(k) . Section 3.3.1 describes in details how they can be selected in practice.
The kernel Kag should satisfy 0  Kag(t)  1 , should be monotonously decreasing and have support on [0, 1] . Besides that, there is a positive number b such that Kag(t) = 1 for t  b . Our default choice is a piecewise linear kernel with b = 1/6 and Kag(t) = 1 - (t - b)+ + . Our numerical results (not shown here) indicate that the particular choice of the kernel Kag has only a minor effect on the final results.

3.3.1 Choice of the parameters zk
The "critical values" zk define the level of significance for the test statistics m(k) . A proper choice of these parameters is crucial for the performance of the procedure. We propose in this section one general approach for selecting them which is similar to the bootstrap idea in the hypothesis testing problem. Namely, we select these values to provide the prescribed performance of the procedure in the parametric situation (under the null hypothesis). For every step k , we require that the estimate (k) is sufficiently close to the "oracle" estimate (k) in the parametric situation f (∑)   in the sense that

sup E NkK (k), (k) r  rr


(3.1)

belomestny, d. and spokoiny, v.

15

for all k = 2, . . . , K with rr from Theorem 2.4. In some cases the risk E NkK (k), (k) r does not depend on  . This is, for example, the case when  is a shift or scale param-
eter, as for Gaussian shift, exponential and volatility families. Then it sufficient to check (3.1) for any single point  . In the general situation, the risk E NkK (k), (k) r depends on the parameter value  . However, our numerical results (not reported here)
indicate that this dependence is minor and usually it suffices to check these conditions for one parameter  . In particular, for the Bernoulli model considered in Section 4 we recommend to only check the condition (3.1) for the "least favorable" value  = 1/2
corresponding to the largest variance of the estimate  .

The values  and r in (3.1) are two global parameters. The role of  is similar to the level of the test in the hypothesis testing problem while r describes the power of the loss function. A specific choice is subjective and depends on the particular application at hand. Taking a large r and small  would result in an increase of the critical values and therefore, improves the performance of the method in the parametric situation at cost of some loss of sensitivity to parameter changes. Theorem 5.1 presents some upper bounds for the critical values zk as functions of  and r in the form a0 +a1 log -1 +a2r(K -k) with some coefficients a0 , a1 and a2 . We see that these bounds linearly depend on r and on log -1 . For our applications to classification, we apply a relatively small value r = 1/2 because the misclassification error corresponds to the bounded loss function. We also apply  = 1 although the other values in the range [0.5, 1] lead to very similar results. Note that in general the such defined parameters zk depend on the model considered, design X1, . . . , Xn and the localizing schemes W (1), . . . , W (K) which in turn can differ from point to point. Therefore, an implementation of the suggested rule would require to compute the parameters separately for every point of estimation. However, in many situations, e.g. for the regular design, this variation from point to point is negligible, and a universal set of parameters can be used. Important is only that the conditions (3.1) are fulfilled for all the points.

16 stagewise aggregation
3.3.2 Simplified parameter choice
The proposal (3.1) is not constructive: we have just K - 1 conditions for choosing K - 1 parameters. Here we present a simplified procedure which is rather simple for the implementation and based on the Monte Carlo simulations. It suggests to first identify the last value zK using the reduced aggregation procedure with only two estimates (K-1) and (K) :
sup E NkK (K), (zK ) r  rr/(K - 1)

where (zK ) = (K) + (1 - )(K-1) ,  = Kag(m/zK ) and m = NK K (K), (K-1) . The other values zk are found in the form zk = zK + (K - k) to provide (3.1). This suggestion is justified by the result of Theorem 5.1 from Section 5.1.
3.3.3 Examples of sequences of local likelihood estimates
This section presents some examples and recommendations for the choice of the localizing schemes W (k) which we also use in our simulation study. Note, however, that the choice of W (k) 's is not a part of the SSA procedure. The procedure applies with any choice under some rather mild growth conditions.
Below we assume that the design X1, . . . , Xn is supported on the unit cube [-1, 1]d . This condition can be easily provided by rescaling the design components. We mention two approaches for choosing the localizing scheme which are usually used in applications. One is based on a given sequence of bandwidths, one more is based on the nearest neighbor structure of the design. In both situations we assume that a location kernel Kloc is a nonnegative function on the unit cube in [-1, 1]d . In general we only assume that this kernel is decreasing alone any radial line, that is, Kloc(x)  Kloc(x) for any x  [-1, 1]d and   1 , and Kloc(x) = 0 for |x|  1 . In the most of applications, one applies an isotropic kernel Kloc which only depends on the norm of x . The recommended choice is the Epanechnikov kernel Kloc(x) = (1 - |x|2)+ .
Bandwidth-based localizing schemes: This way can be recommended for the univariate or bivariate equidistant design. Let {hk}kK=1 be a finite set of bandwidthcandidates. We assume that this set is ordered, that is, h1 < h2 < . . . < hK . Every

belomestny, d. and spokoiny, v.

17

such bandwidth determines the collection of kernel weights wi(k) = Kloc (Xi - x)/hk , i = 1, . . . , n . In all the examples below we apply a geometrically increasing sequence of "bandwidths" hk , that is, hk+1 = ahk for some a > 1 . This sequence is uniquely determined by the starting value h1 , the factor a and the total number K of local schemes. The recommended choice of a is (1.25)1/d although our numerical results (not reported here) indicate no significant change in the results when the other value of a in the range 1.1 to 1.3 is used. The value h1 is to be selected in a way that the starting estimate (1) is well defined for all the points of estimation. In the case of a local constant approximation, this value can be taken very small because even one point can be sufficient for a preliminary estimation. In the case of a regular design, the value h1 is of order n-1/d . The number K of local schemes W (k) or, equivalently, of the "weak" estimates (k) is mostly determined by the values h1 and a in such a way that hK = h1aK-1 is about one, that is, the last estimate behaves like a global parametric estimate from the whole sample. The formula K = a log(hK/h1) suggests that K is at most logarithmic in the sample size n .
k-NN based local schemes: If the design is irregular or the design space is high dimensional ( d > 2 ) then it is useful to apply the local schemes based on the k-nearest neighbor structure of the design. For this approach, an increasing sequence {Nk} of integers has to be fixed. For a fixed x and every k  1 , the bandwidth hk is the minimal one for which the ball of radius hk contains at least Nk design points. The weights are defined again by wi(k) = Kloc (Xi - x)/hk . The sequence {Nk} is selected similarly to the sequence {hk} in the bandwidth-based approach. One starts with a fixed N1 and then multiplies it at every step with some factor a > 1 : Nk+1 = aNk . The number of steps K is such that NK is of order n .
One can easily check that the kernel and k-NN based local schemes coincide in the case of univariate regular design.

4 Application to classification
One observes a training sample (Xi, Yi) , i = 1, . . . , n , with Xi valued in a Euclidean
xspace = IRd with known class assignment Yi  {0, 1} . Our objective is to construct

18 stagewise aggregation

xa discrimination rule assigning every point x  to one of the two classes. The clas-
sification problem can be naturally treated in the context of a binary response model. It is assumed that each observation Yi at Xi is a Bernoulli r.v. with the parameter i = f (Xi) , that is, P (Yi = 0|Xi) = 1 - f (Xi) and P (Yi = 1|Xi) = f (Xi) . The "ideal" Bayes discrimination rule is (x) = 1 (f (x)  1/2) . Since the function f (x) is usually unknown it is replaced by its estimate  . If the distribution of Xi within the class k has density fk then

i = 1f1(Xi)/(0f0(Xi) + 1f1(Xi)).

where k is the prior probability of k th population k = 0, 1 . Nonparametric methods of estimating the function  are typically based on local
averaging. Two typical examples are given by the k -nearest neighbor ( k -NN) estimate
xand the kernel estimate. For a given k and every point x in , denote by Dk(x) the
subset of the design X1, . . . , Xn containing the k nearest neighbors of x . Then the k -NN estimate of f (x) is defined by averaging the observations Yi over Dk(x) :

(k)(x) = k-1

Yi .

Xi Dk(x)

The definition of the kernel estimate of f (x) involves a univariate kernel function K(∑)

and the bandwidth h :

n
(h)(x) = K
i=1

(x, Xi) h

Yi

n
K
i=1

(x, Xi) h

.

Both methods require the choice of a smoothing parameter (the value k for k -NN and

the bandwidth h for the kernel estimate).

Example 4.1. In this example we consider the binary classification problem with the corresponding class densities f0(x) and f1(x) given by two component normal mixtures

f0(x) = 0.2(x; (-1, 0), 0.5I2) + 0.8(x; (1, 0), 0.5I2) f1(x) = 0.5(x; (0, 1), 0.5I2) + 0.5(x; (0, -1), 0.5I2)

where (∑; µ, ) is the density of the multivariate normal distribution with the mean vector µ and the covariance matrix  and I2 is 2 ◊ 2 unit matrix.

belomestny, d. and spokoiny, v.

Data

Bayes Decision

19

-2 -1 0 1 2 3 -3 -2 -1 0 1 2

-2 -1 0 1 2

-2 -1 0 1 2

Figure 4.1: Sample from the binary response model with the normal mixture class densities (left) and results of applying the Bayes discrimination rule for this model (right).

Figure 4.1 shows one typical realization of the training sample with 100 observations in each class (left) and the optimal Bayes classification for a testing sample with 1000 observations in each class (right). First, in order to illustrate the "oracle" property of the SSA we compute the pointwise misclassification errors for all week estimate and SSA estimate at four boundary points. They are obtained using training sample of size 400 , k -NN weighting scheme with N1 = 5, NK = 300, K = 30 and  = 0.5 . Further, we have done 500 simulations runs generating each time 100 training points and 100 testing points. The rates of misclassification on testing sets have been averaged thereafter to give the mean misclassification error which is shown as a reference dotted line in Figure 4.3. We note here that the critical values
zk = 0.0031 + 0.007  (K - k), k = 1, . . . , K
have been computed only once for one design realization and least favorable parameter value  = 0.5 and then used in all runs. The same strategy is used in other examples as well. Next, two "weak" classification methods, k -NN and kernel classifiers, with varying smoothing parameters are applied to the same data set Figure 4.3 (top) shows the dependence of the misclassification error on the bandwidth for kernel classifiers and

20 stagewise aggregation

Misclassification Error 0.00 0.05 0.10 0.15

Misclassification Error 0.00 0.05 0.10 0.15

0 5 10 15 20 25 30 Index of week estimate

0 5 10 15 20 25 30 Index of week estimate

Misclassification Error 0.00 0.05 0.10 0.15

Misclassification Error 0.00 0.05 0.10 0.15

0 5 10 15 20 25 30 Index of week estimate

0 5 10 15 20 25 30 Index of week estimate

Figure 4.2: Pointwise misclassification errors (black dots) at four points for all weak estimates used in the example 4.1. The solid reference lines correspond to the SSA misclassification errors.

on the number of nearest neighbors for the k -NN classifier. One can observe that a careful choice of the smoothing parameter is crucial for getting
a reasonable quality of the classification. A wrong choice leads to a significant increase of the misclassification rate, especially for the kernel classifiers. At the same time, the optimal choice can lead to a reasonable quality of the classification which is only slightly worse than one of the Bayes decision rule.
Example 4.2. Now we consider the example 4.1 with additional 8 independent N (0, 1) distributed nuisance components. So, now Xi = (Xi1, .., Xi10) where
(Xi1, Xi2)  fclass(i), (Xi3, .., Xi10)  N ((0, ..., 0), I8).
8
The SSA procedure is implemented now again using k -NN weights with the number of nearest neighbors exponentially increasing from 5 to 100 . The results are shown in the

0.26 0.28 0.30 0.32 0.34

belomestny, d. and spokoiny, v.
Bayes Classifier Kernel Classifier SSA

Bayes Classifier k-NN Classifier SSA

21

0.26 0.28 0.30 0.32 0.34

012345 Bandwidth

0 10 20 30 40 50 Number of k-NN

Bayes Classifier Kernel Classifier SSA

Bayes Classifier k-NN Classifier SSA

0.20 0.25 0.30 0.35 0.40 0.45 0.50

0.20 0.25 0.30 0.35 0.40 0.45 0.50

012345 Bandwidth

0 10 20 30 40 50 Number of k-NN

Figure 4.3: Misclassification errors as a functions of the main smoothing parameter for k -NN (right) and kernel (left) classifiers. SSA and Bayes misclassification errors are given as reference lines. Top: Example 4.1 (dimension 2). Bottom: Example 4.2 (dimension 10).

bottom row of Figure 4.3. We observe again that the quality of the both standard classifiers depends significantly on the choice of the smoothing parameters. In the considered high dimensional situation, even under the optimal choice the quality of the dimension independent Bayes classifier is not attained. However, the SSA procedure performs again nearly as good as the best k -NN or kernel classifier.

Example 4.3. [BUPA liver disorders] We consider the dataset sampled by BUPA Medical Research Ltd. It consists of 7 variables and 345 observed vectors. The subjects are single male individuals. The first 5 variables are measurements taken by blood tests that are thought to be sensitive to liver disorders and might arise from excessive alcohol consumption. The sixth variable is a sort of selector variable. The seventh variable is the label indicating the class identity. Among all the observations, there are 145 peo-

22 stagewise aggregation
ple belonging to the liver-disorder group (corresponding to selector number 2 ) and 200 people belonging to the liver-normal group. The BUPA liver disorder data set is notoriously difficult for classifying with the usual error rates about 30% . We apply SSA, k -NN and kernel classifiers to tackle this problem. In SSA procedure the kNN weighting scheme was employed with number of k -NN ranging from 2 to 100 . Figure 4.4 shows the corresponding one-leave-out cross-validation errors for the above methods. One can see that the SSA method is uniformly better than kernel or k -NN classifiers.

k-NN Classifier SSA Classifier

Kernel Classifier SSA Classifier

CV Error 0.30 0.32 0.34 0.36 0.38 0.40
CV Error 0.30 0.32 0.34 0.36 0.38 0.40

0 10 20 30 40 50 Number of k-NN

0 5 10 15 20 25 Bandwidth

Figure 4.4: One-leave-out cross-validation errors as a functions of the main smoothing parameters for k -NN (right) and kernel (left) classifiers. The dotted line describes the error of SSA classifier.
Example 4.4. [Bankruptcy Data] The data set from the Compustat repository contains the statistics about bankruptcies (defaults) in private sector of USA economy during the period 2000-2005. There are 14 explanatory variables including different financial ratios, industry indicators and so on. First, the preliminary analysis is conducted and two most informative variables (equity/total assets ratio and net income/total assets ratio (profitability)) are selected. The projection of the default statistics on the corresponding plane is shown in Figure 4.5. Further, the performance of SSA procedure is compared to the performance of k-NN classifier with different numbers of nearest neighbors. Namely, the one-leave-out cross-validation errors are computed for both SSA and k-NN classification methods and the last one is presented in Figure 4.5 as a function of the number of

belomestny, d. and spokoiny, v.

23

nearest neighbors. Again as in previous examples, the quality of classification strongly depends on the choice of the parameter k . The adaptive SSA procedure provides the performance corresponding to the best possible choice of this parameter.

k-NN Classifier SSA Classifier

Equity/Total Assets -4 -3 -2 -1 0 1
CV Error 0.22 0.24 0.26 0.28 0.30

-5 0

5 10

Return on Assets

0 20 40 60 80 100 Number of k-NN

Figure 4.5: Left: Default events (crosses indicate defaulted firms and circles operating ones) are shown in dependence on the two characteristics of a firm. Right: One-leave-out cross-validation error for k-NN classifier as a function of the number of nearest neighbors. The CV error for SSA classifier is given as a red reference line.

5 Some theoretical properties of the SSA method
This section discusses some important theoretical properties of the proposed aggregating procedure. In particular we establish the "oracle" result which claims that the aggregated estimate is up to a log-factor as good as the best one among the considered family {(k)} of local constant estimates.
The majority of the results in the modern statistical literature are stated as asymptotic rate results. It is however well known that the rate optimality of an estimation procedure does not automatically imply its good finite sample properties and cannot be used for comparing different procedures. The rate results are also almost useless for selecting the parameters of the procedure. In our theoretical study we apply another approach which aims to link parametric and nonparametric inference with the focus on

24 stagewise aggregation
the adaptive behaviour of the proposed method. This means in particular that the SSA procedure attains the parametric accuracy if the parametric assumption is fulfilled. In the general situation the procedure attains (up to a unavoidable price for adaptation) the quality corresponding to the best possible local parametric approximation for the underlying model near the point of interest .
The "oracle" result is in its turn a consequence of two important properties of the aggregated estimate  : "propagation" and "stability". "Propagation" can be viewed as the oracle result in the parametric situation with f (∑)   . In this case the oracle choice would be the estimate with the largest value Nk , that is, the last estimate (K) in the family {(k)} . The "propagation" property means that at every step k of the procedure the "aggregated" estimate (k) is close to the "oracle" estimate (k) . In other words, the "propagation" property ensures that at every step the degree of locality is relaxed and the local model applied for estimation is extended to a larger neighborhood described by the weights W (k) . The "propagation" property can be naturally extended to a nearly parametric case when (W (k), ) is small for some fixed  and all k  k . The "propagation" feature of the procedure ensures that the quality of estimation improves and confidence bounds for (k) become tighter as the number of iterations increases provided that the "small modeling bias" condition still holds. Finally, the "stability" property secures that the quality gained in the "propagation" stage will be kept for the final estimate.
Our theoretical study is done under assumptions A1 and A2 on the parametric family P . Additionally we impose an assumption on the sequence of localizing schemes W (k) which was already mentioned in Section 3.

(A3) the set W (k) is ordered in the sense that wi(k)  wi(k ) for all i and all k > k .

Moreover, for some constants u0, u with 0 < u0  u < 1 , values Nk =

n j=1

wi(k)

satisfy for every 2  k  K

u0  Nk-1/Nk  u.

belomestny, d. and spokoiny, v.

25

5.1 Behavior in the parametric situation
First we consider the homogeneous situation with the constant parameter value f (x) =  . Our first result claims that in this situation under condition A3 the parameters zk can be chosen in the form zk = zK + (K - k) to fulfill the "propagation" condition (3.1). The proof is given in the Appendix.
Theorem 5.1. Assume A1 , A2 and A3 . Let f (Xi) =  for all i . Then there are three constants a0, a1 and a2 depending on r and u0 , u only such that the choice
zk = a0 + a1 log -1 + a2r log Nk
ensures (3.1) for all k  K . Particularly, E NK K (K),  r  rr.

5.2 "Propagation" under "small modelling bias"

Now we extend the "propagation" result to the situation when the parametric assumption is not fulfilled any more but the deviation from the parametric structure within the considered local model is sufficiently small. This deviation can be measured for the localizing scheme W (k) by (W (k), ) from (2.3).
We suppose that there is a number k such that the modeling bias (W (k), ) is small for some  and all k  k . Consider the corresponding estimate (k) obtained after the first k steps of the algorithm. Theorem 2.5 implies in this situation the following result.

Theorem 5.2. Assume A1 , A2 and A3 . Let  and k be such that (W (k), )   for some   0 and all k  k . Then

Ef (∑) Nk K (k), (k) r/2 Ef (∑) Nk K (k),  r/2

 

rr e , rr e .

5.3 "Stability after propagation" and "oracle" results
Due to the "propagation" result, the procedure performs well as long as the "small modeling bias" condition (W (k), )   is fulfilled. To establish the accuracy result for the final estimate  , we have to check that the aggregated estimate (k) does not

26 stagewise aggregation

vary much at the steps "after propagation" when the divergence (W (k), ) from the parametric model becomes large.

Theorem 5.3. Under A1 , A2 and A3 , it holds for every k  K

NkK (k), (k-1)  zk.

(5.1)

Moreover, under A3 , it holds for every k with k < k  K

NkK (k ), (k)  a2cu zk

(5.2)

with cu = (u-1/2 - 1)-1/2 and zk = maxlk zl .

Remark 5.1. An interesting feature of this result is that it is fulfilled with probability one, that is, the control of stability "works" not only with a high probability, it always applies. This property follows directly from the construction of the procedure.

Proof. By convexity of the Kullback-Leibler divergence K(u, v) w.r.t. the first argument

K (k), (k-1)  kK (k), (k-1) .

If K (k), (k-1)  zk/Nk , then k = 0 and (5.1) follows. Now, Assumption A2 and Lemma 6.1 yield

kk

K1/2 (k ), (k)  a

K1/2 (l), (l-1)  a

zl/Nl 1/2.

l=k+1

l=k+1

The use of Assumption A3 leads to the bound

k
K1/2 (k ), (k)  a zk/Nk 1/2

u(l-k)/2



 a u(1

-

u)-1

zk /Nk

1/2

l=k+1

which proves (5.2).

Combination of the "propagation" and "stability" statements implies the main result concerning the properties of the adaptive estimate  .
Theorem 5.4. Assume A1 , A2 and A3 . Let k be a "good" choice in the sense that

max (W (k), )  
kk

belomestny, d. and spokoiny, v.

27

for some  and some value  . Then

Ef (∑) Nk K (k),  r/2  2(r-1)+ ar rre + cu zk r/2

where cu is the constant from Theorem 5.3.

We also present a corollary of the "oracle" result concerning the risk of the adaptive estimate  for the special case with r = 1 . The other values of r can be considered as well, one only has to update the constants depending on r . We also assume that   1 .

Corollary 5.5. Let maxkk (W (k), )   for some  and some  . Then

Nk1/2Ef(∑)K1/2 ,   a 2 r1e + cuzk .

Proof. Just observe that by Lemma 6.1

k

K1/2 ,   a K1/2 (k),  + K1/2 (k), (k) +

K1/2 (l), (l-1)

l=k+1

and follow the proof of Theorem 5.3.

Remark 5.2. Recall that in the parametric situation, the risk E NkK (k),  1/2 of (k) is bounded by r1/2 , cf. Theorem 2.2. In the nonparametric situation, the result is only slightly worse: the value r1/2 is replaced by r1e which takes into account the modeling bias. There is also an additional term proportional to zk which can be considered as the payment for adaptation. Due to Theorem 5.1, zk is bounded from above by zK + (K - k) . By Theorem 5.1 K is only logarithmic in the sample size n .
Therefore, the risk of the aggregated estimate corresponds to the best possible risk among the family {(k)} for the choice k = k up to a logarithmic factor. Lepski, Mammen and Spokoiny (1997) established a similar result in the regression setup for the pointwise adaptive Lepski procedure. Combining the result of Corollary 5.5 with Theorem 2.7 yields the rate of adaptive estimation n-1 log n 1/(2+d) under Lipschitz smoothness of the function f and the usual design regularity, see Polzehl and Spokoiny (2005) for more details. It was shown by Lepski (1990) that in the problem of pointwise adaptive estimation this rate is optimal and cannot be improved by any estimation method. This gives an indirect proof of the optimality of our procedure: the factor zk

28 stagewise aggregation
in the accuracy of estimation cannot be removed or reduced in the rate because otherwise the similar improvement would appear in the rate of estimation.

6 Appendix: Proof of Theorem 5.1

The proof utilizes the following simple "metric like" property of K1/2(∑, ∑) .

Lemma 6.1 (Polzehl and Spokoiny, 2005, Lemma 5.2). Under condition A2 it holds for every sequence 0, 1, . . . , m that
K1/2(1, 2)  a K1/2(1, 0) + K1/2(2, 0) , K1/2(0, m)  a K1/2(0, 1) + . . . + K1/2(m-1, m) .

With the given constants zk , define for k > 1 the random sets

Ak = {Nk K((k), (k-1))  bzk},

A(k) = A2  . . .  Ak ,

where b enters in the construction of Kag : Kag(t) = 1 for t  b . Note first that (k) = (k) on A(k) for all k  K . This fact can be proved by
induction in k . For k = 1 , the assertion is trivial because (1) = (1) . Now suppose that (k-1) = (k-1) . Then it holds on Ak that m(k) = NkK((k), (k-1)) = NkK((k), (k-1))  bzk and thus, k = Kag(m(k)/zk)  Kag(b) = 1 yielding (k) = (k) .
Therefore, it remains to bound the risk of (k) on the complement A(k) of A(k) . Define Bk = A(k-1) \ A(k) . On the event Bk , the index k is the first one for which the condition Nk K((k), (k-1))  bzk is violated. It is obvious that A(k) = l<k Bl . First we bound the probability P  Bl . Applying assumption A3 and Lemma 6.1 yields for
every l

Nl K((l), (l-1))  2a2Nl K((l), ) + K((l-1), )  2a2 Nl K((l), ) + u-0 1Nl-1K((l-1), ) .

Therefore, by Theorem 2.2,

P  Bl

 P  Nl K((l), (l-1)) > bzl

 2 exp

-

u0b 4a2

zl

.

belomestny, d. and spokoiny, v.

29

On the set Bl , it holds (l-1) = (l-1) and thus, for every k > l the aggregated estimate (k) by construction is a convex combination of (l-1), . . . , (k) . Convexity of the Kullback-Leibler divergence w.r.t. the second argument, the definition of (k) and Lemma 6.1 ensure that

K1/2 (k), (k) 1 Bl

 max K1/2 (k), (l )
l =l-1,...,k-1
 a max K1/2((k), ) + K1/2((l ), )
l =l-1,...,k-1
 2a max K1/2((l ), ).
l =l-1,...,k

This and Theorem 2.4 imply for every r

E Kr (k), (k) 1 Bl

k

 (2a)2rE

Kr((l ), )1 Bl

l =l-1

k

 (2a)2r

E 1/2 K2r ((l

),

)P

1/2 

Bl

l =l-1

 (2a)2rr12r/2

k

Nl-r2 exp

-

u0b 8a2

zl

l =l-1

 C1Nl-rr12r/2 exp -c2zl

for some fixed constants C1 and c2 . Therefore,

kk

E Kr (k), (k) 

E Kr (k), (k) 1 Bl 

C1Nl-rr12/r2 exp -c2zl .

l=2 l=2

It remains to check that the choice zk = a0 + a1 log -1 + a2r log(NK /Nk) with properly selected a0, a1 and a2 provide the required bound E NkK (k), (k) r  rr .

References
[1] Bickel, P.J., C.A.J. Klaassen, Y. Ritov and J.A. Wellner (1998). Efficient and Adaptive Estimation for Semiparametric Models, 1998, Springer.
[2] Breiman, L. (1996). Stacked regression. Machine Learning, 24 49≠64. [3] Cai, Z., Fan, J. and Li, R. (2000). Efficient estimation and inference for varying coefficients
models. J. Amer. Statist. Ass., 95 888≠902. [4] Catoni, O. (2004). Statistical learning theory and stochastic optimization. Lecture Notes in
Mathematics, 1851. Springer-Verlag, Berlin.

30 stagewise aggregation
[5] Fan, J., and Gijbels, I. (1995). Data driven bandwidth selection in local polynomial fitting: variable bandwidth and spatial adaptation. J. Royal Statist. Soc. Ser. B, 57 371≠394.
[6] Fan, J., Farmen, M. and Gijbels, I. (1998). Local maximum likelihood estimation and inference. J. Royal Statist. Soc. Ser. B, 60 591≠608.
[7] Fan, J. and Gijbels, I. (1996). Local polynomial modelling and its applications. Chapman & Hall, London.
[8] Fan, J. and Zhang, W. (1999). Statistical estimation in varying coefficient models. Ann. Statist. 27 1491≠1518.
[9] Hastie, T.J. and Tibshirani, R.J. (1993). Varying-coefficient models (with discussion). J. Royal Statist. Soc. Ser. B, 55 757≠796.
[10] Juditsky, A. and Nemirovski, A. (2000). Functional aggregation for nonparametric estimation. Ann. Statist., 28 682≠712.
[11] Lepski, O., Mammen, E. and Spokoiny, V. (1997). Ideal spatial adaptation to inhomogeneous smoothness: an approach based on kernel estimates with variable bandwidth selection. Annals of Statistics, 25 929≠947.
[12] Li, J. and Barron, A. (1999). Mixture density estimation. In. S.A. Sola, T.K. Leen, and K.R. Mueller, editors, Advances in Neural Inforamtion proceedings systems.
[13] Loader, C. R. (1996). Local likelihood density estimation. Academic Press.
[14] Polzehl, J. and Spokoiny, V. (2005). Propagation-Separation Approach for Local Likelihood Estimation. Probab. Theory Related Fields, DOI: 10.1007/s00440-005-0464-1.
[15] Rigollet, Ph. and Tsybakov, A. (2005). Linear and convex aggregation of density estimators. Manuscript.
[16] Spokoiny, V. (1998). Estimation of a function with discontinuities via local polynomial fit with an adaptive window choice. Ann. Statist., 26 1356≠1378.
[17] Staniswalis, J.C. (1989). The kernel estimate of a regression function in likelihood-based models. Journal of the American Statistical Association, 84 276≠283.
[18] Tibshirani, J.R., and Hastie, T.J. (1987). Local likelihood estimation. Journal of the American Statistical Association, 82 559≠567.
[19] Tsybakov, A. (2003). Optimal rates of aggregation. Computational Learning Theory and Kernel Machines. B.Scholkopf and M.Warmuth, eds. Lecture Notes in Artificial Intelligence, 2777 Springer, Heidelberg, 303-313.
[20] Yang, Y. (2004). Aggregating regression procedures to improve performance. Bernoulli 10 25≠47

SFB 649 Discussion Paper Series 2006
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Calibration Risk for Exotic Options" by Kai Detlefsen and Wolfgang K. H‰rdle, January 2006.
002 "Calibration Design of Implied Volatility Surfaces" by Kai Detlefsen and Wolfgang K. H‰rdle, January 2006.
003 "On the Appropriateness of Inappropriate VaR Models" by Wolfgang H‰rdle, Zdenk Hl·vka and Gerhard Stahl, January 2006.
004 "Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification" by Harald Uhlig, January/February 2006.
005 "British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis" by Enzo Weber, January 2006.
006 "A Combined Approach for Segment-Specific Analysis of Market Basket Data" by Yasemin Boztu and Thomas Reutterer, January 2006.
007 "Robust utility maximization in a stochastic factor model" by Daniel Hern·ndez≠Hern·ndez and Alexander Schied, January 2006.
008 "Economic Growth of Agglomerations and Geographic Concentration of Industries - Evidence for Germany" by Kurt Geppert, Martin Gornig and Axel Werwatz, January 2006.
009 "Institutions, Bargaining Power and Labor Shares" by Benjamin Bental and Dominique Demougin, January 2006.
010 "Common Functional Principal Components" by Michal Benko, Wolfgang H‰rdle and Alois Kneip, Jauary 2006.
011 "VAR Modeling for Dynamic Semiparametric Factors of Volatility Strings" by Ralf Br¸ggemann, Wolfgang H‰rdle, Julius Mungo and Carsten Trenkler, February 2006.
012 "Bootstrapping Systems Cointegration Tests with a Prior Adjustment for Deterministic Terms" by Carsten Trenkler, February 2006.
013 "Penalties and Optimality in Financial Contracts: Taking Stock" by Michel A. Robe, Eva-Maria Steiger and Pierre-Armand Michel, February 2006.
014 "Core Labour Standards and FDI: Friends or Foes? The Case of Child Labour" by Sebastian Braun, February 2006.
015 "Graphical Data Representation in Bankruptcy Analysis" by Wolfgang H‰rdle, Rouslan Moro and Dorothea Sch‰fer, February 2006.
016 "Fiscal Policy Effects in the European Union" by Andreas Thams, February 2006.
017 "Estimation with the Nested Logit Model: Specifications and Software Particularities" by Nadja Silberhorn, Yasemin Boztu and Lutz Hildebrandt, March 2006.
018 "The Bologna Process: How student mobility affects multi-cultural skills and educational quality" by Lydia Mechtenberg and Roland Strausz, March 2006.
019 "Cheap Talk in the Classroom" by Lydia Mechtenberg, March 2006. 020 "Time Dependent Relative Risk Aversion" by Enzo Giacomini, Michael
Handel and Wolfgang H‰rdle, March 2006. 021 "Finite Sample Properties of Impulse Response Intervals in SVECMs with
Long-Run Identifying Restrictions" by Ralf Br¸ggemann, March 2006. 022 "Barrier Option Hedging under Constraints: A Viscosity Approach" by
Imen Bentahar and Bruno Bouchard, March 2006.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "How Far Are We From The Slippery Slope? The Laffer Curve Revisited" by Mathias Trabandt and Harald Uhlig, April 2006.
024 "e-Learning Statistics ≠ A Selective Review" by Wolfgang H‰rdle, Sigbert Klinke and Uwe Ziegenhagen, April 2006.
025 "Macroeconomic Regime Switches and Speculative Attacks" by Bartosz Makowiak, April 2006.
026 "External Shocks, U.S. Monetary Policy and Macroeconomic Fluctuations in Emerging Markets" by Bartosz Makowiak, April 2006.
027 "Institutional Competition, Political Process and Holdup" by Bruno Deffains and Dominique Demougin, April 2006.
028 "Technological Choice under Organizational Diseconomies of Scale" by Dominique Demougin and Anja Schˆttner, April 2006.
029 "Tail Conditional Expectation for vector-valued Risks" by Imen Bentahar, April 2006.
030 "Approximate Solutions to Dynamic Models ≠ Linear Methods" by Harald Uhlig, April 2006.
031 "Exploratory Graphics of a Financial Dataset" by Antony Unwin, Martin Theus and Wolfgang H‰rdle, April 2006.
032 "When did the 2001 recession really start?" by Jˆrg Polzehl, Vladimir Spokoiny and Ctlin Stric, April 2006.
033 "Varying coefficient GARCH versus local constant volatility modeling. Comparison of the predictive power" by Jˆrg Polzehl and Vladimir Spokoiny, April 2006.
034 "Spectral calibration of exponential LÈvy Models [1]" by Denis Belomestny and Markus Reiﬂ, April 2006.
035 "Spectral calibration of exponential LÈvy Models [2]" by Denis Belomestny and Markus Reiﬂ, April 2006.
036 "Spatial aggregation of local likelihood estimates with applications to classification" by Denis Belomestny and Vladimir Spokoiny, April 2006.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

