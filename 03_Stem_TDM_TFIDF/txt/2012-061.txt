BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2012-061
Variable selection in Cox regression models
with varying coefficients
Toshio Honda * Wolfgang Karl H‰rdle **
* Graduate School of Economics, Hitotsubashi University ** Humboldt-Universit‰t zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Variable selection in Cox regression models with varying coefficients6
Toshio Honda
Graduate School of Economics, Hitotsubashi University Kunitachi, Tokyo 186-8601, Japan
Wolfgang Karl H®ardle
C.A.S.E. - Center for Applied Statistics & Economics, Humboldt-Universita®t zu Berlin Unter den Linden 6, 10099 Berlin, Germany

Abstract
We deal with two kinds of Cox regression models with varying coefficients. The coefficients vary with time in one model. In the other model, there is an important random variable called an index variable and the coefficients vary with the variable. In both models, we have p-dimensional covariates and p increases moderately. However, it is the case that only a small part of the covariates are relevant in these situations. We carry out variable selection and estimation of the coefficient functions by using the group SCAD-type estimator and the adaptive group Lasso estimator. We examine the theoretical properties of the estimators, especially the L2 convergence rate, the sparsity, and the oracle property. Simulation studies and a real data analysis show the performance of these new techniques.
Keywords: Cox regression model, high-dimensional data, sparsity, oracle estimator, B-splines, group SCAD, adaptive group Lasso, L2 convergence rate JEL: C14, C24

6This research is supported by the Deutsche Forschungsgemeinschaft via SFB 649 "Economic Risk , Humboldt-Universit®at zu Berlin.
The first author is also supported by the Global COE Program Research Unit for Statistical and Empirical Analysis in Social Sciences at Hitotsubashi University, Japan.
Email address: honda@econ.hit-u.ac.jp (Toshio Honda )

Preprint submitted to a journal

October 12, 2012

1. Introduction
The Cox regression model is one of the most popular and useful models in survival analysis. In recent years, many nonparametric and semiparametric variants of the Cox regression model have been proposed. Among them, there are varying coefficient models (Cai and Sun [9], Tian et al. [24], Fan et al. [12], Cai et al. [8], Chen et al. [10]), partially linear models and their extensions (Cai et al. [5], [6]), and additive and functional ANOVA models (Huang et al. [17]). In this paper we focus on varying coefficient models and consider two kinds of Cox regression models with varying coefficients. The coefficients vary with time in one model ([9], [24]) and with index variable U (t) in another model ([12], [8], [10]).
In recent years, a dimensional and model selection issue occurs in many applications : only a small part of the variables are relevant. Therefore statistical methods for variable selection are needed. Penalized likelihood estimators such as the Lasso or SCAD estimators have been among the standard tools in carrying out variable selection and estimation simultaneously, Tibshirani [25] and Fan and Li [13]. Zou [33] proposed the adaptive Lasso to correct some deficiencies of the Lasso and proved that the adaptive Lasso estimators choose the relevant variables consistently. Both group SCAD and group Lasso are also popular techniques, Yuan and Lin [27] and Meier et al. [21]. For more references, we refer to Bu®hlmann and van de Geer [4].
Local linear estimators have been used in varying coefficient Cox regression models. However, we employ basis functions such as B-spline basis functions and look at the models from a different perspective by combining these models and variable selection. This is the focus of this research. We deal with the cases where the number of the covariates, p, increase moderately with the sample size, for example p = O(n3/10), where n is the sample size. We conduct variable selection and estimation simultaneously by employing group SCAD-type or adaptive group Lasso estimators.
Variable selection and estimation in Cox regression models are considered in many papers, for example, in Cai et al. [7], Zhang and Lin [32], Du et al. [11], Wang et al. [29], and Bradic et al. [2]. In [29], group SCAD and Lasso estimators are analyzed for linear models. Bradic et al. [2] deals with ultra high-dimensional data and presents useful theoretical results for linear models. However, all of the above papers focus on variable selection in the parametric part of the Cox regression model and the variants. Although an alleviation has been proposed for functional ANOVA models in Leng and
2

Zhang [19], the sparsity or the oracle property of the estimator is not verified. Recently Yan and Huang [26] proposed the adaptive group Lasso in a
Cox regression model with time-varying coefficients and carried out some simulation studies and a real data analysis. There is however still a lacuna of theoretical results that this research aims to fill. We establish the sparsity for the group SCAD-type and adaptive group Lasso estimator and the oracle property for the group SCAD-type estimator under simple and interpretable assumptions. Very recently Bradic and Song [3] considered penalized estimators for additive Cox regression models when the number of the covariates is larger than the sample size. However, they do not deal with any varying coefficient Cox regression models.
We concentrate on the time-varying coefficient model since the model is easier to treat. Besides no identifiability constraint is necessary to the model. We describe only the results on the model having coefficients varying with an index variable U (t) in section 6 because we can deal with the model almost in the same way. The derivation of the theoretical results of this paper crucially depend on the methodology of Huang [16], Huang and Stone [18], and Huang et al. [17].
Variable selection in time-varying coefficients models are also considered in other settings in Wang et al. [28], Noh and Park [22], Wei et al. [30], and Lian [20], where group SCAD-type or group Lasso estimators are used.
This paper is organized as follows. We state the setup of the timevarying coefficient model and define the partial likelihood estimator, the group SCAD-type estimator, and the adaptive group Lasso estimator in section 2. We consider the asymptotics and establish the sparsity and the oracle property of the estimators in section 3. The results of simulation studies and a real example are presented in section 4. Technical assumptions and the proofs of the theorems are given in section 5. We present the results on the model having coefficients varying with an index variable U (t) in section 6. The proofs of propositions and lemmas are confined to section 7.
In this paper, C is a generic positive constant and the value varies from place to place. We denote the Euclidean norm and the transpose of a vector v by |v| and vT, respectively. We omit almost surely or a.s. when it is clear from the context.
3

2. Assumptions and estimators

In this section, we describe the Cox regression model with time-varying coefficients, state some assumptions, and define the group SCAD-type and adaptive group Lasso estimator. In deriving the main results, we repeatedly use insights of [17] and [18], of which we also borrow the notation.
Let T and C be a failure time and a censoring time. The interest is in the failure time. However, we observe only Y = min{T, C} on [0,  ] subject to censoring for some finite  and  = I(T  C). Define

N (t) =  I(Y  t) and Z(t) = I(Y  t).

We also observe a p-dimensional time-dependent covariate X(t). Suppose that (Yi, i, Xi(t)), where Xi(t) = (Xi1(t), . . . , Xip(t)), i = 1, . . . , n, are
i.i.d. observations of (Y, , X(t)). Then our purpose is to simultaneously

carry out variable selection of X(t) and estimation of the time-varying coef-

ficients in (1) below . Assumptions A1-3 on the time-varying Cox regression

models reflect a standard setup of Cox regression models. These assumptions

and two technical assumptions on X(t) and Z(t) are deferred to section 5.

The hazard function of Ti w.r.t. an appropriate filtration is

{ p

}

(t) = 0(t) exp

g0j(t)Xij(t) = 0(t) exp{g0(t)Xi(t)},

j=1

(1)

where 0(t) is an unknown hazard function and g0(t) = (g01(t), . . . , g0p(t)) is a vector of unknown time-varying coefficients and assumed to be twice

continuously differentiable. Details about the sparsity of g0(t) are given later in this section. Technical assumptions on g0(t) are postponed to section 5. Note that we do not have to impose any identifiability constraints on the

time-varying coefficient g0(t) since X(t) has no constant element. We estimate g0(t) by choosing a basis {B1(t), . . . , BKn(t)} on [0,  ] and
maximizing the partial likelihood with/without a penalty term. We allow p to increase moderately (e.g. p = O(n3/10)) and consider variable selection.
More precisely for the basis {B1, . . . , BKn}, we write
B(t) = (B1(t), . . . , BKn(t)) or B = (B1, . . . , BKn).

Then the covariate vector for the partial likelihood is Xi(t)  B(t). The

approximation error n of the basis {B1, . . . , BKn} is defined by

p

n

=

sup
g0

j=1

inf
j RKn

sup
0t

|jB(t)

-

g0j (t)|,

(2)

4

where g0 = (g01, . . . , g0p) is over the set of functions satisfying Assumption G in section 5. Then we have n = O(Kn-2) by the standard theory.
An example of the basis satisfying the following assumption is an equispaced B-spline basis of order m(m  2), see Schumaker [23] for more about B-spline functions.
Assumption B: (i) Bj(t), j = 1, . . . , Kn, are continuous and bounded on [0,  ]. The upper and lower bounds are allowed to depend on n. (ii) n  0.
Here we define two linear function spaces G0 and H0 on [0,  ] and two norms ∑ and ∑L2 on them. The ratio of ∑ to ∑L2 is also considered in (7) below. We define G0 by

G0 = {(1B(t), . . . , pB(t))| j  RKn, j = 1, . . . , p}.

(3)

Let H0 be the linear space spanned by G0 and the true coefficient function
vector g0. A function space similar to G0 is defined and called the estimation
space in [17]. The dimension of our G0 is pKn. Note that the dimension of
G0 in [17] is denoted by Nn and p is fixed there. For h = (h1, . . . , hp)  H0, we define h and hL2 by

p h = sup |hj(t)|
j=1 0t

(4)

and

h2L2

=

p hj 2L2
j=1

=

p
j=1

1 


0

hj2(t)dt,

(5)

where we also write for the jth element of h,

hj L2 2

=

1 


0

hj2(t)dt.

(6)

The ratio of  ∑  to  ∑ L2 over G0 plays an important role and we denote

the ratio by An.

An = sup {g/gL2}.
gG0

(7)

5

When we employ an equi-spaced B-spline basis of order m(m  2), we have An  C(pKn)1/2. The necessary relations between An, n, and pKn are given in :
Assumption RA:

lim
n

Ann

=

0

and

lim
n

n-1A2n

max{pKn,

log

n}

=

0.

When we have Kn = CKn1/5, n = O(Kn-2), and An = O{(pKn)1/2}, Assumption RA implies that p = O(n3/10).
In [17], two norms (equivalent to the L2 norm) are introduced and the norms play an crucial role in the proofs of their main results. We also in-
troduce two similar norms and employ the two norms when we evaluate the
eigenvalues of the Hessian matrix of the partial likelihood. Recall that H0 is spanned by G0 and g0.
The partial likelihood lp(h) is defined by

lp(h) =

1 n

n 

i=1

0 



h(t)Xi(t)dNi(t) [ n

{

}]

- log n-1 Zi(t) exp h(t)Xi(t) dNØ (t),

0 i=1

(8)

where

NØ (t)

=

n-1

n
i=1

Ni(t).

First we estimate g0 by maximizing lp(g)

over G0 and use the estimator as an initial value of the optimization for the

penalized partial likelihood. Define the p(h) :

{ 

}

p(h) = E

h(t)X(t)dN (t)

0  ( [

{

}])

- log E Z(t) exp h(t)X(t) d E{N (t)}.

(9)

0

For h1  H0 and h2  H0, define :

(h1, h2)Z(t) = E[{h1 (t)X(t)}{h2 (t)X(t)}Z(t)]/ E{Z(t)}

(10)

and

(h1, h2)Zn(t) = En[{h1 (t)X(t)}{h2(t)X(t)}Z(t)]/En{Z(t)},

(11)

6

where

En(∑)

is

the

empirical

measure,

for

example,

En {Z (t)}

=

n-1

n
i=1

Zi(t).

As in [17], we define h for h  H0 in terms of the inner product defined

in (12) below.



(h1, h2) = (h1, h2)Z(t)d E{N (t)}.

(12)

0

The empirical version hn of h is defined in terms of the inner product

below.



(h1, h2)n = (h1, h2)Zn(t)dEn{N (t)}.

(13)

0

A centered version h0 of h is used in [17] for the identifiability constraint. However, we define and use h0 and the empirical version h0n only for technical reasons. The centered version is defined in terms of the

inner product in (14) below and the empirical version is defined by replacing

E(∑) with En(∑).

(h1, h2)0

(14)

= [E{Z(t)}]-1 E([h1(t)X(t) - E{h1(t)X(t)Z(t)}/ E{Z(t)}]

0

◊[h2(t)X(t) - E{h2 (t)X(t)Z(t)}/ E{Z(t)}]Z(t))d E{N (t)}.

The norms defined by (12) and (14) are equivalent to the L2 norm in (5) and these norms are also equivalent to the empirical counterparts with probability tending to 1. The details are given in Lemmas 1-4 in section 5.
Finally in this section, we define three estimators of g0. The first one is the partial likelihood estimator and defined by

g~n = argmax lp(g)
gG0

(15)

It will be shown in Theorem 1 below that the L2 convergence rate of g~n is :

rpn = max{(pKn/n)1/2, n}.

(16)

When we have a moderately large p, it is often the case that only a small number of the covariates are relevant or the model is sparse. Therefore we introduce the sparsity assumption :
Assumption S: For some s, g0j = 0, s + 1  j  p.

7

To deal with this sparsity, we present two penalized partial likelihoods Qp(g) and QØp(g) for g = (g1, . . . , gp)  G0.

p Qp(g) = lp(g) - pn(gjL2),

(17)

j=1

where n is a tuning parameter and p(∑) is a SCAD-type penalty function to be specified in Assumption P below. See (6) for the definition of the L2 norm. An example of p(∑) satisfying (i) of Assumption P is the SCAD function. See [13] for the definition of the SCAD function.

Assumption P:

(i) p(t) is a monotone increasing and concave function on [0, ) with p(0) = 0. Besides, there are positive constants a0, b0, and c0 such that p(t) = 0, t  a0, and p (t)  c0, 0 < t  b0. (ii) n/rpn   and min1js g0jL2/n  .

The second assumption in Assumption P means that n should be much larger than the convergence rate and that g0jL2 should be large enough compared to n.
Another penalized partial likelihood QØp(g) is defined by

p QØp(g) = lp(g) - n wjgjL2,

(18)

j=1

where n is another tuning parameter and wj, j = 1, . . . , p, are weights to be constructed from a preliminary estimator. Notice that QØp(g) is a concave

function.

Finally the group SCAD-type estimator g^n and the adaptive group Lasso estimator gØn are given by

g^n = argmax Qp(g) and gØn = argmax QØp(g).

gG0

gG0

(19)

We can also define ls(g), Qs(g), and QØs(g) for the s in Assumption S
by ignoring the last (p - s) elements of the covariates or taking Xi(t) = (Xi1(t), . . . , Xis(t)) and g = (g1, . . . , gs).

The main contribution here is to prove that we can select the relevant

covariates consistently by using g^n or gØn. Besides, both of them achieve the rate of convergence rsn, where rsn = max{(sKn/n)1/2, n} and rsn is the

convergence rate we obtain by maximizing ls(g). Besides, we establish the oracle property of the SCAD-type estimator. We discuss how to compute g~n, g^n, and gØn in section 4.

8

3. Main theorems
The L2 convergence rate of the partial likelihood estimator g~n is derived in Theorem 1. The properties of the group SCAD-type and the adaptive group Lasso estimator are considered in Theorems 2-3 and Theorems 4-5, respectively. We also comment on the semi-varying coefficient model in Remark 2. Recall that Assumptions A1-3, X, M, and G are given later in section 5.

Theorem 1. Suppose that Assumptions A1-3, X, M, G, B, and RA hold and rpn  0. Then with probability tending to 1, there is a unique maximizer g~n = (g~n1, . . . , g~np) of lp(g) over G0 and we have
g~n - g0L2 = Op(rpn).
The existence of the group SCAD-type estimator is verified in Theorem 2 and the sparsity and oracle property is established is Theorem 3.

Theorem 2. Suppose that all the assumptions in Theorem 1 and Assumptions P and S hold. Then for any positive , there is a positive constant M such that

lim P(
n

T here

is

a

local

maximizer

g^n

of Qp(g) over G0 such that g^n - g0L2  M rpn) > 1 - .

Before we present Theorem 3, we define two properties. If the local maximizer of g^n = (g^n1, . . . , g^np) satisfies under Assumption S,

g^nj = 0, j = s + 1, . . . , p,

(20)

with probability tending to 1, we say that g^n has the sparsity. The maximizer of ls(g) is called an oracle estimator since we use the knowledge of the true model under Assumption S. If an estimator is asymptotically equivalent to such an oracle estimator, we say that the estimator has the oracle property.
It is known that (ii) easily follows from (i) in Theorem 3 below due to the flatness of p(t) on [a0, ). For example, see Fan and Lv [14].

Theorem 3. Suppose that the assumptions in Theorem 2 hold and let {dn} be a sequence of positive numbers satisfying dn  , n/(dnrpn)  , and Andnrpn = O(1).

9

(i) With probability tending to 1, any local maximizer g^n of Qp(g) over G0 such that g^n - g0L2  dnrpn satisfies (20). (ii) With probability tending to 1, the local maximizer in (i) is the unique maximizer of ls(g) and satisfies
s g^nj - g0jL2 2 = O(rs2n).
j=1
In Theorems 4 and 5 below, we state the properties of the adaptive group Lasso estimator. We comment on how to choose the weights {wj} in Remark 1 below.

Theorem 4. hold and that

Snuppsomseatxh1atjths ewaj/srsupnm=ptiOonp(s1i)n.

Theorem 1 Then with

and Assumption S probability tending

to 1, there is a unique maximizer gØn = (gØn1, . . . , gØnp) of QØp(g) over G0 and

we have

gØn - g0L2 = Op(rpn).

Remark Theorem

1. Suppose we 4 is satisfied if

take wj we have

=n1/sg/~n(jrpnL2m. inT1hej

assumption on {wj s g0jL2) = O(1)

} in and

min1js g0jL2/rpn  , If we also have n /rp2n  , the assumptions on

{wj} in Theorem 5 below are also satisfied.

Theorem 5. Suppose that the assumptions in Theorem 4 hold and that n mins<jp wj/(rpn)   in probability and Anrpn = O(1). Then with probability tending to 1, the unique maximizer gØn has the sparsity and is equal to the unique maximizer of QØs(g). In addition we have
s gØnj - g0j2L2 = Op(rs2n).
j=1

We have demonstrated that the group SCAD estimator has the sparsity
and oracle property. As for the adaptive group Lasso estimator, we have
established the sparsity and the improved L2 convergence rate. Generally speaking, it is difficult to prove the oracle property of the adaptive group
Lasso estimator due to the property of the Lasso penalty function. On the other hand, the uniqueness easily follows from the concavity of QØp(g). When some statistical inference is necessary, we recommend to ignore the influences

10

of the penalty term and use the inverse of the observed Fisher information matrix of the partial likelihood as the estimate of the variance. The validity of this procedure is a topic of future research.
The condition on p is rather restrictive due to Assumption RA. When we deal with the case of a larger p we will have to use the methodology of [2] by imposing much more restrictive assumptions on the model and the properties of covariates. One of the main purposes of this paper is to establish the desirable properties of the estimators under simple, mild, and interpretable assumptions. See assumptions in section 5.
Finally we comment on how to select a semi-varying coefficient model from the varying coefficient model.

Remark 2. Suppose that the true model is a semi-varying coefficient model.

Then we can detect the semi-varying coefficient model with probability tend-

ing one by modifying the estimators in the following way. We decompose gj of g = (g1, . . . , gp)  G0, by

gj (t)

=

1 


0

gj (s)ds

+

{ gj (t)

-

1 


0

} gj (s)ds

=

gaj

+

gbj (t)

and

gjL2 2 = |gaj|2 + gbjL2 2 . Then we define Qp(g) and QØp(g) by
p Qp (g) = lp(g) - {pn(|gaj|) + pn(gbjL2)}
j=1

and
p QØp(g) = lp(g) - n (w1j|gaj| + w2jgbjL2),
j=1
where w1j and w2j are weights. Note that QØp (g) is similar to (6) of [26]. See also Zhang et al. [31]. We derive almost the same results as in Theorems 2-5 for Qp (g) and QØp (g) in the same way because the decomposition of gj into gaj and gbj does not depend on data and the decomposition is the orthogonal

11

one. This implies that we select only the gaj component consistently for any j such that gjL2 = 0 and gj has no nonlinear component gbj. Therefore we can detect the true semi-varying coefficient model from the varying coefficient model consistently. However, the convergence rate is still rsn even for the linear component. We will have to use a two-step estimator to improve the convergence rate of the linear component or employ another proof to show
that the proposed estimators have the improved convergence rate for the linear component. This remark also applies to the model in section 6.

4. Simulation studies and a real example
We carried out some simulations by using R to examine the finite sample properties of the group SCAD-type and adaptive group Lasso estimators in (19). We considered two models whose hazard functions are given by
h1(t) = 0 exp{-X1 + X2w1 log(t + 1)}
and

h2(t) = 0 exp(-X1 + X2w2t),

where 0 = 0.275,

w12




{log(t + 1)}2dt = 1,

and

w22   t2dt = 1.

0

0

We call them Model 1 and Model 2, respectively. We follow [1] in gener-

ating survival times and took  = 4, p = 14, Kn = 5, and n = 600 in the simulations. Only X1 and X2 are relevant. The replication number is 100 because only one iteration takes several minutes and the standard er-

rors of the simulated results are small enough. The covariates X1, . . . , X14

follow U (0, 2) independently of each other and the censoring time C follows

1 2

U

(0,



)

+

1 2

I(C

=

)

independently

of

the

covariates.

The

censoring

rate

is

about 50% in both models.

We employ the coxph function to compute the partial likelihood estimator

g~n in (15). As for the basis function, we chose an equi-spaced quadratic
B-spline basis B = (B1, . . . , BKn) with Kn = 5. We represent gj in g = (g1, . . . , gp) as gj = jB and define an pKn vector  by  = (1, . . . , Kn)
to describe the algorithms.

12

We approximate lp(g) in a neighborhood of g0 = (g10, . . . , gp0) by

lp(g0)

+

(

-

0)

lp (g0) 

+

1 (
2

-

0)



2lp 

(g0)(

-

0),

(21)

where gj0 = j0B, j = 1, . . . , p, and 0 = (10, . . . , p0). We give the details of the computation of the group SCAD-type estimator.
We use the SCAD function with a = 3.7 and approximate pn(gjL2) in a neighborhood of gj0 by

pn (gj0L2 )

+

p
n

(gj0L2

2gj0L2

)

(gj

L2 2

-

gj0L2 2 )

(22)

as in [13]. The computational algorithm is as follows:
1. Put g^(1) = g~n, where g~n is defined in (15). 2. Approximate Qp(g) in a neighborhood of g^(m) by using (21) and (22) and minimize the approximation. 3. Denote the solution in step 2 by g^(m+1) = (g^1(m+1), . . . , g^p(m+1)). Replace g^j(m+1) with 0 and remove the j from step 2 in the iteration once g^j(m+1)L2  0.01. 4. Iterate steps 2 and 3 until g^(m) - g^(m+1)L2  0.005. Let q be the number of the finally selected covariates.
5. Compute the partial likelihood estimator with the finally selected covariates and denote the partial likelihood estimator by g~qn = (g~qn1, . . . , g~qnp). We put g~qnj = 0 if j is not selected in step 4.
We take n = b(pKn/n)1/2 with b = 0.2, 0.3, 0.4 and select the one with the smallest BIC as the group SCAD estimator, where

BIC = -2 log lp(g~qn) + qKn log n.

We can define AIC by replacing qKn log n with 2qKn. When the number of the covariates is large, it is very time-consuming and impractical to compute the BIC or AIC for all the submodels. It will be very useful to the procedures here and the information criteria simultaneously. We adopt the above definitions of the information criteria and do not enter into the controversy although there is a controversy on how to define the information criteria in the case of the partial likelihood.
We have g~qn = g^n in most replications of the simulations and the average iteration number is 5.8 for selected n.

13

In

Next we describe the computation of the fact we replaced gjL2 with |j|/ Kn in

adaptive group Lasso estimator. (18) as in [26], where gj = jB,

for computational simplicity. Note that Theorems 4 and 5 about the adaptive group Lasso estimator still hold because gjL2 is equivalent to |j|/ Kn.

We also followed the algorithm in [26] except that we used X in the iteration

such that X2 = H and X is symmetric and that we chose the weights as in

Remark 1. They used the Cholesky decomposition of H for X in [26]. They

used the second order approximation in (21) and the KKT condition when

they proposed their algorithm. See [26] for the details of the algorithm. We

apply the same convergence criterion as for the group SCAD estimator. We take n = bpKn/n with b = 0.1, 0.3, 0.5 and select the one with the smallest GCV. In these simulations, we adopt the same GCV as in [26]. The average

iteration number is 6.5 for selected n. We present the simulation results in Tables 1-4. We define the mean

integrated squared error, MISE, of g^j by

E

[

1 


0

{g^j (t)

-

] g0j(t)}2dt .

The MISE of the other estimators is similarly defined. We define IMISE, MISE, and PMISE in Tables 1 and 2 as follows:

IMISE : The MISE of the initial partial likelihood estimator g~j MISE : The MISE of the group SCAD estimator g^j or adaptive group Lasso estimator gØj PMISE : The MISE of the partial likelihood estimator g~qnj, which is computed with the finally selected covariates

The numbers in parentheses are standard errors in Tables 1 and 2. Note
that MISE's of the adaptive group Lasso estimators are large. They are much smaller when we choose and fix a smaller n , for example, b = 0.15.

j=1 j=2

SCAD Lasso SCAD Lasso

Table 1: MISE's of Model 1

IMISE

MISE

0.111(0.004) 0.082(0.003)

0.111(0.004) 0.187(0.003)

0.108(0.004) 0.075(0.002)

0.108(0.004) 0.267(0.004)

PMISE 0.082(0.003) 0.079(0.003) 0.075(0.002) 0.074(0.002)

14

j=1 j=2

SCAD Lasso SCAD Lasso

Table 2: MISE's of Model 2

IMISE

MISE

0.095(0.003) 0.075(0.002)

0.095(0.003) 0.158(0.003)

0.103(0.004) 0.068(0.002)

0.103(0.004) 0.232(0.004)

PMISE 0.075(0.002) 0.074(0.002) 0.068(0.002) 0.067(0.002)

Both estimators selected only the relevant covariates X1 and X2. In every replication of Models 1 and 2, X1 and X2 were selected by the the group SCAD and adaptive group Lasso estimators. Only fifteen and no irrelevant covariates were falsely selected among the total 100 replications of Model 1 by the the group SCAD and adaptive group Lasso estimator, respectively. Only six and two irrelevant covariates were falsely selected respectively in the case of Model 2. In Tables 3 and 4, we present the mean of the number of falsely selected covariates to show how the tuning parameters affect these estimators.
Table 3: Numbers of falsely selected covariates (SCAD)
b 0.2 0.3 0.4 Model 1 6.00 1.10 0.15 Model 2 5.34 0.64 0.06

Table 4: Numbers of falsely selected covariates (Lasso)
b 0.2 0.3 0.4 Model 1 1.98 0.03 0.00 Model 2 1.54 0.02 0.00
We have the following implications from Tables 1-4. 1. Tables 3 and 4 show that selection of tuning parameters n and n is critical to variable selection. 2. The group SCAD estimator is equal to the partial likelihood estimator with the selected variables in most of the replications.
15

3. In Tables 1 and 2, the MISE's of the adaptive group Lasso estimator are much larger than those of the initial partial likelihood estimator. This means that selection of the tuning parameter by GCV may not work well or we should use the adaptive group Lasso estimator only for variable selection. The GCV criterion tends to choose a larger n and cause a larger bias. On the other hand, the tuning parameter selection by BIC works well for the group SCAD estimator.
The simulation studies suggest that the group SCAD estimator with BIC tuning parameter selection works well. We know we should conduct more extensive simulation studies to obtain a conclusion about the tuning parameter selection. However, it takes a lot of time to compute these estimators and the simulation studies of this paper are limited because of the computational time.
Next we present a real example. As in [26], we apply the above two procedures to the well-known PBC (primary biliary cirrhosis) data. The data is often used in the literature of time-varying Cox regression models. PBC is a fatal liver disease and the data is from a trial of comparing the drug D-penicillamine and a placebo. Times to death or censoring are recorded. The details are given in Fleming and Harrington [15] and the survival package of R. We use the first 312 randomized cases and consider only ten covariates among 17 covariates in the data set for numerical stability of the coxph function in the survival package. We remove two cases with missing covariates and our sample size is 310. In this study, we consider the following covariates. We normalize continuous covariates so that the mean is 0 and the variance is 1. 1) treatment indicator (0:placebo, 1:D-penicillamine); 2) normalized age; 3) sex (0:male, 1:female); 4) presence of hepatomegaly (0:no, 1:yes); 5) presence of edema (0, 0.5, 1 according to the severity); 6) normalized log serum bilirubin; 7) normalized serum albumin; 8) normalized urine copper; 9) normalized log prothrombin time; 10) histologic stage of disease (0, 1/3, 2/3, 1)
In this study, we have n = 310, p = 10, and  = 12 years. We take Kn = 4, n = b(pKn/n)1/2 with b = 0.4, 0.5, 0.6, 0.7 for the group SCAD estimator, and n = bpKn/n with b = 0.2, 0.3, 0.4, 0.5, 0.6 for the adaptive group Lasso estimator. We finally select the variables by using AIC and BIC for the group SCAD estimator and AIC, BIC, and GCV for the adaptive group Lasso estimator.
We present the results of variable selection in Table 5. In the table, we also
16

give the squared L2 norms of estimated functions g~nj2L2 for the initial PL estimators. For the SCAD and Lasso estimators, we reestimate the coefficient
functions for selected variables by employing the coxph function and present
the squared L2 norms. Note that we use the two procedures only for variable selection.

Table 5: Selected variables and the squared L2 norms

Initial SCAD SCAD Lasso Lasso

PLE BIC AIC BIC GCV

1) treatment 0.596 0 0.569 0

0

2) age

0.502 0 0.509 0.263 0.263

3) sex

0.616 0 0.509 0

0

4) hepato

0.690 0 1.147 0

0

5) edema

3.320 1.636 2.876 1.708 1.708

6) bilirubin

4.676 4.153 3.638 4.134 4.134

7) albumin

0.841 1.202 0.806 0.650 0.650

8) copper

0.117 0

0 00

9) prothrombin 0.200 0.314 0.234 0.248 0.248

10) stage

0.694 0 1.237 0.610 0.610

Lasso AIC
0 0.259
0 0 1.542 4.749 0.655 0.090 0.265 0.621

Comparing the results here to those in [26], we notice that the urine copper is selected only in the case of Lasso and AIC and that only the Lasso and AIC result coincides with those in [26]. BIC tends to choose smaller sets of covariates due to its penalty term.
We need more extensive simulation studies and real data analysis to examine the finite sample properties of the procedures. It is a topic of future research.
5. Proofs of Theorems 1-5
We prove Theorems 1-5 in this section. First we describe some technical assumptions. Next we state Lemmas 1-4 and Propositions 1-4. Finally we prove the theorems by using the propositions. The proofs of the lemmas and propositions are postponed to section 7. We prove the theorems by following the methodology in [17] and Propositions 1, 2, and 3 correspond to their Lemma 7, Lemma 10, and Lemma 11, respectively.
17

The assumptions A1-3 below are about the Cox regression model with time-varying coefficients.
Assumption A1: There is a suitable filtration {Ft} such that Yi(t) are adapted to {Ft} and (Zi(t), Xi(t)) is predictable w.r.t. {Ft}.
Assumption A2: When we have no censoring time, the hazard function of Ti w.r.t. {Ft} is given by

{ p

}

i(t) = 0(t) exp

g0j(t)Xij(t) = 0(t) exp{g0(t)Xi(t)},

j=1

where 0(t) is an unknown hazard function and g0(t) = (g01(t), . . . , g0p(t)) is a vector of unknown time-varying coefficients.

Assumption A3: The censoring time Ci satisfies the independent censoring condition. This means that the compensator of Ni(t) w.r.t. {Ft} is equal to

t t Zi(s)i(s)ds = Zi(s) exp{g0(s)Xi(s)}0(s)ds.
00

(23)

Then

t Mi(t) = Ni(t) - Zi(s)i(s)ds
0

(24)

is a martingale process w.r.t. {Ft}. We need following technical assumptions on X(t) and Z(t). We set

(t) = Var(X(t)) and (t) = E{X(t)X(t)}.

Let min(A) and max(A) denote the minimum and maximum eigenvalue of a symmetric matrix A.
Assumption X: (i) There are positive constants Cm and CM such that uniformly in t in [0,  ], Cm  min((t))  min((t)) and max((t))  max((t))  CM . (ii) Xij(t) is uniformly bounded in i, j, and t a.s.
The first assumption in Assumption X is easy to check and does not depend on the sample property of X(t). Besides, the density function of X(t) is not necessary. The second one is necessary because we need to evaluate exp{g0(t)Xi(t)}.
Assumption M:

18

(i) There is a positive constant CZ such that E{Z(t)|X(t)}  CZ on [0,  ] a.s. (ii) There are positive constants CL1 and CL2 such that CL1  0(t)  CL2 on [0,  ].
The first assumption in Assumption M is a standard assumption and the second one is necessary since we deal with time-varying coefficient models on [0,  ].
We describe the assumptions on g0(t) = (g01(t), . . . , g0p(t)) on [0,  ].
Assumption G: (i) g0j(t), j = 1, . . . , p, are twice continuously differentiable on [0,  ] and may depend on the sample size n. (ii) There are positive constants Cg0 and Cg2 such that

p p

sup |g0j(t)|  Cg0 and

sup |g0j(t)|  Cg2.

j=1 0t

j=1 0t

We impose the first assumption in Assumption G for simplicity of presentation and the second one is necessary when we evaluate exp{g0(t)Xi(t)} and define the approximation error n.
We state the important properties of the norms introduced in section 2.
The proofs are postponed to section 7. Similar results are given in [17]. We write an  bn when an  C1bn and an  C2bn for some positive constants C1 and C2. The first two lemmas are about the equivalences between the norms and the L2 norm. The last two lemmas evaluates the differences between the norms and the empirical counterparts.

Lemma 1. Suppose that Assumptions A1-3, X, M, B, and G hold. Then uniformly in h  H0,
h  hL2.

Lemma 2. Suppose that Assumptions A1-3, X, M, B, and G hold. Then uniformly in h  H0,
h0  hL2.

Lemma 3. Suppose that Assumptions A1-3, X, M, B, G, and RA hold.

Then

sup
g1,g2G0

(g1, g2)n - (g1, g2) g1g2

= Op(1).

19

Lemma 4. Suppose that Assumptions A1-3, X, M, B, G, and RA hold.

Then

sup
g1,g2G0

(g1, g2)0n - (g1, g2)0 g10g20

= Op(1).

Proposition 1. Suppose that the same assumptions hold as in Theorem 1. Then there is a unique maximizer gn = (gn1, . . . , gnp) of p(g) over G0 satisfying gn - g0 = O(n).

By Assumptions G and RA and Lemma 1 we have gn  C for some
C.
Here we choose an orthonormal basis {1, . . . , pKn} of G0 w.r.t.  ∑  and define an Rp◊(pKn)-valued function (t) on [0,  ] by

(t) = (1(t), . . . , pKn(t)) or  = (1, . . . , pKn).

(25)

This orthonormal basis is just a technical tool as in [17] and the results do
not depend on any particular choice. By using this basis, we can represent gn for some n  RpKn as

gn = n.

(26)

When g = , lp(g) = lp() is represented as

lp()

=

1 n

n 

i=1

0 



{(t)}Xi(t)dNi(t)

( n

[

])

- log n-1 Zi(t) exp {(t)}Xi(t) dNØ (t).

0 i=1

and we write

Sp()

=

lp () 

and

Dp()

=



2lp 

().

(27)

We evaluate Sp(n) in Proposition 2 below and evaluate the eigenvalues of Dp() in Proposition 3 below. Proposition 3 is stated in a more general
form than in [17].

Proposition 2. Suppose that the same assumptions hold as in Theorem 1.

Then we have

|Sp(n )|

=

{( Op

pKn n

)1/2} .

20

Proposition 3. Suppose that the same assumptions hold as in Theorem 1. When   M for some positive M , there are positive constants M1 and M2 such that
-M1  min(Dp())  max(Dp())  -M2
uniformly in  with probability tending to 1.

Proposition 4 below is necessary to the proof of the sparsity.
Proposition 4. Suppose that the same assumptions hold as in Theorem 1. When 1  M for some positive M , we have
(2 - 1)Sp(1) = (2 - 1)Sp(n) + Op(|2 - 1||1 - n|)
uniformly in 1 and 2 with probability tending to 1. Now we start to prove Theorems 1-5.

Proof of Theorem 1. We have only to show that there is a unique maximizer ~n of lp() over G0 such that ~n - n = Op(rpn) with probability tending to 1. Then the desired result follows from Lemma 1 and
Proposition 1.
Define M for a positive constant M by

M = {g =  | | - n| = M (pKn/n)1/2}

and consider the Taylor expansion of lp() at  = n.

lp() = lp(n) + ( - n)Sp(n) + ( - n)Dp(Ø)( - n)(28) = lp(n) + J1() + J2(),

where Ø is between  and n. By Proposition 2, we have uniformly on M ,

J1()

=

M

(pKn) n

Op(1).

(29)

By Proposition 1 and Assumption RA, we have

  n + AnCM (pKn/n)1/2  C.

(30)

21

We have by Proposition 3 and (30) that for some positive constant M2,

J2()  -M 2M2pKn/n

(31)

uniformly on M with probability tending to 1. Combining (28), (29), and (31), we obtain

()

lim lim P
M  n

sup lp() < lp(n)
M

= 1.

(32)

The concavity of lp(), Proposition 3, and (32) imply that there is a unique maximizer ~n of lp() over G0 such that ~n - n = Op(rpn). Hence the proof of Theorem 1 is complete.

Before the proof of Theorem 2, we define gØn by

gØn = (gn1, . . . , gns, 0, . . . , 0).

(33)

Recall that gn is given in Proposition 1. Proposition 1 implies that gØn - g0 = O(n) under Assumption S.

Proof of Theorem 2. We have only to demonstrate that for any positive

, there is a positive constant M such that

()

lim P
n

sup Qp(g) < Qp(gØn)
g Ø M

> 1 - ,

(34)

where ØM = {g =  | | - Øn| = M rpn} and gØn = Øn. Write

Qp(g) - Qp(gØn) = {lp() - lp(Øn)}

(35)

{ p

p }

+ - pn(()jL2) + pn((Øn)jL2)

j=1 j=1

= J3() + J4(),

where ()j and (Øn)j are the jth element of  and Øn. We evaluate J3() on ØM as in the proof of Theorem 1.

J3() = ( - Øn)Sp(Øn) + ( - Øn)Dp(Ø)( - Øn) = J31() + J32(),

(36)

22

where Ø is between  and Øn. By applying Proposition 2 and Proposition 4 with 1 = Øn, we obtain on ØM ,

J31() = M rpnOp(rpn) + Op(M rpnn) = Op(M rp2n).

(37)

Proposition 3 implies that there is a positive constant M2 such that

J32()  -M2M 2rp2n

(38)

uniformly on ØM with probability tending to 1. Thus (37) and (38) yield that there is a positive constant M such that

()

lim lim P
M  n

sup J3()  -M2M 2rp2n/2
ØM

=1

(39)

Next we deal with J4(). Here recall that gØnj = (Øn)j = 0, j = s + 1, . . . , p, min1js g0jL2/n  , and gØnj - g0jL2 = O(n) = O(n), j =
1, . . . , p. By Lemma 1 and the above facts, we have on ØM ,

()jL2 > a0n and (Øn)jL2 > a0n, j = 1, . . . , s, (40)

and ()jL2  Crpn = O(n), j = s + 1, . . . , p.
We obtain by Assumption P, (40), and (41) that

(41)

p

J4() = -

pn(()jL2)  0 on ØM .

j=s+1

(42)

(34) follows from (35), (39), and (42). Hence the proof of Theorem 2 is complete.

Proof of Theorem 3. Let g^n = (g^n1, . . . , g^np) be a local maximizer of Qp(g) satisfying the condition of Theorem 3. First we establish the sparsity (i).
We choose l from {s + 1, . . . , p}. If g^nl = 0, we replace g^nl of g^n with 0 and denote it by g^nl. We also define g^t, 0  t  1, by

g^t = g^n + t(g^nl - g^n) = (1 - t)g^n + tg^ln

(43)

23

and compare Qp(g^t) and Qp(g^n). Conditions on g^n imply that g^n  g0 + Andnrpn = O(1) and 0 < g^nl  dnrpn = O(n). Note that g^nl - g^nL2 = g^nlL2.
We represent Qp(g^t) - Qp(g^n) as

Qp(g^t) - Qp(g^n) = {lp(g^t) - lp(g^n)} + {-pn((1 - t)g^nlL2) + pn(g^nlL2)} = J5 + J6.

(44)

It is easy to see that for some tØ  [0, t],

J6 = tg^nlL2pn((1 - tØ)g^nlL2).

(45)

We evaluate J5 by employing Propositions 2-4. By using the orthonormal basis, we can represent g^n and g^t as g^n = ^n and g^t = ^t for some ^n  RpKn and ^t  RpKn, respectively. Then we have

J5 = lp(^t) - lp(^n) = (^t - ^n)Sp(^n) + (^t - ^n)Dp(Øt)(^t - ^n) = J51 + J52,

(46)

where Øt is between ^t and ^n. By Propositions 2 and 4, we have uniformly in t,

J51 = tg^nl - g^n{Op(rpn) + Op(dnrpn)} = tg^nlL2Op(dnrpn). By Proposition 3, we have uniformly in t,

(47)

J52 = t2g^nl - g^n2Op(1) = tg^nlL2Op(dnrpn). (47) and (48) imply that uniformly in t,

(48)

J5 = tg^nlL2Op(dnrpn).

(49)

Note that Op(dnrpn) in (47) and (48) are independent of any particular choice of g^n.
By combining (45), (49), and Assumption P, we have uniformly in t  (0, 1/2),

Qp(g^t) - Qp(g^n) = tg^nlL2{Op(dnrpn) + p n((1 - tØ)g^nlL2)} > 0 (50)

24

with probability tending to 1. This contradicts the local optimality of g^n. Hence we have g^nl = 0, l = s + 1, . . . , p, for any local maximizer g^n in Theorem 3 with probability tending to 1. Hence the proof of the latter half
is complete.
Next we prove that the local maximizer in Theorem 3 has the oracle
property. Let g^n be the local maximizer in Theorem 3 such that g^nj = 0, j = s + 1, . . . , p. Then we consider g^n + , where  = (1, . . . , p)  G0 and j = 0, j = s + 1, . . . , p. By Assumption P and the local optimality of g^n, there is a small positive constant  such that

s s pn(g^nj + jL2) = pn(g^njL2) and Qp(g^n + )  Qp(g^n)
j=1 j=1

(51)

when L2  . Since ls(g) = lp(g) with gj = 0, j = s + 1, . . . , p, we have by (51) that

ls(g^n + ) = lp(g^n + )  lp(g^n) = ls(g^n).

(52)

(52) means that g^n is a local maximizer of ls(g). Since the assumptions in Theorem 1 are satisfied with p replaced with s, g^n is the unique maximizer of ls(g) with probability tending to 1 and has the desired convergence rate. Hence the latter half of Theorem 3 is established.

Proof of Theorem 4. First as in the proof of Theorem 2, we demonstrate

that for any positive , there is a positive constant M such that

()

lim P
n

sup QØp(g) < QØp(gØn)
g Ø M

> 1 - .

(53)

Write

QØp(g) - QØp(gØn) = {lp() - lp(Øn)}

(54)

{ p

p }

+ - nwj()jL2 + nwj(Øn)jL2

j=1 j=1

= J3() + J7().

We evaluated J3() on ØM as in the proof of Theorem 2. See (39). We evaluate J7() on ØM and obtain

J7()



s n wj ((
j=1

-

Øn ))jL2



nM srp2n max1js wj . rpn

(55)

25

(53) follows from (39) and (55). (53) implies that a local maximizer gØn of QØp(g) exists inside ØM with probability tending to 1. Besides, QØp() is strictly concave inside ØM with probability tending to 1 by Proposition 3. Therefore the local maximizer must be a unique maximizer with probability tending to 1. Hence the proof of Theorem 4 is complete.

Proof of Theorem 5. We proceed as in the proof of Theorem 3. We define g^n, g^nl, g^t, ^n, and ^t as in the proof of Theorem 3. Then choosing l
as in the proof of Theorem 3, we have

QØp(g^t) - QØp(g^n)

= {lp(g^t) - lp(g^n)} + nwltg^nlL2

= =

tg^nl tg^nl

L2 L2

Op(rpn) + {
rpn Op(1)

n +

wlntwg^l n}l>L2 rpn

0

uniformly in t  (0, 1/2) with probability tending to 1. The above inequality contradicts the optimality of g^n. Hence g^nlL2 = 0, l = s + 1, . . . , p with probability tending to 1. The sparsity and the optimality of g^n implies that g^n is equal to the unique maximizer of QØs(g). Since the assumptions in Theorem 4 are satisfied with p replaced with s, we have the desired L2 convergence rate. Hence the proof of Theorem 5 is complete.

6. Coefficients varying with U (t)

In this section, we consider another Cox regression model with varying coefficients. We can establish almost the same theoretical results as in section 3 by using the results in [17]. The proofs are similar to those of Theorems 1-5 and we present only the assumptions, the theorems, and a remark on the proofs.
We observe two kinds of covariates, X(t) and U (t), and U (t) is an index variable. It is reasonable to assume that the coefficients of the p-dimensional covariate X(t) are functions of U (t). Specifically, we observe another important influential covariate U (t) in addition to (Y, , X(t)) and the hazard function of the failure time T is given by

i(t) = 0(t) exp{g0(Ui(t))Xi(t)}

(56)

26

instead of (1). We assume that U (t) is one-dimensional for simplicity of
presentation. In this setup, suppose that we have n i.i.d. observations (Yi, i, Xi(t), Ui(t)) on [0,  ], where Xi(t) = (Xi1(t), . . . , Xip(t)), and carry out variable selection and estimation of g0 simultaneously. In this section, we take Xi1(t) = 1 on [0,  ] and always include Xi1(t) in the model.
We describe the assumptions and the norms before we present the the-
oretical results. We assumed Assumptions A1-3 for the time-varying coeffi-
cient model. We should just replace Xi(t) and g0(t) with (Xi(t), Ui(t)) and g0(Ui(t)) or g0(u), respectively in those assumptions. We call them Assumptions A1'-3' in this section. This model and related ones are considered in
several papers, for example, [12], [8], and [10]. They employed local polyno-
mial regression to estimate the nonparametric components with p fixed and
examined the asymptotics.
We use almost the same notation and assumptions as in the time-varying
coefficient case. However, some conformable changes below are in order. We set (U (t), t) = E{X(t)X(t) | U (t)} and denote the density function of U (t) by ft(u).
Assumption X': (i) There are positive constants Cm and CM such that Cm  min((U (t), t))  max((U (t), t))  CM uniformly in t a.s. (ii) Xij(t) is uniformly bounded in i, j, and t a.s. (iii) The support of U (t) is [0, 1] and there are positive constants CL and CU such that Cl < ft(u) < CU uniformly in t.

Assumption M': (i) There is a positive constant CZ such that E{Z(t)|X(t), U (t)}  CZ uniformly in t a.s.
(ii) There are positive constants CL1 and CL2 such that CL1  0(t)  CL2
on [0,  ].

In Assumptions G and B, t and  should be replaced with u and 1,
respectively and we call them Assumptions G' and B', respectively. We add
an identifiability constraint to Assumption G' later in this section. We define H0, G0,  ∑ , and  ∑ L2 almost in the same way as in section
2. For example, for g(u) = (g1(u), . . . , gp(u)) on [0, 1],

p p  1

gL2 2 =

gjL2 2 =

gj2(u)du.

j=1 j=1 0

(57)

27

We can define n, rpn, rsn, and Assumption RA in the same way as in section 2.

It is crucial to introduce suitable norms and an suitable identifiability

constraint on H0 and G0 when we employ the results of [17]. One reason is that we have lp(h) = lp(h+ce1) in this model, where e1 = (1, 0, . . . , 0)  Rp

and lp(h) is the partial likelihood defined in (64) below. Therefore we adapt

 ∑  and  ∑ 0 to this setup and impose an identifiability constraint as in [17].

For this model, we define h on H0 in terms of the inner product defined

in (58) below.



(h1, h2) = (h1, h2)Z(t)d E{N (t)},

(58)

0

where

(h1, h2)Z(t) = E[{h1(U (t))X(t)}{h2(U (t))X(t)}Z(t)]/ E{Z(t)}.

The identifiability constraint is imposed through the equation


0

E{h(U (t))X(t)Z E{Z (t)}

(t)}

d

E{N

(t)}

=

0.

(59)

As in [17], we define H by

H = {h  H0| h satisfies (59).}.

For any h  H0, we have h - (m, 0, . . . , 0)  H,

(60)

where

m

=

[E{Z( )}]-1


0

E{h(U (t))X(t)Z E{Z (t)}

(t)}

d

E{N

(t)},

and any element of H can be written as in (60). This means the constraint

(59) affects only the first element of h. Hereafter we assume that g0  H and include this into Assumption G'. We define h0 on H in terms of the inner product in (61) below.

(h1, h2)0

(61)

= [E{Z(t)}]-1 E([h1 (U (t))X(t) - E{h1 (U (t))X(t)}/ E{Z(t)}]

0

◊[h2(U (t))X(t) - E{h2 (U (t))X(t)}/ E{Z(t)}]Z(t))d E{N (t)}.

28

We also define the empirical versions of h and h0 by replacing E(∑) with the empirical measure En(∑) in the definitions. We denote the empirical versions by hn and h0n as in [17] and the time-varying case. The empirical version of the constraint (59) is also given by replacing E(∑) with the empirical measure En(∑) in (59). Then we define G by

G = {g  G0| g satisfies the empirical version of (59).}.

For any g  G0, we have g - (mn, 0, . . . , 0)  G,

(62)

where

mn

=

[En{Z( )}]-1


0

En{g

(U (t))X(t)Z En {Z (t)}

(t)}

dEn

{N

(t)}.

On the other hand, any element of G can be written as in (62). This empirical
identifiability constraint also affects only the first element of g. Note that the dimension of G0 is pKn and that of G is pKn - 1.
The equivalence between h and hL2 on H0 and that between h0 and hL2 on H are crucial to the derivation of the main results as in [17] and we state them in Lemmas 5-6. If only those equivalences are established,
we can proceed as in the time-varying case and [17] with just conformable
changes. The proofs of Lemmas 5-8 are postponed to section 7.

Lemma 5. Suppose that Assumptions A1'-3', X', M', B', and G' hold. Then

uniformly in h  H0,

h  hL2.

Lemma 6. Suppose that Assumptions A1'-3', X', M', B', and G' hold. Then uniformly in h  H,
h0  hL2.

Next we consider the empirical versions.

Lemma 7. Suppose that Assumptions A1'-3', X', M', B', G', and RA hold.

Then

sup
g1,g2G0

(g1, g2)n - (g1, g2) g1g2

= Op(1).

29

In [17], another function space G~ is introduced for technical reasons and
we follow them. As we comment in Remark 3 below, the difference between G and G~ defined in (63) below does not affect the proofs of Theorems 6-10.
We define the theoretical version G~ of G by

G~ = {g  G0| g satisfies (59).}

(63)

Lemma 8. Suppose that Assumptions A1'-3', X', M', B', G', and RA hold.

Then

sup
g1,g2G~

(g1, g2)0n - (g1, g2)0 g10g20

= Op(1).

Now we define the partial likelihood lp(h) and the expected value version p(h) for h  G or H as in the time-varying case and [17].

lp(h)

=

1 n

n  

i=1

0 

h(Ui(t))Xi(t)dNi(t)

[ n

{

(64) }]

- log n-1 Zi(t) exp h(Ui(t))Xi(t) dNØ (t)

0 i=1

and

{ 

}

p(h) = E

h(U (t))X(t)dN (t)

(65)

0  ( [

{

}])

- log E Z(t) exp h(U (t))X(t) d E{N (t)}.

0

To carry out variable selection and estimation of g0 simultaneously, we define Qp(g) and QØp(g) for g  G as in section 2. The former gives the group SCAD-type estimator and the latter gives the adaptive group Lasso estimator.

p Qp(g) = lp(g) - pn(gjL2)
j=2
p QØp(g) = lp(g) - n wjgjL2
j=2

(66) (67)

30

We state the properties of the maximum partial likelihood estimator g~n, the group SCAD-type estimator g^n, and the adaptive group Lasso estimator gØn. We define them as in (14) and (19) by replacing G0 with G. Especially we are interested in the sparsity and oracle property of g^n and gØn under Assumption S. We can also define ls(g), Qs(g), and QØs(g) by taking Xi(t) = (Xi1(t), . . . , Xis(t)) and g = (g1, . . . , gs) as in section 2.
Now we state the main theoretical results of this section. Recall that g0  H in this paper. The L2 convergence rate of the maximum partial likelihood estimator is given in Theorem 6. The SCAD-type estimator and the adaptive
group Lasso estimator are considered in Theorems 7-8 and Theorems 9-10,
respectively.

Theorem 6. Suppose that Assumptions A1'-3', X', M', G', B', and RA hold and rpn  0. Then with probability tending to 1, there is a unique maximizer g~n = (g~n1, . . . , g~np) of lp(g) over G and we have
g~n - g0L2 = Op(rpn).
Theorem 7. Suppose that all the assumptions in Theorem 6 and Assumptions P and S hold. Then for any positive , there is a positive constant M such that

lim P(
n

T here

is

a

local

maximizer

g^n

of Qp(g) over G such that g^n - g0L2  M rpn) > 1 - .

Theorem 8. Suppose that the assumptions in Theorem 7 hold and let {dn} be a sequence of positive numbers satisfying dn  , n/(dnrpn)  , and Andnrpn = O(1). (i) With probability tending to 1, any local maximizer g^n = (g^n1, . . . , g^np) of Qp(g) over G satisfying g^n - g0L2  dnrpn has the sparsity. (ii) With probability tending to 1, the local maximizer in (i) is the unique maximizer of ls(g) and satisfies
s g^nj - g0j2L2 = O(rs2n).
j=1

Theorem 9. hold and that

Snuppsomseatxh2atjths ewaj/srsupnm=ptiOonp(s1i)n.

Theorem 6 Then with

and Assumption S probability tending

31

to 1, there is a unique maximizer gØn = (gØn1, . . . , gØnp) of QØp(g) over G and we have
gØn - g0L2 = Op(rpn).

Rtaekme warj k=3.1/Wg~enjcoLn2.sidIefrwtehehawveeighn ts s{/w(rjp}n

in this remark. min2js g0j L2

)

Suppose we = O(1) and

min2js g0jL2/rpn  , the assumption on {wj} in Theorem 9 is satisfied. If we also have n/rp2n  , the assumptions on {wj} in Theorem 10

below are also satisfied.

Theorem 10. Suppose that the assumptions in Theorem 9 hold and that n mins<jp wj/(rpn)   in probability and Anrpn = O(1). Then with probability tending to 1, the unique maximizer gØn has the sparsity and is equal to the unique maximizer of QØs(g). In addition we have
s gØnj - g0j2L2 = Op(rs2n).
j=1
We give two remarks here. One is about the proofs of Theorems 6-10 and the other is about computation of the estimators.
Remark 4. In [17], the authors maximize lp(g) over G~ , not G, in the proofs and they examined the asymptotics of the maximizer over G~ closely. And then they proved that the difference between the maximizer over G~ and that over G is Op(rpn). This method also works well in the setup of this paper because the identifiability constraint affects only the first element of g, g1, and the penalty term does not include the first element. Besides, when we obtain the maximizer over G from that over G~ as in (62), the difference mn does not have any influences on the L2 convergence rate as in the proof of Lemma 9 of [17].

Remark 5. We can use any basis and identifiability constraint on the first
element of g, g1, when we compute g~n, g^n, and gØn. An orthonormal basis is chosen in the proofs only for technical reasons and the partial likelihood or gjL2 does not depend on any particular choice of basis. Besides, identifiability constraints on g1 does not affect gj, j = 2, . . . , p. Therefore we can use a B-spline basis and an identifiability constraint g1(0) = 0 when we compute g~n, g^n, and gØn.

32

7. Technical proofs

Lemmas 1-8 and Propositions 1-4 are proved in this section.

Proof of Lemma 1. It is easy to see that E[{X(t)h(t)}2Z(t)] = E[{X(t)h(t)}2 E{Z(t) | X(t)}]  h(t)(t)h(t)

uniformly in t and h due to Assumptions X and M.

Thus we have uniformly in h,

h2

=

 0 

E[{X

(t)h(t)}2Z E{Z (t)}

(t)]

d

E{N

(t)}

 |h(t)|2 E[exp{g0(t)X(t)}]0(t)dt

0

 h2L2.

Note that |g0(t)X(t)| is uniformly bounded in t by Assumptions M and G. Hence the proof of Lemma 1 is complete.

Proof of Lemma 2. We have uniformly in h, 

h02 =

[E{Z(t)}]-1 E([h(t)X(t) - E{h(t)X(t)Z(t)}/ E{Z(t)}]2

0

  ◊Z(t))d E{N (t)}

= inf([E{Z(t)}]-1 E[{h(t)X(t) - c}2Z(t)])d E{N (t)} 0  c

 inf E[{h(t)X(t) - c}2Z(t)]d E{N (t)}.
0c

We evaluate the integrand and obtain

inf E[{h(t)X(t) - c}2Z(t)]
c
= inf E[{h(t)X(t) - c}2 E{Z(t)|X(t)}]
c
 inf E[{h(t)X(t) - c}2]
c
= h(t)(t)h(t)  |h(t)|2.

uniformly in h due to Assumptions M and X. Thus we have uniformly in h, 
h02  |h(t)|2 E[exp{g0(t)X(t)}]0(t)dt  h2L2.
0
Hence the proof of Lemma 2 is complete.

33

Proof of Lemma 3. We omit the details since we can prove Lemma 3
almost in the same way as Lemma 3 of [17] by just replacing f1(X(t)) and f2(X(t)) there with g1(t)X(t) and g2(t)X(t), respectively. We assume that p can increase moderately. However, this does not affect the application of
Lemma 10 of [16] since Nn in [17] increases , too. Notice the typo in the definition of I3 there.

Proof of Lemma 4. We omit the details since we can also prove Lemma
4 almost in the same way as Lemma 4 of [17] by just replacing f1(X(t)), f2(X(t)), and i(X(t)) with g1(t)X(t), g2(t)X(t), and i (t)X(t), respectively. Note that G~ is replaced with G0 in the time varying coefficient case.
The authors of [17] used their Lemma 5 in the proof of their Lemma 4. We can verify their Lemma 5 by replacing i(X(t)) and hn(X(t)) with i(t)X(t) and 1, respectively.

Before we prove Proposition 1, we state Lemma 9, which corresponds to Lemma 8 of [17]. Recall that g0 is the true coefficient function and g0  H0.

Lemma 9. Suppose that the same assumptions hold as in Theorem 1. Then for any positive M , there are positive constants M1 and M2 such that
-M1h - g02  p(h) - p(g0)  -M2h - g02
for any h  H0 satisfying h  M .

Proof. This lemma can be proved almost in the same way as Lemma 8 of [17]. We should replace  with g0 and set hu = g0 + u(h - g0).
Then we have

d

du

p(hu) { u= 0

}

=E

(h - g0)(t)X(t)dN (t)

0  -
0

E[(h - g0)(t)X(t)Z(t) exp{g0(t)X(t)}] E[Z(t) exp{g0(t)X(t)}]

[   ◊E[Z(t) exp{g0(t)X(t)}]0(t)dt

]

= E (h - g0)(t)X(t)Z(t) exp{g0(t)X(t)}0(t)dt

0 

- E[(h - g0)(t)X(t)Z(t) exp{g0(t)X(t)}]0(t)dt

0

= 0.

34

We used (24) in the above equations. Thus we can proceed as in [17] with conformable changes. The details are omitted.

Proof of Proposition 1. As in [17], we can easily prove that p(h) is strictly concave by calculating the second derivative of p(th1 + (1 - t)h2). The definition of n implies that there is gan  G0 such that gan - g0  2n. Hence gan - g0  2n for any sufficiently large n and we have by Lemma 9,
-4M1n2 + p(g0)  p(gan).
On the other hand, we have
p(g)  p(g0) - M32M22n
on {g  G0 | g - g0 = M3n} for M3 > 2. Here we used the fact that we have eventually

{g  G0 | g - g0 = M3n}  {g  G0 | g  C} for some fixed C by Assumption RA. If we choose M3 such that M3 >
4M1/M2, we have on {g  G0 | g - g0 = M3n},

p(g) < p(gan).

(68)

The existence of a unique maximizer gn satisfying gn - g0 = O(n) follows from (68) and the concavity of p(h). Hence the proof of Proposition
1 is complete.

Proof of Proposition 2. We prove this proposition by following the proof

of Lemma 10 of [17].

Write the jth element of Sp(n) as

Spj (n )

=

1 n

n 

i=1

0 

-

0


j(t)Xi(t)dNi(t)

n
i=1

jin=(t1)ZXi(i(tt))eZxip(t{)genxp({t)gXni((tt))}Xi(t)}

dNØ

(t).

Because of the

 j

p(n )

optimality {

of


gn

=

n ,

we

have }

=E

j (t)X(t)dN (t)

0  -
0

E[j (t)X(t)Z(t) exp{gn(t)X(t)}] E[Z(t) exp{gn(t)X(t)}]

d

E{N

(69) (t)}

= 0.

35

We use (69) to evaluate Spj(n). Write Spj(n) = J8j - J9j - J10j,

(70)

where

J8j

= =

1 n
1 n

n
i=1
n
i=1
1 +
n

 j(t)Xi(t)dNi(t) -
0
 j (t)Xi(t)dMi(t)
0
n   j (t)Xi(t)Zi(t)
[i=1  0

{ 

}

E j (t)X(t)dN (t)

0

exp{g0(t)Xi(t)}0(t)dt ]

(71)

- E j (t)X(t)Z(t) exp{g0(t)X(t)}0(t)dt ,

J9j

=


0

( n 0
i=1

jin=(t1)ZXi(i(tt))eZxip(t{)genxp({t)gXni((tt))}Xi(t)}

-

E[j (t)X(t)Z(t) exp{gn(t)X E[Z(t) exp{gn(t)X(t)}]

(t)}]

) dNØ

(t),

and J10j

=


0

E[j

(t)X(t)Z(t) exp{gn(t)X E[Z(t) exp{gn(t)X(t)}]

(t)}]

d[NØ

(t)

-

E{N (t)}].

We consider J8j, J9j, and J10j.

By (23), (24), and (71), we obtain

E{J82j }

 

2 n
1 n

[ 

]

E {j (t)X(t)}2Z(t) exp{g0(t)X(t)}0(t)dt

2 +
n {

0
E 

([



0

j (t)X(t)Z(t) }

]2) exp{g0 (t)X (t)}0 (t)dt

E j (t)(t)j(t)dt

0



1 n jL2



1 n

j

2.

The above inequality yields

( pKn ) E J82j



C n

pKn j 2

=

CpKn . n

j=1 j=1

(72)

36

As in [17], we have

pKn J92j
j=1



pKn
j=1


0

( n
i=1

jni=(t1)ZXi(i(tt))eZxip(t{)genxp({t)gXni((tt))}Xi(t)}

(73)

-

E[j(t)X(t)Z(t) exp{gn(t)X E[Z(t) exp{gn(t)X(t)}]

(t)}]

)2dNØ

(t).

We can evaluate the expectation of (73) by following the proof of Lemma 5 of [17] with hn(X(t)) and j(X(t)) replaced with 1 and j(t)X(t), respectively. Thus we have

pKn E{J92j }



CpKn . n

j=1

(74)

Finally we deal with J10j. Since J10j is just a sample mean, we have

pKn E(J120j )

(75)

j=1



1 n

pKn {(   E
j=1 0

E[j(t)X(t)Z(t) exp{gn(t)X E[Z(t) exp{gn(t)X(t)}]

(t)}]

dN

)2} (t)



C n

pKn
j=1

 [E{j (t)X(t)Z(t)}]2d
0

E{N (t)}



C n

pKn  
j=1 0

|j (t)|2 dt



C n

pKn j 2L2

j=1



CpKn . n

The desired result follows from (70), (72), (74), and (75). Hence the proof of Proposition 2 is complete.

Proof of Proposition 3. Note that   M in this proposition and take 1  RpKn. With probability tending to 1, we have uniformly in  and

37

1,
1==Dp---()000-1i{i(nnccffinin==inin11==ZZ11 iZZi((tiit(()ni)tt=))11[[1{{Ziinin(((==nittt(=(11)))tt1eZZ))XxZX11iip((}}iitt(i[())({ttXXt)e))xeX(eiixp((txpi)tt[p{))[({[--t}{)(t(ccX)(t]](22t)t)id)e}(xN}tØ})p1X]([eXt{Xx)i(pit(i([)(t{t]t)))]]()}t2)}Xd}iN(ØXt()t]i)d(tNØ)](t)  -102n  -102  -12L2  -12  |1|2
Hence the desired result is established.

Proof of Proposition 4. We have

(2 - 1)Sp(1) = (2 - 1)Sp(n) + (2 - 1)Dp(Ø)(1 - n),
where Ø is between 1 and n. Since 1  M and n = gn  g0 + CAnn for some
positive constant C, Proposition 3 implies there are positive constants M1 and M2 such that with probability tending to 1,
-M1  min(Dp(Ø))  max(Dp(Ø))  -M2

uniformly in 1. Hence we obtain (2 - 1)Dp(Ø)(1 - n) = Op(|2 - 1||1 - n|)

uniformly in 1 and 2 and the proof of Proposition 4 is complete.

Proof of Lemma 5. We have uniformly in t and h = (h1, . . . , hp)  H0,

E[{h(U (t))X(t)}2Z(t)]/ E{Z(t)}

= E[{h(U (t))X(t)}2 E{Z(t)|X(t), U (t)}]/ E{Z(t)}

 E[{h(U (t))X(t)}2]

= E{h(U (t))(U (t), t)h(U (t))}

{ p

}

E

h2j (U (t))  h2L2.

j=1

38

We used Assumptions X' and M' here. Hence

h2

=


0

E[{h(U (t))X(t)}2Z E{Z (t)}

(t)]

d

E{N

(t)}

 hL2 E{N ( )}  h2L2

uniformly in h  H0 and the desired result is established.

Proof of Lemma 6. First recall that the first element of X(t) is 1. We obtain as in the proof of Lemma 5,

E[{h(U (t))X(t) - E{Z (t)}

c}2 Z (t)]



h1

-

c2L2

+

p hj L2 2

j=2

uniformly in c, t, and h = (h1, . . . , hp)  H. Thus we obtain

h20 =


0

1 E{Z (t)}

([ E h(U (t))X(t)

-

E{h(U (t))X(t)Z(t)} ]2 ) Z (t)
E{Z (t)}

◊d E{N (t)}

=

 0 

inf
c

( (

E[{h(U

(t))X(t) - E{Z (t)} p

c}2 Z (t)] )

) d

E{N

(t)}



0

inf
c

h1 - c2L2 +

hj L2 2

j=2

d E{N (t)}

 (

p )

 

inf
c
inf

0


h1 - c2L2 + hj2L2 d E{N (t)}

j=2

(

E[{h(U

(t))X

(t)

-

c}2Z

(t)]

) d

E{N

(t)}

=

c

0

0 E{Z(t)}

E[{h

(U (t))X(t)}2Z E{Z (t)}

(t)]

d

E{N

(t)}



h2L2 .

We used the identifiability constraint (59) in the last line. Hence the proof of Lemma 6 is complete.

Proof of Lemma 7. We omit the details since we can prove Lemma 7 almost in the same way as Lemma 3 of [17] by just replacing f1(X(t)) and

39

f2(X(t)) there with g1(U (t))X(t) and g1(U (t))X(t), respectively. However, this does not affect the application of Lemma 10 of [16] since Nn, which is the counterpart of pKn, also increases in [17].
Proof of Lemma 8. We omit the details since we can also prove Lemma 8 almost in the same way as Lemma 4 of [17] by just replacing f1(X(t)), f2(X(t)), and i(X(t)) with g1(U (t))X(t), g2(U (t))X(t), and i (U (t))X(t), respectively. The authors of [17] used their Lemma 5 in the proof of their Lemma 4. We can also verify their Lemma 5 by replacing i(X(t)) and hn(X(t)) with i(U (t))X(t) and 1, respectively.
References
[1] R. Bender, T. Augustin, M. Blettner, Generating survival times to simulate Cox proportional hazards models, Statist. in Medicine 24 (2005) 1713-1723.
[2] J. Bradic, J. Fan, J. Jiang, Regularization for Cox's proportional hazards model with NP-dimensionality, Ann. Statist. 39 (2011) 3092-3120.
[3] J. Bradic, R. Song, Gaussian oracle inequalities for structured selection in non-parametric partial likelihood, arXiv:1207.4510v1[math.ST].
[4] P. Bu®hlmann, S. van de Geer, Statistics for High-Dimensional Data: Methods Theory and Applications, Springer, New York, Dordrecht, Heidelberg, London, 2011.
[5] J. Cai, J. Fan, J. Jiang, H. Zhou, Partially linear hazard regression for multivariate survival data, J. Amer. Statist. Assoc. 102 (2007) 538-551.
[6] J. Cai, J. Fan, J. Jiang, H. Zhou, Partially linear hazard regression with varying coefficients for multivariate survival data, J. R. Statist. Soc. B 70 (2008) 141-158.
[7] J. Cai, J. Fan, R. Li, H. Zhou. Variable selection for multivariate failure time data, Boimetrika 92 (2005) 303-316.
[8] J. Cai, J. Fan, H. Zhou, Y. Zhou, Hazard models with varying coefficients for multivariate failure time data, Ann. Statist. 35 (2007) 324-354.
40

[9] Z. Cai, Y. Sun, Local linear estimation for time-dependent coefficients in Cox's regression models, Scand. J. Statist. 30 (2003) 93-111.
[10] K. Chen, H. Lin, Y. Zhou, Efficient estimation for the Cox model with varying coefficients, Biometrika 99 (2012) 379-392.
[11] P. Du, S. Ma, H. Liang, Penalized variable selection procedure for Cox models with semiparametric relative risk, Ann. Statist. 38 (2010) 20922117.
[12] J. Fan, H. Lin, Y. Zhou, Local partial-likelihood estimation for lifetime data, Ann. Statist. 34 (2006) 290-325.
[13] J. Fan, R. Li, Variable selection via nonconcave penalized likelihood and its oracle properties, J. Amer. Statist. Assoc. 96 (2001) 1348-1360.
[14] J. Fan, J. Lv, Nonconcave penalized likelihood with NP-dimensionality, IEEE transaction on Information Theory 57 (2011) 5467-5484.
[15] T. R. Fleming, D. P. Harrington, Counting Processes and Survival Analysis, Wiley, Hoboken, NJ, 1991.
[16] J. Z. Huang, Projection estimation in multiple regression with application to functional ANOVA models, Ann. Statist. 26 (1998) 242-272.
[17] J. Z. Huang, C. Kooperberg, C. J. Stone, Y. K. Truong, Functional ANOVA modeling for proportional hazards regression, Ann. Statist. 28 (2000) 961-999.
[18] J. Z. Huang, C. J. Stone, The L2 rate of convergence for event history regression with time-dependent covariates, Scand. J. Statist. 25 (1998) 603-620.
[19] C. Leng, H. H. Zhang, The L2 rate of convergence for event history regression with time-dependent covariates, J. Nonparametric Statist. 18 (2006) 417-429.
[20] H. Lian, Variable selection for high-dimensional generalized varying-coefficient models, forthcoming in Statistica Sinica, doi:10.5705/ss.2010.308.
41

[21] L. Meier, S. van de Geer, P. Bu®hlmann, The group lasso for logistic regression, J. R. Statist. Soc. B 70 (2008) 53-71.
[22] H. S. Noh, B. U. Park, Sparse variable coefficient models for longitudinal data, Statistica Sinica 20 (2010) 1183-1202.
[23] L. L. Schumaker, Spline Functions: Basic Theory 3rd ed, Cambridge University Press, Cambridge, 2007.
[24] L. Tian, D. Zucker, L. J. Wei, On the Cox model with time-varying regression coefficients, J. Amer. Statist. Assoc. 100 (2005) 172-183.
[25] R. J. Tibshirani, Regression shrinkage and selection via the Lasso, J. R. Statist. Soc. B 58 (1996) 267-288.
[26] J. Yan, J. Hunag, Model selection for Cox models with time-varying Coefficients, Biometrics 68 (2012) 419-428.
[27] M. Yuan, Y. Lin, Model selection and estimation in regression with grouped variables, J. R. Statist. Soc. B 68 (2006) 49-67.
[28] L. Wang, H. Li, J. Z. Huang, Variable selection in nonparametric varying-coefficient models for analysis of repeated measurements, J. Amer. Statist. Assoc. 103 (2008) 172-183.
[29] S. Wang, B. Nan, N. Zhou, J. Zhu, Hierarchically penalized Cox regression with grouped variables, Biometrika 96 (2009) 307-322.
[30] F. Wei, J. Huang, H. Li Variable selection and estimation in highdimensional varying-coefficient models, Statistica Sinica 21 (2011) 15151540.
[31] H. H. Zhang, G. Cheng, Y. Liu, Linear or nonlinear? Automatic structure discovery for partially linear models, J. Amer. Statist. Assoc. 106 (2011) 1099-1112.
[32] H. H. Zhang, W. Lu, Adaptive Lasso for Cox's proportional hazards model, Boimetrika 94 (2007) 691-703.
[33] H. Zou, The adaptive Lasso and its oracle properties, J. Amer. Statist. Assoc. 101 (2006) 1418-1429.
42

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "HMM in dynamic HAC models" by Wolfgang Karl H‰rdle, Ostap Okhrin and Weining Wang, January 2012.
002 "Dynamic Activity Analysis Model Based Win-Win Development Forecasting Under the Environmental Regulation in China" by Shiyi Chen and Wolfgang Karl H‰rdle, January 2012.
003 "A Donsker Theorem for LÈvy Measures" by Richard Nickl and Markus Reiﬂ, January 2012.
004 "Computational Statistics (Journal)" by Wolfgang Karl H‰rdle, Yuichi Mori and J¸rgen Symanzik, January 2012.
005 "Implementing quotas in university admissions: An experimental analysis" by Sebastian Braun, Nadja Dwenger, Dorothea K¸bler and Alexander Westkamp, January 2012.
006 "Quantile Regression in Risk Calibration" by Shih-Kang Chao, Wolfgang Karl H‰rdle and Weining Wang, January 2012.
007 "Total Work and Gender: Facts and Possible Explanations" by Michael Burda, Daniel S. Hamermesh and Philippe Weil, February 2012.
008 "Does Basel II Pillar 3 Risk Exposure Data help to Identify Risky Banks?" by Ralf Sabiwalsky, February 2012.
009 "Comparability Effects of Mandatory IFRS Adoption" by Stefano Cascino and Joachim Gassen, February 2012.
010 "Fair Value Reclassifications of Financial Assets during the Financial Crisis" by Jannis Bischof, Ulf Br¸ggemann and Holger Daske, February 2012.
011 "Intended and unintended consequences of mandatory IFRS adoption: A review of extant evidence and suggestions for future research" by Ulf Br¸ggemann, Jˆrg-Markus Hitz and Thorsten Sellhorn, February 2012.
012 "Confidence sets in nonparametric calibration of exponential LÈvy models" by Jakob Sˆhl, February 2012.
013 "The Polarization of Employment in German Local Labor Markets" by Charlotte Senftleben and Hanna Wielandt, February 2012.
014 "On the Dark Side of the Market: Identifying and Analyzing Hidden Order Placements" by Nikolaus Hautsch and Ruihong Huang, February 2012.
015 "Existence and Uniqueness of Perturbation Solutions to DSGE Models" by Hong Lan and Alexander Meyer-Gohde, February 2012.
016 "Nonparametric adaptive estimation of linear functionals for low frequency observed LÈvy processes" by Johanna Kappus, February 2012.
017 "Option calibration of exponential LÈvy models: Implementation and empirical results" by Jakob Sˆhl und Mathias Trabs, February 2012.
018 "Managerial Overconfidence and Corporate Risk Management" by Tim R. Adam, Chitru S. Fernando and Evgenia Golubeva, February 2012.
019 "Why Do Firms Engage in Selective Hedging?" by Tim R. Adam, Chitru S. Fernando and Jesus M. Salas, February 2012.
020 "A Slab in the Face: Building Quality and Neighborhood Effects" by Rainer Schulz and Martin Wersing, February 2012.
021 "A Strategy Perspective on the Performance Relevance of the CFO" by Andreas Venus and Andreas Engelen, February 2012.
022 "Assessing the Anchoring of Inflation Expectations" by Till Strohsal and Lars Winkelmann, February 2012.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
023 "Hidden Liquidity: Determinants and Impact" by Gˆkhan Cebiroglu and Ulrich Horst, March 2012.
024 "Bye Bye, G.I. - The Impact of the U.S. Military Drawdown on Local German Labor Markets" by Jan Peter aus dem Moore and Alexandra Spitz-Oener, March 2012.
025 "Is socially responsible investing just screening? Evidence from mutual funds" by Markus Hirschberger, Ralph E. Steuer, Sebastian Utz and Maximilian Wimmer, March 2012.
026 "Explaining regional unemployment differences in Germany: a spatial panel data analysis" by Franziska Lottmann, March 2012.
027 "Forecast based Pricing of Weather Derivatives" by Wolfgang Karl H‰rdle, Brenda LÛpez-Cabrera and Matthias Ritter, March 2012.
028 "Does umbrella branding really work? Investigating cross-category brand loyalty" by Nadja Silberhorn and Lutz Hildebrandt, April 2012.
029 "Statistical Modelling of Temperature Risk" by Zografia Anastasiadou, and Brenda LÛpez-Cabrera, April 2012.
030 "Support Vector Machines with Evolutionary Feature Selection for Default Prediction" by Wolfgang Karl H‰rdle, Dedy Dwi Prastyo and Christian Hafner, April 2012.
031 "Local Adaptive Multiplicative Error Models for High-Frequency Forecasts" by Wolfgang Karl H‰rdle, Nikolaus Hautsch and Andrija Mihoci, April 2012.
032 "Copula Dynamics in CDOs." by Barbara Choro-Tomczyk, Wolfgang Karl H‰rdle and Ludger Overbeck, May 2012.
033 "Simultaneous Statistical Inference in Dynamic Factor Models" by Thorsten Dickhaus, May 2012.
034 "Realized Copula" by Matthias R. Fengler and Ostap Okhrin, Mai 2012. 035 "Correlated Trades and Herd Behavior in the Stock Market" by Simon
Jurkatis, Stephanie Kremer and Dieter Nautz, May 2012 036 "Hierarchical Archimedean Copulae: The HAC Package" by Ostap Okhrin
and Alexander Ristig, May 2012. 037 "Do Japanese Stock Prices Reflect Macro Fundamentals?" by Wenjuan
Chen and Anton Velinov, May 2012. 038 "The Aging Investor: Insights from Neuroeconomics" by Peter N. C. Mohr
and Hauke R. Heekeren, May 2012. 039 "Volatility of price indices for heterogeneous goods" by Fabian Y.R.P.
Bocart and Christian M. Hafner, May 2012. 040 "Location, location, location: Extracting location value from house
prices" by Jens Kolbe, Rainer Schulz, Martin Wersing and Axel Werwatz, May 2012. 041 "Multiple point hypothesis test problems and effective numbers of tests" by Thorsten Dickhaus and Jens Stange, June 2012 042 "Generated Covariates in Nonparametric Estimation: A Short Review." by Enno Mammen, Christoph Rothe, and Melanie Schienle, June 2012. 043 "The Signal of Volatility" by Till Strohsal and Enzo Weber, June 2012. 044 "Copula-Based Dynamic Conditional Correlation Multiplicative Error Processes" by Taras Bodnar and Nikolaus Hautsch, July 2012
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
045 "Additive Models: Extensions and Related Models." by Enno Mammen, Byeong U. Park and Melanie Schienle, July 2012.
046 "A uniform central limit theorem and efficiency for deconvolution estimators" by Jakob Sˆhl and Mathias Trabs, July 2012
047 "Nonparametric Kernel Density Estimation Near the Boundary" by Peter Malec and Melanie Schienle, August 2012
048 "Yield Curve Modeling and Forecasting using Semiparametric Factor Dynamics" by Wolfgang Karl H‰rdle and Piotr Majer, August 2012
049 "Simultaneous test procedures in terms of p-value copulae" by Thorsten Dickhaus and Jakob Gierl, August 2012
050 "Do Natural Resource Sectors Rely Less on External Finance than Manufacturing Sectors? " by Christian Hattendorff, August 2012
051 "Using transfer entropy to measure information flows between financial markets" by Thomas Dimpfl and Franziska J. Peter, August 2012
052 "Rethinking stock market integration: Globalization, valuation and convergence" by Pui Sun Tam and Pui I Tam, August 2012
053 "Financial Network Systemic Risk Contributions" by Nikolaus Hautsch, Julia Schaumburg and Melanie Schienle, August 2012
054 "Modeling Time-Varying Dependencies between Positive-Valued HighFrequency Time Series" by Nikolaus Hautsch, Ostap Okhrin and Alexander Ristig, September 2012
055 "Consumer Standards as a Strategic Device to Mitigate Ratchet Effects in Dynamic Regulation" by Raffaele Fiocco and Roland Strausz, September 2012
056 "Strategic Delegation Improves Cartel Stability" by Martijn A. Han, October 2012
057 "Short-Term Managerial Contracts and Cartels" by Martijn A. Han, October 2012
058 "Private and Public Control of Management" by Charles Angelucci and Martijn A. Han, October 2012
059 "Cartelization Through Buyer Groups" by Chris Doyle and Martijn A. Han, October 2012
060 "Modelling general dependence between commodity forward curves" by
Mikhail Zolotko and Ostap Okhrin, October 2012
061 "Variable selection in Cox regression models with varying coefficients" by Toshio Honda and Wolfgang Karl H‰rdle, October 2012
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

