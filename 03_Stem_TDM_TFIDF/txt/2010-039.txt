BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2010-039
High Dimensional Nonstationary Time Series
Modelling with Generalized Dynamic Semiparametric Factor
Model
Song Song* Wolfgang K. Härdle**
Ya'acov Ritov***
*Humboldt-Universität zu Berlin & University of California, Berkeley **Humboldt-Universität zu Berlin & National Central University ***The Hebrew University of Jerusalem This research was supported by the Deutsche
Forschungsgemeinschaft through the SFB 649 "Economic Risk". http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

High Dimensional Nonstationary Time Series Modelling with Generalized Dynamic Semiparametric Factor Model 
Song Song , Wolfgang K. H¨ardle , Ya'acov Ritov§
August 2, 2010
Abstract
(High dimensional) time series which reveal nonstationary and possibly periodic behavior occur frequently in many fields of science. In this article, we separate the modeling of high dimensional time series to time propagation of low dimensional time series and high dimensional time invariant functions via functional factor analysis. We propose a two-step estimation procedure. At the first step, we detect the deterministic trends of the time series by incorporating time basis selected by the group Lasso-type technique and choose the space basis based on smoothed functional principal component analysis. We show properties of this estimator under various situations extending current variable selection studies. At the second step, we obtain the detrended low dimensional stochastic process, but it also poses an important question: is it justified, from an inferential point of view, to base further statistical inference on the estimated stochastic time series? We show that the difference of the inference based on the estimated time series and "true" unobserved time series is asymptotically negligible, which finally allows one to study the dynamics of the whole high-dimensional system with a low dimensional representation together with the deterministic trend. We apply the method to our motivating empirical problems: studies of the dynamic behavior of temperatures (further used for pricing weather derivatives), implied volatilities and risk patterns and correlated brain activities (neuro-economics related) using fMRI data, where a panel version model is also presented.
Keywords: Semiparametric model, Factor model, Group Lasso, Seasonality, Spectral Analysis, Periodic, Asymptotic inference, Weather, fMRI, Implied Volatility Surface
AMS 2000 subject classification: 62G08, 62G20, 62M10 JEL classification: C14, C32, G12
1 Introduction
Modeling high-dimensional data is a challenging task in statistics especially when the data come in a dynamic context and are observed at different time points with changing structure and different sample sizes. Such modeling challenges appear in many different fields. In meteorology and agricultural economics, one of the primary interests is to study fluctuations of temperatures at different locations, for a recent summary, see Gleick et al. (2010). Such an analysis is essential for pricing weather
Supported by Deutsche Forschungsgemeinschaft via SFB 649 "O¨ konomisches Risiko", Humboldt-Universit¨at zu Berlin. Ya'acov Ritov's research is supported by an ISF grant and a Humboldt Award. We especially would like to acknowledge the useful comments of Peter Bickel, Qiwei Yao and Enno Mammen, thank Wei Xu, Peter Mohr and Elena Silyakova for preparing the data and thank participants at numerous seminars and conferences for their discussions.
Humboldt-Universit¨at zu Berlin & University of California, Berkeley. Email: ssoonngg123@hotmail.com Humboldt-Universita¨t zu Berlin and National Central University §The Hebrew University of Jerusalem
1

derivatives and hedging weather risks, Odening et al. (2008). In neuro-economics, one uses (high

dimensional) functional magnetic resonance imaging data (fMRI) to analyze the brain's response to

certain (economics related) stimuli as well as identifying its activation area, Worsley et al. (2002). In

financial engineering, one studies the dynamics of the implied volatility surface for risk management,

calibration and pricing purposes, Fengler et al. (2007). Other examples and research fields for very

large dimensional time series include empirical macroeconomics, Stock and Watson (2005); mortality

analysis, Lee and Carter (1992); bond portfolio risk management or derivative pricing, Nelson and

Siegel (1987) and Diebold and Li (2006); limit order book dynamics, Hall and Hautsch (2006); yield

curves, Hautsch and Ou (2008). In the biostatistical field, we refer to Martinussen and Scheike (2000)

for bio-medical research; Kauermann (2000) for radiation treatment of prostate cancer; Gasser et al.

(1983) for Electroence-phalogram (EEG) analysis.

The modeling challenge for high dimensional time series is that there are both high dimensionality

(in space) and dynamics (in time). One approach utilizes a factor type model, which allows low-

dimensional representation of the data by separating high dimensionality and dynamics, see Forni

et al. (2005), Giannone et al. (2005), Stock and Watson (2002a), Stock and Watson (2002b). In an

orthogonal L-factor model, a J-dimensional random vector Yt = (Yt,1, . . . , Yt,J ) can be represented

as

Yt,j = Zt,1m1,j + · · · + Zt,LmL,j + t,j,

(1)

where Zt,l are common factors, t,j are errors and the coefficients ml,j are factor loadings. In the above described applications, the index t = 1, . . . , T reflects the time evolution, and Yt can be considered as a multidimensional not necessarily stationary time series. The study of the time behavior of the high-
dimensional Yt is then simplified to the modeling of Zt = (Zt,1, . . . , Zt,L) , which is a more feasible task when L J. In a variety of applications, one has explanatory variables Xt,j  Rd at hand that may influence the factor loadings ml. An important refinement of the model (1) is to incorporate the existence of observable covariates Xt,j. The factor loadings are then generalized to functions of Xt,j, so that the model (1) is generalized to:

L

Yt,j =

Zt,l ml(Xt,j) + t,j, 1  j  Jt, 1  t  T.

l=1

d=ef Zt m(Xt,j) + t,j

(2)

where Zt = (Zt,1, . . . , Zt,L) (common factors) is an unobservable L-dimensional process (not necessarily stationary), m (factor loading functions) is an L-tuple (m1, . . . , mL) of unknown real-valued functions ml defined on a subset of Rd and t,j are errors. The variables X1,1, . . . , XT,JT , 1,1, . . . , T,JT are independent. Throughout the paper we assume that the Xt,j are deterministic. The errors t,j are i.i.d., have zero mean and finite second moments. Park et al. (2009) consider this model when Zt is stationary and call it a dynamic semiparametric factor model (DSFM). For simplicity of notation, we assume that the covariates Xt,j have support [0, 1]d, and also that Jt  J do not depend on t unless otherwise specified.
The approximation (2) involves unknown "space functions" ml(·) which in Park et al. (2009) are estimated via a B-Spline series:
K

ml(x) = alkk(x)

(3)

k=1

with a possibly multidimensional (as a tensor product of one dimensional) B-spline basis {k}Kk=1. Using the K ×J matrix t = {1(xt), . . . , K(xt)} and the matrix A = (alk), l = 1, . . . , L, k = 1, . . . , K
we can rewrite (2) as Yt = Zt At + t. Expanding the time effect in a series leads us to modeling Zt

2

as a sum of basis functions as well:

R

Ztl = rlur(t)

(4)

r=1

Putting (3) and (4) together we obtain (5) and (6), i.e. we observe (Xt,j, Yt,j) for j = 1, . . . , Jt and t = 1, . . . , T such that

LR

K

Yt,j =

ur(t)rl alkk(Xt,j) + tj

l=1 r=1

k=1

Yt = Ut  At +t d=ef Ut  t + t.

Zt m

(5) (6)

Here Ut = (u1(t), . . . , uR(t)) is a 1 × R matrix with ur(t) as the pre-specified initial time basis, which we introduce to capture the global trend and periodic variations. t = (1(Xt), . . . , K(Xt)) is a K × J matrix with k a space basis function. , A and  are R × L, L × K and R × K
(unknown) underlying coefficient matrices consisting of rl, alk and rk respectively. For every  matrix, we introduce r = (kr, 1  k  K), that is, the column vector formed by the coefficients

corresponding to the r-th time basis. Additionally we define  2,1 =

R r=1

K k=1

r2k

.

Finally we

set R() = {r : r = 0} and M () = |R()| where |R()| denotes the cardinality of set R(). For

sake of simplicity and convenience, we sometimes use | · | to denote the L1 norm for vectors and ·

to denote the L2 norm for vectors or the mixed (2, 1) norm for matrices.

Since certainly not all initially included time basis are fully loading, to avoid overparametrization

in time, basis or variable selection is necessary, i.e. some rs will be shrunk to 0 equivalently. A

popular variable selection method is Lasso, Tibshirani (1996). An extension for factor structured

models is the group Lasso, Yuan and Lin (2006), in which the penalty term is a mixed (2, 1)-norm of

the coefficient matrix.

Under an additional Gaussian error assumption, we first show that this group Lasso type estimator

enjoys sparsity inequalities (upper bounds on the prediction error and the distance between the

estimator and the true regression matrix ) and variable selection properties. Finally, we show how

our results can be extended to more general noise distributions, of which we only require the variance

to be finite. Since the standard assumption on t being independent is often not met in practice,

we further extend our results into the dependent scenario. Since the original model (6) actually

assumes that there is no randomness in time, we face some restrictions in practice. To this end,

we consider an extension incorporating the stochasticity (in time) and call it a generalized dynamic

semiparametric factor model (GDSFM). But it also poses an important question: is it justified, from

an inferential point of view, to base further statistical inference on the detrended stochastic time

series? We show that the difference of the inference based on the estimated time series and "true"

unobserved time series is asymptotically negligible, which finally allows one to study the dynamics of

the whole high-dimensional system with a low dimensional stochastic process representation together

with the deterministic trend.

Another motivation of (4) (the expansion in time), is from the temperature analysis (across China

over the past 50 years). Our data set is taken from Climatic Data Center (CDC), China Meteorological

Administration (CMA), which contains daily observations from 159 weather stations across China

(reduced from 202 after data cleaning) from Jan 1st, 1957 to Dec 31st, 2009, as can be seen from Figure

1 (left) (average over the 159 weather stations' observations). Except the well known seasonality effect,

we may expect a climate change related trend. If we take the moving average of 730 nearby days,

which is (159 · 730)-1

+365 s=-354

159 j=1

Yt+s,j

with

Yt,j

being

the

temperature

of

the

jth

weather

station

at time t, Figure 1 (right) shows a "large period" (around 10 years between peaks) and an upward

trend of the Chinese temperatures. Xt,j = Xj is the three-dimensional geographical information of

3

the jth weather station. Studying the dynamics of temperatures in various places simultaneously using a well calibrated GDSFM model will enable us to forecast temperatures in time and space.

50 40 30 20 10
0 1958 1963 1968 1973 1978 1983 1988 1993 1998 2003 2008

6.2 6
5.8 5.6 5.4 5.2
5 4.8 4.6 4.4 1958 1963 1968 1973 1978 1983 1988 1993 1998 2003 2008

Figure 1: Temperatures of China from Jan 1st, 1957 to Dec 31st, 2009 (left) and the corresponding moving average (of 730 nearby days) view (right).

Another motivation for this research is from neuro-economics. Understanding which part of our brain is activated during risky decisions and whether there is a significant reaction to specific stimuli (neural processes underlying investment decisions) are important goals in neuroscience. We address this problem through the analysis of high dimensional, dynamic fMRI data recorded in an experiment (to be described in more detail later). The fMRI is a noninvasive technique of recording brain's signals on spatial area in a given time period (2.5 sec for our data set). One obtains a series of threedimensional images of the blood-oxygen-level-dependent (BOLD) fMRI signals, when an exercised person is subject to certain stimuli related with financial decisions (periodically), where Yt,j is the BOLD value at voxel j and time t. Xt,j = Xj is the three-dimensional geographical information of the jth voxel. An example of the images at one particular time point is presented in Figure 2.

Figure 2: Typical fMRI data in one particular time point. The brightness corresponds to the strength of the observed signals.
The third motivation for this modeling approach (especially the space part) comes from financial engineering, i.e. the dynamics of the implied volatility surface (IVS) (although considered as stationary time series here), as is observed in Figure 3. The IV is a volatility parameter that matches observed plain vanilla option prices with the ones given by the formula of Black and Scholes (1973), which is a key financial variable for trading, heading and the risk management of option portfolios. Figure 3 shows the "string" structure of the IV data obtained from European option prices on the German stock index DAX (ODAX) for two different days from the whole data set - intraday observations from Jan 1, 2004 to Dec 30, 2004 from Bloomberg. The volatility strings shift towards expiry,
4

which is indicated by the bottom line in the figure. Moreover the shape of the IV strings is subject to stochastic deformation. Apart from the dynamic degeneration, one may also observe nonuniform frequency of the trades with significant greater market activities and the "smile" effect for the options closer to expiry or at-the-money. Fengler et al. (2007) first proposed to study the dynamics of the IV data, where Yt,j are the values of IV on the day t, and Xt,j are the two-dimensional vectors of the moneyness and time-to-maturity, where the dimensionality J (number of transactions) depends also on t.

implied volatility implied volatility

0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1
0.8

1 1.2
moneyness

1.4 0

0.5 0.4 0.3 0.2 0.1 time to maturity

0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1
0.8

1 1.2
moneyness

1.4 0

0.5 0.4 0.3 0.2 0.1 time to maturity

Figure 3: The typical IV data design on two different days. In the maturity direction observations appear in the
discrete points for each particular day. Bottom solid lines indicate the observed maturities. Left panel: observations on 20040701, Jt = 5606. Right panel: observations on 20040819, Jt = 8152.

The rest of the article is organized as follows. In the next section we present the estimation of (6) to extract the complex deterministic trends of the nonstationary time series using the group Lasso type technique. Its properties under various situations are presented in Section 3. Section 4 considers the general framework incorporating the stochasticity (in time) together with the corresponding asymptotic analysis. In Section 5 we present the results of simulation studies that illustrate the theoretical findings. In Section 6 we apply the model to the temperature, IVS and fMRI data, where a panel version of (6) is also presented. All technical proofs are sketched in Section 7.

2 Methodology

2.1 Choice of Time Basis

To capture the global trend in time, one may use an orthogonal Legendre polynomial basis: u1(t) =

1/C1, u2(t) = t/C2, u3(t) = (3t2 - 1)/C3, . . . (throughout this paper, Ci are generic constants). The

rescaling is made here such that

T t=1

u2r

(t)/Cr2

=

1.

To capture periodic variations, we could use

Fourier series, u4(t) = sin(2t/p)/C4, u5(t) = cos(2t/p)/C5, u6(t) = sin{2t/(p/2)}/C6, u7(t) =

cos{2t/(p/2)}/C7, . . . with the given the period p. For example, in the fMRI application, we know

that p = 11.8 (29.5s per trial & 2.5s per scan) and in the weather application, p1 = 365, p2 = 365 · 10.

2.2 Choice of Space Basis
There are various choices for a space basis. For example, Park et al. (2009) use a series estimator as described in (3). However, it has some disadvantages. Firstly, since the B-spline basis {k}kK=1 is possibly multidimensional (d > 1), it is constructed as a tensor product of one dimensional ones. When d 3, this may lead to quite large K, e.g. K = 9 × 9 × 5 = 405 in the fMRI application. More importantly, since the knots of the B-spline are equal-spaced, it could not capture some special

5

structure, e.g. the "smile" effect in the IVS modeling when the options are close to the maturity, as

can be seen in Figure 4 from Park et al. (2009) (adaptive choice of the knots of the B-splines may

solve

this

problem,

but

it

is

omitted m1

here

since

not

primary

interem2st).

2

0

-2

0 0.2 0.4 0.6 0.8
moneyness

10

2

0

1 0.5 time to maturity

-2

0 0.2 0.4 0.6 0.8
moneyness

10

1 0.5 time to maturity

Figure 4: Space basis using the series estimator for the IVS modeling.

To this end, we propose a data driven method to estimate the space basis 1(x), . . . , K(x), motivated by Hall et al. (2006), which combines smoothing techniques with ideas related to functional principal component analysis. We summarize the basic steps as follows:

1 Estimate the covariance operator. Write Xtj = (Xt1j, . . . , Xtdj), u = (u1, . . . , ud) and v = (v1, . . . , vd) (same for b, b, b1, b1, b2 and b2). Given u  [0, 1]d, let hµ and h denote bandwidths, which could be selected as in the usual local polynomial regression setup and select
(a, b) = (a, b) to minimize

T Jt

d

{Ytj - a - bc(uc - Xtcj)}2K

t=1 j=1

c=1

Xtj - u hµ

,

and take µ(u) = a. Then, given u, v  [0, 1]d, choose (a0, b1, b2) = (a0, b1, b2) to minimize

T t=1 1 j=k Jt

dd
{YtjYtk - a0 - bc1(uc - Xtcj) - b2c (vc - Xtck)}2
c=1 c=1
×K Xtj - u K Xtj - v . h h

Denote a0 by (u, v) and construct µ(v) similarly with µ(u). The estimate of the covariance operator is thus:
(u, v) = (u, v) - µ(u)µ(v).
Since the covariance operator is J × J, where J could be very large, to get its consistent estimates, various large covariance matrices regularization techniques, e.g. banding, Bickel and Levina (2008b) and thresholding, Bickel and Levina (2008a), could be further used.

2 Compute the principal space basis. Given the estimated operator, compute the largest K eigenvalues and corresponding orthonormal eigenfunctions as the basis 1(Xt,j), . . . , K(Xt,j) (tt /Jt = IK is thus valid). Computational methods could be found, for example, in Section 8.4 of Ramsay and Silverman (2005), where practical features regarding the operator-eigenfunction implementation are discussed in detail.

6

2.3 Estimation Procedure

We have now accumulated sufficient information to introduce the estimation method, which is summarized as below:

1 Find significantly loaded time basis functions by the group Lasso technique by minimizing:

T

min(J T )-1

Yt - Ut  t

t=1

Yt - Ut  t

+ 2  2,1.

(7)

2 Split the joint matrix  into 2 separate coefficient matrices , A by taking  as the L eigenvectors of  with respect to the L largest eigenvalues, and A =  .
To select K and L here, we could use either the classic "90%" rule in principal component analysis or the "explained variance" type selection method. Alternatively we could also sequentially test the size of the eigenvalues. But since it goes beyond the scope of this paper, we will therefore study its theoretical properties in a separate paper.
In order to study the statistical properties of this estimator, it is useful to derive some optimality condition for a solution of (7). Our implementation of the group Lasso-type estimator comes from Yuan and Lin (2006), which is an extension of the shooting algorithm of Fu (1998) for the lasso. As a direct consequence of the Karush-Kuhn-Tucker conditions, we have a necessary and sufficient condition for  to be a solution to expression (7) is

T
(J T )-1 {t(Yt - t Ut)Ut }r
t=1

=

 r , if r = 0 r

T

(J T )-1

{t(Yt - t Ut)Ut }r

,

if r = 0

t=1

Recall that tt /J = IK. It can be easily verified that the solution to (8) and (9) is

(8) (9)

r = 1 - / Sr Sr, +

(10)

where Sr =

T t=1

{t

(Yt

-

t

-r Ut )Ut

}r,

with

-r

=

(1, . . . , r-1, 0, r+1,

. . . , R).

The

solution

to

expression (7) can therefore be obtained by iteratively applying equation (10) to r = 1, . . . , R. We

choose the ordinary least square estimate OLS as the initial value, with which usually a reasonable convergence tolerance is reached within 5 iterations. However, the computational burden increases

dramatically as the number of initial basis increases.

Since the group Lasso type estimates depend on the unknown tuning parameter parameter , which

needs to be estimated, to select the final models on the solution paths of the group selection methods,

we introduce an easily computable Cp-type criterion as in Yuan and Lin (2006). The solution path is computed by evaluating on 100 equally spaced 's between 0 and max = maxr t tYtUtr / K. We select the  minimizing

Cp() = ~2 = df =

t

Yt

- Ut  ~2

t

2
- JT + 2df

t Yt - Ut OLSt 2 JT - df

1{ r > 0} +
rr

r (K - 1)  OLS

7

Empirical evidence suggests that this approximation works fairly well. In our experience, the performance of this approximate Cp-criterion is generally comparable with that of computationally much more expensive (especially for the high-dimensional data) fivefold cross-validation, as already noted in Yuan and Lin (2006).

3 Estimates' Properties

In this section, we first study the properties of this estimator as defined in (6) when the errors t are Gaussian. Our main results concern upper bounds on the prediction error and the distance between the estimator and the true matrix , (Theorem 3.1). The techniques of proofs are closely build upon
those of Lounici et al. (2009), Bickel et al. (2009) and Lounici (2008). In Theorem 3.2 we discuss how
our results can be extended to more general noise distribution, of which we only require the variance
to be finite. Since the standard assumption on t being independent is often not met in practice, in Theorem 3.3, we further extend our results into the dependent scenario.

LEMMA 3.1 Consider the model (6) for R 2 and T, J 1. Assume that the random vec-

tors 1, . . . , T are i.i.d. Gaussian with zero mean and covariance matrix 2IJ×J , tt /J = IK,

T t=1

Ut

Ut/R

=

1,

and

M ()

s. Let

 = 2

 1/2 1 + A log R/ T ,

JT

 where A > 8 and let q = min(A log R, T ). Then with probability at least 1 - R1-q, for any solution

 of problem (7) and  we have:

T
(J T )-1
t=1

t ( - )Ut

2
+   -  2,1

T
(J T )-1
t=1

t ( - )Ut 2 + 4

r - r ,

rR()

T

(J T )-1 max 1rR

{tt ( - )UtUt }r

t=1

3 ,
2

(11) (12)

and

M ()

42max 2T 2

 - 

2 2

,

where max is the maximum eigenvalue of the matrix

T t=1

UtUt

.

(13)

Before stating the first main result of this section, we make the following assumption first.

ASSUMPTION 3.1 There exists a positive number  = (s) such that

min t t Ut : |R| s,   RK×R\{0}, J R

Rc 2,1 3 R 2,1

,

where Rc denotes the complement of the set of indices R, R denotes the matrix formed by stacking the rows of matrix  w.r.t. row index set R.

8

Assumption 3.1 is essentially a restriction on the eigenvalues of Ut as a function of sparsity s. It actually requires the initially involved time basis not to be too dependent, which is naturally satisfied by the orthogonal polynomials and Fourier series. Low sparsity means that s is big and therefore  is small. (s) is thus a decreasing function of s. For this reason we sometimes refer to it as Assumption RE(s), see also Bickel et al. (2009), but note that in their paper l1 norms are used.
THEOREM 3.1 Assume all conditions in Lemma 3.1 still hold and add Assumption 3.1. Then with probability at least 1 - R1-q, for any solution  of (7):

T
(J T )-1
t=1
T -1/2

t ( - )Ut 2  -  2,1 32s

 642s(1 + A log R/ T )/(2J),
 1 + A log R/ T /(2 J),

(14) (15)

and

M () 64m2 axs/2

(16)

Note that Theorem 3.1 is valid for any fixed J, R, T and therefore yields non-asymptotic bounds. We
could see that dependence on the number of initially specified time basis R can be made negligible for large T . Additionally when the true coefficient matrix 's sparsity level is low (s large,  small,
s/2 large), all the three bounds get larger and the number of nonzero rows of estimated one  is
larger too correspondingly.
From now on, we only assume that the random variables tj are independent with zero mean and finite variance E(t2j) 2. In this case the results remain similar to those of the previous theorem, though the concentration effect is weaker. We use the following mild technical assumption.

ASSUMPTION 3.2 The matrices t and Ut are such that

TJ
(J T )-1
t=1 j=1

K2
max | tkjUtr| r k=1

C,

for a constant C > 0.

THEOREM 3.2 Consider the DSFM (6) for R 3 and T, J 1. Assume that the random

vectors 1, . . . , T are independent with zero mean and finite variance E(2tj) 2, tt /J = IK,

T t=1

Ut

Ut/R

=

1,

and

M ()

s. Let also Assumption 3.2 be satisfied. Furthermore let  be

defined as in Assumption 3.1 and max is the maximum eigenvalue of the matrix

T t=1

UtUt

.

Let

 =  (log R)1+/(JT ),  > 0.

Then with probability at least 1 - (2e log R - e)C/(log R)1+, for any solution  of (7) we have:

T

(J T )-1

t ( - )Ut 2

t=1

T -1/2  -  2,1 16s

162s(log R)1+/(2J ) 
(log R)1+/(2 J )

and

M () 642maxs/2

9

Since the standard assumption on t being independent is often not met in practice, it is important to understand how the proposed estimator behaves under dependent error terms. As far as we know, our result is the first attempt with dependent error terms for (group) Lasso variable selection techniques. The other effort of getting rid of the independence assumption could be found in Jia et al. (2009), where they consider a sparse Possion-like model. Before moving on, similar to Janson (2004), we introduce the following definitions first.
Given a set T and random variables Vt, t  T , we say:
· A subset T of T is independent if the corresponding random variables {Vt}tT are independent.
· A family {Tj}j of subsets of T is a cover of T if j Tj = T .
· A family {(Tj, wj)}j of pairs (Tj, wj), where Tj  T and wj  [0, 1] is a fractional cover of T if j wj1Tj 1T , i.e. j:tTj wj 1 for each t  T .
· A (fractional) cover is proper if each set Tj in it is independent.
· X (T ) is the size of the smallest proper cover of T , i.e. the smallest m such that T is the union of m independent subsets.
· X (T ) is the minimum of j wj over all proper fractional covers {(Tj, wj)}j.
Note that, in spite of our notation, X (T ) and X (T ) depend not only on T but also on the family {Vt}tT . Note further that X (T ) 1 (unless T = ) and that X (T ) = 1 if and only if the variables Vt, t  T are independent, i.e. X (T ) is a measure of the dependence structure of {Vt}tT . For example, if Vt just depends on Vt-1 but independent of all Vs, s < t - 1, e.g. AR(1), X (T ) = 2.
We use the following mild technical assumption similar to Assumption 3.2.

ASSUMPTION 3.3 The matrices t and Ut and random variables t are such that

E(J T )-1

KJ

(J -1

tkj tj Utr)2

k=1 j=1

T KJ

1/2

( tkj tj Utr)2

t=1 k=1 j=1

b2t with a high probabilityp C .
T

for  r and some constants bt, C > 0, t = 1, . . . , T . Note that dropping the sub-index r for all constants here does not matter, since they could be taken as the maximum of all corresponding constants over different rs. Given bt, t = 1, . . . , T , C could be taken as maxt bt for example.

We can now state our main result.

THEOREM 3.3 Consider the DSFM (6) for R 3, T, J 1 and T = {1, . . . , T }. Let also

Assumption 3.3 be satisfied for the random vectors 1, . . . , T and tt /J = IK,

T t=1

Ut

Ut/R

=

1,

and M () s. Furthermore let  be defined as in Assumption 3.1 and max is the maximum

eigenvalue of the matrix

T t=1

UtUt

.

Let

 = C + T

X (T ) t bt2 (log R)1- T 2

,

 > 0.

10

Then with probability at least p(1 - R- ), for any solution  of (7) we have:

T
(J T )-1
t=1

t ( - )Ut 2

T -1/2  -  2,1 16

16 C +

2

X (T ) t b2t (log R)1- T

s/2

C+

X (T ) t b2t (log R)1- T

s/2

and
M () 642maxs/2 Not surprisingly, this theorem tells that the bounds get larger when the dependence level, i.e. X (T ) increases, i.e. the bound is minimized when X (T ) = 1.

4 Generalized Dynamic Semiparametric Factor Model

The original model (6) assumes that there is no stochastic evolution in time. To this end, we consider the following extension of (4) and (6):

R

Ztl =

rl ur (t)

r=1

Yt = (Z0,t + Ut )At + t = Ut At + (Z0,tAt + t),

(17)

with an unobservable L-dimensional random process Z0,t with E(Z0,t|Xt) = 0 and i.i.d. assumption on t. We call (17) a generalized dynamic semiparametric factor model (GDSFM). If we concentrate on prediction, the trend represented by Ut  is enough. However, if we are interested in the stochasticity or dynamics of the original high dimensional time series, Z0,t comes into play, e.g. for pricing weather derivatives and various other financial engineering examples. The estimation procedure is now divided
into 2 steps:

· For the model Yt = Ut At + (Z0,tAt + t), treat Z0,tAt + t as the t in (6) and find the best parametric approximation according to the estimation procedure described in Subsection
2.3 to get the deterministic trend Ut .

· Based on Y t d=ef Yt - Ut t, A and t, use the ordinary least square method to obtain the estimated random process Z0,t.

As we could see from step one here, since t in (6) involves Z0,tAt + t, where Z0,t is a random process inhering dependence structure, Theorem 3.3 shows its necessity again. In the second step, Z0,t is estimated based on  instead of , we need to show the influence of this plug-in estimate is negligible. Our first result this section relies on the following assumptions, which are similar to
Assumptions (A1-8) in Park et al. (2009).

ASSUMPTION 4.1 4.1.1 The variables X1,1, . . . , XT,J , 1,1, . . . , T,J , and Z0,1, ..., Z0,T are independent.
4.1.2 For t = 1, . . . , T the variables Xt,1, . . . , Xt,J are identically distributed, have support [0, 1]d and a density ft that is bounded from below and above on [0, 1]d, uniformly over t = 1, . . . , T .

11

4.1.3 We assume that E t,j = 0 for 1  t  T, 1  j  J, and for c > 0 small enough sup1tT,1jJ E exp{c(t,j)2} < .
4.1.4 The vector of functions m = (m1, . . . , mL) can be approximated by k, i.e.

K d=ef sup inf m(x) - A(x)  0 x[0,1]d ARL×K

as K  . We denote A that fulfills supx[0,1]d m(x) - A(x)  2K by A.
4.1.5 There exist constants 0 < CL < CU <  such that all eigenvalues of the matrix T -1 lie in the interval [CL, CU ] with probability tending to one.

T t=1

Z0tZ0,t

4.1.6 The minimization (7) runs over all values  with

sup max
x[0,1]d 1 t T

Z0,tA(x)

MT ,

where the constant MT fulfils max1 t T Z0,t MT /Cm (with probability tending to one) for a constant Cm such that supx[0,1]d m(x) < Cm.
4.1.7 It holds that 2 = (K + T )MT2 log(JT MT )/(JT )  0. The dimension L is fixed.
Assumption (4.1.6) and the additional bound MT in the minimization is introduced for purely technical reasons.

THEOREM 4.1 Suppose that model (17), all assumptions in Theorem 3.3 and Assumption 4.1 hold. Then we have

1 T

2
Z0,tA - Z0,tA = OP (2 + K2 ).

1tT

(18)

In the following we discuss how a statistical analysis differs if the inference of stochasticity on Z0,t is
based on Z0,t (note that the trend Ut  is deterministic) instead of using (the unobserved) process Z0,t. We will show that the differences are asymptotically negligible (up to an orthogonal transformation). This is the content of the following theorem, where we consider estimators of autocovariances and show that these estimators differ only by second order terms. This asymptotic equivalence carries over to classical estimation and testing procedures in the framework of fitting a vector autoregresssive model. For the statement of the theorem we need the following assumptions, which are similar to Assumptions (A9-11) in Park et al. (2009):

ASSUMPTION 4.2 4.2.1 Z0,t is a strictly stationary sequence with E(Z0,t) = 0, E( Z0,t ) < 

for some  > 2. It is strongly mixing with

 i=1

(i)(-2)/

< .

The matrix E Z0,tZ0,t

has full rank.

The process Z0,t

is independent of

X11, . . . , XT J , 11, . . . , T J .

4.2.2 It holds that [log(KT )2{(KMT /J )1/2 + T 1/2MT4 J -2 + K3/2J -1 +K4/3J -2/3T -1/6} + 1]T 1/2(2 + K2 ) = O(2 + K2 )

Assumption (4.2.2) poses very weak conditions on the growth of J, K, T . Suppose, for example, that MT is of logarithmic order and that K is of order (JT )1/5 so that the variance and the bias are balanced for twice differentiable functions. In this setting, (4.2.1) only requires that T /J2 times a
logarithmic factor converges to zero.

12

Furthermore, please note that the minimization problem (7) has only a unique solution up to ,

but not to , A. If (Z0,t, A) is a minimizer, then also (B Z0,t, B-1A) is a minimizer, where B is an

arbitrary invertible matrix. In particular, with the choice B = (

T t=1

Z0,tZ0,t)-1

T t=1

Z0,tZ0,t,

we

get

for Z0,t d=ef B Z0,t and A d=ef B-1A that

T t=1

Z0,t(Z0,t

-

Z0,t)

= 0. Without loss of generality, we

may assume T -1

T s=1

Z0,s

=

T -1

T s=1

Z0,s

=

0.

Additionally

define

T

Zn,t = (T -1

Z0,sZ0,s)-1/2Z0,t

s=1

T

Zn,t = (T -1

Z0,sZ0,s)-1/2Z0,t.

s=1

THEOREM 4.2 Suppose that model (17) holds. Besides all assumptions in Theorem 3.3, let also Assumption 4.1-4.2 be satisfied. Then there exists a random matrix B such that for h  0

min[T,T -h]

T -1

Z0,t

t=max[1,-h+1]

Z0,t+h - Z0,t

- Z0,t (Z0,t+h - Z0,t) = OP (T -1/2)

and

min[T,T -h]

T -1

Zn,tZn,t+h - Zn,tZn,t+h = OP (T -1/2).

t=max[1,-h+1]

5 Simulation Study

We present three simulations which investigate how the spread of the sparsity level M (), the number
of initial time basis R and the dependence level of the error terms affect the performance. In the first example, we show how changing the values of M () result in changing the two measures of
estimation error in light of Theorem 3.1:

Lpar = 1 - Lpre = 1 -

R r=1

r - r



R r=1

r



R r=1

t (r - r)Ut 

R r=1

t rUt 

All codes were done in Matlab and are available on the author's homepage or www.quantlet.com. We
applied the above algorithm (8), (9) to the following simulated data. We generate random 1, . . . , 179  R5 such that all coordinates are independent and consider an initial model with the parameters such that rk  N{0, exp(-2k/5)}, r = 1, . . . , 179, k = 1, . . . , 5. We randomly pick 179 - M () rs from 1, . . . , 179 and assign them to be 0  R5. We choose the same time basis as in Table 4. For
the space part, inspired by Park et al. (2009), we considered d = 2, L = 3 and the following tuple of
2-dimensional functions:

m0(x1, x2) = 1, m1(x1, x2) = 3.46(x1 - .5), m2(x1, x2) = 9.45 (x1 - .5)2 + (x2 - .5)2 - 1.6,
m3(x1, x2) = 1.41 sin(2x2).

13

0.2 0.15
0.1 0.05
0 -0.05
-0.1 -0.15
-0.2 0

0.2 0.4 0.6 0.8

1

Figure 5: An illustration plot about how group Lasso penalty shrinks the coefficients.

The coefficients in these functions were chosen so that m1, m2, m3 are close to orthogonal. The design points Xt,j were independently generated from a uniform distribution on the unit square. We generate Yt = Ut  t + t, t = 1, . . . , 19345 where t is drawn as i.i.d. N(0, 0.05).
The convergence of the algorithm presented in (10) is usually achieved up to 5 iterations. Figure
5 is an illustration plot about how the group Lasso penalty shrinks the coefficients.
With 250 repetitions, Table 1 displays different Lpar and Lpres w.r.t. different sparsity levels. Our theoretical results in the previous sections suggest that when M () (s) is small, Lpar and Lpre will be large, which is confirmed by the simulation results.

M () = 100 M () = 50 M () = 20

Lpar 0.870 Lpre 0.710

0.918 0.835

0.931 0.859

Table 1: Lpar and Lpre w.r.t. different sparsity levels.
The second experiment compares how Lpar and Lpre react to changing the numbers of initial time basis R, for M () = 50, if we additionally include the quartic term in the orthogonal polynomial and double the number of Fourier series, R = 53 · 4 + 40 = 252 and if we remove the cubic term in the orthogonal polynomial and half the number of Fourier series, R = 53 · 2 + 10 = 116. The Lpar and Lpres are presented in Table 2.
R = 116 R = 179 R = 252 Lpar 0.879 0.918 0.920 Lpre 0.695 0.835 0.841

Table 2: Lpar and Lpre w.r.t. different number of initially involved time basis.
As we could see, when R increases, Lpar and Lpres increase. This indicates us that in practice we need take a relatively large R value, i.e. involve as many as possible time basis.
The third experiment compares how Lpar and Lpre are sensitive to the dependence level of the error items. We generated t from a centered VAR(1) process t = Rt-1+Ut, where Ut is N3(0, U ) random vector, the rows of R from the top equal (0.95, -0.2, 0), (0, 0.8, 0.1), (0.1, 0.0.6), and U = 10-4I3. We choose M () = 50, R = 179 as before. Besides the VAR(1) process indicated before, we also tried the VAR(2) to generate t. Table 3 displays the result, where we use VAR(0) to denote the independent
14

case. The performance decreases when the error terms are more dependent, which is consistent with Theorem 3.3.

V AR(0) V AR(1) V AR(2)

Lpar 0.918 Lpre 0.835

0.854 0.774

0.783 0.712

Table 3: Lpar and Lpre w.r.t. different levels of dependence of t. For more Monte Carlo experiments concerning Theorem 4.2, we refer to Park et al. (2009).

6 Weather, Neuro-economics and IVS

This section presents three applications to the temperature, fMRI and IVS analysis. First, we fit the model to the daily temperature observations by Climatic Data Center (CDC), China Meteorological Administration (CMA), as introduced in Figure 1. To capture the upward trend, seasonal and "large period" effects, for time basis, similar to Racsko et al. (1991), Parton and Logan (1981) and Hedin (1991), we propose the following initial choice of time basis (rescaling factors omitted) in Table 4.

Trend (Year by Year)
Seasonal Effect

Factors 1 t
3t2 - 1 sin 2t/365 cos 2t/365
... cos 20t/365

Large Period

Factors sin 2t/(365 · 10) cos 2t/(365 · 10) sin 4t/(365 · 10) cos 4t/(365 · 10) sin 6t/(365 · 10)
... cos 20t/(365 · 10)

Table 4: Initial choice of 53 · 3 + 20 = 179 time basis.

8000

1

6000

0.8

4000

0.6

2000

0.4

0 5 10 15 20 25 30 35 40 0.2 5 10 15 20 25 30 35 40
Figure 6: Distribution of the eigenvalues and the relative proportion of variance explained by the first K basis.

For the space basis, consider the eigenvalues of the smoothed (with the usual optimal bandwidth for local polynomial regression) covariance operator (Figure 6) and also the climate types of China (Figure 7), the number of space basis K = 5 seems to be satisfactory although K = 10 is needed to pass the "90%" rule. Please note that it is significantly smaller than the number of terms of a

15

Figure 7: China Climate Types

series estimator. Figure 8 displays the estimated coefficients of the 5 factors with respect to the 54 · 3 yearly polynomial time basis under the optimal choice of . The coefficients of constant, linear and quadratic terms are displayed as solid, dashed and dotted lines correspondingly. As one may see, the fact that most of the coefficients are nonnegative (especially for k = 1) shows strong evidence of global warming effect (especially with a quadratic upward trend) in China during the past 50 years. In a climatological context this has also been observed by Karl et al. (1991), while the global climate change has been recently summarized by Gleick et al. (2010). The high estimates over the second half of 1960s are due to the high temperatures then in China (Figure 1). The pattern that all the coefficients display an upward trend further indicates the stronger and stronger warming effect. The coefficients estimates of the 20 Fourier series time basis corresponding to the optimal  are displayed in Table 5. It clearly indicates the 10-year period effect which, as some meteorologists claimed, are related to the solar activity. Figure 9 displays the extracted trends based on Ut , where the five lines correspond to the five factors. The characters of this kind of nonstationary time series further indicate that the autoregressive model may not be a proper tool to capture them. Firstly, since there exists the "stronger and stronger global warming" effect, if we use AR model, the constant, linear and quadratic coefficients should be time variant (increasing). Secondly, the existence of "large period" effect also poses the problem of lag or frequency selections there. Both of these actually introduce bigger technical challenges.

Basis sin 2t/365 cos 2t/365 sin 4t/365 cos 4t/365 ... cos 20t/365 sin 2t/(365 · 10) cos 2t/(365 · 10) ... cos 20t/(365 · 10)

-0.1777 -0.6081
0.0000 -0.0145
0.0000 0.0000 0.0025 0.0000 0.0000 0.0000

0.0076 0.0126 0.0000 0.0028
... ... -0.0006 ... ... ...

Estimates 0.0177 0.0366 0.0000 0.0021
0.0009

-0.0136 -0.0369
0.0000 -0.0022
-0.0008

0.0084 0.0114 0.0000 0.0029
-0.0001

Table 5: Estimated coefficients of the 5 factors w.r.t. the 20 Fourier series time basis. Since the eigenvalues of  are (0.4683, 0.0106, 0.0068, 0.0040, 0.0007, 0.0000, . . .), we choose L = 5 and estimated the remaining 5-dimensional random process Z0,t, e.g. Z0,t,1 as displayed in Figure
16

0.06

0.05

0.04

0.03

0.02

0.01

0 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007
0.02 0.04

0.01 0
-0.01 -0.02

0.03 0.02 0.01

-0.03

0

-0.04 -0.05 -0.06 -0.07
1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 0.01
0 -0.01 -0.02 -0.03 -0.04 -0.05 -0.06
1962 1967 1972 1977 1982 1987 1992 1997 2002 2007

-0.01
-0.02
-0.03 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007
0.025 0.02 0.015 0.01 0.005
0 -0.005 -0.01 -0.015 -0.02 -0.025 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007

Figure 8: Estimated coefficients of the 54 · 3 yearly polynomial time basis w.r.t. k = 1, . . . , 5 from up to down and left to right.

10 (Z0,t,2 - Z0,t,5 are omitted due to the limited space here). The expectation of the random process is close to zero, which indicates our detrending using the group Lasso type technique works well.
The residual multi-dimensional random process could be further modeled by multivariate time series techniques. For example, if we use VAR(1) process Z0,t = RZ0,t-1 + 0,t, where 0,t is a random vector, the estimated coefficient matrix is:

 0.9732 -0.0135 -0.0002 -0.0006 -0.0002



0.0127 0.0358 -0.0001

0.1766 -0.2867 -0.1967

-0.1824 0.4493
-0.1962

-0.0682 -0.1138
0.8010

-0.0009 0.0053
-0.0052

 .

0.0790 0.0492 0.0690 -0.0225 0.8418

In comparison with the existing temperature modeling or weather derivatives pricing techniques, e.g. Benth and Benth (2005), we have the following advantages. Firstly, based on the high dimensional

17

0.01

0.005

0

-0.005

-0.01

1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007

Figure 9: Extracted trends based on Ut .

1000 900 800 700 600 500 400 300 200 100 0 -100 1958 1963 1968 1973 1978 1983 1988 1993 1998 2003 2008

80 60 40 20
0 -20 -40 -60 -80

1978

1983

Figure 10: Estimated Stochastic Process Z0,t,1 and a 10 year zoom.

time series data, we offer integrated analysis considering space (high dimensionality) and time (dynamics) parts simultaneously, while forecasting at places different from the existing weather stations is also possible since the space basis are actually functions of the geographical location information. Secondly, we extract the trend more clearly. Thirdly, we provide the theoretical justification for further inferential analysis of Z0,t instead of Z0,t. However, if we have a closer look at the enlarged estimated stochastic process in Figure 10, we find that the volatility of the random process also has a seasonality, which is actually due to the fact that the variance of the noise (temperature, fMRI etc.) scale linearly with the expectation of the measurements. This motivates to consider (6) under heteroscedasticity (Poisson - like model) as follows:
Yt = Ut At + t, Cov(t) = diag(|Ut At|),
which will be presented in a separate paper. As a second application of the model, we consider a microeconomic experiment based on fitting
an fMRI data set. Here we used a novel investment decision task that uses streams of (past) returns as stimuli to the exercised subjects, where the flowchart of the experiment is presented in Figure 11 (left), and obtain a series of three-dimensional images of the blood-oxygen-level-dependent (BOLD) fMRI signals. Our model helps to identify the corresponding brain's activation areas and to simplify the inference to the analysis of time propagation of a few number of factors (low-dimensional representation). Additionally we classify the risk attitudes of different subjects based on the coefficients of time basis, which performed quite well compared to the classic risky decision making model (risk-return model) which is based on the subjects' answers directly, where the risk attitude can be
18

measured as value reduction in Euro for maximum risk (the case when the subjective perceived risk = 100), as described in Mohr et al. (2010). All subjects were classified as risk averse indicated by a positive risk weight as shown in Figure11 (right). However, for six subjects the risk attitude was quite low (risk weight< 5, colored with blue) resulting in only a small influence of risk on value. For the experimental procedure and the fMRI data description, we refer to Mysickov´a et al. (2010).

Risk Attitude

Risk Attitude of Probands
20
Pb 19
Pb 18

15

Pb 4
10

Pb 15 Pb 16

Pb 10

Pb 1 Pb 3
5 Pb 2

Pb P5b 6

Pb P8b 9 Pb 11

Pb 17

Pb 12
00 5 10 15 20
Probands

Figure 11: Flowchart of the experiment (left) "Returns Pause Decision" and risk attitudes of 16 subjects (right). Subjects with risk attitude < 5 are colored blue, otherwise green.

Since we are analyzing multi subjects 1  i  I here, we obtain a panel version of the original

model (6) to

L
Yti,j = (ti,l + Ut li)ml(Xt,j) + t,j, 1  j  Jt, 1  t  T,
l=1

where the fixed effect ti,l is the individual effect on function ml for subject i at time point t. For identification purpose, we assume
IL
ti,lml(Xt,j) = 0. Please notice that assuming different subjects have the same basis function
i=1 l=1
in space ml makes sense here since the basis function is used to detect which part of the brain is
activated for risky decisions, which should be homogeneous for human beings. Thus for this panel

data, we have:

L
Y t,j = (Ut l)ml(Xt,j) + t,j , 1  j  J,
l=1

and our 2-step estimation procedure is as follows:

1 Take the average of Yti,j across different subjects i, and estimate the common basis function in space ml as in the original approach.
2 Given the common ml, for different subjects i, estimate their specific factors in time Zti,l.

L

Yti,j =

Ut liml(Xt,j) + it,j

l=1

Since most of the technical details have been illustrated in the previous application, it is skipped here, while the differences will be emphasized. Since the significantly larger dimension J = 76176 is observed here, computing eigenvalues of a 76176 × 76176 matrix will encounter significant numerical

19

difficulties. By using the fact that cc has the same eigenvalues as c c (where c is a J × T matrix), we only need to compute eigenvalues of a 722 × 722 matrix. If we additionally take the average of every 10 Yti,js over t, we only need compute eigenvalues from a 73 × 73 matrix.
The third factor loading function m3 shown in Figure 12 could be identified as the Ventromedial prefontal cortex (VMPFC) located in the bottom frontal part in the brain, which is the center for
utility and conform herewith with our experiment (it is why it is presented here). The other functions
ml, which also represent exactly those brain regions which we have expected to be involved during the experiment, are presented in Mysickov´a et al. (2010).

Figure 12: Estimated function m3 shown in 12 axial slices (left) and as a 3D-plot in a posterior view (right) with highlighted Ventromedial prefrontal cortex (VMPFC).

We use the same 3 orthogonal polynomials and 10 Fourier series as before as time basis. Figure 13 displays the response curve (to stimuli) Ut i2 for different subjects. Based on the estimated factors for different individuals, we could further develop a classification method which can predict the risk
aversion only based on the measured fMRI signals. Observing that different probands' response
curves have different patterns and their corresponding Z0,t have different volatilities, for this purpose we use the estimated coefficients 3i since it correspond to the brain activity of the VMPFC, which is linked with utility. To provide the classification analysis, we apply Support Vector Machines (SVM),
which is a widely used nonlinear method based on statistical learning theory. For the learning step, strongly risk averse subjects were labeled by -1 and weakly risk averse subjects by 1. Then, we
applied the leave-one-out method to first train and then estimate the classification rate of the SVM.
The classification rates are 85% for strongly risk averse and 60% for weakly risk averse individuals.
More importantly, these rates hold for a wide range of prior parameters: the radial basis coefficient r (0.25 - 0.35) and the capacity C (20 - 90).

MEAN

Estimated

Data

Strongly 0.85 0.14 Weakly 0.59 0.40

Table 6: Classification rates of the SVM method using median(left) and mean (right) of volatilities of Zt,2.

In the analysis of IVS data, deterministic trends are not present, and do not make sense from a non arbitrage point of view. We may therefore assume stationarity. The first detrending step is therefore omitted, alternatively, we could still use the dynamic semiparametric factor modeling
20

Figure 13: Response curve (to stimuli) Ut i2 for proband 18 (up) and 16, 19 and 11 (down, left to right) w.r.t l = 3.

approach proposed by Park et al. (2009) except for a different space basis. To this end, due to

orrelationthme alitmrixiteedstismpaacteionhere, we only present the new space basis of th3e-1i0mplied volatility surface (IVS)

application in Figure 14 (left). We see that the "smile" effect is captured very well. The corresponding
Basis functionsestimated time series of factors Zt,1, Zt,2 (stationary) are presented in Figure 14 (right). Correlation matrix estimation

1st eigenfunction, interpolated

2nd eigenfunction, interpolated
Estimated time series of factors Zt1, Zt2

3-12

0.4

0.3

0.2

0.1

0

-0.1

2 1.5 1 0.5
time to maturity

00

0.2

0.1

0

-0.1

-0.2

-0.3

-0.4

2

2 1.5

1.5 1 0.5 moneyness

1
0.5 time to maturity

00

2 1.5 1 0.5 moneyness

80 60 40 20
0 -20 -40 -60
0

Z 50 100 150 200 250

Figure 14: Space basis using the FPCA approach for IVS modeling and the estimated time series of
Ffaicgtuorres Z2:t,1E, iZgte,2n.functions as basis functionsGiennteeticrspaonlda/toef dbasket options

enetics a7nd/ofAbapskpeteonptdioinxs
Here we collect one auxiliary result which is used in the proof of Lemma 3.1. LEMMA 7.1 For any I × J matrix A and any J × K matrix B, we have AB 2,1

A 2,1 B 2,1.

21

Proof With Cauchy Schwartz inequality it is not hard to derive:

I
AB 2,1 =
i=1

KJ
( aijbjk)2
k=1 j=1

I KJ

J

( ai2j

bj2k )

i=1 k=1 j=1 j=1

I
(
i=1

JK
a2ij )(
j=1 k=1

= A 2,1 B 2,1

J
bj2k )
j=1

Proof of Lemma 3.1 The proof is in a similar spirit of the one of Lemma 3.1 in Lounici et al. (2009). By the definition of  as a minimizer of (7), for  we have

TR

(J T )-1

t Ut - Yt

2
+ 2

r

t=1 r=1

TR

(J T )-1

t Ut - Yt 2 + 2

t=1 r=1

r ,

which, using Yt = t Ut + t, is equivalent to

TT

(J T )-1

t ( - )Ut 2 (J T )-1

t ( - )Ut 2

t=1 t=1

TR
+2(J T )-1 t t ( - )Ut + 2 ( r - r ).

t=1 r=1

By H¨older's inequality, we have that

(19) (20)

T
t t ( - )Ut
t=1

T
ttUt
t=1

2,  -  2,1

(21)

where

T t=1

tt

Ut

2, = max1 r R

Consider the random event

T t=1

Kk=1(

J j=1

tkj

tj Utr )2 .

T

A = 2(J T )-1

ttUt 2,  .

(22)

t=1

Since tt /J = IK and

T t=1

Ut

Ut/R

=

1, the random variables Vtr

=

 ( J )-1/2

K k=1

J j=1

tkj

tj Utr ,

t = 1, . . . , T , are i.i.d. standard Gaussian. Using this fact, we can write, for any r = 1, . . . , R, and

  1/2

 = 2/ JT 1 + A log R/ T ,

TK
P

J2
tkj tj Utr

t=1 k=1 j=1

2(J T )2/4

= P XT2 = P XT2

2J T 2/(42)
 T + A T log R

22

where XT2 is a chi-square random variable with T degrees of freedom. By the tail property of XT2 distribution (Lemma A.1 of Lounici et al. (2009)), and the fact that A > 8 we get:
 P(Ac) R exp{-A log R/8 min( T , A log R)} R1-q
 with q = min(A log R, T ). It follows from (20) and (21) that, on the event A:

TR

(J T )-1

t ( - )Ut 2 + 

r - r

t=1 r=1

TR

(J T )-1

t ( - )Ut 2 + 2 ( r - r + r - r )

t=1 r=1

T

(J T )-1

t ( - )Ut 2 + 2

( r - r + r - r )

t=1 rR()

+ 2

( r - r + r - r )

rRc()

T

(J T )-1

t ( - )Ut 2 + 4

r - r

t=1 rR()

(23)

which coincides with (11). To prove (12), we use (8) and (9) resulting in the inequality

T

(J T )-1 max 1rR

{t(Yt - t Ut)Ut }r

t=1

.

(24)

Then

(J T )-1 (J T )-1

T
{tt ( - )UtUt }r
t=1
T
{t(t Ut - Yt)Ut }r + (J T )-1
t=1

T
(ttUt )r
t=1

(25)

where we have used Yt = t Ut +t and the triangle inequality. The derived bound (12) then follows by combining (25) with (24) and using the definition of the event A. Finally, we prove (13). First,

observe that,

T TT
t(Yt - t Ut)Ut = tt ( - )UtUt + ttUt .
t=1 t=1 t=1

On the event A, following from (8) and the triangle inequality, we have:

T

(J T )-1

{tt ( - )UtUt }r

t=1

/2, if r = 0.

23

The following arguments yields the bound (13) on the number of nonzero rows of r :

M ()

4 2(J T )2
rR()

T
{tt ( - )UtUt }r 2
t=1

4R 2(J T )2
r=1

T
{tt ( - )UtUt }r 2
t=1

4 = 2T 2

T

{J -1tt

( - )UtUt

}

2 2,1

t=1

4 2T 2

 - 

2 2,1

T
UtUt

t=1

42max 2T 2

 - 

22,1,

2 2,1

which follows from Lemma 7.1, tt /J = IK and max is the maximum eigenvalues of the matrix

T t=1

UtUt

.

Proof of Theorem 3.1 We proceed similarly to the proof of Theorem 3.1 in Lounici et al. (2009) and Theorem 6.2 in Bickel et al. (2009). Let R = R() = {r : r = 0}
By inequality (11) in Lemma 3.1 with  =  we have, on the event A defined in (22):

T

(J T )-1

t ( - )Ut 2

t=1

4 r - r

rR 4 s

(

-

)R

(26)

Moreover by the same inequality, on the event A, we have

R r=1

r - r

4 rR r - r , which

implies that rRc r - r 3 rR r - r . Thus, by Assumption 3.1 with  = ( - ):

( - )R

T t ( - )Ut /( J).
t=1

(27)

Now (14) follows from (26) and (27). Inequality (15) follows by noting that

R
r - r
r=1

4 r - r
rR

 4s

( - )R

and then using (14). Inequality (16) follows from (13) and (14).

Proof of Theorem 3.2 The proofs of this theorem are similar to the one of Theorem 3.1 up to a modification of the bound on P(Ac) in Lemma 3.1. We consider now the event

A=

max
1rR

TK J
( tkj tj Utr)2
t=1 k=1 j=1

1/2

JT .

The Markov inequality yields that

P(Ac)

T
E
t=1

KJ

max (

tkjtjUtr)2 /(J T )2.

1rR

k=1 j=1

24

Then we use Nemirovski's inequality, see Corollary 2.4 of Du¨mbgen et al. (2008)[p.5], with the random

vectors

KK

Wtj =

tkjtjUt1/J, . . . , tkjtjUtR/J  RR, j, t.

k=1 k=1

We get that

P(Ac)

2e log R - e 2(J T )-1 T J 2J T
t=1 j=1

K2
max | tkjUtr| .
1rR k=1

By the definition of  in Theorem 3.2 and Assumption 3.2 we obtain

P(Ac)

(2e log R - e)C .

(log R)1+

Proof of Theorem 3.3 The proofs of this theorem are similar to the one of Theorem 3.1 up to a modification of the bound on P(Ac) in Lemma 3.1. We consider now the event

A=

max
1rR

TK J
( tkj tj Utr)2
t=1 k=1 j=1

1/2

JT .

Thus, following the fact that different space basis k and k are independent, we have:

P(Ac) = P max 1rR

TK J

1/2

( tkj tj Utr)2

> JT

t=1 k=1 j=1

= P max 1rR

T KJ

1/2

(

tkj tj Utr)2

> JT

t=1 k=1 j=1

T KJ

1/2

RP (

tkj tj Utr)2

> JT

t=1 k=1 j=1

= RP

T KJ

1/2

(

tkj tj Utr)2

> JT

t=1 k=1 j=1

= R P{f (V ) > }

where

KJ

Vt d=ef J -1

tkj tj Utr

k=1 j=1

V d=ef (V1r, . . . , VT r)

f (V ) d=ef T -1

T 1/2
Vt2 .

t=1

Since Assumption 3.3 holds, i.e. with a high probability p, for  t and v1r, . . . , vT r, vtr,

|f (v1r, . . . , vtr, . . . , vT r) - f (v1r, . . . , vtr, . . . , vT r)| E f (V )

bt2/T C .
T

25

Then, by the (extended) Mcdiarmid inequality, see Theorem 2.1 of Janson (2004), with the random vectors V and function f , we have

P(Ac)

R P{f (V ) > } R P{f (V ) - E f (V ) >  - C } T

( - C )2T 2

R exp

-

X

(T

T
)

t b2t

= R-

with  = C + T

,X (T ) t bt2
(log R)1- T 2

 > 0.

Proof of Theorem 4.1 Similar to Y t d=ef Yt - Ut t, define Yt d=ef Yt - Ut t with the corresponding estimate Z0,t. Thus

1 T

Z0,tA - Z0,tA 2

1tT

1 T

Z0,tA - Z0,tA

21 + T

Z0,tA - Z0,tA

2
,

1tT

1tT

where the second term is bounded by OP (2 + K2 ) by Theorem 2 of Park et al. (2009). For the first term, since

Z0,t = (Att A )-1AtYt Z0,t = (Att A )-1AtYt Z0,t - Z0,t = (Att A )-1At{t ( - )Ut}

and Theorem 3.3 tells us that (JT )-1

T t=1

t ( - )Ut

2 could be arbitrary small, i.e.  large

enough R, s.t. the first term is dominated by the second one.

Proof of Theorem 4.2 The proof is in a similar spirit of the one of Theorem 3 in Park et al. (2009).

We will prove the first equation of the theorem for h = 0. The second equation follows from the first

equation. We first prove that the matrix T -1

T t=1

Z0,t

Z0,t

is

invertible.

Suppose that

the

assertion

is not true. We can choose a random vector e such that e = 1 and e

T t=1

Z0,tZ0,t

=

0.

Note

that

TT

T -1

Z0,tZ0,tA - T -1

Z0,tZ0,tA

t=1 t=1

T

T -1

Z0,t(Z0,tA - Z0,tA)

t=1

TT

(T -1

Z0,t 2)1/2(T -1

Z0,tA - Z0,tA 2)1/2

t=1 t=1

= OP ( + K),

(28)

because of Assumption (4.1.5) and Theorem 4.1. Thus with f = T -1

T t=1

Z0,tZ0,te,

we

obtain

fm

= f (A) + OP (K)
T
= e T -1 Z0,tZt A + OP ( + K )
t=1
= OP ( + K).

26

This implies that m1, . . . , mL are linearly dependent, contradicting to the construction that all space
basis are independent. Z0,t = B Z0,t and A = B-1A give with (28)

A - A

T
= T -1 Z0,tZt (A - A) OP (1)

t=1

TT

= T -1

Z0,tZ0,tA - T -1

Z0,tZ0,tA OP (1)

t=1 t=1

= OP ( + K)

(29)

From Assumptions (4.1.4), (29) and Theorem 4.1, we get

T

T -1

Zt - Z0,t 2

t=1

T

= T -1

Zt (m1, . . . , mL) - Z0,t(m1, . . . , mL) 2OP (1)

t=1

T

= T -1

Zt A - Zt A 2OP (1)

t=1

T

+ T -1

Zt A - Z0,tA 2OP (1) + OP (K2 )

t=1

T
T -1 Z0,t - Z0,t 2 A - A 2OP (1)

t=1

T

+ T -1

Z0,t 2 A - A 2OP (1)

t=1

T

+ T -1

Zt A - Z0,tA 2OP (1) + OP (K2 )

t=1

= OP (2 + K2 ).

We will show that for h = 0

(30)

T

T -1

{(Z0,t+h - Z0,t+h) - (Z0,t - Z0,t)}Z0,t = OP (T -1/2)

t=h+1

(31)

This implies the first statement of Theorem 4.2, because by (30)

T
T -1 (Z0,t - Z0,t)(Z0,t+h - Z0,t+h) = OP (b2) = OP (T -1/2).
t=-h+1

27

For the proof of (31), define

J
St,Z = J -1 A(Xt,j)(Xt,j) A

j=1

St,Z = AE (Xt,j)(Xt,j) A

TJ

S = (J T )-1

{(Xt,j)  Z0,t}{(Xt,j)  Z0,t}

t=1 j=1

T
S = T -1 E {(Xt,j)  Z0,t}{(Xt,j)  Z0,t} Z0,t

t=1
S = J -1A (Xt,j)(Xt,j) e - E (Xtj)(Xtj) e ,

where e  RK with e = 1. Let a~ be the stack form of A. It can be verified that

J
Z0,t = St-,Z1J -1 {Yt,jA(Xt,j)} ,
j=1

TJ

a~ = S-1(J T )-1

{(Xt,j)  Z0,t}Yt,j.

t=1 j=1

Let  = T -1/2/b. We argue that

(32) (33)

sup St,Z - St,Z = OP (), S - S = OP ().
1tT
We show the first part of (34). The second part can be shown similarly. Since

(34)

Att A = (A - A + A)(tt - E tt + E tt )(A - A + A) ,

to prove the first part it suffices to show that, uniformly for 1 t T ,

J
J -1 A (Xt,j)(Xt,j) - E (Xt,j)(Xt,j)

(A - A) = OP ()

j=1

J
J -1 (A-A) (Xt,j)(Xt,j) -E (Xt,j)(Xt,j) (A-A) = OP ()

j=1

J
J -1 A (Xt,j)(Xt,j) - E (Xt,j)(Xt,j)

A = OP ()

j=1

J
J -1 A E (Xt,j)(Xt,j) (A - A) = OP ()

j=1

J
J -1 (A - A) E (Xt,j)(Xt,j)T (A - A) = OP ()

j=1

(35) (36) (37) (38) (39)

The proof of (35)-(37) follows by simple arguments. We now show (38). Claim (39) can be shown similarly. For the proof of (38), we use Bernstein's inequality for the following sum:

J
P | Wj| > x
j=1

1 x2

2 exp -

.

2 V + M x/3

(40)

28

Here for a value of t with 1  t  T , the random variable Wj is an element of the L × 1-matrix

S = J-1A (Xt,j)(Xt,j) e - E (Xtj)(Xtj) e where e  RK with e = 1. In (40), V is

an upper bound for the variance of

J j=1

Wj

and

M

is

a

bound

for

the

absolute

values

of

Wj ,

i.e.

|Wj|  M for 1  j  J, a.s. With some constants C1 and C2 that do not depend on t and the row

number we get V  C1J-1 and M  C2K1/2J-1. Application of Bernstein's inequality gives that,

uniformly for 1  t  T and e  RK with e = 1, all L elements of S are of order OP (). This shows

claim (35).

From (29), (30), (32), (33) and (34) it follows that uniformly for 1 t T ,

J

Z0,t - Z0,t = St-,Z1J -1

t,j A (Xt,j )

j=1

J
+St-,Z1J -1 t,j(A - A)(Xt,j) + OP (T -1/2)
j=1

d=ef t,1,Z + t,2,Z + OP (T -1/2).

(41)

For the proof of the theorem it remains to show that for 1 j 2

T

T -1

(t+h,j,Z - t,j,Z )Z0,t = OP (T -1/2).

t=-h+1

(42)

This can be easily checked for j = 1. For j = 2 it follows from A - A = OP ( + K) and

TJ

E (J T )-1

t,jSt-,Z1M(Xt,j) 2 = O(K(J T )-1),

t=1 j=1

for any L × K matrix M with M = 1.

References
Benth, F. and Benth, J. (2005). Stochastic modelling of temperature variations with a view towards weather derivatives. Applied Mathematical Finance, 12(1):53­85.
Bickel, J. and Levina, E. (2008a). Covariance regularization by thresholding. Ann. Statist, 36(6):2577­ 2604.
Bickel, J. and Levina, E. (2008b). Regularized estimation of large covariance matrices. Ann. Statist, 36(1):199­227.
Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. Annals of Statists, 37(4):1705­1732.
Black, F. and Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of Political Economy, 81:637­654.
Diebold, F. X. and Li, C. (2006). Forecasting the term structure of government bond yields. Journal of Econometrics, 130:337­364.
Du¨mbgen, L., van de Geer, S., Veraar, M., and Wellner, J. A. (2008). Nemirovski's Inequalities Revisited. ArXiv e-prints.
29

Fengler, M. R., H¨ardle, W., and Mammen, E. (2007). A semiparametric factor model for implied volatility surface dynamics. Journal of Financial Econometrics, 5(2):189­218.
Forni, M., Hallin, M., Lippi, M., and Reichlin, L. (2005). The generalized dynamic factor model: One-sided estimation and forecasting. Journal of the American Statistical Association, 100:830­ 840.
Fu, W. J. (1998). Penalized regressions: The bridge versus the Lasso. Journal of Computational and Graphical Statistics, 7(3):397­416.
Gasser, T., Mo¨cks, R., and Verleger, R. (1983). Selavco: A method to deal with trial-to-trial variability of evoked potential. Electroencephalography and Clinical Neurophysiology, 55:717­723.
Giannone, D., Reichlin, L., and Sala, L. (2005). Monetary policy in real time. In NBER Macroeconomics Annual 2004, Volume 19, NBER Chapters, pages 161­224. National Bureau of Economic Research, Inc.
Gleick et al., P. H. (2010). Climate change and the integrity of science. Science, 328:689­691.
Hall, A. and Hautsch, N. (2006). Order aggressiveness and order book dynamics. Empirical Economics, 30(4):973­1005.
Hall, P., Mller, H. G., and Wang, J. L. (2006). Properties of principal component methods for functional and longitudinal data analysis. The Annals of Statistics, 34(3):1493­1517.
Hautsch, N. and Ou, Y. (2008). Yield curve factors, term structure volatility, and bond risk premia. SFB 649 Discussion Papers SFB649DP2008-053, Sonderforschungsbereich 649, Humboldt University, Berlin, Germany.
Hedin, A. E. (1991). Extension of the msis thermosphere model into the middle and lower atmosphere. Journal of Geophysical Research, 96:1159­1172.
Janson, S. (2004). Large deviations for sums of partly dependent random variables. Random Structures Algorithms, 24(3):234­248.
Jia, J., Rohe, K., and Yu, B. (2009). The lasso under heteroscadecity. Technical Report 783, Statistics Department, UC Berkeley.
Karl, T. R., Kukla, G., Razuvayev, V. N., Changery, M. J., Quayle, R. G., Heim, R. R., and Easterling, D. R. (1991). Global warming: Evidence for asymmetric diurnal temperature change. Geophysical Research Letters, 18:2253­2256.
Kauermann, G. (2000). Modeling longitudinal data with ordinal response by varying coefficients. Biometrics, 56(3):1692­698.
Lee, R. D. and Carter, L. (1992). Modeling and forecasting the time series of u.s. mortality. Journal of the American Statistical Association, 87(419):659­671.
Lounici, K. (2008). Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators. Electronic Journal of Statistics, 2:90­102.
Lounici, K., Pontil, M., Tsybakov, A. B., and van de Geer, S. (2009). Taking advantage of sparsity in multi-task learning. Proceedings of Conference on Learning Theory (COLT) 2009.
30

Martinussen, T. and Scheike, T. (2000). A nonparametric dynamic additive regression model for longitudinal data. Annals of Statistics, 28(4):1000­1025.
Mohr, P. N. C., Biele, G., Krugel, L. K., Li, S.-C., and Heekeren, H. R. (2010). Neural foundations of risk-return trade-off in investment decisions. NeuroImage, 49(3):2556­2563.
Mysickov´a, A., Song, S., Mohr, P. N., Heekeren, H. R., and Ha¨rdle, W. K. (2010). Risk patterns and correlated brain activities. Submitted to Neuroimage.
Nelson, C. R. and Siegel, A. F. (1987). Parsimonious modeling of yield curves. Journal of Business, 60:473­489.
Odening, M., Berg, E., and Turvey, C. (2008). Management of climate risk in agriculture. Special Issue of the Agricultural Finance Review, 68(1):83C97.
Park, B. U., Mammen, E., Ha¨rdle, W., and Borak, S. (2009). Time series modelling with semiparametric factor dynamics. Journal of the American Statistical Association, 104(485):284­298.
Parton, W. J. and Logan, J. A. (1981). A model for diurnal variation in soil and air temperature. Agricultural Meteorology, 23:205 ­ 216.
Racsko, P., Szeidl, L., and Semenov, M. (1991). A serial approach to local stochastic weather models. Ecological Modelling, 57(1-2):27 ­ 41.
Ramsay, J. and Silverman, B. (2005). Functional Data Analysis, 2nd Edition. Springer. Stock, J. H. and Watson, M. W. (2002a). Forecasting using principal components from a large number
of predictors. Journal of the American Statistical Association, 97:1167­1179. Stock, J. H. and Watson, M. W. (2002b). Macroeconomic forecasting using diffusion indexes. Journal
of Business & Economic Statistics, 20(2):147­62. Stock, J. H. and Watson, M. W. (2005). Implications of dynamic factor models for var analy-
sis. NBER Working Papers 11467, National Bureau of Economic Research, Inc. available at http://ideas.repec.org/p/nbr/nberwo/11467.html. Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267­288. Worsley, K., Liao, C., Aston, J., Petre, V., Duncan, G., Morales, F., and Evans, A. (2002). A general statistical analysis for fmri data. NeuroImange, 15:1­15. Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B, 68(1):49­67.
31

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Volatility Investing with Variance Swaps" by Wolfgang Karl Härdle and Elena Silyakova, January 2010.
002 "Partial Linear Quantile Regression and Bootstrap Confidence Bands" by Wolfgang Karl Härdle, Ya'acov Ritov and Song Song, January 2010.
003 "Uniform confidence bands for pricing kernels" by Wolfgang Karl Härdle, Yarema Okhrin and Weining Wang, January 2010.
004 "Bayesian Inference in a Stochastic Volatility Nelson-Siegel Model" by Nikolaus Hautsch and Fuyu Yang, January 2010.
005 "The Impact of Macroeconomic News on Quote Adjustments, Noise, and Informational Volatility" by Nikolaus Hautsch, Dieter Hess and David Veredas, January 2010.
006 "Bayesian Estimation and Model Selection in the Generalised Stochastic Unit Root Model" by Fuyu Yang and Roberto Leon-Gonzalez, January 2010.
007 "Two-sided Certification: The market for Rating Agencies" by Erik R. Fasten and Dirk Hofmann, January 2010.
008 "Characterising Equilibrium Selection in Global Games with Strategic Complementarities" by Christian Basteck, Tijmen R. Daniels and Frank Heinemann, January 2010.
009 "Predicting extreme VaR: Nonparametric quantile regression with refinements from extreme value theory" by Julia Schaumburg, February 2010.
010 "On Securitization, Market Completion and Equilibrium Risk Transfer" by Ulrich Horst, Traian A. Pirvu and Gonçalo Dos Reis, February 2010.
011 "Illiquidity and Derivative Valuation" by Ulrich Horst and Felix Naujokat, February 2010.
012 "Dynamic Systems of Social Interactions" by Ulrich Horst, February 2010.
013 "The dynamics of hourly electricity prices" by Wolfgang Karl Härdle and Stefan Trück, February 2010.
014 "Crisis? What Crisis? Currency vs. Banking in the Financial Crisis of 1931" by Albrecht Ritschl and Samad Sarferaz, February 2010.
015 "Estimation of the characteristics of a Lévy process observed at arbitrary frequency" by Johanna Kappusl and Markus Reiß, February 2010.
016 "Honey, I'll Be Working Late Tonight. The Effect of Individual Work Routines on Leisure Time Synchronization of Couples" by Juliane Scheffel, February 2010.
017 "The Impact of ICT Investments on the Relative Demand for HighMedium-, and Low-Skilled Workers: Industry versus Country Analysis" by Dorothee Schneider, February 2010.
018 "Time varying Hierarchical Archimedean Copulae" by Wolfgang Karl Härdle, Ostap Okhrin and Yarema Okhrin, February 2010.
019 "Monetary Transmission Right from the Start: The (Dis)Connection Between the Money Market and the ECB's Main Refinancing Rates" by Puriya Abbassi and Dieter Nautz, March 2010.
020 "Aggregate Hazard Function in Price-Setting: A Bayesian Analysis Using Macro Data" by Fang Yao, March 2010.
021 "Nonparametric Estimation of Risk-Neutral Densities" by Maria Grith, Wolfgang Karl Härdle and Melanie Schienle, March 2010.

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
022 "Fitting high-dimensional Copulae to Data" by Ostap Okhrin, April 2010. 023 "The (In)stability of Money Demand in the Euro Area: Lessons from a
Cross-Country Analysis" by Dieter Nautz and Ulrike Rondorf, April 2010. 024 "The optimal industry structure in a vertically related market" by
Raffaele Fiocco, April 2010. 025 "Herding of Institutional Traders" by Stephanie Kremer, April 2010. 026 "Non-Gaussian Component Analysis: New Ideas, New Proofs, New
Applications" by Vladimir Panov, May 2010. 027 "Liquidity and Capital Requirements and the Probability of Bank Failure"
by Philipp Johann König, May 2010. 028 "Social Relationships and Trust" by Christine Binzel and Dietmar Fehr,
May 2010. 029 "Adaptive Interest Rate Modelling" by Mengmeng Guo and Wolfgang Karl
Härdle, May 2010. 030 "Can the New Keynesian Phillips Curve Explain Inflation Gap
Persistence?" by Fang Yao, June 2010. 031 "Modeling Asset Prices" by James E. Gentle and Wolfgang Karl Härdle,
June 2010. 032 "Learning Machines Supporting Bankruptcy Prediction" by Wolfgang Karl
Härdle, Rouslan Moro and Linda Hoffmann, June 2010. 033 "Sensitivity of risk measures with respect to the normal approximation
of total claim distributions" by Volker Krätschmer and Henryk Zähle, June 2010. 034 "Sociodemographic, Economic, and Psychological Drivers of the Demand for Life Insurance: Evidence from the German Retirement Income Act" by Carolin Hecht and Katja Hanewald, July 2010. 035 "Efficiency and Equilibria in Games of Optimal Derivative Design" by Ulrich Horst and Santiago Moreno-Bromberg, July 2010. 036 "Why Do Financial Market Experts Misperceive Future Monetary Policy Decisions?" by Sandra Schmidt and Dieter Nautz, July 2010. 037 "Dynamical systems forced by shot noise as a new paradigm in the interest rate modeling" by Alexander L. Baranovski, July 2010. 038 "Pre-Averaging Based Estimation of Quadratic Variation in the Presence of Noise and Jumps: Theory, Implementation, and Empirical Evidence" by Nikolaus Hautsch and Mark Podolskij, July 2010. 039 "High Dimensional Nonstationary Time Series Modelling with Generalized Dynamic Semiparametric Factor Model" by Song Song, Wolfgang K. Härdle, and Ya'acov Ritov, July 2010.

