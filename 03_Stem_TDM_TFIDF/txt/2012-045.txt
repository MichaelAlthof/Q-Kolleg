BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2012-045
Additive Models: Extensions and Related
Models.
Enno Mammen* Byeong U. Park** Melanie Schienle***
* Universität Mannheim, Germany. ** Seoul National University, Korea *** Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Additive Models: Extensions and Related Models.
Enno Mammen Byeong U. Park Melanie Schienle July 23, 2012

Abstract
We give an overview over smooth backfitting type estimators in additive models. Moreover we illustrate their wide applicability in models closely related to additive models such as nonparametric regression with dependent error variables where the errors can be transformed to white noise by a linear transformation, nonparametric regression with repeatedly measured data, nonparametric panels with fixed effects, simultaneous nonparametric equation models, and non- and semiparametric autoregression and GARCH-models. We also discuss extensions to varying coefficient models, additive models with missing observations, and the case of nonstationary covariates.
Keywords: smooth backfitting, additive models
JEL classification: C14, C30

1 Introduction

In this chapter we continue the discussion of the last chapter on additive models. We come back to the

smooth backfitting approach that was already mentioned there. The basic idea of the smooth backfitting

is to replace the least squares criterion by a smoothed version. We now explain its definition in an

additive model

E(Y |X) = µ + f1(X1) + · · · + fd(Xd).

(1.1)

We assume that n i.i.d. copies (Xi1, . . . , Xid, Yi) of (X1, . . . , Xd, Y ) are observed, or more generally, n stationary copies. Below, in Section 4, we will also weaken the stationarity assumption.
Department of Economics, Mannheim University, Germany. E-mail: emammen@rumms.uni-mannheim.de. Enno Mammen gratefully acknowledges research support of the German Science Foundation through the Collaborative Research Center 884 "Political Economy of Reforms".
Department of Statistics, Seoul National University, Korea. E-mail: bupark@stats.snu.ac.kr. Byeong U. Park's research was supported by the NRF Grant funded by the Korea government (MEST)(No. 2010-0017437).
School of Business and Economics, Humboldt University Berlin, Germany. E-mail: melanie.schienle@wiwi.hu-berlin.de. Melanie Schienle gratefully acknowledges research support of the German Science Foundation through the Collaborative Research Center 649.

1

In an additive model (1.1) the smooth backfitting estimators µ, f1, . . . , fd are defined as the minimizers

of the smoothed least squares criterion

n
Yi - µ - f1(x1) - · · · - fd(xd) 2 K
i=1

Xi1 - x1 h1

×···×K

Xid - xd hd

dx1 · · · dxd

(1.2)

under the constraint

f1(x1)pX1 (x1)dx1 = · · · = fd(xd)pXd (xd)dxd = 0.

(1.3)

Here K is a kernel function, i.e. a positive probability density function and h1, . . . , hd are bandwidths.

Furthermore, pXj is the kernel density estimator of the density pXj of Xj defined by

pXj (xj ) =

1 nhj

n
K
i=1

Xij - xj hj

.

Below, we will outline that the smooth backfitting estimator can be calculated by an iterative backfitting

algorithm. While the estimator got its name from the corresponding algorithm, it could, however, better

be described as smooth least squares estimator highlighting its statistical motivation.

If there is only one additive component, i.e. if we have d = 1, we get a kernel estimator f1(x1) =

µ + f1(x1) as the minimizer of

f1

n
Yi - f1(x1) 2 K
i=1

Xi1 - x1 h1

dx1.

(1.4)

The minimizer of this criterion is given as

n

f1(x1) =

K

i=1

Xi1 - x1 h1

-1 n
YiK
i=1

Xi1 - x1 h1

.

Thus, f1(x1) is just the classical Nadaraya-Watson estimator. We get the smooth backfitting estimator

as a natural generalization of Nadaraya-Watson smoothing to additive models.

In this chapter we present a broad discussion of estimators based on minimizing a smoothed least

squares criterion. We do this for two reasons. First, we argue that, even for additive models, this method

is a powerful alternative to the two-step procedures that were extensively discussed in the last chapter

and in the Chapter "Oracly efficient two-step estimation for additive regression". Furthermore, smooth

least squares estimators also work in models that are closely related to the additive model but are not of

the form that is directly suitable for two-step estimation. We illustrate this with an example. Suppose

that one observes (Xi, Yi) with Yi = f (Xi) + i where i is a random walk, i.e. i = i+1 - i are zero

mean i.i.d. variables that are independent of X1, . . . , Xn. In this model the Nadaraya-Watson estimator (1.4) is not consistent. Consistent estimators can be based on considering Zi = Yi+1 - Yi. For this

variables we get the regression model

Zi = f (Xi+1) - f (Xi) + i.

The smooth least squares estimator in this model is based on the minimization of

f

n
Zi - f (x1) + f (x2) 2 K
i=1

Xi+1 - x1 h1

K

Xi - x2 h2

dx1dx2.

2

Clearly, an alternative approach would be to calculate estimators f1 and f2 in the model Zi = f1(Xi+1)+ f2(Xi) + i and to use f1(x) - f2(x) as an estimator of f . We will come back to related models below.
The additive model is important for two reasons:
(i) It is the simplest nonparametric regression model with several nonparametric components. The theoretical analysis is quite simple because the nonparametric components enter linearly into the model. Furthermore, the mathematical analysis can build on localization arguments from classical smoothing theory. The simple structure allows for completely understanding of how the presence of additional terms influences estimation of each one of the nonparametric curves. This question is related to semiparametric efficiency in models with a parametric component and nonparametric nuissance components. We will come back to a short discussion of nonparametric efficiency below.
(ii) The additive model is also important for practical reasons. It efficiently avoids the curse of dimensionality of a full-dimensional nonparametric estimator. Nevertheless, it is a powerful and flexible model for high-dimensional data. Higher-dimensional structures can be well approximated by additive functions. As lower-dimensional curves they are also easier to visualize and hence to interpret than a higher-dimensional function.
Early references that highlight the advantages of additive modelling are [54], [55], [2] and [21]. In this chapter we concentrate on the discussion of smooth backfitting estimators for such additive structures. For a discussion of two-step estimators we refer to the last chapter and the chapter on two-step estimation. For sieve estimators in additive models, see [6] and the references therein. For the discussion of penalized splines we refer to [11].
In this chapter we only discuss estimation of nonparametric components. Estimation of parametric components such as  = (f1) = f1(x1)w(x1) dx1 for some given function w requires another type of analysis. In the latter estimation problem natural questions are e.g. whether the plug-in estimator  = (f1) = f1(x1)w(x1) dx1 for a nonparametric estimator f1 of f1 converges to  at a parametric 
n-rate, and whether this estimator achieves the semiparametric efficiency bound. Similar questions arise in related semiparametric models. An example is the partially linear additive model: Yi =  Zi + µ + f1(X1i) + · · · + fd(Xdi ) + i. Here, Z is an additional covariate vector. A semiparametric estimation problem arises when µ, f1, . . . , fd are nuisance components and  is the only parameter of interest. Then naturally the same questions as above arise when estimating . As said, such semiparametric considerations will not be in the focus of this chapter. For a detailed discussion of the specific example we refer to [52] and [58].
In this chapter, we concentrate on the description of estimation procedures. Smooth backfitting has been also used in testing problems by [19], [20] and [36]. For related tests based on kernel smoothing, see also the overview article [15]. In [36] additive models are used to approximate the distribution of spatial Markov random fields. The conditional expectation of the outcome of the random field at a point, given the outcomes in the neighborhood of the point, are modeled as sum of functions of the neighbored
3

outcomes. They propose tests for testing this additive structure. They also discuss the behavior of smooth backfitting if the additive model is not correct. Their findings are also interesting for other applications where the additive model is not valid but can be used as a powerful approximation.
Another approach that will not be pursued here is parametrically guided nonparametrics. The idea is to fit a parametric model in a first step and then apply nonparametric smoothing in a second step, see [16] for a description of the general idea. The original idea was suggested by [22] in density estimation. See also [50] for a similar idea.
The next section discusses the smooth backfitting estimator in additive models. In Section 3 we discuss some models that are related to additive models. The examples include nonparametric regression with dependent error variables where the errors can be transformed to white noise by a linear transformation, nonparametric regression with repeatedly measured data, nonparametric panels with fixed effects, simultaneous nonparametric equation models, and non- and semiparametric autoregression and GARCH-models. Other extensions that we will shortly mention are varying coefficient models and additive models with missing observations. In Section 4 we discuss the case of nonstationary covariates. Throughout the chapter we will see that many of the discussed models can be put in a form of noisy Fredholm integral equation of second kind. We come back to this representation in the last section. We show that this representation can be used as an alternative starting point for the calculation and also for an asymptotic understanding of smooth least squares estimators.

2 Smooth least squares estimator in additive models

2.1 The backfitting algorithm.

In the additive model (1.1) the smooth backfitting estimator can be calculated by an iterative algorithm.

To see this, fix a value of x1 and define µ1 = µ + f1(x1). One can easily see that µ1 minimizes

µ1

n
K
i=1

Xi1 - x1 h1

Yi - µ1 - f2(x2) + · · · + fd(xd) 2

×K Xi2 - x2 × · · · × K Xid - xd dx2 · · · dxd. h2 hd

(2.1)

This holds because we have no constraint on the function x1 µ + f1(x1). Thus we can minimize the criterion pointwise in this function and we do not integrate over the argument x1 in (2.1). Thus, we get

 µ1 = 

nd
K
i=1 j=1

Xij - xj hj

-1 dx2 · · · dxd

nd
× Yi - f2(x2) - · · · - fd(xd) K
i=1 j=1

Xij - xj hj

dx2 · · · dxd.

4

The expression on the right hand side of this equation can be simplified by noting that

K1 Xij -xj
hj hj

dxj =

1 for i = 1, . . . , n; j = 1, . . . , d. We get

d
µ1 = µ + f1(x1) = f1(x1) -
k=2

pX

1 ,X k
pX 1

(x1, xk (x1)

)

fk

(xk

)

dxk .

(2.2)

Here, for 1  j  d

n

fj(xj ) =

K

i=1

Xij - xj hj

-1 n
K
i=1

Xij - xj hj

Yi

=

pX

j

(xj

)-1

1 nhj

n
K
i=1

Xij - xj hj

Yi.

This is the marginal Nadaraya-Watson estimator, based on smoothing the response Yi versus one covariate Xij. Furthermore, pXj,Xk is the two-dimensional kernel density estimator of the joint density pXj,Xk of two covariates Xj and Xk: for 1  j = k  d

pXj,Xk (xj , xk)

=

1 nhj hk

n
K
i=1

Xij - xj hj

K

Xik - xk hk

.

Similarly to Eq. (2.2) we get for all j = 1, ..., d that

fj(xj) = fj(xj) - µ -
k=j

pX

j ,Xk
pX j

(xj (xj

, xk )

)

fk

(xk

)

dxk .

(2.3)

One can show that

1n

µ= n

Yi.

i=1

A proof of this equation is postponed to the end of this subsection.

(2.4)

We are now in the position to define the smooth backfitting algorithm. Our main ingredients are Eq.

(2.3) and the formula for µ. After an initialization step the backfitting algorithm proceeds in cycles of d

steps:

·

Initialization

step:

Put

µ=

1 n

n i=1

Yi

and

fj[0](xj )



0

for

j

=

1, ..., d.

· lth iteration cycle:

­ jth step of the lth iteration cycle: in step j of the lth iteration cycle one updates the

estimator fj of the jth additive component fj

j-1
fj[l](xj ) = fj(xj ) - µ -
k=1

pX

j ,Xk
pX j

(xj , xk (xj )

)

fk[l]

(xk

)

dxk

d
-
k=j+1

pX

j ,Xk
pX j

(xj , xk (xj )

)

fk[l-1]

(xk

)

dxk .

(2.5)

We now discuss some computational aspects of the smooth backfitting algorithm. One can show that there exist constants C > 0 and 0 <  < 1 that do not depend on n such that with probability tending to one

[fj[l](xj ) - fj (xj )]2pXj (xj ) dxj  C2l.

(2.6)

5

For a detailed statement, see Theorem 1 in [37] where a proof of (2.6) can be also found. The essential

argument of the proof is that the approximation error

d j=1

[fj[l](xj

)

-

fj (xj )]

behaves

like

a

function

that is cyclically and iteratively projected onto d linear subspaces of a function space. Each cycle of

projections reduces the norm of this function by a factor , for some fixed  < 1, with probability tending

to one.

The bound (2.6) allows for two important conclusions.

(i) For a fixed accuracy, the number of iterations of the algorithm can be chosen as constant in n: in particular, it does not need to increase with n.

(ii) Furthermore, for an accuracy of order n- it suffices that the number of iterations increases with a logarithmic order. This implies, in particular, that the complexity of the algorithm does not explode but increases only slowly in n. For example, assume that an accuracy of order n- with  > 2/5 is required. We will see in the next subsection that for an optimal choice of bandwidth the rate of fj(xj) - fj(xj) is of order Op(n-2/5). Then a choice of  with  > 2/5 guarantees that the numerical error is of smaller order than the statistical error.

When numerically implementing smooth backfitting, estimators fj[l](xj) are only calculated on a finite

grid of points and integrals in (2.6) are replaced by discrete approximations. Suppose that the number

of grid points is of order n for some  > 0. Then in the initialization step we have to calculate n2

two-dimensional kernel density estimators. This results in O(n1+2) calculations. Let us briefly discuss

this for the case where all functions fj(xj) have bounded support and all bandwidths are chosen so that fj(xj) - fj(xj) is of order Op(n-2/5). It can be shown that one has to choose  > 4/19 to obtain a

numerical error of smaller order than the statistical error. Then the computational complexity of the

algorithm

is

of

order

O(n log(n) + n1+2) = O(n1+2) = O(n(27/19)+2)

with



=-

4 19

.

This

amount

of calculations can still be carried out even for large values of n in reasonable time.

Proof of (2.4): To get Eq. (2.4) we multiply both sides of equation (2.3) with pXj (xj) and integrate both sides of the resulting equation over xj. Because of the norming (1.3) this yields:

0=

fj (xj )pXj (xj ) dxj

= fj(xj )pXj (xj ) dxj - µ pXj (xj ) dxj -

pXj,Xk (xj , xk)fk(xk) dxk dxj

k=j

=

1n K
nhj i=1

Xij - xj hj

Yi dxj - µ -
k=j

pXk (xk)fk(xk) dxk

=

1 n

n

Yi - µ,

i=1

where we use the facts that completes the proof.

K1 Xij -xj
hj hj

dxj = 1 and that

pXj,Xk (xj , xk) dxj = pXk (xk). This

6

2.2 Asymptotics of the smooth backfitting estimator

Under appropriate conditions, the following result holds for the asymptotic distribution of each component function fj(xj), j = 1, . . . , d:

nhj fj(xj) - fj(xj) - j(xj) -d N 0,

K 2 (u)

du

j2(xj ) pXj (xj )

.

(2.7)

Here the asymptotic bias terms j(xj) are defined as minimizers of

(1, . . . , d)

[(x) - 1(x1) - · · · - d(xd)]2pX (x) dx

under the constraint that

j (xj )pXj (xj )dxj

=

1 2

hj2

[2fj (xj )pXj (xj ) + fj (xj )pXj (xj )] dxj

u2K(u) du,

(2.8)

where pX is the joint density of X = (X1, . . . , Xd) and

1 (x) =
2

d

h2j

2fj (xj

)



log pX xj

(x)

+

fj

(xj

)

j=1

u2K(u) du.

In [37] and [40] this asymptotic statement has been proved for the case that fj is estimated on a compact
interval Ij. The conditions include a boundary modification of the kernel. Specifically, the convolution kernel h-j 1K(h-j 1(Xij - xj )) is replaced by Khj (Xij , xj ) = h-j 1K(h-j 1(Xij - xj ))/ Ij h-j 1K(h-j 1(Xij - uj)) duj. Then it holds that Ij Khj (Xij, xj) dxj = 1. In particular, this implies Ij pXj,Xk (xj, xk)dxj = pXk (xk) and Ij pXj (xj)dxj = 1 if one replaces hj-1K(hj-1(Xij - xj)) by Khj (Xij, xj) in the definitions of the kernel density estimators. In fact, we have already made excessively use of these properties of kernel
density estimators in the previous subsection.
Before illustrating how the asymptotic result (2.7) is obtained, we discuss its interpretations. In
particular, it is illustrative to compare fj with the Nadaraya-Watson estimator fj in the classical nonparametric regression model Yi = fj(Xij) + i. Under standard smoothness assumptions it holds that

nhj fj(xj) - fj(xj) - j(xj) -d N 0,

K 2 (u)

du

j2(xj ) pXj (xj )

(2.9)

with

the

asymptotic

bias

j(xj )

=

1 2

hj2

2fj

(xj

)



log pXj xj

(xj

)

+

fj

(xj )

u2K(u) du. We see that fj(xj)

has the same asymptotic variance as fj(xj) but that the two estimators differ in their asymptotic bias.

Thus, as long as one only considers the asymptotic variance, one has not to pay any price for not knowing

the other additive components fk (k = j). One gets the same asymptotic variance in the additive model as in the simplified model Yi = fj(Xij) + i where all other additive components fk (k = j) are set equal to 0. As said, the bias terms differ. The asymptotic bias of fj(xj) may be larger or smaller than that of fj(xj). This depends on the local characteristics of the function fj at the point xj and also
on the global shape of the other functions fk (k = j). It is a disadvantage of the Nadaraya-Watson smooth backfitting estimator. There may be structures in fj(xj) that are caused by other functions.

7

We will argue below that this is not the case for the local linear smooth backfitting estimator. For the local linear smooth backfitting estimator one gets the same asymptotic bias and variance as for the local linear estimator in the classical model Yi = fj(Xij) + i. In particular, both estimators have the same asymptotic normal distribution. In the last chapter this was called oracle efficiency. This notion of efficiency is appropriate for nonparametric models. Typically in nonparametric models there exists no asymptotically optimal estimator, in contrast to parametric models and to the case of estimating the parametric parts of semiparametric models.
We now come to a heuristic explanation of the asymptotic result (2.7). For a detailed proof we refer to [37] and [40]. The main argument is based on a decomposition of the estimator into a mean part and a variance part. For this purpose one applies smooth backfitting to the "data" (X1, . . . , Xd, f1(X1) + · · · + f (Xd)) and to (X1, . . . , Xd, ). We will argue below that fj(xj) is the sum of these two estimators.
Justification of (2.7): We start with a heuristic derivation of the asymptotic bias and variance of the smooth backfitting estimator fj(xj). For this purpose note first that the smooth backfitting estimators µ, f1, . . . , fd are the minimizers of

(µ, f1, . . . , fd)

[f (x) - µ - f1(x1) - · · · - fd(xd)]2pX (x) dx

(2.10)

under the constraint (1.3), where pX is the kernel density estimator of pX and f is the Nadaraya-Watson estimator of the regression function f (x) = E(Y |X = x):

pX (x)

=

1n nh1 · · · hd i=1 K

Xi1 - x1 h1

×···×K

Xid - xd hd

,

f (x)

=

pX

(x)-1

nh1

1 ··

·

hd

n
K
i=1

Xi1 - x1 h1

×···×K

Xid - xd hd

Yi.

One may show that this minimization problem leads to (2.3) and (2.4). We omit the details. For a

geometric argument see also [38].

For heuristics on the asymptotics of fj, 1  j  d, we now decompose f into its bias and variance component f (x) = f A(x) + f B(x), where

f A(x)

=

pX

(x)-1

nh1

1 ··

·

hd

n
K
i=1

Xi1 - x1 h1

×···×K

Xid - xd hd

i,

f B(x)

=

pX

(x)-1

nh1

1 ··

·

hd

n
K
i=1

Xi1 - x1 h1

×···×K

Xid - xd hd

[µ + f1(x1) + · · · + fd(xd)].

Denote by (µA, f1A, . . . , fdA) the minimizer of

(µ, f1, . . . , fd)

[f A(x) - µ - f1(x1) - · · · - fd(xd)]2pX (x) dx

under the constraint (1.3), and by (µB, f1B, . . . , fdB) the minimizer of

(µ, f1, . . . , fd)

[f B(x) - µ - f1(x1) - · · · - fd(xd)]2pX (x) dx

under the constraint (1.3). Then, we obtain µ = µA + µB, f1 = f1A + f1B, . . . , fd = fdA + fdB. By standard smoothing theory, f B(x)  µ + f1(x1) + · · · + fd(xd) + (x). This immediately implies that

8

fjB(xj)  cj + fj(xj) + j(xj) with a random constant cj. Our constraint (2.8) implies that cj can be chosen equal to zero. This follows by some more lengthy arguments that we omit.
For an understanding of the asymptotic result (2.7) it remains to show that

nhj fjA(xj) - fj(xj) -d N 0,

K 2 (u)

du

j2(xj ) pXj (xj )

.

(2.11)

To see this claim we proceed similarly as in the derivation of (2.3). Using essentially the same arguments

as there one can show that

fjA(xj ) = fjA,(xj ) - µA -
k=j

pX

j ,Xk
pX j

(xj , xk (xj )

)

fkA

(xk

)

dxk ,

(2.12)

where

n

fjA,(xj ) =

K

i=1

Xij - xj hj

-1 n
K
i=1

Xij - xj hj

i

is the stochastic part of the marginal Nadaraya-Watson estimator fj(xj). We now argue that

pX

j ,Xk
pX j

(xj , xk (xj )

)

fkA

(xk

)

dxk



pX

j ,Xk
pX j

(xj (xj

, xk )

)

fkA

(xk

)

dxk



0.

The basic argument for the second approximation is that a global average of a local average behaves like

a global average, or more explicitly, consider e.g. the local average rj(xj) = (nhj)-1

n i=1

K

Xij -xj hj

i.

This local average is of order Op(n-1/2h-j 1/2). For a smooth weight function w we now consider the

global average j = Ij w(xj)rj(xj) dxj of the local average rj(xj). This average is of order Op(n-1/2) = op(n-1/2h-j 1/2) because of

j =

w(xj)rj(xj) dxj

Ij

=

n
w(xj)(nhj)-1 K
Ij i=1

Xij - xj hj

i dxj

n
= n-1 whj (Xij )i

i=1

with whj (Xij ) = Ij w(xj )hj-1K

Xij -xj hj

dxj .

2.3 Smooth backfitting local linear estimator

In the additive model (1.1) the smooth backfitting local linear estimators µ, f1, f1, ..., fd, fd are defined as the minimizers of the smoothed least squares criterion

n Yi - µ - f1(x1) - f1(x1)(Xi1 - x1) - · · · - fd(xd) - fd(xd)(Xid - xd) 2
i=1
× K Xi1 - x1 × · · · × K Xid - xd dx1 · · · dxd h1 hd

(2.13)

under the constraint (1.3). This is a natural generalization of the local linear estimator. For the case

d = 1 the minimization gives the classical local linear estimator as the minimization of (1.4) leads to the

9

classical Nadaraya-Watson estimator. The estimators, fj, 1  j  d, are estimators of the derivatives of the additive components fj.
The smooth backfitting local linear estimator is given as the solution of a random integral equation. Similarly to Eq. (2.3), the tuples (fj, fj) fulfill now a two-dimensional integral equation. This integral equation can be used for the iterative calculation of the estimators. For details we refer to [37]. We only mention the following asymptotic result from [37] for the smooth backfitting local linear estimator that holds under appropriate conditions: for 1  j  d

nhj fj(xj) - fj(xj) - j(xj) -d N 0,

K 2 (u)

du

j2(xj ) pXj (xj )

,

(2.14)

where now the asymptotic bias terms j(xj) are defined as

j (xj )

=

1 2

h2j

fj (xj) -

fj (uj )pXj (uj ) duj

u2K(u) du.

Up to an additive norming term, the asymptotic bias of fj(xj) coincides with the asymptotic bias of local linear estimator fj in the classical nonparametric regression model Yi = fj(Xij) + i. Moreover, we get the same asymptotic distribution for both estimators (up to an additive norming term). Asymptotically one does not lose any efficiency by not knowing the additive components fk : k = j compared to the oracle model where these components are known. This is an asymptotic optimality result for the local linear smooth backfitting. It achieves the same asymptotic bias and variance as in the oracle model. As discussed above, the Nadaraya-Watson smooth backfitting estimator achieves only the asymptotic variance of the oracle model. For an alternative implementation of local linear smooth backfitting, see [41].

2.4 Smooth backfitting as solution of a noisy integral equation

We write the smooth backfitting estimators as solutions of an integral equation. We discuss this briefly for Nadaraya-Watson smoothing. Put f (x1, . . . , xd) = (f1(x1), . . . , fd(xd)) and f (x1, . . . , xd) = (f1(x1), . . . , fd(xd)) . With this notation we can rewrite (2.3) as

f (x) = f (x) - H(x, z)f (z) dz,

(2.15)

where for each value of x, z  R the integral kernel H(x, z) is a matrix with element (j, k) equal to pXj,Xk (xj, xk)/pXj (xj). This representation motivates an alternative algorithm. One can use a discrete approximation of the integral equation and approximate the integral equation (2.15) by a finite linear equation. This can be solved by standard methods of linear algebra. Eq. (2.15) can also be used as an alternative starting point for an asymptotic analysis of the estimator f . We will come back to this in Section 5 after having discussed further on those models in Section 3 whose estimation can be formulated as solving an integral equation.

10

2.5 Relations to classical backfitting and two-stage estimation

Smooth backfitting (2.5) is related to classical backfitting and to two-stage estimation. In the classical backfitting, the jth step of the lth iteration cycle (2.5) of the smooth backfitting is replaced by

fj[l](Xij )

=

pX j

(xj )-1

1 nhj

n
K
i=1

Xij - xj hj

 j-1


d

Yi - µ - fk[l](Xik) -

fk[l-1] (Xik )

k=1

k=j+1

(2.16)

for 1  j  d and 1  i  n. This iteration equation can be interpreted as a limiting case of (2.5) where one lets the second bandwidth hk in the definition of the kernel density estimator pXj,Xk (xj, xk) converge to zero.
If the backfitting algorithm runs through O(log n) cycles, the algorithm needs O(n log n) calculation steps. This is slightly faster than the smooth backfitting. In contrast to the smooth backfitting, the backfitting estimator is only defined as the limit of the iterative algorithm (2.16). Note that the smooth backfitting is explicitly defined as minimizer of the smoothed least squares criterion (1.2). The fact that backfiitng estimators are only implicitly defined as limit of an iterative algorithm complicates the asymptotic mathematical analysis. Note also that the algorithm runs in Rn, i.e. in spaces with increasing dimension. An asymptotic treatment of the classical backfitting can be found in [48] and [49]. [47] illustrated by simulation that smooth backfitting, in comparison with the classical backfitting, is more robust against degenerated designs and a large number of additive components. The reason behind this is that the iteration equation (2.5) is a smoothed version of (2.16). The smoothing stabilizes the "degenerated integral equation" (2.16). In [48] and [49] stronger assumptions are made on the joint density of the covariates than are needed for the study of the smooth backfitting. This may be caused by the same reasons, but there has been made no direct theoretical argument that supports the empirical finding that the classical backfitting is more sensitive to degenerate designs than smooth backfitting. For another modification of the classical backfitting that takes care of correlated covariates, see [24].
Two-stage estimation differs from smooth backfitting in several respects. First of all, only two steps are used instead of an iterative algorithm that runs until a convergence criterion is fulfilled. Furthermore, different bandwidths are used in different steps: undersmoothing is done in the first-step, but an optimal bandwidth is chosen in the second-step. The algorithm of two-step estimation is as simple as that of backfitting. On the other hand, choice of the bandwidth in the first-step is rather complex. Asymptotically, optimal choices will not affect the first order properties of the outcomes of the secondstep. But for finite samples the influence of the first-step bandwidth is not clear. The calculation of theoretically optimal values would require a second-order optimal theory that is not available and, as other higher-order theory, may not be accurate for small to moderate sample sizes. In particular, in models with many nonparametric components, backfitting may be preferable because it does not require an undersmoothing step.

11

2.6 Generalized Additive Models

We now discuss nonlinear extensions of the additive models. In a generalized additive model a link

function g is introduced and it is assumed that the following equation holds for the regression function

E(Y |X1, . . . , Xd)

E(Y |X1, . . . , Xd) = g-1{µ + f1(X1) + · · · + fd(Xd)}.

It has been considered that the link function is known or that it is unknown and has to be estimated.

An important example where generalized additive models make sense are models for binary responses Y . If Y is {0, 1}-valued, the function g-1 maps the additive function onto the interval [0, 1]. In the gener-

alized additive model, the additive functions f1, . . . , fd can be estimated by smoothed least squares. An alternative approach for heterogenous errors is a smoothed quasi-likelihood criterion. Quasi-likelihood

is motivated for regression models where the conditional variance of the errors is equal to V (µ) with µ

equal to the conditional expectation of Y . Here, V is a specified variance function. Quasi-likelihood co-

incides with classical likelihood if the conditional error distribution is an exponential family. It also leads

to consistent estimators if the conditional variances have another form. The quasi-likelihood criterion

Q(µ, y) is defined as:

 y-µ

Q(µ, y) =

.

µ V (µ)

An early reference to quasi-likelihood approaches in additive models is [21]. For the discussion of local

linear smoothing in generalized partially linear models see also [14]. For a discussion of the asymptotics

of classical backfitting in generalized additive model, see [26]. The Smoothed Quasi-Likelihood criterion

is defined as follows: Minimize for f = (µ, f1, . . . , fd)

SQ(f ) =

n
Q(g-1(f +(x)), Yi)K
i=1

Xi1 - x1 h1

×···×K

Xid - xd hd

dx1 · · · dxd.

where f +(x) = µ + f1(x1) + · · · + fd(xd). Minimization of the smoothed quasi-likelihood criterion over f results in the smoothed maximum quasi-liklihood estimator. Algorithms for the calculation of

this estimator were discussed in [57]. In that paper an asymptotic theory for this estimator was also

developed. In other applications the quasi-likelihood criterion may be replaced by other M-functionals.

We do not discuss this here. An example is quantile regression. For a discussion of backfitting and

smooth backfitting in additive quantile models, see [28].

3 Some models that are related to additive models.
In linear regression, the standard least squares method produces consistent estimators when the errors are uncorrelated. When the errors are correlated, the method may not give consistent or efficient estimators of the regression parameters. In the latter case it is often appropriate to take a linear transformation of the response variable in such a way that it corrects for the correlations between the errors. Linear transformations may be also used to remove some unobserved effects in a regression model that are
12

correlated with the regressors or errors. Taking a linear transformation in parametric linear models does not alter the linear structure of the model, so that conventional methods still work with the transformed data. In nonparametric regression models, however, it often yields an additive model where classical smoothing methods can not be applied, as we illustrate on several cases in this section. Some of the models of this section were also discussed in the overview papers [31] and [44]. A general discussion of smooth least squares in a general class of nonparametric models can also be found in [39].

3.1 Nonparametric regression with time series errors

Suppose we observe (Xt, Yt) for 1  t  T such that Yt = f (Xt) + ut, where the errors ut have an AR(1) time series structure so that t = ut - ut-1 is a sequence of uncorrelated errors. The transformed model Zt()  Yt - Yt-1 = f (Xt) - f (Xt-1) + t has uncorrelated errors, but has an additive structure in the mean function. For simplicity, assume that the errors ut are independent of the covariates Xt. Then, the target function f minimizes

1 QT (m) = T

T

E [Zt() - m(Xt) + m(Xt-1)]2

t=1

over m, so that it satisfies

[E(Zt()|Xt = x, Xt-1 = y) - f (x) + f (y)] [g(x) - g(y)] f0,1(x, y) dx dy = 0

(3.1)

for all square integrable functions g. Here f0,1 denotes the joint density of (Xt, Xt-1) and f0 is the density of Xt. The equation (3.1) holds for all square integrable functions g if and only if

f (x) = f(x) - H(x, y)f (y) dy

(3.2)

where

f(x)

=

1 1 + 2

[E(Zt()|Xt

=

x)

- E(Zt()|Xt-1

=

x)] ,

 H(x, y) = - 1 + 2

f0,1(x, y) + f0,1(y, x)

f0(x)

f0(x)

.

An empirical version of the integral equation (3.2) may be obtained by estimating f0, f0,1, E(Zt()|Xt = ·) and E(Zt()|Xt-1 = ·). Let f (·, ) denotes the solution of the latter integral equation. In case  is known, f (·, ) can be used as an estimator of f . Otherwise, the parameter  can be estimated by  that

minimizes

1T

2

T Zt() - f (Xt, ) + f (Xt-1, ) ,

t=1

and then f by f = f (·, ). We note that the estimator f (·, ) is consistent even if the autoregressive coef-

ficient  = 1. In contrast, smoothing of the original untransformed data (Yt, Xt) leads to an inconsistent

estimator. We mentioned this example already in the introduction.

The above discussion may be extended to a general setting where the errors ut admit a time series

structure such that t =

 j=0

aj

ut-j

is a sequence of uncorrelated errors.

In this general case, if we

13

take the transformation Zt(a0, a1, . . .) =

 j=0

aj

Yt-j

,

then

the

transformed

model

Zt(a0, a1, . . .)

=

 j=0

aj

f

(Xt-j

)

+

t

has

an

additive

structure

with

uncorrelated

errors.

For

a

discussion

of

this

general

case, see [33]. There weaker assumptions are made on the errors ut. In particular, it is not assumed that

the errors ut are independent of the covariates Xt.

3.2 Nonparametric regression with repeated measurements.

Suppose that one has J repeated measurements on each of n subjects. Let (Xij, Yij) be the jth observation on the ith subject. Write Xi = (Xi1, . . . , XiJ ) and Yi = (Yi1, . . . , YiJ ) . Assume that (Xi, Yi), i = 1 . . . , n, are i.i.d. copies of (X, Y). Consider the simple nonparametric regression model

Yij = f (Xij ) + ij ,

(3.3)

where the errors ij have zero conditional mean, but are allowed to be correlated within each subject. Let i = ( i1, . . . , iJ ) and  = cov( i). The kernel regression estimator based on the ordinary least squares criterion is consistent even in this case where  is not the identity matrix. However, we may find a better estimator which is based on a weighted least squares criterion. This is in line with parametric linear regression with repeated measurements, where a weighted least squares estimator outperforms the ordinary least squares estimator. A weighted least squares estimation is equivalent to taking a linear transformation of the response and then applying the ordinary least squares criterion to the transformed model. In contrast to the parametric case, introducing weights in the nonparametric model (3.3) leads to a more complicated estimation problem, as is demonstrated below.
Let f (x1, . . . , xJ ) = (f (x1), . . . , f (xJ )) . The regression function f at (3.3) minimizes

E[{Y - m(X1, . . . , XJ )} -1{Y - m(X1, . . . , XJ )}]

(3.4)

over all square integrable functions m, where m(x1, . . . , xJ ) = (m(x1), . . . , m(xJ )) . Note that the transformed response vector -1/2Y admits an additive model and the variance of the transformed error vector -1/2 equals the identity matrix. The minimizer f satisfies
JJ
jkE[Yj - f (Xj)]g(Xk) = 0
j=1 k=1
for all square integrable functions g, where jk denotes the (j, k)th entry of the matrix -1. This gives
the following integral equation for f ;

f (x) = f (x) - H(x, z)f (z) dz,

(3.5)

where


J

-1
JJ

f (x) =  jjpj(x)

jkE(Yk|Xj = x)pj(x),

j=1

j=1 k=1


J

-1
JJ

H(x, z) =  jjpj(x)

jkpjk(x, z).

j=1

j=1 k=j

14

Here, pj and pjk denote the densities of Xj and (Xj, Xk), respectively. The quantities f , pj and pjk can be estimated by the standard kernel smoothing techniques. Plugging these into (3.5) gives an integral

equation for estimating f . One may apply other weighting schemes replacing -1 at (3.4) by a weight matrix W. It can be
shown the choice W = -1 leads to an estimator with the minimal variance, see [4] for details. The

foregoing weighted least squares regression may be extended to the additive regression model Yij =

D d=1

fd(Xidj )

+

ij with covariates Xij = (Xi1j, . . . , XiDj )

. Details are also given in [4].

3.3 Panels with individual effects

Suppose we have panel data (Xij, Yij) for i = 1, . . . , n and j = 1, . . . , J. We assume that

Yij = f (Xij ) + i + ij ,

(3.6)

where i are the unobserved random or nonrandom individual effects that are invariant over time j, and ij are errors such that E( ij|Xi1, . . . , XiJ ) = 0. The individual effect i can be uncorrelated or correlated with the regressors Xi1, . . . , XiJ and the error variables ij. If E(i|Xi1, . . . , XiJ ) = 0, then the model

reduces to the model considered in Subsection 3.2. An interesting case is when the individual effect is

correlated with the regressors so that E(i|Xi1, . . . , XiJ ) = 0. In this case, the ordinary nonparametric kernel regression fails to obtain a consistent estimator. Recall that the latter is also the case with

parametric linear regression.

Here again, we may use a simple linear transformation to remove the unobserved individual effect

from the regression model. Let Zi =

J j=1

aj Yij

for

some

constants

aj

such

that

J j=1

aj

=

0.

Examples

include

(i) a1 = · · · = ak-2 = 0, ak-1 = -1, ak = 1, ak+1 = · · · = aJ = 0 for some 1  k  J ;

(ii) a1 = · · · = ak-1 = -J -1, ak = 1 - J -1, ak+1 = · · · = aJ = -J -1 for some 1  k  J .

For the transformed response variables Zi, we obtain

J
Zi = aj f (Xij ) + ui,
j=1

(3.7)

where ui =

J j=1

aj

ij

has

zero

conditional

mean

given

Xi1, . . . , XiJ .

Let

Z

and

Xj

denote

the

generics

of Zi and Xij, respectively. Since f minimizes the squared error risk E[Z -

J j=1

aj m(Xj )]2

over

m,

it

satisfies


J


J

E Z - ajf (Xj) ajg(Xj) = 0

j=1

j=1

(3.8)

for all square integrable functions g. The equation (3.8) is equivalent to


J

JJ


J

 ajE(Z|Xj = x)pj(x) -

ajakE[f (Xk)|Xj = x]pj(x) - f (x) a2j pj(x) g(x) dx = 0,

j=1

j=1 k=j

j=1

15

where pj and pjk denote the density of Xj and (Xj, Xk), respectively. This gives the following integral

equation

f (x) = f (x) - H(x, z)f (z) dz,

(3.9)

where


J

-1
J

f (x) =  a2j pj(x)

ajE(Z|Xj = x)pj(x),

j=1

j=1


J

-1
JJ

H(x, z) =  a2j pj(x)

ajakpjk(x, z).

j=1

j=1 k=j

As in the additive regression model we need a norming condition for identification of f in the

transformed model (3.7). The reason is that in the transformed model we have

J j=1

aj

f

(Xij

)

=

J j=1

aj

[c

+

f

(Xij

)]

for

any

constant

c

since

J j=1

aj

=

0.

We

may

also

see

this

from

the

integral

equation

(3.9) since H(x, z) dz = -1. For a norming condition, we may define i such that E(Yij) = Ef (Xij).

This motivates the normalizing constraint

J
J -1
j=1

nJ

f (x)pj(x) dx = n-1J -1

Yij

i=1 j=1

for an estimator f of f .

The differencing technique we have discussed above may also be applied to a more general setting that

allows for discrete response variables. For example, consider a binary response model where each of the n

subjects has matched observations (Xij, Yij) such that the responses Yij, conditionally on the regressors

Xi1, . . . , XiJ and the individual effect i, are independent across j and have Bernoulli distributions with

success probabilities p(Xij, i), respectively. Assume that

log

p(Xij , i) 1 - p(Xij, i)

= f (Xij) + i

and consider the case where J = 2 for simplicity. Let Zi = I(Yi1 = 1) and Ni = Yi1 + Yi2, where I

denotes the indicator function. Then, it can be shown that

log

E(Zi|Xi1, Xi2, Ni = 1) 1 - E(Zi|Xi1, Xi2, Ni = 1)

= f (Xi1) - f (Xi2).

(3.10)

This follows from the equation

E(Zi|Xi1, Xi2, Ni = 1) = E

E p(Xi1, i)(1 - p(Xi2, i)) Xi1, Xi2 p(Xi1, i)(1 - p(Xi2, i)) + p(Xi2, i)(1 - p(Xi1, i)) Xi1, Xi2

and the fact that

p(Xi1, i)[1 - p(Xi2, i)] p(Xi1, i)[1 - p(Xi2, i)] + p(Xi2, i)[1

- p(Xi1, i)]

=

exp[f (Xi1) - f (Xi2)] 1 + exp[f (Xi1) - f (Xi2)]

does not involve i.

Let Z, Xj, Yj denote the generics for Zi, Xij, Yij, respectively. The function f in the transformed

model (3.10) maximizes the expected log-likelihood, so that it satisfies

E I(N = 1) [Z - (X1, X2; f )] [g(X1) - g(X2)] = 0

16

for all square integrable function g, where

exp[m(x) - m(y)]

(x, y;

m)

=

1

+

exp[m(x)

-

. m(y)]

It can be shown that f satisfies F (f ) = 0, where F is a nonlinear operator defined by

F (m)(x) = E I(N = 1)(Z - (X1, X2; m)) X1 = x p1(x)-E I(N = 1)(Z - (X1, X2; m)) X2 = x p2(x)

and pj denotes the density of Xj, j = 1, 2. Here, we also need a norming condition for identifiability of f . The integral equation F (m) = 0 is nonlinear, but it can be linearized in the same way as the nonlinear
equation in Section 2. The linear approximation basically puts the problem back to the framework for the model (3.6). To detail this, define 1(x, y; m) = [1 + exp(m(x) - m(y))]-2, let f [0] be an a function f [0] close to f . Note that F (m) F (f [0]) + F1(f [0])(m - f [0]) where F1(f [0]) is a linear operator and F1(f [0])(g) denotes the Fr´echet differential of F at f [0] with increment g. Put  = f - f [0] and

H0(x, y) = E [I(N = 1)|X1 = x, X2 = y] 1(x, y; f [0])p12(x, y) + E [I(N = 1)|X1 = y, X2 = x] 1(y, x; f [0])p12(y, x),

where p12 denotes the density of (X1, X2). Then, the approximating linear integral equation F (f [0]) + F1(f [0])() = 0 is equivalent to

(x) = (x) - H(x, y)(y) dy,

(3.11)

where

(x) =

-1
H0(x, y) dy F (f [0])(x),

-1
H(x, y) = - H0(x, z) dz H0(x, y).

We may estimate F and H0 by kernel methods. Plugging the estimators F and H0 into (3.11) gives an integral equation for the update f [1] of the starting estimator f [0]. The statistical properties of the resulting backfitting algorithm and the limit of the algorithm f which satisfies F (f ) = 0 have been studied by [23].

3.4 Additive models for panels of time series and factor models

Similar to (3.6), one can consider models with an unobserved time effect t instead of an individual effect. We now denote time by t. Suppose that we have panel data (Xi1t, . . . , Xidt, Yit) for individuals 1  i  n and time points 1  T . We assume that

d
Yit = mj (Xijt) + t + it.
j=1

(3.12)

This model naturally generalizes linear panel data models. It has been studied in [43] for two asymptotic frameworks: n  , T fixed and n, T  . Their asymptotic analysis includes the case where {Xijt},

17

j = 1, . . . , p, are time lagged values of Yit. No assumptions are made on the unobserved temporary effects t. They may be deterministic or random, and they may be correlated with covariates or error

terms. The basic idea of [43] is to use difference schemes that cancel out the time effects t, simliar to

the approaches in the last subsection that cancel out individual effects. Here, the values t are nuissance parameters.

In [35] also the model (3.12) is considered, but the statistical aim there is inference on the structure of

t. It is assumed that t is a random process following a parametric specification. A two-step procedure is

proposed where the process t is fitted in the first-step. In their mathematics they compare parametric inference based on the fitted values of t with an infeasible statistical inference that is based on the

unobserved t. The main result is that these two approaches are asymptotically equivalent. This can be

interpreted as an oracle property and it can be used to construct efficient estimators of the parameters.

Another modification of model (3.12) is the factor model

d
Ytl = m0(Xt0l) + Ztj mj (Xtjl) + tl
j=1

(3.13)

for l = 1, . . . , L. Here, the dynamics of the L-dimensional process Yt is approximated by the unobserved

d-dimensional time series Zt. The basic idea is that elements Ytl of Yt with similar characteristics (Xtjl : 1  j  d) show similar dynamics and that the dynamics of Yt can be accurately modeled by choices of d that are much smaller than L. This model has been applied in [8] to the analysis of stock returns Ytl with characteristics (Xtjl : 1  j  d). Again, a two-step procedure is proposed where in the first-step the unobserved process Zt is fitted. Also, an oracle property applies: inference based on

estimates Zt of Zt is asymptotically equivalent to infeasible inference based on the unobserved Zt. In [18] and [51] the following model has been considered

d
Ytl = m0(Xtl) + Ztj mj (Xtl) + tl.
j=1
This model differs from (3.13) because now the nonparametric components mj are functions of a single
characteristic Xtl. As a result, the multivariate time series Zt is only identified up to linear transfor-

mations. Again, an oracle property for parametric inference based on fitted values has been shown in

[51]. The model has been used in functional principal component analysis. One application in [18] and

[51] is for implied volatility surfaces that develop over time. The surfaces are approximated by a finite-

dimensional process and the random movement of the surfaces is then analyzed by a VAR representation

of the finite-dimensional process.

3.5 Semiparametric GARCH models

Another example that leads to an additive model is a semiparametric GARCH model. In this model we

observe a process Yt such that E(Yt|Ft-1) = 0, where Ft-1 denotes the sigma field generated by the entire past history of the Y process, and t2  E(Yt2|Ft-1) assumes a semiparametric model

t2 = t2-1 + f (Yt-1).

(3.14)

18

This model is a natural generalization of the GARCH(1,1) model of [1] where a parametric assumption is

made on f such that f (x) = +x. The generalization was introduced by [13] to allow for more flexibility

in the `news impact curve', i.e., the function f , which measures the effect of news onto volatilities in

financial markets.

The parameters  and the function f in the semiparametric model (3.14) are unknown. Since

E(Yt2|Ft-1) =

 j=1

j-1

f

(Yt-j

),

the

parameter



and

the

function

f (·, )

together

minimize

E[Y02

-

 j=1

j-1f

(X-j

)]2.

For

each

,

let

f

denote

the

minimizer

of

the

criterion.

Then,

it

satisfies





j+k-2f(Y-j )g(Y-j ) = E[Y02g(Y-j )]

j=1 k=1

j=1

for all square integrable functions g. This gives the following integral equation.

f(x) = f(x) - H(x, y)f(y) dy,

(3.15)

where



f(x) = (1 - 2) j-1E(Y02|Y-j = x),

j=1


H(x, y) = j
j=1

p0,-j (x, y) + p0,j (x, y) p0(x)

,

p0 and p0,j are the densities of Y0 and (Y0, Yj), respectively. For an asymptotic and empirical analysis of the estimators based on the integral equation (3.15), we refer to [32]. For a recent extension of the model, see also [5].

3.6 Varying coefficient models

Suppose we are given a group of covariates X1, . . . , Xd and a response Y . The most general form of varying coefficient model was introduced and studied by [30]. It is given by



E(Y |X1, . . . , Xd) = g-1  Xkfk1(X1) + · · · + Xkfkp(Xp) ,

kI1

kIp

(3.16)

where g is a link function and p  d. The index sets Ij may intersect with each other, but each Ij does not include j. It is also allowed that the two groups of covariates, {Xj : 1  j  p} and {Xk : k  pj=1Ij} may have common variables. The coefficient functions are identifiable if we put the following constraints: for nonnegative weight functions wj, (i) fkj(xj)wj(xj) dxj = 0 for all k  pj=1Ij and 1  j  p; (ii)
xjfkj(xj)wj(xj) dxj = 0 for all j, k  {1, . . . , p}  (jp=1Ij). In this model, the effect of the covariate Xk for k  pj=1Ij is set in a nonparametric way as j:Ij k fkj(Xj). The model is flexible enough to include various types of varying coefficient models as special cases. For example, it is specialized to
the generalized additive model discussed in Section 2.6 if one takes I1 = · · · = Ip = {p + 1} and set Xp+1  1. The model also reduces to the varying coefficient model studied by [29] and [56] if the two

19

groups, {Xj : 1  j  p} and {Xk : k  pj=1Ij}, are disjoint and the sets Ij contain only one element (1  j  p). In this case one can rewrite model (3.16) as


p



Yi = g-1  Zij fj (Xij ) + i.

j=1

With an identity link g and with the additional constraint fj  f , this model has been used in [34] for
nonparametric estimation of yield curves by smoothed least-squares. There, Yi was the trading price of a coupon bond, Zij denotes the payment returned to the owner of bond i at date Xij and f is the discount function. In case p = 1 and I1 = {2, . . . , d}, the approach with disjoint sets of covariates results in the
model studied, for example, by [17].
For simplicity, suppose that the link g is the identity function. In this case, the coefficient functions
2
fkj minimize E Y - kI1 Xkfk1(X1) - · · · - kIp Xkfkp(Xp) . This gives the following system of integral equations for fkj: for 1  j  p,

fj (xj ) = E(Xj Xj |Xj = xj )-1E(Xj Y |Xj = xj ) - E(Xj Xj |Xj = xj )-1

p
×
l=1,=j

E

Xj Xl

Xj = xj , Xl = xl

fl(xl)

pjl(xj , xl) pj (xj )

dxl,

where Xj = (Xk : k  Ij) and fj = (fkj : k  Ij). Note that Xj does not contain Xj as its entry. To get an empirical version of the above integral equations, one may replace the conditional expectations, the joint density pjl of (Xj, Xl) and the marginal density pj of Xj, by kernel estimators. [30] presented complete theory for the estimation of the general model (3.16). Their theory includes sieve and penalized

quasi-likelihood estimation as well as the smooth backfitting method described above.

3.7 Missing observations
Additive models can also be consistently estimated if the tuples (Yi, Xi1, ..., Xid) are only partially observed. We will discuss this for the following simple scheme of missing observations.
Denote
- by Njk the set of indices i where Xij and Xik are observed, - by Nj the set of indices i where Xij is observed, - by N0j the set of indices i where Xij and Yi are observed, and - by N0 the set of indices i where Yi is observed.
These sets may be random or nonrandom. We denote the number of elements of these sets by Njk, Nj, N0j or N0, respectively. We assume that the observations {(Xij, Xik) : i  Njk}, {Xij : i  Nj}, {(Xij, Yi) : i  N0j} and {Yi : i  N0} are i.i.d. This assumption holds under simple random missingness schemes and also in the case of pooling samples where different subsets of covariates were observed.
Then, under the assumption that Njk  , Nj  , N0j   and N0  , the estimators of pXj,Xk , pXj , fj and µ that are based on the subsamples Njk, Nj, N0j or N0, respectively, are consistent.
20

More precisely, for 1  j = k  d, put

pXj,Xk (xj , xk)

=

1 K
Njkhj hk iNjk

Xij - xj hj

K

Xik - xk hk

,

pXj (xj ) =

1 K
Nj hj iNj

Xij - xj hj

,

fj(xj )

=

pX

j

(xj

)-1

1 N0j

hj

iN0j

K

Xij - xj hj

Yi,

1n µ = N0 iN0 Yi.

Under appropriate conditions on the bandwidths hj these estimators converge to pXj,Xk (xj, xk), pXj (xj), fj(xj) and µ, respectively, in probability. Similarly as in Eq. (2.2), we consider the solutions f1, . . . , fd of the equations

fj(xj) = fj(xj) - µ -
k=j

pX

j ,Xk
pX j

(xj (xj

, xk )

)

fk

(xk

)

dxk .

Using the stochastic convergence of pXj,Xk (xj, xk), pXj (xj), fj(xj) and µ, one can show that fj(xj) converges in probability to fj(xj) for 1  j  d. These consistency proofs can be generalized to more complex missingness schemes. Furthermore, under appropriate conditions one can study normal distribution limits of these estimators. We remark that these identification, consistency and asymptotic normality results are not available for the full-dimensional model specification: Y = f (X1, . . . , Xd) + .

3.8 Additive diffusion models
Some multivariate diffusion models are based on additive parametric specifications of the mean. Nonparametric generalizations of such models were considered in [19]. There also nonparametric specifications of the volatility term were considered.

3.9 Simultaneous nonparametric equation models
Additive models also naturally occur in economic models, where some covariates are correlated with the disturbance. Despite these so-called endogenous regressors, such models can be identified via a control function approach. In particular, [46] proposed the following model with additive error terms
Y = f (X1, Z1) + e ,
where X1 and Z1 are observed covariates and Y is a one-dimensional response. While Z1 is independent of the error variable e, no assumptions are made on the dependence between X1 and e at this stage. For identification, however, assume that the following control equation holds for the endogenous variable X1
X1 = h(Z1, Z2) + V,

21

where Z2 is an observed covariate not contained in the original equation and (Z1, Z2) is independent of the joint vector of errors (e, V ).
Under the stated independence conditions, it follows that

E(Y |X1, Z1, Z2) = f (X1, Z1) + (V ) = E[Y |X1, Z1, V ]

(3.17)

with (V ) = E(e|V ). Thus, we get an additive model where the regressor in the second additive

component is not observed but can be estimated as residual of the control equation. This additive model

can be also obtained under slightly weaker conditions than the above independence conditions, namely

under the assumption that E(e|Z1, Z2, V ) = E(e|V ) and E(V |Z1, Z2) = 0. The corresponding system

of integral equations to be solved for (3.17) is

f (x1, z2) = f (x1, z2) -

pX1 ,Z2 ,V pX1 ,Z2

(x1, (x1,

z2, v) z2)

(v)dv

(v) = (v) -

pX1,Z2,V (x1, z2, v) f (x1, z2)d(x1, z2) pV (v)

where f (z1, z2) = E[Y |(X1, Z1) = (x1, z2)] and (v) = E(Y |V = v). Note that some ingredients of the

smooth backfitting iteration algorithm thus require nonparametric pre-estimates of marginal objects with

the nonparametrically generated regressor V = X1 - h(Z1, Z2). The paper [42] studies how asymptotic

theory in nonparametric models has to be adjusted to take care of nonparametrically generated regressors.

4 Nonstationary observations
Additive models are a powerful tool in case of stochastically nonstationary covariates. For this data generality, consistent estimation of a fully nonparametric model requires that the whole compound vector fulfills a specific recurrence condition, i.e. it has to be guaranteed that the full dimensional process X returns infinitely often to local neighborhoods. For an additive model, however, recurrence conditions are only needed for two-dimensional subvectors of X. An illustrative example is a multivariate random walk. A fully nonparametric model cannot be consistently estimated for dimensions greater two, since beyond dimension two random walks become transient and do not fulfill the above recurrence property. For an additive model, however, there is no dimension restriction, as any pair of bivariate random walks is recurrent. Here we briefly outline the main ideas. The detailed theory of additive models for nonstationary covariates is developed in [53].
The setting is as follows: Suppose we want to estimate a standard additive model (1.1) where covariates and response are potentially nonstationary Markov chains but satisfy a pairwise recurrence condition, and the residual is stationary mixing. Instead of a stationary data generating process density function, a nonstationary pairwise recurrent Markov chain can be characterized by the densities of pairwise bivariate invariant measures jk with j, k  {1, . . . , d}. For the specific kind of recurrence imposed, it is guaranteed that such a bivariate invariant measure exists for each pair and is unique up to a multiplicative constant; but it is generally only finite on so-called small sets and only -finite on the full support. Note that e.g., for random walks any compact set is small.
22

Furthermore, under the type of pairwise recurrence imposed, bivariate component Markov chains (Xj, Xk) = (Xjk) can be decomposed into i.i.d. parts of random length depending on the recurrence times of the chain. In particular, the stochastic number of recurrence times T jk(n) characterizes the amount of i.i.d. block observations and thus corresponds to the effective sample size available for inference with the particular pair of components. Thus for different components and pairs of components available effective sample sizes are path dependent and generally vary depending on the recurrence frequency being smaller for more nonstationary processes and closer to the stationary deterministic full sample size n for more stationary processes. Correspondingly, consistent kernel type estimators are weighted averages of T jk(n) i.i.d. block elements

j k (xj k )

=

1 T jk(n)

K

iIjk

Xijk - xjk hjk

,


fj(xj) =  K
iIj

Xij - xj hj

-1
K
iIj

j(k)(xj )

=

1 T jk(n)

K

iIjk

Xij - xj hj

,



fj(k)(xj ) = 

K

iIjk

Xij - xj hj

-1
K
iIjk

Xij - xj hj
Xij - xj hj

Yi, Yi.

(4.1) (4.2)

The estimators in (4.1) provide pointwise consistent estimates of the corresponding bivariate invari-

ant measure density jk and a general nonparametric link function fj, respectively (see [27]). Their
rates of convergence are driven by respective recurrence frequencies and occupation times Ljk(xjk) = iIjk Kxjk,hjk (Xijk) and Lj, respectively, which are generally of different order on average over all
sample paths. Asymptotically in both cases, they are on average of size (njk h)-1/2 and (nj h)-1/2,

respectively, where the global jk-parameter  [0, 1] characterizes the underlying type of nonstationarity

of the corresponding recurrent chain as the tail index on the distribution of recurrence times. For a

bivariate random walk we have jk = 0, for a stationary process jk = 1 recovering standard rates,

and generally jk  j. The kernel estimators in (4.2) artificially "downgrade" their univariate speed of

convergence to the respective bivariate one. Note that the index sets Ijk ensure that only T jk(n) i.i.d. sub-blocks are considered of the T j(n) original ones.

For balancing terms in the empirical version of the smooth backfitting integral equations, such potentially slower than standard estimators j(k), j(kl ) and fj(k) of bivariate nonstationary type jk are necessary. Also in the backfitting operator for component j, the impact of other directions on any pair

of components containing Xj might now differ depending on respective occupation times of component

pairs. Both aspects are reflected by a respectively generalized procedure ensuring consistent estimates.

The generalized smooth backfitting estimates (fj)jd=1 are defined as



fj (xj )

=

d

1 -

1



k=j

fj(k)(xj ) - f0(,kj)

-1 k=j jk l=j

Gl

fl(xl)

j(kl )(xjl) j(k)(xj )

 dxl

,

(4.3)

23

where fj(k)(xj) are the marginal local constant estimates with bivariate speed of convergence as defined above and constants

f0(,kj) =

Gj fj(k)(xj )j(k)(xj ) dxj Gj j(k)(xj ) dxj

=

1

T jk(n)

Yi,

iIjk

(4.4)

which follow from appropriate analogues of the standard norming constraints

fj (xj )j(k)(xj ) dxj = 0.
k=j Gj

(4.5)

Note that asymptotically in the projection part of (4.3) only those elements jl prevail, where jl = jk while all others vanish. The projection property of standard backfitting only prevails in a generalized sense, since in general an invariant measure for the full-dimensional compound process does not exist for pairwise recurrent X. For each j and k, jk counts the number of such elements in the sample. In a nonstationary setting also the regions of integration Gj must be chosen with some care to ensure that integrals exist. Related to small sets, e.g., in a random walk case compact areas are appropriate. If all pairs of components of X have the same type of nonstationarity, the backfitting equations reduce to

fj (xj )

=

d

1 -

1

k=j

fj(k)(xj ) - f0(,kj)

-
k=j

Gk

fk

(xk

)

j k (xj k ) j(k)(xj )

dxk

,

since jk = d - 1 and j(kl ) = jl in this case. In particular, for the special case of identical one- and two-dimensional scales, generalized smooth backfitting reduces to the standard case. This usually occurs for sufficiently stationary data.
Asymptotic results for the generalized backfitting are univariate in form, i.e, the standard curse of dimensionality can be circumvented. However, they are driven by the worst case bivariate type of nonstationarity in the data. In particular, the difference between the true component function fj and the backfitting estimate fj is asymptotically normal when inflated with the stochastic occupation time factor mink=j Lj(k)(xj)h. As L(jk) is asymptotically of the same order as T jk(n), the rate of convergence is on average of size nj++ h, where j+ is the highest degree of nonstationarity and thus the smallest number among the jk, and > 0 is very small. That means, if all components are random walks, i.e. jk = 0, estimation of each component is possible, but with logarithmic rate. This should be compared to the fact that a fully nonparametric model cannot be estimated in this case where the compound vector is transient. If one component Xj0 follows a random walk and all others are stationary, all components
 are estimated at rate nj0 h = n1/2h.

5 Noisy fredholm integral equations of second kind
As outlined in Subsection 2.4, we can define the smooth backfitting estimators in the additive models as solutions of an integral equation f (x) = f (x)- H(x, z)f (z) dz, where f (x1, . . . , xd) = (f1(x1), . . . , fd(xd)) , f (x1, . . . , xd) = (f1(x1), . . . , fd(xd)) and the integral kernel H(x, z) equals to a matrix with elements
24

pXj,Xk (xj, xk)/pXj (xj). We also rewrite this noisy integral equation as f = f  - Hf .

In Section 3 we have also seen that smooth least squares for various models leads to estimators that are given as solutions of such noisy integral equations. There are several approaches to the numerical solution of the integral equation. As already mentioned in Subsection 2.4, one can use a discrete approximation of the integral equation for the numerical solution. This results in a finite system of linear equations that can be solved by standard methods. One approach would be based on a iterative scheme that uses a discrete approximation of the iteration steps:
f NEW = f  - Hf OLD.

If f is a d-dimensional vector of functions with d  2, one can also use an iteration scheme that runs cyclically through component-wise updates

fjNEW = fj - Hj f OLD, 1  j  d

with an obvious definition of Hj. This was the algorithm we discussed in Subsection 2.1. Compare also

the Gauss-Seidel method and the Jacobi method in numerical linear algebra.

We now use the definition of the estimators by a noisy integral equation for an asymptotic under-

standing of the distributional properties of the estimators. We consider the case of one-dimensional f

and f  and we rewrite the equation as f = f  - Hf . We now suppose that f  is a smoothing estimator

with

f   fA + f  + fB ,

where fA is the stochastic part of f  that is of order (nh)-1/2. The function f  is the stochastic limit of f  and fB is a bias term that we suppose to be of the standard order h2. Here, h is a bandwidth that is chosen of order n-1/5 so that the stochastic term and the bias term are of order n-2/5. A similar

discussion applies to Hf . This variable has stochastic limit Hf where H is the stochastic limit of H. We

now get

Hf  (Hf )A + Hf + (Hf )B,

where (Hf )A is the stochastic part of Hf . Again this term is of order (nh)-1/2. Although H is a higher dimensional smoother, all variables up to one are integrated out in Hf . Furthermore, (Hf )B is a bias term that is of order h2. By subtracting f = f  - Hf from f = f  - Hf we get

f - f = f  - f  - Hf + Hf = f  - f  - H(f - f ) - (H - H)f - (H - H)(f - f )  f  - f  - H(f - f ) - (H - H)f.

Now, simple algebra gives f - f  (I + H)-1(f  - f  - (H - H)f )  (I + H)-1(fA + fB - (Hf )A - (Hf )B).

25

We now argue that (I +H)-1fA  fA and (I +H)-1(Hf )A  (Hf )A. These claims follow immediately from (I + H)-1 = I - (I + H)-1H, HfA  0 and H(Hf )A  0. Here, the first equality can be easily seen by multiplying both sides of the equation with (I + H). For the two approximations one

notes that the integral over an interval of the stochastic part of a kernel smoother is typically of order

n-1/2. For example, one has

w(x)n-1

n i=1

Kh(x

-

Xi)i

dx

=

n-1

n i=1

wh

(Xi

)i

with

wh(u)

=

w(x)Kh(x - u) dx, which is of order n-1/2. Using the above approximations we get that

f - f  (I + H)-1(fA + fB - (Hf )A - (Hf )B) = fA - (Hf )A - (I + H)-1H(fA - (Hf )A) + (I + H)-1(fB - (Hf )B)  fA - (Hf )A + (I + H)-1(fB - (Hf )B)

The expressions on the right hand side of this expansion can be easily interpreted. The first term fA - (Hf )A is of order (nh)-1/2 and asymptotically normal with mean zero. This can be shown as in classical kernel smoothing theory. The second term (I + H)-1(fB - (Hf )B) is purely deterministic and it is of order h2 because already fB - (Hf )B is of this order. For a more detailed discussion of the above arguments we refer to [43] and [44].
We conclude this section by noting that the above noisy integral equations are quite different from
the case that estimators are given by integral equations of the form

0 = f  - Hf .

This is called an ill-posed inverse problem because, typically, the eigenvalues of the operator H accumulate at 0. For this reason the inverse of the operator H is not continuous. The integral equation studied in this chapter leads to the inversion of the operator (I + H). The eigenvalues of this operator accumulate around 1 and allow for a continuous inverse of (I + H). Thus our set-up is quite different from ill-posed problems. For a discussion of ill-posed problems we refer to [3], [7], [9], [10], [12], [25] and [45].

References
[1] Bollerslev, T. (1986). Generalized autoregressive conditional heteroscedasticity. J. Econometrics, 31, 307-327.
[2] Buja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoothers and additive models (with discussion), Ann. of Statist., 17, 453­510.
[3] Carrasco, M., Florens, J. P. and Renault, E. (2006). Linear Inverse Problems in Structural Econometrics: Estimation Based on Spectral Decomposition and Regularization. In: Handbook of Econometrics, J.J. Heckman and E.E. Leamer (eds.), vol. 6, North Holland, 2006.
[4] Carroll, R. J., Maity, A., Mammen, E. and Yu, K. (2009). Nonparametric additive regression for repeatedly measured data. Biometrika, 96, 383-398. doi:
26

[5] Chen, X. and Ghysels, E. (2011). News - good or bad - and its impact on volatility predictions over multiple horizons. Rev. Financ. Stud., 24, 46-81.
[6] Chen, X. (2007). Large Sample Sieve Estimation of Semi-nonparametric Models. Chapter 76 in: Handbook of Econometrics, Vol. 6B (James J. Heckman and Edward E. Leamer, eds.), pp 55495632, Elsevier.
[7] Chen, X. and Reiss, M. (2011). On rate optimality for ill-posed inverse problems in econometrics. Econometric Theory 27, 497-52.
[8] Connor, G., Hagmann, M. and Linton, O. (2012). Efficient estimation of a semiparametric characteristic-based factor model of security returns. Econometrica, 18, 730-754.
[9] Darolles, S., Florens, J. P. and Renault, E. (2011). Nonparametric instrumental regression. Econometrica, 79, 1541-1565.
[10] Donoho, D. L. (1995). Nonlinear solutions of linear inverse problems by wavelet-vaguelette decomposition. J. Applied and Comput. Harmonic Anal., 2, 101-126.
[11] Eilers, P. H. C. and Marx, B. D. (2002). Generalized linear additive smooth structures. J. Comput. Graph. Statist., 11, 758­783.
[12] Engl, H., Hanke, M. and Neubauer, A. (1996). Regularization of Inverse Problems, Kluwer Academic Publishers, London
[13] Engle, R. F., and Ng, V. K. (1993). Measuring and testing the impact of news on volatility. J. Finance, 48, 987-1008.
[14] Fan, J., Heckman, N. E. & Wand, M. P. (1995). Local polynomial kernel regression for generalized linear models and quasi-likelihood functions. J. Amer. Statist. Assoc., 90, 141­150.
[15] Fan, J. and Jiang, J. (2007). Nonparametric inference with generalized likelihood ratio tests (with discussion). Test, 16, 409-478.
[16] Fan, J., Wu, Y. and Feng, Y. (2009). Local quasi-likelihood with a parametric guide. Ann. Statist., 37, 4153-4183.
[17] Fan, J. and Zhang, W. (1999). Statistical estimation in varying coefficient models. Ann. Statist., 27, 1491­1518.
[18] Fengler, M., H¨ardle, W. and Mammen, E. (2007). A semiparametric factor model for implied volatility surface dynamics J. Financial Econometrics, 5, 189-218.
[19] Haag, B. (2006). Model Choice in Structured Nonparametric Regression and Diffusion Models. PhD dissertation, Universit¨at Mannheim.
27

[20] Haag, B. (2008). Non-parametric regression tests using dimension reduction techniques. Scand. J. Statist., 35, 719-738
[21] Hastie, T. J. and Tibshirani, R. J. (1990). Generalized Additive Models. Chapman and Hall, London.
[22] Hjort, N. L. and Glad, I. K. (1995). Nonparametric density estimation with a parametric start. Ann. of Statist., 23, 882-904.
[23] Hoderlein, S., Mammen, E. and Yu, K. (2011). Nonparametric models in binary choice fixed effects panel data. Econometrics J., 14, 351-367.
[24] Jiang, J., Fan, Y. and Fan, J. (2010). Estimation of additive models with highly or nonhighly correlated covariates. Ann. Statist., 38, 1403-1432.
[25] Johnstone, I. M. and Silverman, B. W. (1990). Speed of estimation in positron emission tomography and related inverse problems. Ann. Statist., 18, 251-280.
[26] Kauermann, G. and Opsomer, J. D. (2003). Local likelihood estimation in generalized additive models, Scand. J. Statistics, 30, 317-337.
[27] Karlsen, H. A. , Myklebust, T. and Tjøstheim, D. (2007). Nonparametric estimation in a nonlinear cointegration type model, Ann. Statist., 35, 1-57.
[28] Lee, Y. K., Mammen, E. and Park, B. U. (2010). Backfitting and smooth backfitting for additive quantile models. Ann. Statist., 38, 2857­2883.
[29] Lee, Y. K., Mammen, E. and Park, B. U. (2012a). Projection-type estimation for varying coefficient regression models. Bernoulli, 18, 177­205.
[30] Lee, Y. K., Mammen, E. and Park, B. U. (2012b). Flexible generalized varying coefficient regression models. Forthcoming in Ann. Statist.
[31] Linton, O. and Mammen, E. (2003). Nonparametric smoothing methods for a class of non-standard curve estimation problems. In: Recent advances and trends in nonparametric statistics (M.G. Akritas and D. N. Politis, eds.), Elsevier, Amsterdam.
[32] Linton, O. and Mammen, E. (2005). Estimating semiparametric ARCH() models by kernel smoothing methods. Econometrica, 73, 771-836
[33] Linton, O. and Mammen, E. (2008). Nonparametric transformation to white Noise. J. Econometrics, 142, 241-264.
[34] Linton, O. and Mammen, E., Nielsen, J. and Tanggaard, C. (2001). Estimating yield curves by kernel smoothing methods. J. Econometrics, 105, 185-223.
28

[35] Linton, O. and Nielsen, J. (2009). Nonparametric regression with a latent time series. Econometrics J., 12, 187-207.
[36] Lundervold, L., Tjøstheim, D. and Yao, Q. (2007). Exploring spatial nonlinearity using additive approximation. Bernoulli, 13, 447­472.
[37] Mammen, E., Linton, O. and Nielsen, J. (1999). The existence and asymptotic properties of a backfitting projection algorithm under weak conditions. Ann. Statist., 27, 1443 - 1490.
[38] Mammen, E., Marron, J. S., Turlach, B. A. and Wand, M. P. (2001). A general framework for constrained smoothing. Statist. Sci., 16, 232-248
[39] Mammen, E. and Nielsen, J. (2003). Generalised structured models. Biometrika, 90, 551-566
[40] Mammen, E. and Park, B. U. (2005). Bandwidth selection for smooth backfitting in additive models. Ann. Statist., 33, 1260-1294.
[41] Mammen, E. and Park, B. U. (2006). A simple smooth backfitting method for additive models. Ann. Statist., 34, 2252-2271.
[42] Mammen, E., Rothe, C. and Schienle, M. (2012). Nonparametric regression with nonparametrically generated covariates. Forthcoming in Ann. Statist.
[43] Mammen, E., Støve, B. and Tjøstheim, D. (2009). Nonparametric additive models for panels of time series. Econometric Theory, 25, 442-481.
[44] Mammen, E. and Yu, K. (2009). Nonparametric estimation of noisy integral equations of the second kind. J. Korean Stat. Soc., 38, 99-110.
[45] Newey, W. K. and Powell, J. L. (2003): Instrumental variables estimation for nonparametric models, Econometrica, 71, 1565-1578
[46] Newey, W., Powell, J. and Vella, F. (1999). Nonparametric estimation of triangular simultaneous equations models, Econometrica, 67, 565-603.
[47] Nielsen, J. and Sperlich, S. (2005). Smooth backfitting in practice. J. Roy. Statist. Soc., B 67, 43-61
[48] Opsomer, J. D. (2000). Asymptotic properties of backfitting estimators. J. Mult. Anal., 73, 166­179.
[49] Opsomer, J. D. and Ruppert, D. (1997). Fitting a bivariate additive model by local polynomial regression. Ann. Statist., 25, 186 ­ 211.
[50] Park, B. U., Kim, W. C. and Jones, M. C. (2002). On local likelihood density estimation. Ann. Statist., 30, 1480-1495.
[51] Park, B. U., Mammen, E., Ha¨rdle, W. and Borak, S. (2009). Time series modelling with semiparametric factor dynamics. J. Amer. Statist. Assoc., 104, 284-298.
29

[52] Schick, A. (1996). Root-n-consistent and efficient estimation in semiparametric aditive regression models. Statist. Probab. Lett. 30, 45-51.
[53] Schienle, M. (2008). Nonparametric Nonstationary Regression. PhD dissertation, Universita¨t Mannheim.
[54] Stone, C. J. (1985) Additive regression and other nonparametric models. Ann. Statist., 13, 689­705. [55] Stone, C. J. (1986) The dimensionality reduction principle for generalized additive models. Ann.
Statist., 14, 590­606. [56] Yang, L., Park, B. U., Xue, L. & Ha¨rdle, W. (2006). Estimation and testing for varying coefficients
in additive models with marginal integration. J. Amer. Statist. Assoc., 101, 1212­1227. [57] Yu, K, Park, B. U. and Mammen, E. (2008). Smooth backfitting in generalized additive models.
Ann. Statist., 36, 228-260. [58] Yu, K., Mammen, E. and Park, B. U. (2011). Semiparametric regression: efficiency gains from
modeling the nonparametric part. Bernoulli, 17, 736-748.
30

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "HMM in dynamic HAC models" by Wolfgang Karl Härdle, Ostap Okhrin and Weining Wang, January 2012.
002 "Dynamic Activity Analysis Model Based Win-Win Development Forecasting Under the Environmental Regulation in China" by Shiyi Chen and Wolfgang Karl Härdle, January 2012.
003 "A Donsker Theorem for Lévy Measures" by Richard Nickl and Markus Reiß, January 2012.
004 "Computational Statistics (Journal)" by Wolfgang Karl Härdle, Yuichi Mori and Jürgen Symanzik, January 2012.
005 "Implementing quotas in university admissions: An experimental analysis" by Sebastian Braun, Nadja Dwenger, Dorothea Kübler and Alexander Westkamp, January 2012.
006 "Quantile Regression in Risk Calibration" by Shih-Kang Chao, Wolfgang Karl Härdle and Weining Wang, January 2012.
007 "Total Work and Gender: Facts and Possible Explanations" by Michael Burda, Daniel S. Hamermesh and Philippe Weil, February 2012.
008 "Does Basel II Pillar 3 Risk Exposure Data help to Identify Risky Banks?" by Ralf Sabiwalsky, February 2012.
009 "Comparability Effects of Mandatory IFRS Adoption" by Stefano Cascino and Joachim Gassen, February 2012.
010 "Fair Value Reclassifications of Financial Assets during the Financial Crisis" by Jannis Bischof, Ulf Brüggemann and Holger Daske, February 2012.
011 "Intended and unintended consequences of mandatory IFRS adoption: A review of extant evidence and suggestions for future research" by Ulf Brüggemann, Jörg-Markus Hitz and Thorsten Sellhorn, February 2012.
012 "Confidence sets in nonparametric calibration of exponential Lévy models" by Jakob Söhl, February 2012.
013 "The Polarization of Employment in German Local Labor Markets" by Charlotte Senftleben and Hanna Wielandt, February 2012.
014 "On the Dark Side of the Market: Identifying and Analyzing Hidden Order Placements" by Nikolaus Hautsch and Ruihong Huang, February 2012.
015 "Existence and Uniqueness of Perturbation Solutions to DSGE Models" by Hong Lan and Alexander Meyer-Gohde, February 2012.
016 "Nonparametric adaptive estimation of linear functionals for low frequency observed Lévy processes" by Johanna Kappus, February 2012.
017 "Option calibration of exponential Lévy models: Implementation and empirical results" by Jakob Söhl und Mathias Trabs, February 2012.
018 "Managerial Overconfidence and Corporate Risk Management" by Tim R. Adam, Chitru S. Fernando and Evgenia Golubeva, February 2012.
019 "Why Do Firms Engage in Selective Hedging?" by Tim R. Adam, Chitru S. Fernando and Jesus M. Salas, February 2012.
020 "A Slab in the Face: Building Quality and Neighborhood Effects" by Rainer Schulz and Martin Wersing, February 2012.
021 "A Strategy Perspective on the Performance Relevance of the CFO" by Andreas Venus and Andreas Engelen, February 2012.
022 "Assessing the Anchoring of Inflation Expectations" by Till Strohsal and Lars Winkelmann, February 2012.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
023 "Hidden Liquidity: Determinants and Impact" by Gökhan Cebiroglu and Ulrich Horst, March 2012.
024 "Bye Bye, G.I. - The Impact of the U.S. Military Drawdown on Local German Labor Markets" by Jan Peter aus dem Moore and Alexandra Spitz-Oener, March 2012.
025 "Is socially responsible investing just screening? Evidence from mutual funds" by Markus Hirschberger, Ralph E. Steuer, Sebastian Utz and Maximilian Wimmer, March 2012.
026 "Explaining regional unemployment differences in Germany: a spatial panel data analysis" by Franziska Lottmann, March 2012.
027 "Forecast based Pricing of Weather Derivatives" by Wolfgang Karl Härdle, Brenda López-Cabrera and Matthias Ritter, March 2012.
028 "Does umbrella branding really work? Investigating cross-category brand loyalty" by Nadja Silberhorn and Lutz Hildebrandt, April 2012.
029 "Statistical Modelling of Temperature Risk" by Zografia Anastasiadou, and Brenda López-Cabrera, April 2012.
030 "Support Vector Machines with Evolutionary Feature Selection for Default Prediction" by Wolfgang Karl Härdle, Dedy Dwi Prastyo and Christian Hafner, April 2012.
031 "Local Adaptive Multiplicative Error Models for High-Frequency Forecasts" by Wolfgang Karl Härdle, Nikolaus Hautsch and Andrija Mihoci, April 2012.
032 "Copula Dynamics in CDOs." by Barbara Choro-Tomczyk, Wolfgang Karl Härdle and Ludger Overbeck, May 2012.
033 "Simultaneous Statistical Inference in Dynamic Factor Models" by Thorsten Dickhaus, May 2012.
034 "Realized Copula" by Matthias R. Fengler and Ostap Okhrin, Mai 2012. 035 "Correlated Trades and Herd Behavior in the Stock Market" by Simon
Jurkatis, Stephanie Kremer and Dieter Nautz, May 2012 036 "Hierarchical Archimedean Copulae: The HAC Package" by Ostap Okhrin
and Alexander Ristig, May 2012. 037 "Do Japanese Stock Prices Reflect Macro Fundamentals?" by Wenjuan
Chen and Anton Velinov, May 2012. 038 "The Aging Investor: Insights from Neuroeconomics" by Peter N. C. Mohr
and Hauke R. Heekeren, May 2012. 039 "Volatility of price indices for heterogeneous goods" by Fabian Y.R.P.
Bocart and Christian M. Hafner, May 2012. 040 "Location, location, location: Extracting location value from house
prices" by Jens Kolbe, Rainer Schulz, Martin Wersing and Axel Werwatz, May 2012. 041 "Multiple point hypothesis test problems and effective numbers of tests" by Thorsten Dickhaus and Jens Stange, June 2012 042 "Generated Covariates in Nonparametric Estimation: A Short Review." by Enno Mammen, Christoph Rothe, and Melanie Schienle, June 2012. 043 "The Signal of Volatility" by Till Strohsal and Enzo Weber, June 2012. 044 "Copula-Based Dynamic Conditional Correlation Multiplicative Error Processes" by Taras Bodnar and Nikolaus Hautsch, July 2012
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
045 "Additive Models: Extensions and Related Models." by Enno Mammen, Byeong U. Park and Melanie Schienle, July 2012.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

