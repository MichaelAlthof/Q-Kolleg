BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2012-012
Confidence sets in nonparametric calibration
of exponential Lévy models
Jakob Söhl*
* Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Confidence sets in nonparametric calibration of exponential L´evy models
Jakob So¨hl Humboldt-Universita¨t zu Berlin
February 7, 2012
Abstract Confidence intervals and joint confidence sets are constructed for the nonparametric calibration of exponential L´evy models based on prices of European options. This is done by showing joint asymptotic normality for the estimation of the volatility, the drift, the intensity and the L´evy density at finitely many points in the spectral calibration method. Furthermore, the asymptotic normality result leads to a test on the value of the volatility in exponential L´evy models.
Keywords: European option · Jump diffusion · Confidence sets · Asymptotic normality · Nonlinear inverse problem
MSC (2000): 60G51 · 62G10 · 62G15 · 91B28
JEL Classification: G13 · C14
1 Introduction
The unknown future development of financial markets faced by its participants, whether as investors, as traders or as companies, can be understood to consist of model risk and "Knightian uncertainty" [10, 17]. The first describes the risk for a given calibrated model and can be evaluated by probabilistic methods, whereas the second incorporates the lack of knowledge on the underlying probability measure and is typically treated by worst case scenarios, for example, by stress testing, which amounts to taking the supremum or infimum over a range of probability measures.
I thank Markus Reiß for fruitful discussions at all stages of this work and Mathias Trabs for helpful comments on the manuscript. This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk". E-mail address: soehl@math.hu-berlin.de
1

By the choice of the model, there is a trade-off in the uncertainty between the calibration error and the misspecification of the model, where larger models reduce the latter. Since the calibration error is statistically traceable and the misspecification is hard to track, we choose a rich model such as a nonparametric one and then asses the calibration error by constructing confidence sets.
More precisely, we consider the nonparametric calibration when the riskneutral price of a stock (St) follows an exponential L´evy model

St = Sert+Xt with a L´evy process Xt for t  0.

(1.1)

A thorough discussion of this model is given in the monograph by Cont and Tankov [7]. They introduced in [8, 9] a nonparametric calibration method for this model based on prices of European call and put options, in which a least squares approach is penalized by relative entropy. Belomestny and Reiß [2] used a different approach to the same estimation problem, where they regularized by a spectral cut-off and constructed estimators that achieve the minimax rates of convergence. We show asymptotic normality and construct confidence sets and intervals for their estimation procedure. Methods similar to theirs were also applied by Belomestny [1] to estimate the fractional order of regular L´evy processes of exponential type and by Trabs [21] to estimate self-decomposable L´evy processes.
Confidence sets measure how reliable the estimation is. This is particularly important if the calibrated model is to be used for pricing and hedging. For a recent review on pricing and hedging in exponential L´evy models see [20] and the references therein. For the influence of model uncertainty on the pricing see [6].
Nonparametric confidence intervals and sets for L´evy triplets have not been studied with the notable exception of the work by Figueroa-Lo´pez [12]. The work is more general in the sense that beyond pointwise confidence intervals also confidence bands are constructed. On the other hand, the method is based on high-frequency observations so that the statistical problem of estimating the L´evy density reduces to the problem of density estimation. In contrast to this direct observation scheme our method is based on option prices and thus the calibration is a nonlinear inverse problem, which is mildly ill-posed for volatility zero and severely ill-posed for positive volatility.
Confidence intervals and sets in nonparametric problems are a subtle issue. Whether adaptive confidence intervals for the estimators of the volatility, the drift and the intensity exist is an open question. We show asymptotic normality for the parametric estimators of the volatility, the drift and the intensity. We also proof asymptotic normality for the pointwise estimators of the L´evy density. The joint asymptotic distribution of these estimators is derived in both the mildly and the severely ill-posed case. This is used for the construction of confidence intervals and joint confidence sets as well

2

as for a test on the value of the volatility. The asymptotic normality results are based on undersmoothing and on a linearization of the stochastic errors.
The paper is organized as follows. The model and the estimation method are given in Section 2. The main results are formulated in Section 3. They are applied to confidence intervals and to a hypotheses test on the value of the volatility in Section 4. We conclude in Section 5. Proofs are deferred to Section 6 and Section 7. Uniform convergence is treated in Section 8.

2 The model and the estimators

We denote by C(K, T ) and P(K, T ) prices of European call and put options on the underlying (St) with strike price K and maturity T . We suppose that the risk-neutral price of the stock (St) follows the exponential L´evy model (1.1) with respect to an equivalent martingale measure Q, where S > 0 is the present value of the stock and r  0 is the riskless interest rate. We fix some T and assume that the observations are given by option prices for different maturities (Kj) corrupted by noise:

Yj = C(Kj, T ) + j j, j = 1, . . . , n.

The noise levels (j) are assumed to be positive and known. The minimax

result in [2] is shown for general errors ( j) which are independent centered

random

variables

with

E[

2j ]

=

1

and

supj E[

4 j

]

<

.

We transform the

observations to a regression problem on the function

O(x) :=

S-1C(x, T ), S-1P(x, T ),

x  0, x < 0,

where x := log(K/S) - rT denotes the log-forward moneyness. The regression model may then be written as

Oj = O(xj) + j j.

(2.1)

We call the volatility of a L´evy process , the drift  and the inten-

sity . As in [2] we consider only L´evy processes (Xt) with a jump com-

ponent of finite intensity and absolutely continuous jump distribution. For

a jump density  we denote by µ(x) := ex(x) the corresponding expo-

nentially weighted jump density. The aim is to estimate the L´evy triplet

T = (2, , µ). In the remainder of this section we present the spectral

calibration method of Belomestny and Reiß [2]. The method is based on a

pricing formula by Carr and Madan [5], which relates the Fourier transform

FO(u) :=

 -

O(x)eiuxdx

to

the

characteristic

function

T

(u)

:=

E[eiuXT

].

That is why we define

(u)

:=

1 T

log (1

+

iu(1

+

iu)F O(u))

=

1 T

log(T (u

-

i))

=

2u2 -

+

i(2

+

)u

+

(2/2

+



-

)

+

F µ(u),

2

(2.2)

3

where the first equality is given by the above mentioned pricing formula and the second by the L´evy-Khintchine representation. This equation links the observation of O to the L´evy triplet that we want to estimate. Let O be an approximation on the true function O. For example, O can be obtained by linear interpolation of the data (2.1). We further define the empirical counterpart of  by



(u)

:=

1 T

log(u)

(1

+

iu(1

+

iu)F O

(u)) ,

C Cwhere the trimmed logarithm log : \{0}  is given by

log(z) :=

log(z), log( z/|z|),

|z|   |z| < 

.

The logarithms are taken in such a way that  and  are continuous with (0) =  (0) = 0 and (u)  (0, 1) is specified in [2]. Considering (2.2) as a quadratic polynomial in u disturbed by Fµ motivates the following definition of the estimators for a cut-off value U > 0:

U
^2 := Re( (u))wU (u)du,
-U

U
^ := -^2 + Im( (u))wU (u)du,
-U

^ := ^2 + ^ - 2

U
Re( (u))wU (u)du,
-U

(2.3) (2.4) (2.5)

where the weight functions wU , wU and wU satisfy

U -U

-u2 2

wU

(u)du

=

1,

U
uwU (u)du = 1,
-U

U
wU (u)du = 0,
-U

U
u2wU (u)du = 0.
-U

U
wU (u)du = 1, -U (2.6)

The estimator for µ is defined by a smoothed inverse Fourier transform of the remainder

µ^(x) := F -1

 (u) + ^2 (u - i)2 - i^(u - i) + ^ 2

wµU (u) (x).

(2.7)

The weight functions for all U > 0 can be obtained from w1 , w1, w1 and wµ1 by rescaling:
wU (u) = U -3w1 (u/U ), wU (u) = U -2w1(u/U ), wU (u) = U -1w1(u/U ), wµU (u) = wµ1 (u/U ).

4

Since  (-u) =  (u) only the symmetric part of w1 , w1 and the antisymmetric part of w1 matter. The antisymmetric part of wµ1 contributes a purely imaginary part to µ^(x). Without loss of generality we will always assume w1 , w1, wµ1 to be symmetric and w1 to be antisymmetric. We further assume that the support of w1 , w1, w1 and wµ1 is contained in [-1, 1]. We define the estimation error ^2 := ^2 - 2 and likewise for the other
estimators. We will also use the notation  :=  - . The estimation error ^2 can be decomposed as

1
^2 : = 2U -2 Re(F µ(U u))w1 (u)du
0
1
+ 2U -2 Re( (U u))w1 (u)du.
0

(2.8)

The first term is the approximation error and decreases in U due to the decay of Fµ. The second is the stochastic error and increases in U by the growth of  . The cut-off value U is the crucial tuning parameter in this method and allows a trade-off between the error terms. The other estimation errors allow similar decompositions as ^2 in (2.8).
We shall analyze the asymptotic properties of the stochastic errors in depth. To bound the approximation errors some smoothness assumption is necessary. We assume that the L´evy triplet belongs to a smoothness class
NGs(R, max) with s  and R, max > 0 specified in [2, Definition 4.1]. The
assumption T  Gs(R, max) includes a smoothness assumption of order s on µ leading to a decay of Fµ. To profit from this decay when bounding the approximation error, we assume the weight functions to be of order s, this means
RF (w1 (u)/us), F (w1(u)/us), F (w1(u)/us), F (1 - wµ1 (u))/us  L1( ).

(2.9)
3 Asymptotic normality
3.1 The main results
The aim of this section is to establish asymptotic normality results for the estimators. We would like to state that the appropriately scaled errors of the estimators converge to normal random variables. The starting point of the error analysis is the decomposition (2.8) into the approximation error and the stochastic error. The approximation error is deterministic and only the stochastic error can be expected to converge with appropriate scaling to a normal random variable. It is common practice to resolve this problem by undersmoothing, which means that the tuning parameter is

5

chosen such that the approximation error becomes asymptotically negligible. Thus the cut-off value has to grow fast enough. This is ensured by the condition U ( )(2s+5)/2   in the case of volatility zero and by U ( )s+1 exp(T 2U ( )2/2)   in the case of positive volatility.
Since the approximation errors are negligible by these conditions we focus on the stochastic errors. To simplify the asymptotic analysis we do not work with the regression model (2.1) but with the Gaussian white noise model. This is an idealized observation scheme, where the terms are easier to analyze. At the same time asymptotic results may be transferred to the regression model. The Gaussian white noise model is given by

dO (x) = O(x)dx + (x)dW (x),

(3.1)

Rwhere W is a two-sided Brownian motion,   L2( ) and > 0. Here
the empirical counterpart O of O is given directly by the model and no further approximation is necessary. Transferring asymptotic results from the Gaussian white noise model to the regression model is formally justified by the concept of asymptotic equivalence. Due to the asymptotic equivalence between regression with regular errors and regression with Gaussian errors in [14] we will consider for our asymptotic analysis only the case of Gaussian errors. The asymptotic equivalence of regression (2.1) with Gaussian errors and the Gaussian white noise model (3.1) is given in Brown and Low [4], where j = (xj) and corresponds to 1/ n up to a logarithmic factor. Further assumptions for the asymptotic equivalence to hold are given in Section 6.
The stochastic errors involve the term  (U u), which is a difference
Cbetween two logarithms. For z, z  \{0} and  > 0 it holds log(z) -
log(z ) = log/|z | (z/z ). That yields

 (U u)

1 = T logU (u)

iU u(1 + iU u) 1+
1 + iU u(1 + iU u)FO(U u)


eiUux(x)dW (x) ,
-

where U (u) := (U u)/|1 + iU u(1 + iU u)FO(U u)|  1/2, see [2, (6.3)]. We define a linearization L ,U of the logarithm and the remainder term R ,U by

L

,U (u)

:=

T (1

+

iU u(1 iU u(1 +

+ iU u) iU u)FO(U u))

R ,U (u) :=  (U u) - L ,U (u).


eiUux(x)dW (x),
-

(3.2) (3.3)

we aTsosuemnseutrheactonthtienrueiitsyaofpt>he1Gsauucshsitahnatpro-ces(s1X+(|xu|))p=(x-)2dexiu<x(x.)dIWn [(1x9)] it is shown that on this assumption X satisfies the Kolmogorov-Chentsov
criterion [16, p. 57] and thus has a continuous version. In the sequel we are
always working with this version.

6

The remainder term R ,U in (3.3) is small when the argument of the logarithm is close to one, that is when L ,U is small. Since we are integrating over the unit interval in (2.8) we want L ,U to be uniformly small. We shall use the notation A(x) B(x) as x   synonymously with the Landau notation A(x) = O(B(x)) as x  , meaning that there exist M > 0 and
Rx0  such that A(x)  M B(x) for all x  x0.
Proposition 1. For all q  1 holds

1/q
E sup |L ,U (u)|q
u[-1,1]

U 2 log(U ) exp(T 2U 2/2)

as U  .

This proposition is proved in Section 6 by metric entropy arguments. In

the following theorems we control the supremum of L ,U and thus the remain-

der term R ,U by the condition U ( )2 log(U ( )) exp(T 2U ( )2/2)  0.

Then the asymptotic distribution of the stochastic errors

1 0



(U u)w(u)du

is governed by the linearized stochastic errors

1 0

L

,U

(u)w(u)du

and

the

re-

mainder term

1 0

R

,U (u)w(u)du

is

asymptotically

negligible.

In the case

 = 0 the stronger condition U ( )5/2  0 is assumed, which is needed for

the stochastic errors to converge to zero.

In the results on asymptotic normality we will also include the estimator

µ^(0) of the jump density at zero. This only makes sense by our smoothness

assumption on µ since there is no way of detecting jumps of height zero.

Unlike for points x = 0 it will turn out that not the weight function wµ1 determines the asymptotic distribution but the effective weight function

11
w0(u) := wµ1 (u) + w1 (u) v2wµ1 (v)dv/2 - w1(u) wµ1 (v)dv.
-1 -1
The first theorem states the joint asymptotic normality result for the mildly ill-posed case of volatility zero.

R RTheorem 1. Let  = 0. Let  be continuous at T , x1 + T , . . . , xn + T 
and let F 2  L1( ). For j = 1, . . . , n let xj  \{0} be distinct and
let V0, W0, Wx1 . . . , Wxn be independent Brownian motions. If U ( )5/2  0 and U ( )(2s+5)/2   as  0, then it holds

 U ( )+1/2

 U ( )-1/2



 U ( )-3/2

1

 



U(

)-5/2

 

U(

)-5/2





 U ( )-5/2

^2   d(0)

^   d(0)

 ^   d(0)


µ^(0)

 

-d

 

d(0)

µ^(x1)

 

 

d(x1)

...

  

µ^(xn)

d(xn)

1 0

u2w1

(u)dW0

(u)



1 0

u2w1

(u)dV0(u)

1 0

u2 w1 (u)dW0 (u)

   

1 0

u2w0(u)dW0(u)/(2)

 

,

1 0

u2

wµ1

(u)dWx1

(u)/(2)

...

    

1 0

u2

wµ1

(u)dWxn

(u)/(2)

7

 as  0, where d(x) := 2 (x + T ) exp(T ( - ))/T .

RNext we consider the case  > 0. Let  be in L( ) and
We set

d := 2  RL2( ) exp(T ( -  - 2/2))T -2-2.

 RL2( ) > 0.
(3.4)

and define real-valued random variables W ,U and V ,U by W ,U + iV ,U :=

2d-1

1 0

L

,U (u)du.

By

Lemma

3

below

1 exp(T 2U 2/2)

W ,U V ,U

-d

W V

(3.5)

as U  , where W and V are independent standard normal random variables.
The following theorem treats the stochastic errors in the case of positive volatility. Since the theorem contains no statement on the approximation errors, the condition (2.9) on the order of the weight functions may be omitted.
RTheorem 2. Let  > 0 and   L( ). Assume for the cut-off value
U ( )   and U ( )2 log(U ( )) exp(T 2U ( )2/2)  0 as  0. Let
Rw1 , w1, w1, wµ1 : [0, 1]  be Riemann-integrable, in L([0, 1]) and continRuous at one. For x  it holds with the notation U := U ( )



2

1 0

Re(

(U u))w1 (u)du

-

d

w1 (1)W

,U



1

 

2

1 0

Im(

(U u))w1(u)du

-

d w1(1)

V

,U

eT 2U 2/2

 

2

1 0

Re(

(U u))w1(u)du

-

d w1(1)W

,U

  -Q 0,  

F -1  (U u)wµ1 (u) (U x) - d wµ1 (1) Z ,U (x)/(2)

as  0, where Z ,U (x) := cos(U x)W ,U + sin(U x)V ,U .

The assumption T  Gs(R, max) includes   [0, max]. The condition U ( )2 log(U ( )) exp(T 2U ( )2/2)  0 is especially fulfilled if U ( )  ¯-1(2 log( -1)/T )1/2 for any ¯ > max. For the estimation it suffices to know some upper bound max of . The theorem shows that regardless whether one undersmoothes or not the stochastic errors converge with appropriate scaling to normal random variables. For the statement on asymptotic normality we have to undersmooth and further knowledge on the volatility is necessary.
In many situations the volatility  is known or can be estimated easily. It can, for example, be determined from high frequency data of the underlying asset since the volatility is the same for equivalent measures. In the following we will assume either that the volatility  is known as in Cont and Tankov [8] or that we have a sufficiently good estimator of the volatility. To control the remainder term we choose U ( ) such that

8

U ( )2 log(U ( )) exp(T 2U ( )2/2)  0 as  0. We also assume the undersmoothing condition U ( )s+1 exp(T 2U ( )2/2)   as  0. A smoothness parameter s  2 is implicitly assumed so that both condition
can be satisfied simultaneously. A possible choice of U ( ) is

2 -1 U ( ) := T 2 log log( -1) ,

(3.6)

where   (1, (s + 1)/2). Then it holds

   > 2

 U ( ) exp(T 2U ( )2/2) 

2 T 2

 = 2

 0  < 2

as  0. Especially the term diverges for  = s + 1 and converges to zero for   (2, 2) so that both conditions on U ( ) are fulfilled.
Next we state the joint asymptotic normality result for the severely illposed case of positive volatility.

Theorem 3. Let  > 0 and let the cut-off value U ( ) be chosen such that U ( )2 log(U ( )) exp(T 2U ( )2/2)  0 and U ( )s+1 exp(T 2U ( )2/2) 
R as  0. Let   L( ). It holds

 U ( )2 ^2 - d w1 (1)W ,U( )



1

 U( ) 

eT 2U (

)2/2

  

U(

)-1

^ ^
µ^(0)

- - -

d w1(1)V ,U( ) d w1(1)W ,U( ) d w0(1)W ,U( )/(2)



 

-Q

0,





U ( )-1 µ^(x) - d wµ1 (1)Z ,U( )(x)/(2)

Ras  0, where x  \{0}, Z ,U (x) := cos(U x)W ,U + sin(U x)V ,U and d is
given by (3.4).

3.2 Discussion of the results
Theorems 1 and 3 include the asymptotic distribution of ^2, which may be used for testing the hypotheses H0 :  = 0. If  is known, we can set ^2 = 2. Then the statements of the theorems hold with w1 constant to zero. The estimation method can give negative values for ^2, ^ and ^(x). By a postprocessing step the estimated values can be corrected to be nonnegative.
In Theorem 1 the noise level  enters only locally into the asymptotic variance whereas in Theorems 2 and 3 the asymptotic variance depends on the L2-norm of  through the factor d. In fact for  = 0 it is possible to estimate  and  directly from local properties of the option function O at T as remarked in [2]. This local dependence on the noise level resembles some similarity to deconvolution, for instance, to the case of ordinary smooth

9

error density, where the density of the observations enters locally into the

asymptotic variance [11, 22]. For the weight functions the local and global dependence is vice versa. In Theorem 1 the weight functions w1 , w1, w1, w01 and wµ1 enter globally into the asymptotic variance while in Theorems 2 and 3 only the values of the weights functions at their endpoints appear in

the asymptotic variance.

The asymptotic variance depends on the maturity. For positive volatility

this dependence is through d in (3.4). The martingale condition is equivalent

to 2/2 +  -  +

 -

ex(x)dx

=

0,

especially

it

holds

that



-



-

2/2



0

with equality if and only if  = 0 that is in the Black-Scholes case. In the

case of positive volatility  the asymptotic variance grows exponentially as

T   if the jump intensity  is positive and grows quadratic as T  0.
RFor w1 (1), w1(1), w1(1), wµ1 (1)  \{0} Theorem 2 describes the asymp-
totic distribution of the leading stochastic error term of ^2, ^, ^ and µ^(x),

x = 0, i.e., the other stochastic error terms are of smaller order. The

variances in Theorems 2 and 3 converge by (3.5) and by the definition of Z ,U (x). If one only considers the stochastic errors of ^2, ^, ^ and µ^(0),

then the covariances converge, too. But for x = 0 the covariance of the

stochastic errors of µ^(x) and of ^2 does not converge. The same holds for the covariance of the stochastic errors of µ^(x) and ^ as well as µ^(x) and ^.

The phenomenon that the covariances do not convergence comes from the

fact that the stochastic error centers more and more at the cut-off frequency.

The sequence of cut-off values has a crucial influence on the covariance. For

estimators of the generalized distribution function of the L´evy density this

is likely to lead to a similar dependence on the sequence of cut-off values as

observed in [23] for deconvolution with a supersmooth error density.

4 Applications

4.1 Construction of confidence intervals and confidence sets

For  = 0 we define confidence intervals

I, := I, := Iµ(0), := Iµ(x), :=

[^ - s^ U 1/2q/2, [^ - s^ U 3/2q/2, [µ^(0) - s^µ(0) U 5/2q/2, [µ^(x) - s^µ(x) U 5/2q/2,

^ + s^ U 1/2q/2], ^ + s^ U 3/2q/2], µ^(0) + s^µ(0) U 5/2q/2], µ^(x) + s^µ(x) U 5/2q/2],

(4.1)

10

Rwhere x  \{0}, q denotes the (1 - )-quantile of the standard normal
distribution and



s^(0)


  

s^ s^ s^µ(0)







 

:=







s^(0) s^(0)

s^µ(x)

 

s^(x)

1 0

u4

w1

(u)2du

1/2



1 0

u4

w1 (u)2

du

1/2

   

1 0

u4

w0(u)2du

1/2
/(2)

  

1 0

u4wµ1 (u)2du

1/2
/(2)



with

s^(x) :=

 2 (x

+

T

^)

exp(T

(^

-

^))/T

.

We

fix

some

arbitrarily

slowly

decreasing function h with h(u)  0 as |u|  . We denote by Hs(R, max) the subset of L´evy triplets in Gs(R, max) that satisfy in addition

RFµ   R, |Fµ(u)|  R h(u), u  .

(4.2)

The additional conditions are used to extend the convergence in the theorems to uniform convergence, see Theorem 4.

Corollary. Let  = 0. On the assumptions of Theorem 1 and on the assumption that  is positive and continuous

lim
0

T

inf
Hs(R,0)

QT

(



I,

)

=

1

-



Rholds for the intervals (4.1) and for all   {, , µ(x)|x  }.

Remark. We can take the two parameters  and  and define A := {(^ + U 1/2s^x, ^+ U 3/2s^y) |x2+y2  k}, where k denotes the (1-)-quantile of the chi-square distribution 22 with two degrees of freedom. Then it holds

lim
0

T

inf QT
Hs(R,0)

((, )

 A ) = 1 - .

For  > 0 we define confidence intervals

I, := I, := Iµ(0), := Iµ(x), :=

[^ - s^ U -1eT 2U2/2q/2, [^ - s^ eT 2U2/2q/2, [µ^(0) - s^µ(0) U eT 2U2/2q/2, [µ^(x) - s^µ U eT 2U2/2q/2,

Rwhere x  \{0},

^ + s^ U -1eT 2U2/2q/2], ^ + s^ eT 2U2/2q/2], µ^(0) + s^µ(0) U eT 2U2/2q/2], µ^(x) + s^µ U eT 2U2/2q/2],
(4.3)

 s^ 



 |w1(1)|



  

s^ s^µ(0)

 

:=



2  RL2( )



exp(T

(2/2

+

^

-

^))T

22

 

|w1 (1)| |w0(1)|/(2)

  

s^µ |wµ1 (1)|/(2)

and q denotes the (1 - )-quantile of the standard normal distribution.

11

Corollary. Let  > 0. On the assumptions of Theorem 3

lim
0

inf
T

QT

(



I,

)

=

1

-



Rholds for the intervals (4.3) and for all   {, , µ(x)|x  }, where the

infimum is over all T  Hs(R, max) with volatility .

For (, ) a uniform confidence set may be obtained similarly as in the
Rcase  = 0. Since for x  \{0} the covariance of Z ,U( )(x) and V ,U( )
and the covariance of Z ,U( )(x) and W ,U( ) do not converge, confidence sets for (, µ(x)) and (, µ(x)) have to be constructed differently. Let us
illustrate how to proceed in this case by constructing a confidence set for
R(µ(x1), µ(x2)) , x1, x2  \{0}. By Theorem 3 the convergence

11 eT 2U( )2/2 U ( )

µ^(x1) µ^(x2)

- d wµ1 (1) 2

Z ,U( )(x1) Z ,U( )(x2)

-Q 0

holds for  0. We define

MU :=

cos(U x1) sin(U x1) cos(U x2) sin(U x2)

,

and observe that the components of MU-1 are bounded for U for which the absolute value of the determinant is bounded from below by some c > 0,
i.e., | sin(U (x2 - x1))|  c. For such U ( )

1 eT 2U ( )2/2

MU-(1 ) U( )

µ^(x1) µ^(x2)

- d wµ1 (1) 2

W ,U( )(x1) V ,U( )(x2)

-Q 0

holds for  0. By Slutsky's lemma we obtain

s^µ

U(

1 )eT 2U(

)2/2 MU-(1 )

µ^(x1) µ^(x2)

-d

W V

for  0 such that | sin(U ( )(x2 - x1))|  c. We define

C :=

µ^(x1) µ^(x2)

+

cos(U ( )x1) sin(U ( )x1) cos(U ( )x2) sin(U ( )x2)

B,

where B := {s^µ U ( )eT 2U( )2/2(x, y) | x2 + y2  k} and k denotes the (1-)-quantile of the chi-square distribution 22 with two degrees of freedom.
Then

lim QT ((µ(x1), µ(x2))  C ) = 1 - 
| sin(U ( )(x2-x1))|c 0
holds for all T  Gs(R, max)  { > 0}.

12

4.2 Inference on the volatility
The results on asymptotic normality allow to test for the value of the volatility. Let 0  [0, max] and define the hypotheses H0 : T  Hs(R, max)  { = 0} and the alternative H1 : T  Hs(R, max)  {| - 0|   } for some  > 0. For   (0, 1/2] there is a test, which reaches asymptotically the level , i.e., lim 0 supT ET [0] = , where the supremum is over all T in the hypotheses H0. Moreover, the error of the second kind vanishes asymptotically, i.e., lim 0 supT ET [1 - 0] = 0, where the supremum is over all T in the alternative H1. This family of tests can be used to construct a confidence set for . The precise construction of the test and of the confidence set is given in Section 7.

4.3 A numerical example

We consider the Merton jump diffusion model [18], where the jump density is specified by

R(x) =   exp 2 v

-

(x

- )2 2v2

,

x

,

R Rwith parameters ,   0, v > 0,   and where   is determined by

the martingale condition. We simulate data with the parameters  = 0.1,  = 5,  = -0.1, v = 0.2, which implies  = 0.379. The interest rate

is taken to be r = 0.06. We observe prices of n = 100 European options

with maturity T = 0.25. The strike prices are obtained from sampling the

data points (xj) from a centered normal distribution with variance 1/2, so that more strike prices are sampled at the money than in or out of the

money. The observation error is chosen to be a centered normal distribution with variance 2O(xj)2,  = 0.01. Belomestny and Reiß [3] describe the implementation of the estimation method in detail.

We interpolate the corresponding European call prices as a function of

the strike prices linearly. The weight functions are chosen as in [3] with

smoothness parameter s = 2, there denoted by r. In the simulations the

confidence intervals based on the asymptotic distribution turn out to be to

conservative. The asymptotic confidence intervals are based on the asymp-

totic variance of the linearized stochastic errors that is the stochastic errors,

where the linearization (3.2) is used. Instead of taking the asymptotic vari-

ance of the linearized stochastic errors we derive confidence intervals from

the finite sample variance (6.17) of the linearized stochastic errors. In the

finite sample variance we substitute ,  and  by their estimators. The

asymptotic variance does not depend on µ and the influence of µ one the

finite sample variance is negligibly small. So µ may be set to be constant

to zero in the variance. This yields feasible confidence intervals. We fix

the cut-off value at U = 32 and perform 1000 Monte Carlo iterations. The

13

Figure 1: True L´evy density (solid) with pointwise confidence intervals (dashed) and 100 estimated L´evy densities (grey).
coverage probabilities of 95% confidence intervals for 2,  and  are 0.96, 0.91 and 0.96, respectively. We see that the that the coverage probabilities are close to the prescribed confidence level.
Figure 1 illustrates the true L´evy density and the pointwise 95% confidence intervals based on the finite sample variance and the cut-off value U = 32. The 100 estimated L´evy densities show that the confidence intervals perform well in terms of coverage probabilities.
5 Conclusion
We have shown asymptotic normality in a nonparametric calibration method for exponential L´evy models. These results were used to derive confidence intervals and confidence sets as well as to construct a test on the value of the volatility. We have seen in a numerical example that confidence intervals based on finite sample variance perform well in terms of coverage probabilities. The confidence intervals extend the calibration method beyond a pure point estimate and enable an assessment of the calibration error. Although parametric models might be fitted better, the parametric approach is always exposed to the risk of model misspecification and the obtained confidence
14

results should be used for a goodness-of-fit test. The estimation method and the asymptotic normality results may be
adapted to other models as long as there is an equation relating the option function to the characteristic function and the parameters of interest appear in the characteristic function. The constructed confidence intervals and sets may be used to quantify the errors in pricing, hedging and risk management.

6 Proof of the asymptotic normality

First, we write ^, ^ and µ^(x) similarly as in (2.8) for ^2:

1
^ = -^2 + 2U -1 Im(F µ(U u))w1(u)du
0

1
+ 2U -1 Im( (U u))w1(u)du,
0

1
^ = ^2/2 + ^ - 2 Re(F µ(U u))w1(u)du
0

1
- 2 Re( (U u))w1(u)du,
0
µ^(x) = U F -1  (U u)wµ1 (u) (U x)

+ ^2 U F -1 2

(U u - i)2wµ1 (u)

(U x)

- i^U F -1 (U u - i)wµ1 (u) (U x)

+ ^U F -1 wµ1 (u) (U x)

- U F -1 (1 - wµ1 (u))F µ(U u) (U x).

(6.1) (6.2) (6.3)

In (6.1) we can substitute ^2 using (2.8) and obtain two error terms in-

volving Fµ and two error terms involving  . By similar substitutions in

(6.2) and (6.3) we see that all error terms either involve Fµ or  , which

we will call approximation errors and stochastic errors, respectively.

We will now state the conditions more precisely on which the regression

model (2.1) and the Gaussian white noise model (3.1) are equivalent. We

restrict the Gaussian white noise model to a growing sequence of intervals

[n, n] and assume as a simplification that the observations in the regression

model are equidistant on these intervals with mesh size n. We assume

that (n - n)n  0. We suppose 2 > 0 to be an absolutely continuous

function and

 x

log (x)



C

to hold for some C < .

The functions O

are uniformly bounded O(x) = S-1C(x, T ) - (1 - ex)+  1 and uniformly

Lipschitz |O (x)| = |

x -

O

(x)dx - 1{x>0} + e(-)T 1{x>T,=0}|  4 + eRT ,

where we used Proposition 2.1 in [2] and ||  R. These properties of O are used to apply Corollary 4.2 in [4]. Then corresponds to (n - n)/ n,

15

especially for logarithmically growing intervals one loses only a logarithmic factor.

6.1 Proof of Proposition 1

First we define X p > 1 such that

(u) := -(1 +

 -

eiux

(x)dW

|x|)p(x)2dx <

(x). .

We assumed that there It is shown in [19] that

is an there

Rexists a number c > 0 such that E[|X(u) - X(v)|2]  c|u - v| for all
u, v  with  := min(p/2, 1)  (1/2, 1]. Denote by N(I, r) the covering

number, that is the minimum number of closed balls of radius r in the

metric  with centers in I that cover I. We define (u, v) := c|u - v| and

d(u, v) := E[|X(u) - X(v)|2]. A ball of radius r in the metric  covers an

interval of length 2(r/c)1/. Thus, it holds

N([-U, U ], r) = U (c/r)1/ .

The radius of the smallest ball with center in [-U, U ] that contains [-U, U ] is cU  with respect to the metric . There exists D <  such that d(u, v)  D
Rfor all u, v  . For U large enough such that cU   D we have the entropy
bound


J([-U, U ], d) = (log(Nd([-U, U ], r)))1/2 dr

0

D
= (log(Nd([-U, U ], r)))1/2 dr
0

D
 (log(N([-U, U ], r)))1/2 dr

0



D
log 2U (c/r)1/

1/2
dr

0



D
log (2U c/r)1/

1/2
dr

0

D
 -1/2 (log (2U c/r))1/2 dr,

0

(6.4)

here we substitute r = 2U cs,

D/(2U c)

 -1/22U c

(log (1/s))1/2 ds.

0

This integral is solved by

x



log y-1dy =

-

 Erf(

0 22

log x-1) + x

log x-1,

(6.5)

16

where Erf(y) = exp(-y2)/(y)

2 

y 0

e-t2

dt.

For

holds. For each 

all y > > 0 this

0 the yields

estimate 1 c~ > 0 such

- Erf(y) that for

< all

x ]0, 1/2[

x
log y-1dy  c~x log x-1.

0

Thus, (6.5) can be bounded by

(-1/2c~D) log(2U c/D)

log(U )

as U  . Thus, log(U ) is an asymptotic upper bound of the entropy integral (6.4). We apply Dudley's theorem [15, p. 219] to the real part of X. For all q  1 this yields a continuous version X of Re(X) with

E sup |X (u)|q
u[-U,U ]

(log(U ))q/2

(6.6)

as U  . Since X and Re(X) are both continuous they are indistinguishable and (6.6) holds for Re(X) likewise. We obtain analogously for all q1

E sup |Im(X(u))|q
u[-U,U ]

(log(U ))q/2

as U  . We estimate from above for all q  1

E sup |L ,U (u)|q
u[-1,1]

 sup
u[-U,U ]

iu(1 + iu)

q

T (1 + iu(1 + iu)FO(u)) E

sup |X(u)|q
u[-U,U ]

q

U 1 + U2  T exp(T (-2U 2/2 + 2/2 +  -  - F µ ))

E sup |X(u)|q
u[-U,U ]

U 2 log(U ) exp(T 2U 2/2) q

(6.7)

as U  .

6.2 The linearized stochastic errors

The linearized stochastic errors are of the form

1 0

fj

(u)L

,U (u)du,

where

fj with j = 1, . . . , n are Riemann-integrable function in L([0, 1]). Next

we will show that these are jointly normal distributed. Almost surely L ,U

is continuous. Thus, almost surely the fjL ,U are Riemann-integrable and

almost surely

1 m

m

fj(k/m)L ,U (k/m) 

k=1

1
fj(u)L ,U (u)du
0

17

as m  . Let C > 0 be such that fj   C for all j = 1, . . . , n. For each m the n sums are joint, centered normal random variables. For m   the

covariance matrix converges by the dominated convergence theorem with the dominating function C2 supu[0,1] |L ,U (u)|2, where supu[0,1] |L ,U (u)|2 is an integrable random variable by Proposition 1. Thus, the characteristic

function converges pointwise. By L´evy's continuity theorem this shows that

the sums convergence jointly in distribution to normal random variables. So

1 0

fj (u)L

,U (u)du

are

jointly

normal

distributed.

For a fixed cut-off value U the linearized stochastic errors are jointly

normal distributed. So the natural question is whether the appropriately

scaled covariance matrix converges for U  .
RLet wj, wk : [0, 1]  be Riemann-integrable functions in L([0, 1]). It

holds

1
T wj (u)L ,U (u)e-iUuxj du

0

1

= U 2e-T (2/2+-)

fU (u)

0


eiUu(x-j)+T 2U2u2/2(x)dW (x)du,
-

where

fU (u)

:=

wj(u)(-u2 + iu/U ) exp(T Fµ(U u))

(6.8)

and j := xj + T 2 + T . We define analogously

gU (u)

:=

wk(u)(-u2 + iu/U ) exp(T Fµ(U u))

(6.9)

and k := xk + T 2 + T . We extend fU and gU by zero outside the interval [0, 1].
Since E supu[-1,1] |L ,U (u)|2 <  we may apply Fubini's theorem and then we apply the Ito^ isometry to obtain

T 2e2T (2/2+-)E

11
wj (u)L ,U (u)e-iUuxj du wk(v)L ,U (v)e-iUvxk dv
00

11

= 2U4

fU (u)eiU u(x-j )+T 2U 2u2/2

0 0 -

gU (v)eiUv(x-k)+T 2U2v2/2(x)2dxdudv.

(6.10)

To separate real and imaginary part we will also need

T 2e2T (2/2+-)E

11
wj (u)L ,U (u)e-iUuxj du wk(v)L ,U (v)e-iUvxk dv
00

11

= 2U4

fU (u)eiU u(x-j )+T 2U 2u2/2

0 0 -

gU (v)eiUv(x-k)+T 2U2v2/2(x)2dxdudv.

(6.11)

18

R RLemma 1. Let  = 0. For j = 1, . . . , n let xj  and let wj : [0, 1]  be RRiemann-integrable functions in L([0, 1]). Let  be continuous at x1 + T ,
x2 + T , . . . , xn + T  and let F 2  L1( ). Let Wx1, . . . , Wxn, Vx1, . . . , Vxn be Brownian motions. If xj = xk let Wxj = Wxk and Vxj = Vxk otherwise let the Brownian motions be distinct. Let the set {Wx1, . . . , Wxn, Vx1, . . . , Vxn} consist of independent Brownian motions. Then

1 U 3/2

1
wj (u)L ,U (u)e-iUuxj du
0

converge jointly in distribution to

 (xj + T )
T exp(T ( - ))

11
u2wj(u)dWxj (u) + i u2wj(u)dVxj (u)
00

as U  .

Proof. We will first consider the case xj = xk. We have seen that

T 2e2T (-)E

11
wj (u)L ,U (u)e-iUuxj du wk(v)L ,U (v)e-iUvxj dv
00



= 2U4

F fU (U (x - j))F gU (U (x - j))(x)2dx,

-

where fU and gU are defined as in (6.8) and (6.9), respectively, and j = xj + T ,



= 2U3

F fU (y)F gU (y)(y/U + j)2dy.

-

R RWe notice that F2  L1( ) implies 2  L( ) and we obtain by the
Plancherel identity

1
= 2 2U 3 fU (u)(gU (v)  F -1((y/U + j)2)(v))(u)du,
0

(6.12)

since the support of fU is [0, 1]. Because we are only interested in the limit U   we may assume U  1. By the Riemann-Lebesgue lemma Fµ(u)
tends to zero as |u|  . The factor fU (u) converges for each u  [0, 1] to -u2wj(u) as U   and the functions are dominated by a constant independent of U . In order to apply dominated convergence it suffices that
the second factor is dominated by a constant independent of U and converges
Rstochastically with respect to the Lebesgue measure on .

(gU (v)  F -1((y/U + j)2)(v))(u)

= gU (u - v)F -1((y + j)2)(U v)U dv
-

19

RBy assumption
constant is 2

F 2 lies in L1( ) wk  exp(T F µ

and )

so does F -1((y +j)2).
F -1((y + j)2) RL1( ).

A It

dominating holds



F -1((y + j)2)(U v)U dv =

F -1((y + j)2)(v)dv

- -

= F F -1((y + j)2)(0) = (j)2.

(6.13)

U (v) := F -1((y +j)2)(U v)U is the multiple of what is called approximate
identity or nascent delta function. The basic theorem on approximate iden-
R Rtities states that h  n converges to h in L1( ) as n   for h  L1( ). RThus, (-v2wk(v)1[0,1](v))U (v)(u) converges to -u2wk(u)1[0,1](u)(j)2 for
U   in L1( ) [13, p. 28] and in particular stochastically. If u = 0, then there is a neighborhood of u where gU (u) + u2wk(u)1[0,1](u) converges uniformly to zero. (gU (v) + v2wk(v)1[0,1](v))  U (v)(u) converges almost surely and in particular stochastically to zero. Therefore, gU (v)U (v)(u) converges
Rto -u2wk(u)1[0,1](u)(j)2 stochastically with respect to the Lebesgue mea-
sure on .
We obtain under the limit U   by the dominated convergence theo-
rem

1

lim
U 

2U3 E

11
wj (u)L ,U (u)e-iUuxj du wk(v)L ,U (v)e-iUvxj dv
00

=

T2

2(j )2 exp(2T ( -

))

=

T2

2(j )2 exp(2T ( -

))


-u2 wj (u)1[0,1] (u)
-
1
u4 wj (u)wk (u)du.
0

-u2wk(u)1[0,1](u) du (6.14)

Without taking the complex conjugate in (6.12) we obtain

11

T 2e2T (-)E

wj (u)L ,U (u)e-iUuxj du wk(v)L ,U (v)e-iUvxj dv

00



= 2

fU (u)(gU (-v)  F -1((y/U + j)2)(v))(u)du.

-

The same argumentation as before leads to

11

lim E
U 

wj (u)L ,U (u)e-iUuxj du wk(v)L ,U (v)e-iUvxj dv
00

=

T2

2(j )2 exp(2T ( -

))

 -

-u2 wj (u)1[0,1] (u)

-u2wk(-u)1[0,1](-u) du

= 0. (6.15)

20

We combine (6.14) and (6.15) to obtain

lim
U 

1 2U3 E

Re

1 wj(u)L ,U (u) du Re 0 exp(iU uxj)

1 wk(u)L ,U (u) dv 0 exp(iU uxj)

= lim
U 

1 2U3 E

Im

1 wj(u)L ,U (u) du Im 0 exp(iU uxj)

1 wk(u)L ,U (u) dv 0 exp(iU uxj)

=

T2

(j )2 exp(2T ( -

))

1
u4 wj (u)wk (u)du.
0

From (6.14) and (6.15) it also follows that the covariance between real and imaginary part vanishes asymptotically.
In the case xj = xk we have to show that the covariance vanishes asymptotically. Without loss of generality we assume xj < xk. We define  := (j + k)/2. Then j <  < k.

T 2e2T (-) 2U3 E

11
wj (u)L ,U (u)e-iUuxj du wk(v)L ,U (v)e-iUvxk dv
00


= F fU (U (x - j))F gU (U (x - k))(x)2U dx
-

= F fU (y + U ( - j))F gU (y + U ( - k))(y/U + )2dy
-

By the Plancherel identity and by the dominated convergence theorem

F fU  F -u2wj(u) and F gU  F -u2wk(u)
R Rin L2( ) for U   and especially the L2( ) norms converge. From the R Rassumption F2  L1( ) follows that 2  L( ). By the Cauchy-Schwarz
inequality

0
lim F fU (y + U ( - j))F gU (y + U ( - k))(y/U + )2dy
U  -

 lim
U 



2 

F fU

RL2( )

U(-k) |F gU (y)|2 dy

1/2
= 0.

-

A similar calculation shows that the integral over (0, ) converges to zero and consequently

T 2e2T (-)

lim
U 

2U3 E

1

wj (u)L

,U

(u) du

1 wk(v)L ,U (v) dv

= 0.

0 exp(iU uxj) 0 exp(iU vxk)

The same way follows

T 2e2T (-)

lim
U 

2U3 E

1 wj(u)L ,U (u) du

1

wk(v)L

,U

(v) dv

= 0.

0 exp(iU uxj) 0 exp(iU vxk)

21

The

1/( U 3/2)

1 0

wj (u)L

,U (u)e-iUuxj du

are

centered

normal

random

variables and their covariance matrix converges to the covariance matrix

of the claimed limit. Thus, the characteristic function converges pointwise.

By L´evy's continuity theorem this shows the convergence in distribution.

R CLemma 2. Let  > 0 and   L( ). Let wU , w~U  L([0, 1], ) be
Riemann-integrable and let there be a constant C > 0 such that for all U  1
CwU , w~U   C. Let there be a, a~ : [1, )  such that the condition

lim sup sup |wU (u) - a(U )| = 0
0 U 1 u[1-/U,1]

(6.16)

and the corresponding condition for w~U and a~ hold. Then

1

lim
U 

2 exp(T 2U 2) E

11
wU (u)L ,U (u)du w~U (v)L ,U (v)dv
00

lim
U 

1 2 exp(T 2U 2) E

11
wU (u)L ,U (u)du w~U (v)L ,U (v)dv
00

-

a(U )a~(U ) exp(2T (2/2

 -
+

(y)2dy - ))T 44

= 0.

= 0,

Remark. Obviously a(U ) := wU (1) is the only possible definition. Thus, a describes the dependence of wU on U at one.

Proof. We notice that (6.10) applies to the complex-valued functions and
yields for wj := wU and wk := w~U with the definitions (6.8) and (6.9) of fU and gU , respectively, and with  := T 2 + T 

T 2e2T (2/2+-)E

11
wU (u)L ,U (u)du w~U (v)L ,U (v)dv
00



= 2U4

F (fU (u)eT 2U2u2/2)(U (x - ))

-

F (gU (v)eT 2U2v2/2)(U (x - ))(x)2dx,



= 2 2U 3

fU (u)eT 2U2u2/2

-

gU (v)eT 2U2v2/2  F -1((y/U + )2)(v) (u)du

11

= 2 2U 4

fU (u)gU (v)

00

F -1((y + )2)(U (u - v))eT 2U2(u2+v2)/2dudv.

(6.17)

For all  > 0 we have

1

lim T 2U 2e-T 2U2/2

ueT 2U 2u2/2du

U 

1-/U

= lim e-T 2U 2/2 eT 2U 2u2/2 1

U 

1-/U

22

= 1 - lim e-T 2U2(1-(1-/U)2)/2
U 
= 1 - lim e-T 2U 2(2/U -2/U 2)/2
U 
= 1 - lim e-T 2U+T 22/2 = 1.
U 

(6.18)

For the product of two such sequences we obtain for all  > 0

11

lim T 24U 4e-T 2U2

uveT 2U2(u2+v2)/2dudv = 1.

U 

1-/U 1-/U

(6.19)

Likewise

11

lim T 24U 4e-T 2U2

uveT 2U2(u2+v2)/2dudv = 1

U 

00

holds. We scale the integral in (6.17) appropriately:

(6.20)

11

lim T 24U 4e-T 2U2

fU (u)gU (v)

U 

00

F -1((y + )2)(U (u - v))eT 2U2(u2+v2)/2dudv

- a(U )a~(U ) 1


(y)2dy

2 -

11

= lim T 24U 4e-T 2U2

uveT 2U 2(u2+v2)/2

U 

00

F -1((y + )2)(U (u - v))fU (u)gU (v)/(uv)dudv

- a(U )a~(U ) 1


(y)2dy

2 -

(6.21) (6.22)

We recall that in the Gaussian white noise model we assumed  to be
Rin L2( ). Since F -1((y + )2)(U (u - v))fU (u)gU (v)/(uv) is bounded in
L([0, 1]2) independently of U for U  1 and since the difference between (6.20) and (6.19) is zero, only the integral over [1 - /U, 1]2 contributes to
the limit. For all  > 0 the limit equals

11

= lim T 24U 4e-T 2U2

uveT 2U 2(u2+v2)/2

U 

1-/U 1-/U

F -1((y + )2)(U (u - v))fU (u)gU (v)/(uv)dvdu

- a(U )a~(U ) 1


(y)2dy = 0,

2 -

(6.23)

which can be seen the following way. F -1((y + )2) is continuous and
|U (u - v)|   for all u, v  [1 - /U, 1]. So by choosing  small enough F -1((y + )2)(U (u - v)) gets arbitrarily close to F -1((y + )2)(0) =

23

(1/2)

 -

(y)2dy.

By (6.16), wU (u) tends to a(U ) and w~U (v) tends

to a~(U ) for  tending to zero. By choosing U large the factor (-u +

i/U )/ exp(T Fµ(U u)) gets close to minus one for all u  [1 - /U, 1]. Thus,

for small  and large U the term fU (u)gU (v)/(uv) is close to a(U )a~(U ) for all u, v  [1 - /U, 1].

Rescaling (6.17) and taking the limit U   leads to

lim
U 

1 2 exp(T 2U 2) E

11
wU (u)L ,U (u)du w~U (v)L ,U (v)dv
00

-

a(U )a~(U ) exp(2T (2/2

 -
+

(y)2dy - ))T 44

2U 4 exp(-T 2U 2)

11

= lim
U 

T 2 exp(2T (2/2 +  - ))

0

fU (u)gU (v)
0

F -1((y + 0)2)(U (u - v))eT 2U2(u2+v2)/2dudv

-

2 exp(2T (2/2

+



-

))

a(U )a~(U ) T 442


(y)2dy
-

= 0,

(6.24)

where we used that (6.21) is zero. By (6.11) we have

11

T 2e2T (2/2+-)E

wU (u)L ,U (u)du w~U (v)L ,U (v)dv

00



= 2U4

F (fU (u)eT 2U2u2/2)(U (x - ))

-

F (gU (v)eT 2U2v2/2)(-U (x - ))(x)2dx



= 2U4

F (fU (u)eT 2U2u2/2)(U (x - ))

-

F (gU (-v)eT 2U2v2/2)(U (x - ))(x)2dx



= 2 2U 3

fU (u)eT 2U2u2/2

-

gU (-v)eT 2U2v2/2  F -1((y/U + )2)(v)

(u)du

10

= 2 2U 4

fU (u)gU (-v)

0 -1

F -1((y + )2)(U (u - v))eT 2U2(u2+v2)/2dvdu

11

= 2 2U 4

uveT 2U 2(u2+v2)/2

00

F -1((y + )2)(-U (u + v))fU (u)gU (v)/(uv)dvdu.

24

Rescaling as in (6.24) leads to

1

lim
U 

2 exp(T 2U 2) E

11
wU (u)L ,U (u)du w~U (v)L ,U (v)dv
00

2U 4 exp(-T 2U 2)

=

lim
U 

T

2

exp(2T

(2/2

+



-

))

1 0

1
uveT 2U 2(u2+v2)/2
0

F -1((y + )2)(-U (u + v))fU (u)gU (v)/(uv)dudv = 0,

(6.25)

since F -1((y + )2)(u)  0 for |u|  .

R RLemma 3. Let  > 0 and   L( ). Let x0  and for j = 1, . . . , n let Rwj : [0, 1]  be continuous at one, Riemann-integrable and in L([0, 1]).

Then

1 exp(T 2U 2/2)

1
wj (u)L ,U (u)e-iUux0 du
0

converge jointly in distribution to

  L2(R)wj(1)

(W + iV )

2 exp(T (2/2 +  - ))T 22

as U  , where W and V are independent standard normal random variables.

Proof. We define wU (u) := wj(u)/ exp(iU ux0), w~U (u) := wk(u)/ exp(iU ux0), a(U ) := wj(1)/ exp(iU x0) and a~(U ) := wk(1)/ exp(iU x0) and apply Lemma 2. Condition (6.16) is satisfied since wj and wk are continuous at one and since exp(-iU ux0) = exp(-iU x0) exp(iU (1 - u)x0) where U (1 - u)   for u  [1 - /U, 1]. We note that a(U )a~(U ) = wj(1)wk(1) is real. By Lemma 2 the covariances converge to the covariances of the claimed limit.

The convergence in distribution follows by L´evy's continuity theorem.

R RLemma 4. Let  > 0 and   L( ). Let w1, w2 : [0, 1]  be RiemannRintegrable, in L([0, 1]) and continuous at one. Let x1, x2  and denote
x2 - x1 by . Then

1 eT 2U 2/2

w1(1)

1 0

w2(u)L ,U eiU ux2

(u)

du

-

w2(1) eiU 

1 0

w1

(u)L ,U eiU ux1

(u)

du

-Q 0,

as U  .

Proof. We define

wU (u)

:=

w1(1)w2(u) exp(iU ux2)

-

w2(1)w1(u)

.

exp(iU ) exp(iU ux1)

wU (u) fulfills condition (6.16) with a(U ) = 0 for all U  1. Lemma 2 yields

1

lim
U 

2 exp(T 2U 2) E

12
wU (u)L ,U (u)du = 0
0

and the statement follows by L´evy's continuity theorem.

25

6.3 The remainder term

In this section, we show that the contribution of the remainder term to the estimation vanishes asymptotically. We recall that the remainder term R ,U depends on the L´evy triplet.

CLemma 5. Let 0 > 0. Let wU  L([0, 1], ) be Riemann-integrable and
let there be a constant C > 0 such that wU   C for all U  1. If U ( )2 log(U ( )) exp(T 02U ( )2/2)  0 as  0, then for all L´evy triplets with   0

1 exp(T 02U ( )2/2)

1
wU( )(u)R ,U( )(u)du -Q 0,
0

as  0.

Proof. By the identity R ,U (u) = (1/T ) logU (u)(1 + T L ,U (u)) - L ,U (u), where U (u)  1/2, we have to show that for U = U ( )

11 exp(T 02U 2/2) 0 wU (u) logU (u)(1 + T L ,U (u)) - T L ,U (u) du
(6.26)
converges in probability to zero.
CFor z  holds log(1 + z) - z = O(|z|2) as |z|  0. We define g by
g(z) := (log(1 + z) - z)/|z|2 for z = 0 and g(0) := 0. There are M and  > 0 such that |g(z)|  M for all |z|  . We may assume that   1/2.
If the logarithm in the definition of g is replaced by the trimmed logarithm log with   (0, 1/2] then g remains unchanged for |z|  1/2. Thus, the statement holds uniformly for all g(z) := (log(1 + z) - z)/|z|2 with  ]0, 1/2].
By Proposition 1 we have supu[-1,1] |L ,U (u)| -Q 0. Let  > 0 be given. Eventually we have

Q u  [-1, 1] : | logU (u)(1 + T L ,U (u)) - T L ,U (u)| > M T 2|L ,U (u)|2

 Q T sup |L ,U (u)| >  < .
u[-1,1]

Except on a set with probability less than  we have eventually

11

exp(T 02U 2/2)

wU (u) logU (u)(1 + T L ,U (u)) - T L ,U (u) du
0



MT2 exp(T 02U 2/2)

1
|wU (u)L ,U (u)2|du.
0

(6.27)

Hence (6.26) converges in probability to zero if (6.27) converges in probability to zero. The convergence

1 exp(T 02U 2/2)

1
|wU (u)L ,U (u)2|du  0
0

26

holds even in L1 since

1 exp(T 02U 2/2) E

1
|wU (u)L ,U (u)2|du
0

(6.28)



C exp(T 02U 2/2) E

1
|L ,U (u)2|du
0

C  exp(T 02U 2/2)

1

iU u(1 + iU u)

2

0 T (1 + iU u(1 + iU u)F O(U u)) E

2
eiUux(x)dW (x) du
-



C exp(T 02U 2/2)

1 0

T

2 2

(U 2 + U 4)u exp(T  exp(2T (2/2 +  -

2U )

2u2) - 2T



R2
L2( )
Fµ 

)

du,

(6.29)

for  = 0 this converges to zero and for  > 0 we further calculate,

=

C (1 + U 2) 

R2
L2( )

1 0

2T

2U

2

u

exp(T

2

U

2u2

)du

exp(T 02U 2/2)2T 32 exp(2T (2/2 +  - ) - 2T F µ

)

=

C

(1 + U 2)



2 L2

(R)

(exp(T

2U

2)

-

1)

exp(T 02U 2/2)2T 32 exp(2T (2/2 +  - ) - 2T

Fµ

)



C



R2
L2( )

(1 + U 2)(exp(T 02U 2/2))

2T 32 exp(2T (2/2 +  - ) - 2T F µ

)



0

as  0. Thus, (6.26) converges in probability to zero.
CLemma 6. Let wU  L([0, 1], ) be Riemann-integrable and let there be
a constant C > 0 such that wU   C for all U  1. If U ( )   and U ( )5/2  0 as  0, then for all L´evy triplets with  = 0

as  0.

1 U ( )3/2

1
wU( )(u)R ,U( )(u)du -Q 0,
0

Proof. We follow the proof of Lemma 5. supu[-1,1] |L ,U (u)| -Q 0 holds by Proposition 1. We set 0 = 0 and divide by U 3/2 in (6.26) and (6.27). Then
we use that (6.28) is bounded by (6.29), where we set 0 =  = 0 and divide by U 3/2 again. We obtain

1 U 3/2 E

1
|wU (u)L ,U (u)2|du
0



C (U 1/2 + U 5/2)  T 2 exp(T (2( - ) - 2

R2
L2( )
F µ ))

0

as  0, which implies the desired convergence.

27

6.4 The approximation errors

The approximation error can be controlled as in [2] using the order conditions (2.9) on the weight functions. The L´evy triplet T = (2, , µ) was assumed
to be contained in Gs(R, max), especially µ is s-times weakly differentiable and max0ks µ(k) L2(R)  R, µ(s)   R.
We use (iu)sF µ(u) = F µ(s)(u) and the Plancherel identity to bound the
approximation error by

2 U2

1
Re(F µ(U u))w1 (u)du
0

1 = U2
2 = U2

1
F µ(U u)w1 (u)du
-1

µ(s)(x/U )U -1F -1(w1 (u)/(iU u)s)(x)dx
-

 U -(s+3) µ(s)  F (w1 (u)/us) RL1( ).

(6.30)

Analogously we obtain

2 U

1
Im(F µ(U u))w1(u)du  U -(s+2) µ(s)  F (w1(u)/us) RL1( ),
0

(6.31)

1
2 Re(F µ(U u))w1(u)du  U -(s+1) µ(s)  F (w1(u)/us) RL1( ). 0 (6.32)

The last error term in (6.3) can be bounded by

U F -1 (1 - wµ1 (u))F µ(U u) (U x)

U =
2


(1 - wµ1 (u))F µ(U u)e-iUuxdu
-

1 = 2U s-1

 -

1

-

wµ1 (u) us

eiU

uxF

µ(s)(U

u)du

= U -s


F -1
-

1

-

wµ1 us

(u)

eiU

ux

(y)µ(s) y U



µ(s)  2U s

F

1 - wµ1 (u) us

.
RL1( )

dy

(6.33)

6.5 Proof of Theorem 1
We consider the decompositions of the estimation errors (6.1)­(6.3). The undersmoothing U ( )(2s+5)/2   is equivalent to U ( )-(s+3) = o( U ( )-1/2)

28

and implies in view of (6.30) that the approximation error of ^2 is asymp-

totically negligible. Likewise the error terms 2U -1

1 0

Im(F

µ(U

u))w1

(u)du,

2

1 0

Re(F

µ(U

u))w1 (u)du

and

U

F

-1

(1 - wµ1 (u))F µ(U u)

(U x) can be bounded

as in (6.31), (6.32) and (6.33) and vanish asymptotically. Since ^2 converges

with a faster rate than ^ and ^ converges with a faster rate than ^, the error

^2 vanishes asymptotically in (6.1) and in (6.2) as well as ^ is asymp-

totically negligible in (6.2). For x = 0 we can apply the Riemann-Lebesgue

lemma to the second, the third and the fourth error term in (6.3) and we see

that they are of order oQ( U ( )5/2). For x = 0 due to the symmetry of wµ1 the third term vanishes asymptotically but the second and the fourth term

do not. The error terms of µ^(x) we have to consider are in the case x = 0

U F -1  (U u)wµ1 (u) (U x)

U =2
2

1
wµ1 (u)Re
0

 (U u)e-iUux

du

and in the case x = 0

F -1  (U u))wµ1 (u) (0)

1
+ Re( (U u))w1 (u)du F -1 u2w1 (u) (0)
0

1
- 2 Re( (U u))w1(u)du F -1 wµ1 (u) (0)
0

11

=2 2

0

Re( (U u))w0(u)du.

(2.9) implies that w1 , w1, w1 and wµ1 are continuous and bounded, especially they are Riemann-integrable and in L([-1, 1]). Applying Lemma 1 to the linearized stochastic errors and Lemma 6 to the remainder terms yields the theorem.

6.6 Proof of Theorem 2
To see the first line we set x1 = x2 = 0, w1  1 and w2 = w in Lemma 4 and wU = w1 in Lemma 5. The second and third line follow analogously. In order to derive the last line we observe
F -1  (U u)wµ1 (u) (U x)
1
= 2 Re( (U u)e-iUux)wµ1 (u)du/(2)
0
and apply Lemma 4 with x1 = 0, x2 = x, w1  1 and w2 = wµ1 , . The remainder term vanishes by setting wU (u) = wµ1 (u)e-iUux in Lemma 5.

29

6.7 Proof of Theorem 3

By the undersmoothing U s+1 exp(T 2U 2/2)   we have U -(s+3) = o( U -2 exp(T 2U 2/2)) so that the approximation error of ^ vanishes. A
similar reasoning applies to the approximation errors of the other estima-
tors. Since ^ converges with a faster rate than ^ and ^ with a faster rate than ^ the leading stochastic error terms are given in Theorem 2 and the conver-
gence of the first three lines follows by this theorem. For x = 0 all stochastic
errors in (6.3) are negligible except the first one. We obtain the convergence in the last line by Theorem 2. We observe that F -1[uwµ1(u)](0) = 0, since wµ1 is symmetric. For x = 0 the relevant stochastic error terms are

F -1  (U u)wµ1 (u) (0)

1
+ Re( (U u))w1 (u)du F -1 u2wµ1 (u) (0)
0

1
- 2 Re( (U u))w1(u)du F -1 wµ1 (u) (0)
0

11

=2 2

0

Re( (U u))w0(u)du.

We apply Lemma 4 with x1 = x2 = 0, w1  1 and w2 = w0 to this term. The remainder term converges to zero by Lemma 5. This shows the convergence in the next to last line.

7 Tests and a confidence set for the volatility

In this section, we test the hypotheses H0 :  = 0 against the alternative

H1 : | - 0|   ,  > 0, and construct a confidence set for . We assume

that T  Hs(R, max) especially we assume that   [0, max].

For 0 > 0 the most natural test statistic is the following. In or-

der to apply the uniform version of Theorem 3 under H0 we choose a cut-off value U ( ) such that U ( )2 log(U ( ))1/2 exp(T 02U ( )2/2)  0 and U ( )s+1 exp(T 02U ( )2/2)   as  0. Let ^2 be the estimator corresponding to this cut-off value U ( ). We ensure ^2  [0, m2 ax] by taking the maximum with zero and the minimum with m2 ax. Likewise we ensure ^  [-R, R] and ^  [0, R]. We choose a weight function with w1 (1) = 0

and define

S0

:=

d^0

U ( )2(^2 - 02) , exp(T 02U ( )2/2)



d^0

:=

2 |w1 (1)| exp(T (02/2 +

R L2( )
^ - ^))T

202

.

Under H0 the test statistic S0 converges uniformly in distribution to a standard normal random variable by the uniform version of Theorem 3. We

30

decompose

S0 = d^0

U ( )2^2 exp(T 02U (

)2/2)

+

d^0

U ( )2(2 - 02) . exp(T 02U ( )2/2)

(7.1)

We will show that for   0 -  the first term converges uniformly in probability to zero. The approximation error contributes a term that con-
verges deterministically to zero by the bound (6.30) and by the assumption U ( )s+1 exp(T 02U ( )2/2)   as  0. The remainder term of the stochastic error in the first summand of (7.1) converge uniformly in prob-
ability to zero by Subsection 8.3 and the linearized stochastic error by the
following lemma.

R CLemma 7. Let   L( ). Let wU  L([0, 1], ) be Riemann-integrable
and let there be a constant C > 0 such that wU   C for all U  1. Then
for all ,  > 0

sup QT
T Hs(R,0- )

1 exp(T 02U 2/2)

1
wU (u)L ,U (u)du > 
0

 0,

as U  

Proof. It is equivalent to consider for all  > 0 the supremum over Hs(R, 02 -  ), such that 2  02 -  is satisfied for each T . By (6.17) we have

11

1

sup
T

2 exp(T 02U 2) E

wU (u)L ,U (u)du wU (v)L ,U (v)dv
00

2U 4 exp(2T ( -  - 2/2)) 1 1

= sup
T

T 2 exp(T 02U 2)

fU (u)fU (v)
00

F -1((y + )2)(U (u - v))eT 2U2(u2+v2)/2dudv

4U 4 exp(6T R)C2  T 2 exp(T  U 2)

F -12

  0,

 as U  , where we used fU   2C exp(T R).

We have seen that for   0 -  the first term in (7.1) converges to zero uniformly in probability. The second term is d^-01 times a deterministic sequence converging to -. We note that d^0 is bounded from above and
Rbelow. Consequently it holds lim 0 infHs(R,0-) Q(S0 < c) = 1 for all
c  . We would like to make a similar statement in the case   0 +  . Unfortunately for  > 0 the variance of L ,U( )(1) does not converge to zero. So it is not possible to find a bound like in Proposition 1, which converges to
zero. Consequently the remainder term cannot be controlled by Lemma 5.
We modify the test statistic in the following way. We choose ¯ > max and

31

let U¯ ( ) be a cut-off value with U¯ ( )2 log(U¯ ( ))1/2 exp(T ¯2U¯ ( )2/2)  0 and U¯ ( )s+1 exp(T ¯2U¯ ( )2/2)   as  0. We further assume that

U¯ ( U(

)2 exp(-T ¯2U¯ ( )2 exp(-T 02U (

)2/2) )2/2)



.

(7.2)

This can for example be ensured by choosing the cut-off values U ( ) and U¯ ( ) according to (3.6) with  and ¯ > , respectively. Let ~2 be the estimator of 2 corresponding to the cut-off value U¯ ( ). We define

S~0 := S0 + = S0 +

U¯ ( )2(~2 - 02) exp(T ¯2U¯ ( )2/2)
U¯ ( )2(~2 - 2) exp(T ¯2U¯ ( )2/2) +

U¯ ( )2(2 - exp(T ¯2U¯ (

02) )2/2)

.

(7.3) (7.4)

Under H0 the statistic S0 can be written as in Lemma 8. The second
term in (7.3) converges uniformly in probability to zero. Thus, under H0 the modified statistic S~0 converges uniformly in distribution to a standard normal random variable by Lemma 8. For   0 -  the sec-
ond term in (7.4) converges uniformly in probability to zero and the third
term is deterministic sequence converging to -. As for S0 it holds lim 0 infHs(R,0-) Q(S~0 < c) = 1 for all c  R. But now we are also able to make a similar statement for   0 +  . Since we bounded the estimators S0 cannot diverge faster than -1U ( )2 exp(-T 02U ( )2/2). For   [0, max] the second term in (7.4) converges uniformly in probability to
zero. Owing to (7.2) the third term in (7.4) tends to infinity faster than the bound of S0. It holds lim 0 infT Q(S~0 > c) = 1 for all c  R, where the infimum is over all T  Hs(R, max) with   0 +  .
Let q denote the (1 - )­quantile of the standard normal distribution.
For 0  (0, max) we define the tests

0 :=

0, 1,

if |S~0 |  q/2 if |S~0 | > q/2

,

max :=

0, if Smax  q1- . 1, if Smax < q1-

For 0 = 0 we would like to apply Theorem 1. To this end we choose the cut-off value U ( ) such that U ( )5/2  0 and U ( )(2s+5)/2  . Let ^2 be
the estimator corresponding to this cut-off value U ( ), where ^2  [0, m2 ax] is ensured. We assume that  is positive on [-T R, T R] and define

U ( )1/2^2

U¯ ( )2~2

S0 := d^0

+ exp(T m2 axU¯ ( )2/2) ,



d^0

:=

2 (T ^) exp(T (^ - ^))T

1 1/2

u4w1 (u)2du

.

0

Under H0 : 2 = 0 the test statistic S0 converges uniformly in distribution to a standard normal random variable by a similar argument as for S~0.

32

We observe that the first term of S0 is nonnegative. Under H1 :   
the second term of S0 may be decomposed as in (7.4) into a part that
converges uniformly in distribution to zero and into a deterministic sequence
Rconverging to infinity. It holds lim 0 infT Q(S0 > c) = 1 for all c  ,
where the infimum is over all T  Hs(R, max) with    . We define the
test

0 :=

0, 1,

if S0  q if S0 > q

.

Since we have a test on  = 0 for each 0  [0, max] we may use this family of tests to define a confidence set for . We define M := {| = 0} and obtain lim 0 infT QT (  M ) = 1 - , where the infimum is over all T  Hs(R, max) with volatility . The set M is not necessarily an interval. For 0  (0, max) the cut-off value U ( ) may be chosen as a continuous function of 0 by (3.6). The estimators ^2, ^ and ^ depend continuously on the cut-off value U ( ), which can be seen by substituting v = u/U in
(2.3), (2.4) and (2.5) and applying the continuity theorem on parameter
Rdependent integrals. Thus, S~ : (0, max)  ,   S~ is continuous and
M  (0, max) may be written as the preimage S~-1([-q/2, q/2]) of the continuous function S~.

8 Uniform convergence
The asymptotic normality results hold for each L´evy triplet T  Gs(R, max). The speed of convergence might depend on T . To make statements on confidence sets and on hypotheses tests it is useful to control the speed of convergence uniformly over a class of L´evy triplets. We fix some arbitrarily slowly decreasing function h with h(u)  0 as |u|  . We will show uniform convergence for the class Hs(R, max) consisting of all L´evy triplets in Gs(R, max) satisfying the additional conditions (4.2), which we recall to be
RFµ   R, |Fµ(u)|  R h(u), u  .
The first condition can easily be ensured by µ RL1( )  R. For h(u) = |u|-1 the second condition can be ensured by µ(1) RL1( )  R. For each L´evy
triplet in Gs(R, max) the function F µ(u) tends to zero as |u|  0 by the Riemann-Lebesgue lemma. Especially for each T  Gs(R, max) there are h and R > 0 such that T  Hs(R , max).
In the case  > 0 some covariances do not converge. We show uniform convergence for the joint distribution only in such cases, where for  > 0 the covariances do converge. As it turns out it is also important that the covariance matrix of the limit is nondegenerated. We cover uniform convergence of ^2, ^, ^, µ^(x) and of (^, ^).

33

A sequence of random variables Xn converges to X in total variation if

sup |P(Xn  B) - P(X  B)|  0
B
as n  , where the supremum is taken over all measurable sets B. For sequences X,n,   , we say that they converge to X in total variation uniformly over  if

sup sup |P(X,n  B) - P(X  B)|  0
 B
as n  . Motivated by the Portmanteau theorem we say for X,n,   , and X with values in some metric space that X,n converge to X in distribution uniformly over  if for all Borel sets B with P(X  B) = 0 we have

sup |P(X,n  B) - P(X  B)|  0


as n  .

Theorem 4. Let the assumptions of Theorem 1 be fulfilled and let  be continuous and positive. Each marginal convergence in Theorem 1 is a uniform convergence in distribution over Hs(R, 0) if the standard deviation is positive and both sides are divided by it.
Let  > 0 and let the assumptions of Theorem 3 be fulfilled. Each marginal convergences in Theorem 3 is a uniform convergence in distribution over all T  Hs(R, max) with volatility  if the standard deviation is positive and both sides are divided by it.

Remark. In both cases uniform convergence in distribution does also hold for ^ and ^ jointly and in the standard deviation on the left side  and 
may be replaced by their estimators.

The following lemma may be seen as a generalization of Slutsky's lemma for uniform convergence. It is the key step to show the uniform convergence in distribution in Theorem 4.
NLemma 8. Let X,n, Y,n,   , n  , and X be random vectors such that PX,n converge to X in total variation uniformly over  and sup (|Y,n|  P)  0 as n   for all  > 0. Let Z,n be random variables with
sup (|Z,n - 1|  )  0 for all  > 0. Then Z,nX,n + Y,n converge to X in distribution uniformly over .

Proof. For  > 0 we define
RB := {y  d| |x - y| <  for some x  B}, RB- := {y  d|x  B for all x with |x - y| < }.

34

As B and B- are open and closed, respectively, they are Borel sets. It holds >0 B = B and >0 B- = B. For B with P(X  B) = 0, we have P(X  B) = P(X  B) = P(X  B). Consequently

lim P(X  B) = lim P(X  B-) = P(X  B).

0

0

(8.1)

Let  > 0 be given. For all  > 0 it holds

sup P(Z,nX,n + Y,n  B)

 sup P(Z,nX,n  B) + sup P(|Y,n|  ),
 

for large n the second term is smaller than  owing to the assumptions on Y,n,

 sup P(Z,nX,n  B) + .


As a single random vector X is tight meaning that for each  > 0 there
is M such that P(|X|  M ) < . By taking the set {x | x  M } in the
definition of uniform convergence in total variation we obtain for n large
enough sup P(|X,n|  M ) < 2. By considering possibly larger n we can also ensure sup P(|Z,n - 1|  /M )  . But if |X,n| < M and |Z,n - 1| < /M , then |X,nZ,n - X,n| < . For n large enough it holds

sup P(Z,nX,n + Y,n  B)  sup P(Z,nX,n  B) + 
 
 sup P(X,n  B2) + 4

 P(X  B2) + 5
 P(X  B) + 6,

(8.2)

for  small enough by (8.1). Let  > 0 be given. For n large enough and  > 0 small enough we
obtain similarly

inf P(Z,nX,n + Y,n  B)  inf P(Z,nX,n  B-) - 
 



inf


P(X,n



B -2 )

-

4

 P(X  B-2) - 5

 P(X  B) - 6.

(8.3)

The statement follows by combining (8.2) and (8.3).

Lemma 8 outlines how to proceed in showing uniform convergence. X,n will be the leading term of the linearized stochastic error, Y,n will be the sum

35

of the smaller stochastic errors, the remainder term and the approximation error and Z,n will be the quotient of standard deviation and estimated standard deviation. The approximation error is uniformly controlled over Gs(R, max) and thus over Hs(R, max), too.
The substitution of the standard deviation by its empirical counterpart works as follows. We fix some x and write d instead of d(x) in Theorem 1 to unify the notation with Theorem 3 and to treat both simultaneously. For  continuous and positive the standard deviation depends continuously on  and  through d.  and  are restricted to a compact set. By the uniform convergence and by an upper bound of the standard deviation we obtain supT QT (|^| > )  0 for all  > 0 and for   {, }, where the supremum is over all T  Hs(R, max) with a fixed volatility . Since d is uniformly continuous in  and , we obtain supT QT (|d^| > )  0 for all  > 0, which gives the assumption on d/d^ corresponding to Z,n in Lemma 8 by a lower bound on d.
By the following lemma uniform convergence in total variation of the linearized stochastic error follows from uniform convergence in each component of the covariance matrix.

Lemma 9. Let X be a normal random vector with symmetric positive def-
R Ninite covariance matrix A  d×d. Let X,n, n  ,   , be normal Rrandom vectors with covariance matrices A,n  d×d. If A,n converge to
A in each component uniformly over  as n  , then X,n converge to X
in total variation uniformly over  as n  .

Proof. We have to show that

sup sup

exp(-

x, 

A-,1nx

/2)

-

exp(-

x, A-1x

/2) dx

 B B det( 2A,n)

det( 2A)

(8.4)

converges to zero as n  . The determinantis a continuous function of

Nthe components of a matrix. For all   (0, det( 2A))
that sup | det( 2A,n) - det( 2A)|   holds for

there all n

is 

N N

 and

such A-,1n

is well-defined for all    if n  N . Expression (8.4) equals

1 = sup
 2 1
= sup  2

exp(-

x, 

A-,1nx

/2)

-

exp(-

x, A-1x

/2)

dx

Rd det( 2A,n)

det( 2A)

exp(- x, A-1x /2)

Rd

| det( 2A)|

 det( 2A)

exp(- x, 

(A-,1n

-

A-1

)x

/2)

-1

dx.

det( 2A,n)

(8.5)

A,n converges to A in each component uniformly over . Likewise A-,1n converges to A-1 in each component uniformly over . We now addition-

36

ally require   (0, min(A-1)/d), where min(A-1) is the smallest eigenvalue of A-1. By going over to a possibly larger N we may assume that
sup |(A-,1n - A-1)jk|   for all j, k = 1, . . . , d, for all n  N . Then for all n  N

d

| x, (A-,1n - A-1)x | =

xj (A-,1n - A-1)jkxk

j,k=1


d

2

   |xj|  d x 22,

j=1

where in the last step the Cauchy-Schwarz inequality is used. We see that (8.5) can be bounded by

1 exp(- x, A-1x /2)

2 Rd | det( 2A)|



| det(

2A)| exp(d

x

2 2

/2)

-

| det(

2A)| exp(-d x 22/2)

| det( 2A)| - 

| det( 2A)| + 

dx. (8.6)

The integrand converges pointwise to zero for   0. For fixed  the func-
tion is at the same time a dominating function, which is integrable since A-1 - dId is positive definite by the choice   (0, min(A-1)/d). By the dominated convergence theorem (8.6) converges to zero and likewise
(8.4).

We will show uniform convergence in each component of the covariance

matrix. By Lemma 9 this leads to uniform convergence in total variation of

the linearized stochastic errors provided that the covariance matrix of the

limit is nondegenerated. To this end we assume in the case  = 0 that 

is positive and that

1 0

u4w1(u)2du

>

0

for

the

weight

functions

w1,





{, , , 0, µ}, involved. Joint distributions may further only involve more

than one of the estimators ^2, ^ or µ^(0), if the covariance matrix is positive

definite. in the case  > 0 we assume that w1 (1), w1(1), w1(1), wµ1 (1) = 0
and 2 RL2( ) > 0. By the uniform version of Lemma 5 the remainder term
converges uniformly to zero. By Lemma 8 each of the rescaled stochastic

error terms

1 exp(T 2U 2/2)w1 (1) 2

1
Re( (U u))w1 (u)du,
0

1 exp(T 2U 2/2)w1(1) 2

1
Im( (U u))w1(u)du,
0

1 exp(T 2U 2/2)w1(1) 2

1
Re( (U u))w1(u)du,
0

37

exp(T

2 2U 2/2)wµ1 (1)

F

-1

 (U u)wµ1 (u)

(U x)

converges uniformly in distribution to



exp(T

2 2 (2/2 +

RL2( )
 - ))T

2

2

W,

where W is a standard normal random variable. By the uniform versions of Lemma 3 and Lemma 5 we also obtain that for the second and third of the above stochastic error terms holds joint uniform convergence in distribution to independent normal variables.

8.1 Uniform convergence in the case  = 0

The convergence in distribution of the linearized stochastic errors is shown in Lemma 1 by the convergence of the components in the covariance matrix. We restrict ourselves to the case x1 = · · · = xn and show uniform convergence in each component of the covariance matrix. This implies uniform convergence in total variation by Lemma 9. We assume that  is continuous at all x  [x1 - T R, x1 + T R]. We note that fU , gU and j depend on the L´evy triplet T . The uniform convergence of the rescaled covariances (6.12) will be shown by the following easy lemma.
C NLemma 10. Let f,n, f, g,n, g  L1([0, 1], ), n  ,   , and M > N0 such that f,n , g   M for all n  , for all   . Let
C Csup f,n - f L1([0,1], )  0 and sup g,n - g L1([0,1], )  0 as
n  . Then

1
sup |f,n(x)g,n(x) - f(x)g(x)|dx  0
 0

as n  .

Proof. For all    it holds

|f,ng,n - fg|  |f,ng,n - f,ng| + |f,ng - fg|  M |g,n - g| + M |f,n - f|.

By assumption sup f,n-f CL1([0,1], )  0 as n   and sup g,n- g CL1([0,1], )  0 as n   and the claimed statement follows.

Let us verify the assumptions of Lemma 10 for the first factor in (6.12). fU and u2wj(u) will correspond to f,n and f, respectively. It holds

|fU (u) + u2wj(u)| = |u2wj(u)(1 - exp(-T F µ(U u))) + iu exp(-T F µ(U u))/U |  u2|wj(u)|(exp(T R(1  h(U u))) - 1) + iu exp(T R)/U.

(8.7)

38

This bound does not depend on T and converges everywhere to zero. Further fU (u), u2wj(u)  2 wj  exp(T R) is a bound that does not depend on U nor on T . By the dominated convergence theorem

sup fU (u) + u2wj(u) CL1([0,1], )  0 T

(8.8)

as U   and the conditions on the first factor in Lemma 10 are satisfied. To show the assumptions of Lemma 10 on the second factor, which is
the complex conjugate of

(gU (v)  F -1((y/U + j)2)(v))(u)

= gU (u - v)F -1((y + j)2)(U v)U dv,
-

we apply the following lemma. It is a uniform version of the basic theorem on approximate identities.

R C NLemma 11. Let f, f,n  L1( , ), n  ,   . Let sup f,n - R Cf L1(R,C)  0 for n  . Let ,n  L1( , ) fulfill the following proper-
ties:

N(i) There exists c > 0 such that ,n L1(R,C)  c for all n  , for all   .

N(ii)

 -

,n(y)dy

=

c

for

all

n



, for all   .

(iii) For any neighborhood V of zero we have sup V c |,n(y)|dy  0 as n  .

Then sup ,n  f,n - cf L1(R,C)  0 as n  .
Proof. By the triangle inequality

sup ,n  f,n - cf L1(R,C)

 sup ,n  (f,n - f ) L1(R,C) + sup ,n  f - cf L1(R,C).  
The first term in the triangle inequality (8.9) can be bounded by

(8.9)

sup ,n  (f,n - f ) L1(R,C)  sup R C,n L1( , ) f,n - f L1(R,C)
 
 c sup f,n - f L1(R,C), 
which converges to zero by assumption. To bound the second term in the triangle inequality (8.9) proceed as in the proof of Theorem 1.2.19 in [13,
p. 26]. Without loss of generality we may assume that f L1(R,C) > 0.
R CContinuous functions with compact support are dense in L1( , ). Since

39

continuous functions g with compact support are bounded we obtain by the dominated convergence theorem


|g(x - y) - g(x)|dx  0
-

R Cas y  0. We approximate f  L1( , ) by a continuous function with
compact support and see that for all  > 0 there is some neighborhood V of
zero such that

 |f (x - y) - f (x)|dx <
- 2c

for all y  V.

(8.10)

It further holds

(,n  f )(x) - cf (x)



= (,n  f )(x) - f (x)

,n(y)dy

-



= (f (x - y) - f (x)),n(y)dy

-

= (f (x - y) - f (x)),n(y)dy + (f (x - y) - f (x)),n(y)dy.
V Vc

We take L1 norms with respect to x and obtain for the first term by (8.10)

sup


(f (x - y) - f (x)),n(y)dy
V

L1(R,C)

 sup f (x - y) - f (x) L1(R,C)|,n(y)|dy  V

 sup


V

 2c

|,n(y)|dy

<

 2

and for the second term

sup


(f (x - y) - f (x)),n(y)dy
Vc

L1(R,C)



 sup


2f
Vc

L1(R,C)|,n(y)|dy

<

, 2

where n is taken large enough such that



sup


|,n(y)|dy <
Vc

4

f

.
L1(R,C)

The lemma is a consequence of (8.11) and (8.12).

(8.11) (8.12)

40

Let us first verify the assumptions (i), (ii) and (iii) for F -1((y+j)2(U v)U , which will correspond to ,n. We have

F -1((y + )2)(v) = eivF -1((y)2)(v).

(8.13)

RBy the assumptions of Lemma 1 it holds F2  L1( ). The equality

|F -1((y + j)2)(U v)U | = |F -1((y)2)(U v)U |

shows that the absolute value does not depend on T and that conditions (i) and (iii) are satisfied. Condition (ii) is satisfied by (6.13). We have

sup gU (u) + u2wk(u) CL1([0,1], )  0 T

(8.14)

R Cas in the corresponding equation (8.8) for fU . By extending gU and wk
by zero outside [0, 1] this holds in L1( , ), too. We apply Lemma 11 to gU (v) and v2wk(v), which correspond to f,n and f in this lemma. v2wk(v) is bounded and since condition (i) is satisfied we see that (j)2v2wk(v) is uniformly bounded over all L´evy triplets. By Lemma 10 the convergence of
the covariances (6.14) holds uniformly. The convergence in the analogous
equation without conjugation (6.15) holds uniformly, too.

8.2 Uniform convergence in the case  > 0
Let us first fix  > 0 and prove uniform convergence for all L´evy triplets with this fixed value of . To this end we show uniform convergence in Lemma 2. In order to control the error when going over to smaller domain of integration in (6.22) the term F -1((y + )2)(U (u - v))fU (u)gU (v)/(uv) needs to be bounded uniformly. The inverse Fourier transform is bounded by the L1-norm of 2. The functions fU and gU are uniformly bounded since Fµ   R. The crucial step is the limit (6.23), where the refined dirac sequence argument is applied. As stated in (8.13), a translation before the Fourier transform is equal to a multiplication by a complex unit after the Fourier transform. Since ||  T (m2 ax + R) the complex unit tends uniformly to one. wU and w~U do not depend on the L´evy triplet. Since there is h with |Fµ(u)|  R h(u) and h(u)  0 as |u|   the factor (-u + i/U )/ exp(T F µ(U u)) converges to -u uniformly over Hs(R, max). This leads to uniform convergence in (6.23) and thus in (6.24). To see that the covariance without conjugation converges in (6.25) uniformly to zero we observe that F -1((y + )2)(u)  0 for |u|   uniformly since the translation by  corresponds to a multiplication by a complex unit by equation (8.13). We immediately obtain the uniform convergence in each component of the covariance matrix in Lemma 3.

41

8.3 Uniform convergence of the remainder term

To begin with, we observe that Proposition 1 holds with the same constant

for all L´evy triplets in Hs(R, max). This follows from the bound (6.7) in the

proof and from the fact that X(u) =

 -

eiux(x)dW (x)

does

not

depend

on the L´evy triplet.

Lemma 5 and Lemma 6 hold uniformly, meaning that for all  > 0

sup QT
T Hs(R,0)
sup QT
T Hs(R,0)

1 exp(T 02U ( )2/2)

1
wU( )(u)R ,U( )(u)du > 
0

1 U ( )3/2

1
wU( )(u)R ,U( )(u)du > 
0

 0,

 0,

as  0. For Lemma 6 this follows from the uniform convergence of the bound in the proof. For Lemma 5 this can be seen by the corresponding uniform statements along the lines of the proof up to the bound (6.29). Then we bound (6.29) in two different ways depending on whether 2  [0, 02/2] or 2  (02/2, 02]. In the latter case we can proceed as in the proof for  > 0. Since 2 is bounded from below by 02/2 > 0 the convergence is uniform. For 2  [0, 02/2] we estimate

C exp(T 02U 2/2)

1 0

T

2(U 2 + U 4)u exp(T 2U 2 exp(2T (2/2 +  - )

2u2) - 2T



R2
L2( )
Fµ )

du



T2

C (U 2 + U 4) exp(2T (2/2 +  -

R

2 L2(

)

) - 2T

Fµ

, )

which converges uniformly to zero by 0 > 0 and by the assumption

U 2 log(U ) exp(T 02U 2/2)  0

of Lemma 5. The maximum of the bounds is a bound that holds for all L´evy triplets in the class. This shows the uniform version of Lemma 5.

References
[1] Denis Belomestny. Spectral estimation of the fractional order of a L´evy process. Ann. Statist., 38(1):317­351, 2010.
[2] Denis Belomestny and Markus Reiß. Spectral calibration of exponential L´evy models. Finance and Stochastics, 10(4):449­474, 2006.
[3] Denis Belomestny and Markus Reiß. Spectral calibration of exponential L´evy models [2]. Discussion Paper 2006-035, SFB 649, Humboldt-Universit¨at zu Berlin, Germany, 2006. Available at http://sfb649.wiwi.hu-berlin.de/papers/pdf/SFB649DP2006-035.pdf.

42

[4] Lawrence D. Brown and Mark G. Low. Asymptotic equivalence of nonparametric regression and white noise. Ann. Statist., 24(6):2384­ 2398, 1996.
[5] P. Carr and D. Madan. Option valuation using the fast Fourier transform. Journal of Computational Finance, 2(4):61­73, 1999.
[6] Rama Cont. Model uncertainty and its impact on the pricing of derivative instruments. Math. Finance, 16(3):519­547, 2006.
[7] Rama Cont and Peter Tankov. Financial modelling with jump processes. Chapman & Hall/CRC Financial Mathematics Series. Chapman & Hall/CRC, Boca Raton, FL, 2004.
[8] Rama Cont and Peter Tankov. Non-parametric calibration of jump­ diffusion option pricing models. Journal of computational finance, 7(3):1­50, 2004.
[9] Rama Cont and Peter Tankov. Retrieving L´evy processes from option prices: regularization of an ill-posed inverse problem. SIAM J. Control Optim., 45(1):1­25 (electronic), 2006.
[10] D. Ellsberg. Risk, ambiguity, and the savage axioms. The Quarterly Journal of Economics, pages 643­669, 1961.
[11] Jianqing Fan. Asymptotic normality for deconvolution kernel density estimators. Sankhy¯a Ser. A, 53(1):97­110, 1991.
[12] J.E. Figueroa-L´opez. Sieve-based confidence intervals and bands for L´evy densities. Bernoulli, 17(2):643­670, 2011.
[13] Loukas Grafakos. Classical and modern Fourier analysis. Pearson Education, Inc., Upper Saddle River, NJ, 2004.
[14] I. Grama and M. Nussbaum. Asymptotic equivalence for nonparametric regression. Math. Methods Statist., 11(1):1­36, 2002.
[15] Jean-Pierre Kahane. Some random series of functions, volume 5 of Cambridge Studies in Advanced Mathematics. Cambridge University Press, Cambridge, second edition, 1985.
[16] Olav Kallenberg. Foundations of modern probability. Probability and its Applications (New York). Springer-Verlag, New York, second edition, 2002.
[17] F.H. Knight. Risk, uncertainty and profit. New York Houghton Mifflin, 1921.
43

[18] R.C. Merton. Option pricing when underlying stock returns are discontinuous. Journal of financial economics, 3(1-2):125­144, 1976.
[19] Jakob So¨hl. Polar sets for anisotropic Gaussian random fields. Statistics & Probability Letters, 80(9-10):840 ­ 847, 2010.
[20] Peter Tankov. Pricing and hedging in exponential L´evy models: review of recent results. In Paris-Princeton Lectures on Mathematical Finance 2010, volume 2003 of Lecture Notes in Math., pages 319­359. Springer, Berlin, 2011.
[21] Mathias Trabs. Calibration of selfdecomposable L´evy models. Discussion Paper 2011-073, SFB 649, Humboldt Universit¨at zu Berlin, Germany, 2011. Available at http://sfb649.wiwi.huberlin.de/papers/pdf/SFB649DP2011-073.pdf.
[22] A. J. van Es and H.-W. Uh. Asymptotic normality of nonparametric kernel type deconvolution density estimators: crossing the Cauchy boundary. J. Nonparametr. Stat., 16(1-2):261­277, 2004.
[23] Bert van Es and Hae-Won Uh. Asymptotic normality of kernel-type deconvolution estimators. Scand. J. Statist., 32(3):467­483, 2005.
44

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "HMM in dynamic HAC models" by Wolfgang Karl Härdle, Ostap Okhrin and Weining Wang, January 2012.
002 "Dynamic Activity Analysis Model Based Win-Win Development Forecasting Under the Environmental Regulation in China" by Shiyi Chen and Wolfgang Karl Härdle, January 2012.
003 "A Donsker Theorem for Lévy Measures" by Richard Nickl and Markus Reiß, January 2012.
004 "Computational Statistics (Journal)" by Wolfgang Karl Härdle, Yuichi Mori and Jürgen Symanzik, January 2012.
005 "Implementing quotas in university admissions: An experimental analysis" by Sebastian Braun, Nadja Dwenger, Dorothea Kübler and Alexander Westkamp, January 2012.
006 "Quantile Regression in Risk Calibration" by Shih-Kang Chao, Wolfgang Karl Härdle and Weining Wang, January 2012.
007 "Total Work and Gender: Facts and Possible Explanations" by Michael Burda, Daniel S. Hamermesh and Philippe Weil, February 2012.
008 "Does Basel II Pillar 3 Risk Exposure Data help to Identify Risky Banks?" by Ralf Sabiwalsky, February 2012.
009 "Comparability Effects of Mandatory IFRS Adoption" by Stefano Cascino and Joachim Gassen, February 2012.
010 "Fair Value Reclassifications of Financial Assets during the Financial Crisis" by Jannis Bischof, Ulf Brüggemann and Holger Daske, February 2012.
011 "Intended and unintended consequences of mandatory IFRS adoption: A review of extant evidence and suggestions for future research" by Ulf Brüggemann, Jörg-Markus Hitz and Thorsten Sellhorn, February 2012.
012 "Confidence sets in nonparametric calibration of exponential Lévy models" by Jakob Söhl, February 2012.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

