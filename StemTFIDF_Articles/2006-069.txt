BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2006-069
Constrained General Regression in Pseudo-
Sobolev Spaces with Application to Option
Pricing
Zdenk Hl·vka* Michal Pesta*
* Department of Probability and Mathematical Statistics, Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Constrained General Regression in Pseudo-Sobolev Spaces with Application to Option Pricing
Zdenek Hl¥avka Michal Pesta Charles University in Prague Faculty of Mathematics and Physics Department of Probability and Mathematical Statistics Sokolovsk¥a 83, 18675 Prague 8, Czech Republic hlavka@karlin.mff.cuni.cz, tel. +420/221913284 pesta@karlin.mff.cuni.cz This version: September 20, 2006
We gratefully acknowledge the support of Deutsche Forschungsgemeinschaft, FEDC Guest Researcher Program for Young Researchers at SFB 649 "Economic Risk", and by MSM 0021620839 "Modern Mathematical Methods and Their Applications".
1

Abstract
State price density (SPD) contains important information concerning market expectations. In existing literature, a constrained estimator of the SPD is found by nonlinear least squares in a suitable Sobolev space. We improve the behavior of this estimator by implementing a covariance structure taking into account the time of the trade and by considering simultaneously both the observed Put and Call option prices.

Keywords and Phrases: isotonic regression, Sobolev spaces, monotonicity, multiple observations, covariance structure, option price

JEL classification: C10, C13, C14, C20, C88, G13

Let Yt(K, T ) denote the price of a European Call with strike price K on day t and with expiry date T . The payoff at time T is given by (ST - K)+ = max(ST - K, 0), where ST denotes the price of the underlying asset at time T . The price of such an option may be expressed as the expected value of the payoff

+
Yt(K, T ) = exp{-r(T - t)} (ST - K)+f (ST )dST ,
0

(1)

discounted by the known risk-free interest rate r. The expectation in (1) is evaluated with respect to the so-called State Price Density (SPD) f (.). The SPD contains important information on the expectations of the market and its estimation is a statistical task of great practical interest (Jackwerth, 1999).
Similarly, we can express the price Zt(K, T ) of the European Put with payoff (K - ST )+ as:

+
Zt(K, T ) = exp{-r(T - t)} (K - ST )+f (ST )dST .
0

(2)

In the following, the symbol denotes the vector of all Put option prices corresponding to a fixed date of
expiry T observed on a given day t. Similarly, denotes a vector containing all Call option prices. The
‹ ‹corresponding vectors of the strike prices for the Call and Put options are denoted by  and , respectively.
Calculating the second derivative of (1) and (2) with respect to the strike price K, we can express the
SPD as the second derivative of the European Call and Put option prices (Breeden and Litzenberger, 1978):

f (K)

=

exp{r(T

-

t)}

2Yt(K, K2

T

)

=

exp{r(T

-

t)}



2

Zt(K, K2

T

)

.

(3)

Both parametric and nonparametric approaches to SPD estimation are described in Jackwerth (1999). Nonparametric estimates of the SPD based on (3) are considered, among others, in A®it-Sahalia and Lo (2000);

2

A®it-Sahalia et al. (2001); Yatchew and Ha®rdle (2005); Ha®rdle and Hla¥vka (2006). In this paper, we will generalize the nonlinear least squares method suggested in Yatchew and Ha®rdle
(2005) by including the covariance of the observed option prices suggested in Ha®rdle and Hl¥avka (2006). The estimation of the SPD will be further improved by considering simultaneously both Put and Call option prices.
The investigation will be based on constrained (isotonic and convex) regression in pseudo-Sobolev spaces (Yatchew and Bos, 1997; Yatchew and Ha®rdle, 2005). In Sections 1 and 2, we will describe the mathematical foundation of the method. In Section 3, we will discuss problems arising in the real life application on the observed option prices. The covariance structure suggested in Ha®rdle and Hl¥avka (2006) is explained in Section 4. Finally, the SPD estimated from option prices on DAX are calculated in Section 5. The proofs of all theorems are given in Appendix B.

1 Pseudo-Sobolev Spaces

In this section, we give a brief overview on basic results of the theory of the Pseudo-Sobolev spaces. We assemble and prove necessary preliminaries and theorems for statistical regression in these spaces. The crux of this section lies in Theorem 1.2 (Representors in Pseudo-Sobolev Space) from Yatchew and Bos (1997). We have continued in examining representors' properties and proved Theorem 1.3, which provides the way of construction of the representors and their exact form.
We consider function f :   R and denote by

‹ ‹Df(

)

:=

||1 f ( ) x1 1 . . . xqq

(4)

‹its partial derivatives of order ||1 for  int()(  := \), where  = (1, . . . , q)  N0q is a

multiindex of the modulus ||1 =

q i=1

i.

Definition 1.1 (Sobolev Norm). Let f  Cm()  Lp() (see Definitions A.2 and A.3). We introduce a

Sobolev norm ∑ p,Sob,m:

‹ ‹


p 1/p

f p,Sob,m := 
||m

Df( )


d

.

(5)

We can write ∑ p,,Sob,m since the multiindex of modulus || = maxi=1,...,q i is taken with respect to maximum-norm.

Definition 1.2 (Pseudo-Sobolev Space). A Pseudo-Sobolev space of rank m, Wpm(), is the completion of intersection of space Cm() and space Lp() with respect to the Sobolev norm ∑ p,Sob,m.

3

Remark 1.1. Cm()  Lp() is dense in Wpm() according to ∑ p,Sob,m.

Definition 1.3 (Sobolev Inner Product). Let f, g  W2m(). The Sobolev inner product ∑, ∑ Sob,m is

defined as:

f, g Sob,m :=

‹ ‹ ‹Df ( )Dg( )d .

||m 

(6)

We denote the Sobolev norm ∑ 2,Sob,m := ∑ Sob,m for simplicity. The correctness of Definition 1.3 is guaranteed by the denseness of the space Cm()  L2() in W2m() (see Remark 1.1). The Sobolev inner product ∑, ∑ Sob,m induces in space W2m() the Sobolev norm ∑ 2,Sob,m. We denote the Pseudo-Sobolev space Hm() := W2m().

Theorem 1.1 (Hilbert Space). Hm() is a Hilbert space.

The theory of the Sobolev spaces is vast and more general than we could have presented in this short introduction. However, our simplified theory of the Sobolev spaces is sufficient for our statistical needs. For more detailed information on Sobolev spaces we refer to Adams (1975).

1.1 Construction of Representors in Pseudo-Sobolev Space

The space Hm() is a Hilbert space. Hence, Hm() can be expressed as a direct sum of subspaces that are orthogonal to each other and we can take projections of the elements of Hm() into its subspaces. This property is very important in the regression.
In the following Theorem 1.2 we quote a representation theorem for Pseudo-Sobolev spaces derived in Yatchew and Bos (1997)--analogous to well-known Riesz Representation Theorem A.2. From now on, let us suppose that m  N. The symbol Qq denotes closed unit cube in Rq.

Theorem 1.2 (Representors in Pseudo-Sobolev Space). For all f  Hm(Qq),
€ ‹| |  m - 1, there exists a function €( )  Hm(Qq), s.t.

€ Qq and  N0q,

€, f Sob,m = D€f ( ).

(7)

€€ is called a representor at the point with the rank . Furthermore,

q
‹€( ) = awii (xi)
i=1

(8)

‹for all  Qq, where awii (∑) is the representor in the Pseudo-Sobolev space of functions of one variable on

4

Q1 with inner product

‹wif ( xwi i

)

=

m

awii , f (x1, . . . , xi-1, ∑, xi+1, . . . , xq)

=
Sob,m =0

Q1

dawii (xi) dxi

df ( dxi

) dxi.

(9)

The proof given in Appendix B is using the idea of Yatchew and Bos (1997). In addition, we derive the exact form of the representor for Pseudo-Sobolev spaces Wpm() of both odd and even rank m.
In order to calculate the representor a  a0 of the function f  Hm [0, 1] (see (79)), we start with functions La and Ra defined in (96) and (97) where a  (0, 1). The existence and uniqueness of the coefficients k(a) of the representor is demonstrated in the proof of Theorem 1.2. The coefficients k(a) are obtained as a solution of a system linear equations corresponding to the boundary conditions (85)≠(89) of the differential equation (84).

Theorem 1.3 (Obtaining Coefficients k(a)). The coefficients k(a) of the representor a are the unique solution of the following 4m ◊ 4m system of linear equations:

m
k(a) k(m-j)(0) + (-1)j (km+j)(0)
k=0 k=
m
+ m+1+k(a) m(m+-1j+)k(0) + (-1)j (mm++1j+)k(0) = 0, j = 0, . . . , m - 1;
k=0 k=
m
2m+2+k(a) k(m-j)(1) + (-1)j (km+j)(1)
k=0 k=
m
+ 3m+3+k(a) m(m+-1j+)k(1) + (-1)j (mm++1j+)k(1) = 0, j = 0, . . . , m - 1;
k=0 k=
m
(k(a) - 2m+2+k(a)) (kj)(a)
k=0 k=
m
+ (m+1+k(a) - 3m+3+k(a)) (mj)+1+k(a) = 0, j = 0, . . . , 2m - 2;
k=0 k=
m
(k(a) - 2m+2+k(a)) (k2m-1)(a)
k=0 k=
m
+ (m+1+k(a) - 3m+3+k(a)) (m2m+1-+1k)(a) = (-1)m-1;
k=0 k=

(10) (11) (12) (13)

5

where and k are defined in (94a)≠(95d).



  := 

m 2

,

m+1 2

,

m even, m odd

(14)

The square system of the above system of 4m linear equations (10)≠(13) can be written in a more

illustrative way using matrix notation:



 

k(m-j) (0) +(-1)j k(m+j)(0)



 k(m-j)(1)
+(-1)j k(m+j) (1)

k(j) (a)

-k(j) (a)

(k2m-1) (a)

-(k2m-1) (a)

{j,k (a)}

 



0(a) ...
-1(a)
+1(a) ...
m(a)
m+1(a) ...
m+(a)
m+2+(a) ...
2m+1(a)
2m+2(a) ...
2m+1+(a)
2m+3+(a) ...
3m+2(a)
3m+3(a) ...
3m+2+(a)
3m+4+(a) ...



=

 

0 ...
0
0 ...
0
0 ...
0 (-1)m-1

 

j=
0
...
m-1
0
...
m-1
0
...
2m-2
2m-1

.

4m+3(a)

(15)

{k (a)}

6

Hence, the coefficients k(a) are the solution of:

(a) = (-1)m-1 {(a)}-1

.

∑,4m

(16)

yaxis 0.8 0.9 1.0 1.1 1.2

0.0 0.2 0.4 0.6 0.8 1.0 xaxis

yaxis -1 0 1 2 3

-4 -2

0 xaxis

2

4

Figure 1:

‹Representors in Pseudo-Sobolev space H4 [0, 1] for data points

=

(0.05, 0.27, 0.41, 0.53, 0.57, 0.75, 0.81, 0.83, 0.87, 0.9, 0.96)--dashed vertical lines. Zoomed view in the

upper picture--interval [0, 1], reduced view in the lower picture--interval [-4.5, +5.5].

Theorem 1.4 (Embedding). The embedding Hm(Qq)  Cm-1(Qq) is compact.

7

2 General Least Squares
Connection of features of L2-spaces and Cm-spaces can yield an interesting background for the nonparametric regression. L2-spaces are special types of Hilbert spaces that facilitate the calculation of least square projection. On the other hand, we regard Cm-spaces as one of the common classes of functions that we want to approximate the data with.
Definition 2.1 (General Single Equation Model). The weighted single equation model is

‹Yi = f ( i) + i, i = 1, . . . , n

(17)

with these assumptions:
‹i) i are q-dimensional non-stochastic design points (knots);

ii) i are random variables so that

∑ Ei = 0, i,

∑

 cov (i, j) = 

ij , i2

i = j, i = j;

iii)

f

 F , where F

is a family of functions in the Pseudo-Sobolev space Hm(Qq) from Rq

to R1, m >

q 2

,

F=

f  Hm(Qq) :

f

2 Sob,m



L

.

Our setting is concerned with random variables {Yi}ni=1, respectively {i}ni=1. It is common terminology to refer to this setting as the fixed design model, which is concerned with controlled, non-stochastic variables
‹{ i}ni=1.
From now on, we denote Hm  Hm(Qq), where Qq is the unit cube in Rq. We define the variance matrix  := (ij )in,,jn=1, where i2  ii.
Our regression problem can be characterized by one of these ways:

a)

‹1
min f Hm n

n

[Yi - f (

i)]2

i=1

s.t.

f

2 Sob,m



L,

(18)

b)

min
f Hm

‹1
n

n

[Yi - f ( i)]2 + 

f

2 Sob,m

i=1

.

(19)

The Sobolev norm bound L and the smoothing parameter (bandwidth parameter)  control the trade-off between the infidelity to the data and the roughness of the estimator.

8

Definition 2.2 (Penalizing Using General Least Squares). Optimizing using General Least Squares

is

‹ ‹min 1 [
f Hm n

- f ( )] -1 [

- f(

)] + 

f

2 Sob,m

(20)

‹ ‹ ‹where is an n ◊ 1 vector of q-dimensional vector data points 1, . . . , n,  is an n ◊ n positive definite ‹and symmetric matrix, is an n ◊ 1 vector of constants, f is a real function of a real value, f ( ) = ‹ ‹(f ( 1), . . . , f ( n)) and  > 0.

Definition 2.3 (Representor Matrix). Let ‹1 , . . . , ‹n be the representors for function evaluation at
‹ ‹ ‹1, . . . , n respectively, i.e. ‹i, f Sob,m = f ( i) for all f  Hm, i = 1, . . . , n. Let  be the n◊ n representor ‹ ‹matrix whose columns (and rows) equal the representors evaluated at 1, . . . , n; i.e.

‹ ‹i,j = ‹i , ‹j Sob,m = ‹i ( j ) = ‹j ( i).

(21)

Theorem 2.1 (Infinite to Finite). Let matrix and define

= (Y1, . . . , Yn),  an n ◊ n positive definite and symmetric

‹ ‹^2

=

min 1 [ f Hm n

- f ( )] -1 [

- f(

)] + 

f

2 Sob,m

,

   s2 = min 1 [ -  ] -1 [ -  ] +   Rn n

(22) (23)

 ‹ ‹where is an n ◊ 1 vector, f is declared in Definition 2.2 and  is the representor matrix at 1, . . . , n.
Then ^2 = s2. Furthermore, there exists a solution to (22) of the form

n
f = c^i‹i
i=1
where = (c^1, . . . , c^n) solves (23). The estimator f is unique a.e.

(24)

Theorem 2.1 transforms the infinite dimensional problem into a finite dimensional quadratic optimization problem. A similar theorem in Yatchew and Bos (1997) uses different penalization.

Corollary 2.2 (Form of the Regression Function Estimator). The regression function estimator from

9

Theorem 2.1 in one-dimensional case is:

 

n
c^i Lxi (x),
i=1
...
n

j

f (x) = 

c^iLxi (x) + c^iRxi (x),

i=j+1
...

i=1

n

c^iRxi (x),

i=1

0  x  x1, ...
xj < x  xj+1, j = 1, . . . , n - 1; ... xn < x  1,

(25)

where = (c^1, . . . , c^n) solves (23) and the functions Lxi(x) and Rxi(x) are defined in (79). ‹Remark 2.1. Corollary 2.2 can be easily extended for a q-dimensional vector variable if we recall how

the representor  is produced in the proof of Theorem 1.2. We apply (79) on the form of each factor

a of the product of representors  in (98). The only difference in (25) will be the number of cases.
‹We will obtain (n + 1)q decision conditions (vector has q components) instead of actual number n + 1

(0  x  x1, . . . , xj < x  xj+1, . . . , xn < x).

The form of the regression function estimator can be written alternatively:

n 2m
f (x) = cj exp  eik x I[xxj]k(xj ) cos  eik x + I[x>xj]2m+k(xj ) sin  eik x . (26)
j=1 k=1

Note that f is not estimated using goniometric splines neither kernel functions!
Lemma 2.3 (Symmetry of Representor Matrix). Representor Matrix is symmetric.
Theorem 2.4 (Positive Definiteness of Representor Matrix). Representor Matrix is positive definite.
In the linear model, the unknown coefficients are estimated using Least Squares. Gauss-Markov Theorem (Andel, 2002) says that the estimate of these coefficients is the best linear unbiased estimate and underlies so-called "normal equations". The analogy can be found in our model.
Theorem 2.5 (Normal Equation for ). Assume General Single Equation Model 2.1. Let  be a representor matrix. Then the vector of coeficients of the unique minimizer f from (24) is the unique solution with respect to = (c1, . . . , cn) of the equation system

-1 + n = -1

(27)

for response vector = (Y1, . . . , Yn).

10

Remark 2.2 (Hat Matrix). We know that

= f(‹) = 

(28)

and from previous Normal Equation for Theorem 2.5 we easily obtain the form of the projection "hat"

matrix

 :=  -1 + n -1 -1

(29)

such that

= .

(30)

If the variance matrix  has full rank, its inverse matrix has also full rank and it can be decomposed using its square root matrix  (formally proceeded by spectral decomposition).

-1 = .

(31)

Notice that this square root matrix  has full rank. According to the Infinite to Finite Theorem 2.1 and Lagrange Multiplier Theorem it can be easily seen
that there is a correspondence between Sobolev bound L and and smoothing parameter . It would be only a technical exercise to prove the 1≠1 mapping between these two parameters (Pesta, 2006).
Theorem 2.6 (1≠1 Mapping of Smoothing Parameters). Let L > 0,  is positive definite and symmetric matrix and

‹ ‹f  = arg min 1 [ f Hm n

- f ( )] -1 [

- f ( )]

s.t.

f

2 Sob,m



L

(32)

then there exists a unique   0 such that

‹ ‹f  = arg min 1 [ f Hm n

- f ( )] -1 [

- f(

)] + 

f

2 Sob,m

.

(33)

Hence, there exists a 1≠1 mapping Z : R+  R+ : L  .
Theorem 2.7 (Bijection Between the Smoothing Parameters). Let  > 0,  is positive definite and symmetric matrix and

‹ ‹f  = arg min 1 [ f Hm n

- f ( )] -1 [

- f(

)] + 

f

2 Sob,m

(34)

11

then there exists a unique L > 0 such that

‹ ‹f  = arg min 1 [ f Hm n

- f ( )] -1 [

- f ( )]

s.t.

f

2 Sob,m

=

L.

(35)

 If   < L, then we are talking about the interpolation not the approximation because
This is very unusual case for a real statistical situation and our problem, too.

=  .

Theorem 2.8 (Asymptotic Behavior). Suppose ~ :=  is n ◊ 1 vector of i.i.d. random variables. Then

‹ ‹ ‹ ‹1
n

f(

) - f(

)  -1

f(

) - f(

)

= OP

n-

2m 2m+q

,

n  .

(36)

2.1 Choice of the Smoothing Parameter
The smoothing parameter  corresponds to the diameter of the set of functions over which the estimation takes place. Heuristically, for large bounds ( smaller ), we obtain consistent but less efficient estimator. On the other hand, for smaller bounds, i.e., large , we obtain more efficient but inconsistent estimators.
Well-known selection method is minimization of the cross-validation criterion

› ‹ › ‹CV(L) = 1 - f ( )  -1 - f ( ) n

where f  = f-1, . . . , f-n is obtained by solving

(37)

› ‹f-i

=

arg

min
f Hm

n

1 -

1

n

[j,∑

- j,∑f (

)]2 + 

f

2 Sob,m

,

j=1

j=i

i = 1, . . . , n,

(38)

where  is the square root of the inverse of matrix  defined in (31). The idea of selection of the smoothing parameter by Cross-Validation is based on its ability to predict
outside the sample. We omit the i-th observation from the estimation when the i-th observation is being predicted. Then we use the minimum of the Cross-Validation function CV to estimate the smoothing parameter  (which correspodents to appropriate Sobolev bound L). Relationship between the data-fit and the smoothness of estimator is shown in Figure 2.
We can also use weighted version of cross-validation, called generalized cross-validation

› ‹ › ‹ - f ( ) -1 - f ( )

GCV(L) =

tr (I - )2

.

(39)

12

CV(L) 0.04 0.05 0.06 0.07 0.08 0.09 0.10

Dependent data

Smoothing parameter chi(L)

Independent data

-5 -4 -3 -2 log(chi(L))

Figure 2: Left--changing monotone curve in H2 depending upon smoothing parameter. Right--optimal value of smoothing parameter according to Cross-Validation.

Detailed information concerning a choice of the smoothing parameter  can be found in Eubank (1999). Cross-Validation is a commonly used Leave-One-Out method for choosing a smoothing parameter in the nonparametric regression. However, there are many different methods based on penalizing functions or plug-in selectors. Specific types of "smoothing choosers"--such as Akaike's Information Criterion, Finite Prediction Error, Shibata's model selector or Rice's bandwidth selector--can be found in Ha®rdle (1990).

3 Application to Option Prices

In the regression in Pseudo-Sobolev spaces we have demanded only smoothness constraint on the regres-

sion function f  F =

f  Hm(Qq) :

f

2 Sob,m



L

.

Now, the estimators should underlie additional

constraints. We focus on the imposition of additional constraint--such as isotonia--on the nonparametric

regression estimator.

We estimate f  F  F where F combines smoothness with further functional properties. We consider

properties such as monotonicity of particular derivatives of the function, i.e., monotonicity, convexity, etc.

The following discussion concerns only the one-dimensional case. From now on, we assume that q = 1.

Definition 3.1 (Derivative of Representor Matrix). Let x1 , . . . , xn be the representors for function evaluation at x1, . . . , xn, i.e. xi , f Sob,m = f (xi) for all f  Hm(Q1), i = 1, . . . , n. Let (k) be the k-th

13

derivative of n ◊ n representor matrix whose columns are equal to the k-th derivatives of the representors

evaluated at x1, . . . , xn; i.e.

i(,kj) = x(kj)(xi), i, j = 1, . . . , n.

(40)

It is very important that the k-th derivative of the representor matrix is defined in a "column" way. In spite of Theorem 2.3, derivative of representor matrix needn't to be a symmetric one.

Definition 3.2 (Estimate of the Derivative). Define the estimate of the regression function derivative as the derivative of the regression function estimate, i.e.

f (s) := f (s), s  N.

(41)

Theorem 3.1 (Consistency of Estimator). Suppose ~ :=  is n ◊ 1 vector of i.i.d. random variables,
the design points are equidistantly distributed on interval [a, b] such that a = x1 < . . . < xn = b and  is positive definite covariance matrix of  with its largest eigenvalue less or equal to a positive constant  > 0
∆for all n  . Then for s = 0, . . . , m - 2

sup f (s)(x) - f (s)(x) ---P- 0.

x[a,b]

n

(42)

Now we can show the relationship between the operator of derivative of the representor matrix and the isotonia, especially in the application to the Call and Put option properties.

3.1 Call and Put options

Let's have multiple Call and Put option prices for some strike prices. Suppose that the Call and the Put option prices are repeated observations at distinct fixed design points i, i = 1, . . . , , called the strike price knots. The Call option prices represents ni  N0 responses Yik for their strike prices xik  {1, . . . , } in each strike price knot i where k = 1, . . . , ni, xi1 = . . . = xini , i,


I[ni1] = Y ,
i=1 
niI[ni1] = n.
i=1

(43) (44)

14

Similarly the Put option prices represents mj  N0 responses Zjl for their strike prices xjl  {1, . . . , } in each strike price knot j where l = 1, . . . , nj, xj1 = . . . = xjmj , j,


I[mj 1] = Z ,
j=1 
mj I[mj1] = m.
j=1

(45) (46)

Let  be the connectivity n ◊ Y matrix for Call option strike prices such that

  1 ij :=  0

if xi = j, otherwise

(47)

for

i  { | 1    n & n  1} , j  { | 1     & n  1}

(48) (49)

and also let  be the connectivity m ◊ Z matrix for Put option strike prices such that

  1 ij :=  0

if xi = j, otherwise

for

(50)

i  { | 1    m & m  1} , j  { | 1     & m  1} .

(51) (52)

Similar matrix has been already defined in Yatchew and Ha®rdle (2005).
Definition 3.3 (Call and Put Option Model). Invoke the notation from the beginning of this section 3. The Call and Put option model is

Yik = f (xik ) + ik , Zjl = g(xjl ) + jl ,

k = 1, . . . , n, {i1, . . . , in}  {1, . . . , } , l = 1, . . . , m, {j1, . . . , jm}  {1, . . . , }

with these assumptions:

15

(53) (54)

i) {xi}i=1 are non-stochastic design points such that xi  {1, . . . , } , i;

ii) ik and jl are random variables so that

∑ Eik = 0, k,

∑ Ejl = 0, l, 

∑

 cov (ik , il ) = 


ik ,il , i2k

∑

 cov (jl , jk ) = 

jl,jk , j2l

k = l, k = l;
l = k, l = k;

∑ cov (ik , jl ) = ik,jl , k, l;

iii)

f, g  F , where F

is a family of functions in the Pseudo-Sobolev space Hp(Qq) from Rq

to R1, p >

q 2

,

F=

h  Hp(Qq) :

f

2 Sob,p



L

.

The second derivatives of functions f and g have to be the same SPD. Hence, Infinite to Finite Theorem 2.1 provides a key to determining how to handle multiple (repeated) observations for our set-up in option prices model 3.3.

Theorem 3.2 (Call and Put Option Optimizing). Invoke the assumptions from Call and Put Option Model 3.3. Define

 ^2 = min 
f Hp,gHp


 -   0





‹0   f ( )  ‹ g ( )

     

-1 

‹ -   0   f ( ) 

‹0 

g( )

(55)

+

f

2 Sob,p

+



g

2 Sob,p

subject to

‹-1  f  ( )  0, ‹0  g ( )  1,
‹f  ( )  0, ‹g ( )  0, ‹ ‹f  (  ) = g ( )

(56a) (56b) (56c) (56d) (56e)

16

and



s2

=

min RY , RZ




 -   0


0    0

 
0  

 



 -1 



    

 -   0    0   

0

0

(57)

 +   +  

subject to

-1  (1)  0,

(58a)

0  (1)  1,
(2)  0,

(58b) (58c)

(2)  0,
(2)  = (2) 

(58d) (58e)

where  > 0,  > 0,  is the (n + m) ◊ (n + m) positive definite and symmetric matrix,  is the connectivity
n ◊ Y matrix from (47),  is the connectivity m ◊ Z matrix from (50),  is the Y ◊ Y representor matrix at (x){ | n1},  is the Z ◊ Z representor matrix at (x){ | m1}, = (Y1, . . . , Yn), =
‹ ‹(Z1, . . . , Zm), f ( ) = (f (x)){ | n1}, g( ) = (g(x)){ | m1} and  :=  = ( | n  1 & m  1)
is the vector of indices in increasing order. Then ^2 = s2. Furthermore, there exists a solution to (55) with
respect to (56) of the form

f=

c^ixi ,

{i | ni1}

g=

d^j xj

{j | mj 1}

(59) (60)

where = (c^i)i{i | ni1} and = (d^j )j{j | mj1} solves (57), xi is the representor at xi for vector
(x){ | n1} and xj is the representor at xj for vector (x){ | m1}. The estimators f and g are unique a.e.
The structure of the (n + m) ◊ (n + m) covariance matrix  of the random errors (1, . . . , n, 1, . . . , m) will be investigated in Section 4. The minimization problem (57) under the constraints (58) can be implemented using GNU≠R statistical software with function pcls() in the library mgcv.

17

4 Covariance Structure
Let us denote the vector of the true SPD in the  distinct observed strike prices 1, . . . ,  as f (2) = (f (2)(1), . . . , f (2)()). Assume that the expected values of the option prices given in (1) and (2) can be approximated by a linear combination of this discretized version of the SPD, i.e., we assume a linear model

Yi = (xi)f (2) + i

(61)

for the Call option prices and

Zj = (xj )f (2) + j

(62)

for the Put option prices, i = 1, . . . , n, j = 1, . . . , m. We assume that the vectors of the coefficients (x) and (x) depend only on the strike price x and can be interpreted as rows of a design matrices X and X, respectively. In the following, the state price density f (2) may depend on the time of the observation and the symbol fi(2) = (fi(2)(1), . . . , fi(2)()) will denote the true value of the SPD at the time of the i-th trade, i = 1, . . . , n + m.

4.1 Constant SPD
Assuming that the random errors  = (1, . . . , n+m) in the linear model
     = X f (2) + ,
X

(63)

are independent and identically distributed, the model (63) for the i-th observation, corresponding to the strike price xi, can be written as

Yi = (xi)fi(2) + i fi(2) = f (2)

if the ik-th observation is a Call option price or

Zi = (xi)fi(2) + i fi(2) = f (2)

18

if the i-th observations is a Put option price. Here, the estimated parameter (SPD) does not change during the observation period (one day).
This simplified model has been estimated in Yatchew and Ha®rdle (2005) only for Call option prices.

4.2 Dependencies due to the time of the trade
Let us now assume that the observations are sorted according to the time of the trade ti  (0, 1) and denote by i = ti - ti-1 > 0 the time between the (i - 1)-st and the i-th trade.
The model described in Subsection 4.1 can now be generalized by moving the iid random errors i to the SPD fi(2) rather than to the observed call option price:
Yi = (xi)fi(2), fi(2) = fi(-2)1 + i1/2i.
Expressing all observations in terms of the parameter fn(2+)1, corresponding to the "end of the day", it follows that the covariance of any two observed call option prices depends only on the time of the trade and their strike prices:

Cov{Yi-u, Yi-v} = Cov((xi-u)fi(-2)u, (xi-v )fi(-2)v)

min(u,v)

= 2(xi-u)(xi-v )

i+1-m.

m=1

Similarly, we obtain the covariances between the observed Put option prices:

(64)

Cov{Zi-u, Zi-v} = Cov((xi-u)fi(-2)u, (xi-v )fi(-2)v(k))

min(u,v)

= 2(xi-u)(xi-v )

i+1-l .

l=1

and the covariance between the observed Put and Call option prices is:

(65)

Cov{Yi-u, Zi-v} = Cov(xi-u fi(-2)u, (xi-v)fi(-2)v(k))

min(u,v)

p-1

= 2

i+1-l


xi-u



(xi-v

).

l=1 k=2

(66)

Hence, the knowledge of the time of the trades allows us to estimate the covariance matrix of the observed option prices. Note that with this covariance structure we can estimate arbitrary future value of the SPD. It

19

is natural that more recent observations are more important for the construction of the estimator and that observations corresponding to the same strike price and taken at approximately same time will be highly correlated.
5 DAX Option Prices
In this section, the theory developed in the previous sections is applied on real data set containing intra day Call and Put DAX option prices in year 1995. The data set, Eurex Deutsche B®orse, was provided by the Financial and Economic Data Center (FEDC) at Humboldt-Universita®t zu Berlin in the framework of the SFB 649 Guest Researcher Program for Young Researchers.
In Figures 3 and 4, we present the analysis for the first two trading days in January 1995. On the first trading day, the time to expiry was T - t = 0.05 years, i.e., 18 days. Naturally, on the second trading day, the time to expiry was 17 days.
In both figures, the first two plots contain the fitted Put and Call option prices and the estimated SPD. Both smoothing parameters were chosen as 2 ◊ 10-5 leading to a reasonably smooth SPD estimate in the upper right plot in Figures 3 and 4. Smaller values of the smoothing parameters would lead to a more variable and less smooth SPD estimates that would be difficult to interpret.
The second two plots in Figures 3 and 4 show ordinary residual plots separately for the observed Put and Call option prices. The size of each plotting symbol denotes the number of residuals lying in the respective area. The shape of the plotting symbols corresponds to the time of the trade, circles occurred in the morning, squares around the noon and the stars in the afternoon. We observe a strong heteroscedasticity and strong dependencies due to the time of the trade.
In the last two plots in Figures 3 and 4, we plot the same residuals transformed by Mahalanobis transformation, i.e., multiplied by the inverse square root of their assumed covariance matrix, see Section 4.2. This transformation removes most of the dependencies caused by the time of the trade. However, some outlying observations have now appeared. For example, for the Call options on the second day, plotted in Figure 4, we can see a very large positive and a very large negative residual at the same strike price 2050.
The outlying observations can be explained if we have a closer look at the original data set. In Table 1, we show the Call option prices, times of the trades, and the transformed residuals for all trades with the strike price K = 2050. The two observations with larg residuals, 358.7 and -342.2, occurred at approximately the same time, the time difference between them is approximately 0.13 hours, i.e., approximately five minutes. Simultaneously, the price difference of these two observations is quite large. Hence, the large correlation of these two very different prices leads to the large (suspicious) residuals appearing in the residual plot.
20

0.004

100 150

0.003

0.002

50

0.001

0

1950

2000

2050

2100

2150

2200

Residuals (Call Options)

1950

2000

2050

2100

2150

2200

Residuals (Put Options)

price -4 -2 0 2 4 6

price -2 0 2 4

2000

2050

2100 strike

2150

2200

Transformed Residuals (Call Options)

1950

2000

2050 strike

2100

2150

Transformed Residuals (Put Options)

2200

50 100

100 200

price

price

0

0

-100 -50

-100

2000

2050

2100 strike

2150

2200

1950

2000

2050 strike

2100

2150

2200

Figure 3: Estimates and residual plots on the 1st trading day in 1995 (January 2nd). The first plot shows fitted Call and Put option prices, the estimated SPD is plotted in the second plot. The remaining four graphics contain respectively residual plots for Call and Put option prices on the left and right hand side. The residuals plotted in the last two plots were corrected by the inverse square root of the covariance matrix.

21

0.004

0.003

20 40 60 80 100 120 140

0.002

0.001

0

1950

2000

2050

2100

2150

2200

Residuals (Call Options)

1950

2000

2050

2100

2150

2200

Residuals (Put Options)

price -4 -2 0 2 4 6 8 10

10 15

price

5

0

1950

2000

2050 strike

2100

2150

Transformed Residuals (Call Options)

2200

1950

2000

2050 strike

2100

2150

Transformed Residuals (Put Options)

2200

200 400

200 400

0

price

0

price

-200

-200

-400

-400

1950

2000

2050 strike

2100

2150

2200

1950

2000

2050 strike

2100

2150

2200

Figure 4: Estimates and residual plots on 2nd trading day in 1995 (January 3rd). The first plot shows fitted Call and Put option prices, the estimated SPD is plotted in the second plot. The remaining four graphics contain respectively residual plots for Call and Put option prices on the left and right hand side. The residuals plotted in the last two plots were corrected by the inverse square root of the covariance matrix.

22

Call price (K = 2050) 50.62296 51.12417 50.62296 50.02150 48.11687 46.61322 47.31492 48.11687 49.01906 49.01906 50.32223 46.61322 47.61565 45.00932 48.11687 45.10957 48.11687 48.11687 48.11687 47.51541 44.10713 42.10226 42.10226 40.99958 41.60104 42.10226 42.10226 40.69885 41.60104 42.60348 42.10226 41.60104 42.10226

time (in hours) 9.690 9.702 9.785 9.807 9.826 9.864
10.121 10.171 10.306 10.361 10.534 10.666 10.672 11.187 11.690 12.100 12.647 12.766 13.170 14.205 14.791 15.137 15.138 15.232 15.250 15.283 15.288 15.638 15.658 15.711 15.715 15.796 15.914

transformed residual
337.4 73.2 33.8
6.5 -10.3 -11.5 -6.9
26.5 24.3 26.3 358.7 -342.2 32.8 -62.2 28.2 -72.6 53.9 13.3 28.3 11.2 -4.8 -34.1 -93.4 -32.4 -14.2 -2.4 -87.6 -31.2 -48.9 -46.6
6.7 -39.2 -49.5

Table 1: Subset of observed prices of Call options on 2nd trading day in 1995 for strike price K = 2050, time of the trade in hours and residuals transformed by the Mahalanobis transformation. The fitted value for the strike price K = 2050 is f^(2)(2050) = 42.37. This value can be interpreted as an estimate corresponding to 16:00 o'clock.

23

6e-04

4e-04

0 200 400 600 800 1000 1200

2e-04

0e+00

4000

4500

5000

5500

6000

Residuals (Call Options)

4000

4500

5000

5500

6000

Residuals (Put Options)

10 20 30

80 100

price

price 40 60

-20 -10 0

20

0

4500

5000

strike

5500

6000

Transformed Residuals (Call Options)

4000

4500 strike

5000

5500

Transformed Residuals (Put Options)

3000

3000

2000

2000

1000

1000

price 0

price

0

-2000 -1000

-2000 -1000

4500

5000

strike

5500

6000

4000

4500 strike

5000

5500

Figure 5: Estimates and residual plots on the 1st trading day in 2002 (January 2nd). The first plot shows fitted Call and Put option prices, the estimated SPD is plotted in the second plot. The remaining four graphics contain respectively residual plots for Call and Put option prices on the left and right hand side. The residuals plotted in the last two plots were corrected by the inverse square root of the covariance matrix.

24

An example of a more recent data set is plotted in Figure 5. In year 2002, the range of the traded strike prices was much wider than in 1995. The estimated SPD is plotted in the upper right plot. The estimate could be described as a unimodal probability density function with the right tail cut off. It seems that, especially on the right hand side, the traded strike prices do not cover the support of the SPD entirely.
The residual plots in Figure 5 look very similar to the residual plots in Figures 3 and 4. The residual analysis suggests that the simple model for the covariance structure presented in Section 4 is more appropriate for this estimation problem than the unrealistic iid assumptions. In practice, the traded strike prices do not cover the entire support of the SPD. Hence, our estimators recover only the central part of the SPD in Figures 3 and 4 or the left hand part of the SPD in Figure 5. Unfortunately, this implies that we cannot impose any conditions on the expected value of the SPD without additional distributional assumptions.
6 Conclusion
The mathematical foundation of the constrained regression in pseudo-Sobolev spaces is explained in Section 1, see also Yatchew and Bos (1997); Yatchew and Ha®rdle (2005). In Section 2, we generalize the method to dependent observations and introduce the constrained general regression in pseudo-Sobolev spaces. The application of the method to the observed option prices is developed in Section 3. The resulting algorithm, using the covariance structure given in Section 4, see also Ha®rdle and Hla¥vka (2006), is applied on a real data set in Section 5.
The main achievement of this paper is the simultaneous estimation of the SPD from both Put and Call option prices and the incorporation of the covariance structure in the nonparametric estimator that has been previously considered in Yatchew and Ha®rdle (2005). The constrained general regression in pseudo-Sobolev spaces will certainly be very useful in various practical problems.
25

A Used definitions and theorems

A.1 Definitions
Definition A.1 (Domain). A connected Lebesgue-measurable (open or closed) bounded subset  of an Euclidean space Rq with non-empty interior is called a domain.

Definition A.2 (Lebesgue Space). Consider a measurable real-valued function on a given Lebesgue-

‹ ‹measurable domain. Simply f :   R,   Mq(q). The Lebesgue integral of function f is  f ( )dq( ) 

‹ ‹ f ( )d . Let



‹ ‹f

 Lp() := 

1/p
 f p( )d inf C  0 : |f |  C a.e.

for 1  p < , for p = .

(67)

We define a Lebesgue space by Lp() := f : f Lp() <  , 1  p  .
Definition A.3 (Spaces of Continuously Differentiable Functions). Let m  N0. We define Cm() space of m-times continuously differentiable scalar functions upon bounded domain . Simply

Cm() := f :   R Df  C0(), ||  m ,

(68)

where C0()  f :   R f continuous on  and || = maxi=1,...,q |i|.

Definition A.4 (General Definition of Sobolev Space). A Sobolev space can be also defined in more

general way:

Wpm() := {f  Lp()|Dwf  Lp(), ||  m}

(69)

where Dw denotes an operator of the weak derivative (Maz'ja, 1985).

A.2 Theorems
Theorem A.1 (Lp Complete). Lp(), 1  p   is a Banach space.
Theorem A.2 (Riesz Representation Theorem). For every continuous linear functional f on a Hilbert space H, there is a unique u  H such that f (x) = x, u for all x  H.
Theorem A.3 (Kolmogorov-Tihomirov). Let F be a compact non-empty subset of a metric space. Then for all  > 0 exists A > 0 and 0 <  < 1 such that metric entropy

H(; F ) < A-2 .

(70)

26

Theorem A.4 (Schur Decomposition). Eigenvalues 1, . . . , n of symmetric matrix An◊n are always real. Without losing of generality suppose that 1  . . .  n. Let Wn◊n = diag {1, . . . , n}. Then there exists an orthogonal matrix Un◊n such that

An◊n = Un◊nWn◊nUn◊n, In◊n = Un◊nUn◊n = Un◊nUn◊n.

(71) (72)

Theorem A.5 (Cauchy-Schwartz Inequality). If f  L2() and f  L2(), then f g  L1() and

‹ ‹ ‹|f ( )g( )| d  f L2() g L2() .


(73)

Lemma A.6. Suppose (fn)n=1 are non-negative Lipschitz functions on interval [a, b] with a constant T > 0
∆for all n  . If

fn

---L1-
n

0

(74)

then

fn

,[a,b]

:=

sup |fn(x)|
x[a,b]

----
n

0.

(75)

27

B Proofs

Correctness of Definition 1.1. Let f, g  Cm()  Lp(), the triangle inequality for the p-norms on Lp()

and lp ({ : ||  m}) implies

 
f + g p,Sob,m = 
|| m
 

|| m

1/p

 

1/p

Df + Dg

p
Lp()



Df

p Lp()

+

Dg

p Lp ()



|| m

1/p

 

1/p

Df

p
Lp () 

+

Dg

p
Lp()

= f p,Sob,m + g p,Sob,m .

||m

(76)

Proof of Theorem 1.1. It is straightforward to verify that Hm() is a normed linear space. It is also complete by construction, so it is a Banach space. The inner product ∑, ∑ Sob,m has been defined on Hm(), so it is a Hilbert space.

Proof of Theorem 1.2. We divide the proof into two steps. i) Construction of a representor a( a0)
For simplicity, let's set Q1  [0, 1]. We know that for functions of one variable we have

m

g, h Sob,m =

g (k) (x)h(k) (x)dx,

k=0 Q1

so all we need to do is to construct a representor

(77)

a  Hm [0, 1] s.t. a, f Sob,m = f (a)

(78)

for all f  Hm [0, 1]. It suffices to demonstrate the result for all f  C2m because of the denseness of C2m

(see Remark 1.1), hence assume that f  C2m. This representor will be of the form:



a(x)

 = 

La(x) Ra(x)

0  x  a, a  x  1,

(79)

where La(x)  C2m [0, a] and Ra(x)  C2m [a, 1]. As a  Hm [0, 1], it suffices that La(k)(a) = Ra(k)(a), 0  k  m - 1. We get:

am

1m

f (a) = a, f Sob,m =

L(ak)(x)f (k)(x)dx +

Ra(k)(x)f (k)(x)dx.

0 k=0

a k=0

(80)

28

Integrating by parts, we have:



ma

m k-1

aa



k=0

0

La(k)(x)f (k)(x)dx =

 (-1)j La(k+j)(x)f (k-j-1)(x) + (-1)k

k=0 j=0

0

0

L(a2k)(x)f (x)dx

m k-1

a am

(81)

= (-1)j La(k+j)(x)f (k-j-1)(x) +

(-1)k L(a2k)(x) f (x)dx.

k=0 j=0

0 0 k=0

Let's try to substitute i = k - j - 1 and rewrite it:

ma

m k-1

a am

L(ak)(x)f (k)(x)dx =

(-1)k-i-1 La(2k-i-1)(x)f (i)(x) +

(-1)k L(a2k)(x) f (x)dx

k=0 0

k=1 i=0

0 0 k=0

m-1 m

a am

=

(-1)k-i-1 La(2k-i-1)(x)f (i)(x) +

(-1)k L(a2k)(x) f (x)dx

i=0 k=i+1

0 0 k=0

m-1
= f (i)(a)
i=0

m
(-1)k-i-1 La(2k-i-1)(a)
k=i+1

m-1
- f (i)(0)
i=0

m
(-1)k-i-1 La(2k-i-1)(0)
k=i+1

am
+ (-1)k L(a2k)(x) f (x)dx.
0 k=0

(82)

Similarly:

m1

m-1

Ra(k)(x)f (k)(x)dx =

f (i)(1)

k=0 a

i=0

m
(-1)k-i-1 Ra(2k-i-1)(1)
k=i+1

m-1
- f (i)(a)
i=0

m
(-1)k-i-1 Ra(2k-i-1)(a)
k=i+1

1m
+ (-1)k Ra(2k)(x) f (x)dx.
a k=0

(83)

These two results hold for all f (x)  Cm [0, 1]. Thus we require that both La and Ra are the solutions of the constant coefficient differential equation

m
(-1)k (2k)(x) = 0.
k=0

(84)

Boundary conditions are obtained by equality of the functional values of L(ai)(x) and Ra(i)(x) at the point a

29

and the coefficient comparison1 of f (i)(0), f (i)(1) and f (i)(a):

ra  Hm [0, 1]  L(ai)(a) = Ra(i)(a) . . . 0  i  m - 1,

(85)

f (i)(0)  0 

m
(-1)k-i-1 La(2k-i-1)(0) = 0 . . . 0  i  m - 1,

(86)

k=i+1

f (i)(1)  0 

m
(-1)k-i-1 Ra(2k-i-1)(1) = 0 . . . 0  i  m - 1,

(87)

k=i+1

f (i)(a)  0 

m
(-1)k-i-1 La(2k-i-1)(a) - Ra(2k-i-1)(a) = 0 . . . 1  i  m - 1, (88)

k=i+1

m

f (a)  1 

(-1)k-1 La(2k-1)(a) - Ra(2k-1)(a) = 1;

(89)

k=1

together m+m+m+(m-1)+1 = 4m boundary conditions. To obtain the general solution of this differential equation we need to find the roots of its characteristic polynomial

Hence it follows

m
Pm() = (-1)k2k.
k=0
(1 + 2)Pm() = 1 + (-1)m2m+2,  = ±i.

(90) (91)

Solving the last equation (91), we get characteristic roots

k = eik ,

(92)

where





(2k+1) 2m+2

k   k
m+1

m even, m odd,

k  {0, 1, . . . , 2m + 1} \

m 2

,

3m+2 2

,

k  {0, 1, . . . , 2m + 1} \

m+1 2

,

3m+3 2

.

(93)

We have (2m + 2) - 2 = 2m different complex roots together, but each has a pair that is conjugate with it.

Thus if m is even then we have m complex conjugate roots with multiplicity one. We also have 2m base

elements alike complex roots:

m even

k(x) = exp

(k) x cos

(k) x , k  {0, 1, . . . , m} \

m 2

;

m+1+k(x) = exp

(k) x sin

(k) x , k  {0, 1, . . . , m} \

m 2

.

1   denotes that  has a coefficient  in a specific equation.

(94a) (94b)

30

On the other hand if m is odd then we have 2m - 2 different complex roots together (each has a pair that is

conjugate with it) and two real roots. Two real roots are ±1 and m - 1 complex conjugate roots have the

multiplicity one. We also have 2(m - 1) + 2 = 2m base elements alike all roots, too. So these base elements

are:

m odd

0(x) = exp {x} ;

(95a)

k(x) = exp

(k) x cos

(k) x , k  {1, 2, . . . , m} \

m+1 2

;

(95b)

m+1(x) = exp {-x} ;

(95c)

m+1+k(x) = exp

(k) x sin

(k) x , k  {1, 2, . . . , m} \

m+1 2

.

(95d)

These vectors generate a subspace of Cm [0, 1] that is the space of solutions of the differential equation (84). In this case, the general solution is given by the linear combination:

La(x) Ra(x)

 

m

= k(a) exp (k) x cos

kk==0m2 m

+ m+1+k(a) exp (k) x

k=0

k=

m 2

(k) x sin (k) x ,

m even;



m

= 0(a) exp {x} +

k(a) exp (k) x

k=1

k=

m+1 2

m

+ m+1(a) exp {-x} +

m+1+k(a) exp

k=1

cos (k) x (k) x sin

(k) x ,

m odd;

(96)

 

k=

m+1 2

m

= 2m+2+k(a) exp (k)

kk==0m2 m

+ 3m+3+k(a) exp (k)

k=0

k=

m 2

x x

cos sin

(k) x (k) x ,

m even;



m

= 2m+2(a) exp {x} +

k=1

k=

m+1 2

m

+ 3m+3(a) exp {-x} +

k=1

2m+2+k(a) exp 3m+3+k(a) exp

(k) x cos (k) x sin

(k) x (k) x

,

(97) m odd;

k=

m+1 2

where the coefficients k(a) are arbitrary constants that satisfy the boundary conditions (85)≠(89). It can

31

be easily seen that we have obtained 4(m + 1) - 4 = 4m coefficients k(a), because the first index of k(a) is 0 and the last one is 4m + 3. Thus we have 4m boundary conditions and 4m unknowns of ks that lead us to the square 4m ◊ 4m system of the linear equations. Does a exist and is it unique? To show this, it suffices to prove that the only solution of the associated homogeneous system of linear equations is the zero vector. Suppose La(x) and Ra(x) are functions corresponding to the solution of the homogeneous system, because in linear system of equations (85)≠(89) the right side has all zeros--coefficient of f (a) in the last boundary condition is 0 instead of 1. Then, by the exactly the same integration by parts, it follows that a, f Sob,m = 0 for all f  Cm [0, 1]. Hence a(x), La(x) and Ra(x) are zero almost everywhere and thus by the linear independence of base elements k(x), so we have unique coefficients k(a).

ii) Producing a representor € Let's produce the representor € by setting

q
‹ ‹€( ) = awii (xi) for all  Qq,
i=1

(98)

where awii (xi) is the representor at ai in Hm Q1 . We know that Cm is dense in Hm, so it is sufficient to show the result for f  Cm(Qq). For simplicity let's suppose Qq  [0, 1]q. After rewriting the inner product
and using Fubini theorem we have

€, f Sob,m =

q
awii , f
i=1

‹ ‹=
Sob,m ||m

Qq

1 aw11 (x1) x1 1

∑∑∑



q awqq (xq xq q

)

D

f

(

)d

‹ ‹=
i1,...,iq =0,...,m

Qq

i1 aw11 (x1)  x1i1

∑∑∑



iq

awqq (xq  xiqq

)

i1,...,iq f ( ) xi11 . . . xqiq

d

‹m
=
i1 =0

1 i1 aw11 (x1) 0 x1i1

∑∑∑

m iq =0

1 0



iq

awqq (xq  xqiq

)

.

i1,...,iq f ( ) xi11 . . . xiqq

dxq

. . . dx1.

(99)

According to Definition 1.3 and notation in (7) we can rewrite the centermost bracket

‹m
iq =0

1 0



iq

awqq (xq  xiqq

)

.

i1,...,iq f ( ) xi11 . . . xiqq

dxq

=

awqq , D(i1,...,iq-1)f (x1, . . . , xi-1, ∑) Sob,m

‹= D(i1,...,iq-1,wq)f ( -q , aq).

(100)

Chain proceeding in this way we obtain the value for the whole expression to be equal to D€f ( ).

Proof of Theorem 1.3. Existence and uniqueness of coefficients k(a) has already been proved in the proof of Theorem 1.2.

32

Let's define



 L(al)(0),

(al,)I

:= 

Ra(l)(1), L(al)(a) - Ra(a)(a),

for I = L; for I = R; for I = D.

From (86)≠(89) we easily see

m
(-1)k-i-1a(2,Ik-i-1) = 0, 0  i  m - 1, I  {L, R, D} , [i, I] = [0, D] ;
k=i+1
m
(-1)k-1a(2,Dk-1) = 1.
k=1

If m = 1 it directly follows from (102)≠(103):

(101)
(102) (103)

a(1,I) = 0, I  {L, R} , (a1,D) = 1.

(104) (105)

If m = 2 it also directly follows from (102)≠(103):

(a2,I) (a1,I) - (a3,I) (a1,D) - a(3,D)

= 0, = 0, = 1.

I , I  {L, R} ,

(106) (107) (108)

Suppose m  3. We would like to prove this important step in our proof:

a(m,I-j) + (-1)j (am,I+j) = 0, j = 0, . . . , m - 2, I, a(1,I) + (-1)m-1(a2,Im-1) = 0, I  {L, R} , a(1,D) + (-1)m-1(a2,Dm-1) = 1,

(109) (110) (111)

where j := m - i - 1. For j = 0 is i = m - 1 and from (102)≠(103) we have straightforwardly

(am,I) = 0, I,

(112)

which is correct according to (109). Consider j = 1 and thus i = m - 2. In the same way we get a

33

correspondent result to (109)

a(m,I-1) - (am,I+1) = 0, I.

For j = 2 and thus i = m - 3 we have

(113)

a(m,I-2) - (am,I) + (am,I+2) = 0, I,

(114)

so it forces us to apply (112) and for j = 3 and thus i = m - 4 we have

a(m,I-3) - a(m,I-1) + (am,I+1) - (am,I+3) = 0, I,

(115)

so we can apply (113). We could continue in this way finite times (formally we can proceed this by something like a finite double induction). We finish when j = m - 1. The last step ensures the correctness of (110) in case I  {L, R}, eventually (111) in case I = D instead of (109). To finish this proof all we need to do is not to forget to think of (85). From (85) we obtain

(aj,)D = 0, j  {0, . . . , m - 1} .

(116)

According to (109) for I = D and (111) we further see:

(aj,)D = 0, j  {m + 1, . . . , 2m - 2} ; (a2,Dm-1) = (-1)m-1.

(117) (118)

Alltogether we have obtained these 4m linear equations

a(m,L-j) + (-1)j (am,L+j) = 0, j = 0, . . . , m - 1, a(m,R-j) + (-1)j (am,R+j) = 0, j = 0, . . . , m - 1,
(aj,)D = 0, j = 0, . . . , 2m - 2, (a2,Dm-1) = (-1)m-1,

(119) (120) (121) (122)

which after rewriting them using (101), (96)≠(97) and (94a)≠(95d) bring us to a close. Proof of Theorem 1.4. See Yatchew and Bos (1997).

34

Proof of Theorem 2.1. Let M = span {‹i : i = 1, . . . , n} and its orthogonal complement

M  = h  Hm : ‹i , h Sob,m = 0, i = 1, . . . , n .

(123)

Representors exist by Theorem 1.2 and we can write the Pseudo-Sobolev space as a direct sum of its
orthogonal subspaces, i.e. Hm = M  M  since Hm is a Hilbert space. Functions h  M  take on the
‹ ‹value zero at 1, . . . , n. Each f  Hm can be written in form

n
f = cj‹j + h,
j=1

h  M.

(124)

Then

‹ ‹[

- f ( )] -1 [

- f(

)] + 

f

2 Sob,m

 

n



n


n

2

=  ∑ - ‹∑ , cj xj + h

 -1  ∑ - ‹∑ , cj xj + h

+

cj ‹j + h


n
= ∑-

j=1

Sob,m

 

‹∑ , cj xj


Sob,m

-1 

n
∑-

j=1

Sob,m



n

‹∑ , cj xj


Sob,m

+



cj ‹j

j=1 2

+

Sob,m

h

2 Sob,m

j=1

n

j=1

 

n

j=1


Sob,m

= ∑-

cj

‹∑ , xj


Sob,m

-1 

∑-

cj

‹∑ , xj


Sob,m

j=1

j=1

+
 =

nn
cj ‹j , cj ‹j

+

h

2 Sob,m

j=1 n

j=1


Sob,m


n


n

∑ - ∑,jcj  -1  ∑ - ∑,jcj  + 

n
cj

j=1

j=1

j=1 k=1

‹j , ‹k

Sob,m ck + 

h

2 Sob,m

   = [

-  ] -1 [

-  ] +  

+

h

2 Sob,m

(125)

where for an arbitrary g  Hm


‹∑ , g Sob,m = x1 , g Sob,m , . . . , xn , g Sob,m .

(126)

Hence, there exists a function f  minimizing the infinite dimensional optimizing problem that is a linear

 combination of the representors. We note also that

f

2 Sob,m

=

 .

Uniqueness is clear, since ‹i are the base elements of M , and adding a function that is orthogonal to

the spaces spanned by the representors will increase the norm.

35

Proof of Corollary 2.2. Trivial. It can be directly seen from the definition of form (79) of the representor and from (24).
Proof of Lemma 2.3. Trivial. The representor matrix is symmetric by Definition 2.3, because

i,j = ‹i , ‹j Sob,m = ‹j , ‹i Sob,m = j,i,

(127)

i.e.  = .
Proof of Theorem 2.4. We proceed this proof only for one dimensional variable x. Extention into the multi-
variable case is clearly simple (see Remark 2.1). For an arbitrary  Rn we obtain

 =
=

ci ij cj =

ij

i

cixi , cj xj
ij

ci xi , xj Sob,m cj =
ji
2

j

=
Sob,m

cixi

 0.

i Sob,m

cixi , cj xj Sob,m

 Hence  = 0 iff i cixi = 0 a.e. According to (96)≠(97), (94a)≠(95d) and (16) we have2

xi (x) = (xi)(x) = (-1)m-1

{(xi )}-1


(x)
∑,4m

(128) (129)

where (x) is vector which elements are linear independent base elements of space of the differential equation's (84) solutions, i.e. k(x) (see (94a)≠(95d)). Thus linear independence of k(x) it follows that

cixi
i

=

(-1)m-1

ci {(xi)}-1  

i ∑,4m

= (-1)m-1

ci {(xi)}-1

k = 0 a.e.

ik

4m,k





m 2

,

3m+2 2

k

= 0 a.e.

k  {0, 1, . . . , 2m + 1} \ 

m+1 2

,

3m+3 2



m even, m odd;

(130) (131)

xi = 0 a.e. i = 1, . . . , n.

2If x > xi then (xi) = (0, . . . , -1, +1, . . . , m+, m+2+, . . . , 2m+1) (xi) else

(2m+2, . . . , 2m+1+, 2m+3+, . . . , 3m+2+, 3m+4+, . . . , 4m+3) (xi).

Similarly with elements

h{(xi)}-1i .
∑,4m

(132)
(xi) = of vector

36

And xi = 0 a.e. is a zero element of the space Hm. Proof of Theorem 2.5. According to the Theorem 2.1, we want to find the minimum of function

    L( ) := 1 [ -  ] -1 [ -  ] +   . n
 Therefore the first partial derivatives of L( ) have to be equal zero at the minimum point :

(133)

If -1 =: (ij )in,,jn=1, we have

 L( ) =! 0, i = 1, . . . , n.
ci

(134)

     nL( ) = -1 - 2 -1 + -1 + n 

nn

nnn

nnnn

nn

= YrrsYs - 2

Yrrsstct +

crrssttucu + n

cr rs cs

r=1 s=1

r=1 s=1 t=1

r=1 s=1 t=1 u=1

r=1 s=1

(135)

hence

nn

nnn

nn

n

0 =! -2

Yrrssi + 2

crrsstti + 2

ciisstti + 2n crri + 2nciii

r=1 s=1

r=1 s=1 t=1 r=i

r=1 s=1

r=1 r=i

 = -2 -1∑,i + 2 -1∑,i + 2n ∑,i, i = 1, . . . , n.

(136)

Then we obtain our system of "normal" equations

 -1∑,i + n∑,i = -1∑,i, i = 1, . . . , n.

(137)

Proof of Theorem 2.6. The solution of (32) always exists and is unique according to the proof of Theorem 2.1.
From the same proof of Theorem 2.1 follows that finding f --optimizing (32)--is the same as searching
optimal  such that

 ›  ›   

=

arg

mRinn

1 n

[

-  ] -1 [

- ]

s.t.   L

(138)

and again from the proof of Theorem 2.1 the existence and the uniqueness of  is guaranteed. Let's fix L.

37

   If   = L, we can simply apply Lagrange Multiplier Theorem on our bond condition  = L using

the Lagrange function

 ›  ›   J ( , ) = 1 [ -  ] -1 [ -  ] +   - L n

(139)

and it provides us a unique Lagrange multiplier . We do not care about -L because it does not depend
on .
Quadratic form J (∑, ) have to be positive definite according Lagrange Multiplier Theorem (we are
minimizing J ). That implies  > 0.
 If   < L, we just set  = 0 and we are done.

Proof of Theorem 2.7. Trivial. Just apply Lagrange Multiplier Theorem.
Proof of Theorem 2.8. Let's have fixed  > 0. Hence we have obtained unique f and also according to Theorem 2.1. Theorems 2.1 and 2.7 say that there esists a unique L > 0 such that is also a unique solution
of optimizing problem

    = arg min 1 [ -  ] -1 [ -  ] s.t.  = L. Rn n

(140)

Let's define

‹ ‹f( ) := f( ),
:=  ,
 := arg min 1 eRn n

-   -1

  - 

s.t. -1-1-1  L.

(141) (142) (143)

We can easily find out that

 = -1

(144)

and hence

f(‹) = .

(145)

Finally, there must exists L > 0 such that

    = arg min 1 -   -1 - 

s.t.  = L

eRn n

(146)

38

and hence this exactly same have to be a unique solution of optimizing problem

    = arg min 1 -   -1 - 

s.t.   L

eRn n

 because  is positive definite matrix (  is the volume of n-dimensional ellipsoid).

Now we think of model

‹Yi = f ( i) + i, i  i.i.d., i = 1, . . . , n

(147) (148)

with least-squares estimator f . Using Kolmogorov-Tihomirov Theorem A.3 it can be shown that exists

A > 0 such that for  > 0, we have log N (; F ) < A-q/m. Consequently applying Lemma 3.5 from de Geer

(1990), we obtain that there exist positive constants C0, K0 such that for all K > K0



P 

g

sup

2 Sob,m

Le

 n
1

-

2 n

n

n i=1



n i=1

i

f (xi) - g(xi)

2

1 2

-

q 4m

 KA1/2  exp

f (xi) - g(xi)

-C0 K 2

.

(149)

Since f  F =

g  Hm(Qq) :

g

2 Sob,m



L

and f minimizes the sum of squared residuals over g  F ,

1n n
i=1

1n n

2
Yi - f (xi)



1n n

2
Yi - g(xi) ,

gF

i=1 i=1

f (xi) - f (xi)

2
+ i



1n n

i=1

2
f (xi) - g(xi) + i ,

gF

 realize that f  F

1n n

2
f (xi) - f (xi)



-2 n

n

i

f (xi) - f (xi)

.

i=1 i=1

(150) (151)

Now combine (149) and (151) to obtain the result that K > K0

Thus

1n P
n
i=1

2
f (xi) - f (xi) 

K2A n

2m 2m+q

 exp -C0K2 .

(152)

‹ ‹ ‹ ‹1 f ( ) - f ( )  -1 f ( ) - f ( ) = 1 n
nn

2

f (xi) - f (xi)

= OP

n-

2m 2m+q

,

n  .

i=1

(153)

39

Proof of Lemma A.6. Reduction ad proof. Suppose that
∆ > 0 n0  n  n0 x  [a, b] fn(x)  .

(154)

Then according to Lipschitz property of each fn  0 we have for fixed , n0, n and x  [a, b] (drawing a picture could be helpful)

b

fn L1[a,b] = fn(t)dt
a

 min

fn(x) 2

(x

-

a)

+

fn(x) 2

(b

-

x),

fn(x) 2

(x

-

a)

+

fn(x) 2

fn(x) , T

fn(x) 2

fn(x) T

+

fn(x) 2

(b

-

x),

fn(x) 2

fn(x) T

+

fn(x) 2

fn(x) T

 min

 2

(b

-

a),

 2

(x

-

a)

+

2 2 2T , 2T

+

 2

(b

-

x),

2 T

=: K > 0.

(155)

But K is a positive constant which does not depend on n. Hence this is an absurdum because it should hold (according to the assumption of this lemma)

∆ > 0 n1 

n  n1 fn L1[a,b] < .

(156)

Proof of Theorem 3.1. We divide the proof into two steps. i) s = 0
The covariance matrix  is symmetric and positive definite with equibounded eigenvalues for all n. Hence it can be decomposed using Schur decomposition (Theorem A.4)

 = 

(157)

where  is orthogonal,  is diagonal (with eigenvalues on this diagonal) such that

Hence

0 < ii   i = 1, . . . , n, n. -1 = diag 1-1, . . . , -n 1 .

(158) (159)

40

Then

‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹1 f ( ) - f ( )  -1 f ( ) - f ( )  1 f ( ) - f ( )  -1I f ( ) - f ( ) = 1 n

nn

n

f^(xi) - f (xi) 2

i=1

(160)

Let's define hn := f^ - f . We know

f^

2 Sob,m



L

for

all

n

and

f

2 Sob,m

 L.

For every function

t  Hm[a, b] with

t

2 Sob,m



L

holds

 t L2[a,b]  t Sob,1  t Sob,m  L.

(161)

Then t has equibounded derivative and hence there exists a Lipschitz constant T > 0 such that

|t() - t()| < T | - | , ,   [a, b].

(162)

We easily see

|hn() - | -

hn ( )| |

=

f^() - f () - f^() - f () | - |



f^() - f () - f^() - f () | - |



f^() - f^() + f () - f () | - |

< 2T,

,   [a, b].

(163)

Since hn is T -Lipschitz function for all n and

hn L2[a,b] =

f^ - f L2[a,b] 

f^ - f Sob,1 

f^ - f Sob,m 

f^ Sob,m +

 f Sob,m  2 L,

n,

(164)

we obtain that hn is equibounded for all n with a positive constant M such that

hn ,[a,b]  M > 0, n.

(165)

Hence hn2 is also a Lipschitz function for all n, because for ,   [a, b]

h2n() - h2n() | - |

=

|hn() |

- -

hn ( )| |

[hn()

+

hn ( )]



T

◊

2

hn

,[a,b] = 2M T =: U > 0,

n.

(166)

Since h2n is U -Lipschitz function for all n and design points (xi)ni=1 are equidistantly distributed on [a, b],

41

we can write (drawing a picture could be helpful)

b a

h2n(u)du



n-1 i=1

xi+1 - 2

xi

hn2 (xi) + hn2 (xi) + U (xi+1 - xi)

1 n

n

hn2 (xi)

+

U (b - 2n

a) .

i=1



1 2n

n-1
2 hn2 (xi) + U (b - a)

i=1

(167)

According to Theorem 2.8

‹ ‹ ‹ ‹ > 0 P 1 f ( ) - f ( )  -1 f ( ) - f ( ) >  ---- 0, n n

(168)

so it means

∆ ‹ ‹ ‹ ‹ > 0  > 0 n0 

n  n0

P

1 f ( ) - f ( )  -1 f ( ) - f ( ) >  n

< .

(169)

Let's fix an arbitrary  > 0 and  > 0. Hence fix

U n0 := 2

(170)

and for all n  n0 we can write

‹ ‹ ‹ ‹ > P

1 n

f(

) - f(

)  -1

f(

) - f(

)

2(b - a) > 2

P

1n n

f^(xi) - f (xi)

2

>

2(b - a) 2

 i=1



  P 

hn

2 L2 [a,b]

>

2(b - 2

a)

+

U (b - 2n

a)  

 ~

P

~ hn L1[a,b] > 1 L2[a,b]

 P hn L1[a,b] > 

by (169) by (160)

by (167)

by Cauchy-Schwarz A.5

by (170).

(171)

Thus

hn

L1 [a,b]

---P-
n

0.

(172)

According to Lemma A.6 and the fact that the almost sure convergence implies convergence in probability,

42

we have

sup f (x) - f (x) ---P- 0.

x[a,b]

n

(173)

ii) s  1

If m = 2, we are done. Let gn := f - f . According to the assumptions of our model, gn  Hm[a, b].

By Theorem 1.4 (Embedding), all the functions in the estimating set have derivatives up to order m - 1

uniformly bounded in supnorm. Then all the gn are also bounded in supnorm (m  3) and that implies the

uniform boundedness of gn:

M > 0 n  N gn ,[a,b] < M.

(174)

Let's have fixed M > 0. For any fixed  > 0, define ~ := M  and there exists n0  N, such that n  n0 :

[cn, dn]  [a, b] and

gn (cn) = gn (dn) = ~ & gn () > ~,   (cn, dn)

(175)

because gn is continuous on [cn, dn] (drawing a picture is helpful). If there does not exist such [cn, dn], we are done.

Figure 6: Uniform convergence of gn .

Otherwise there exists n1  n0 such that n  n1 holds:

dn

|~(dn - cn)| 

gn ()d = |gn(dn) - gn(cn)|  22

cn

because gn -n--- 0 uniformly in supnorm on interval [a, b]. Hence

|dn

-

cn|



2 M

.

43

(176) (177)

Uniform boundedness of gn implies Lipschitz property (see Figure 6):

|gn (x)| 

~

+

M

dn

- 2

cn

 M  + M   (M + 1). M

(178)

We could continue in this way finitely times (formally we can proceed this by something like a finite induction). In fact, if (m - 1)-th derivatives are uniformly bounded (gn  Hm[a, b]), then this ensures that f (s) for s  m - 2 converges in supnorm. Finally, we have to realize that convergence almost sure implies convergence in probability and each convergent sequence in probability has a convergent subsequence that converges almost sure.

Proof of Theorem 3.2. The proof is very similar to the proof of Infinite to Finite Theorem 2.1. We use the same argumentation. Each f, g  Hm can be written in form

f=

cixi + hf , hf  {span {‹i : ni  1}} ,

{i | ni1}

g=

dj xj + hg, hg  span ‹j : mj  1  .

{j | mj 1}

For 1    n, we easily note that

     

‹  -   0   f ( ) 

‹0 

g ( )





 

= Y - 

if (xi) +

ig(xi)

{i | ni1}

{i | mi1}

= Y -

i

{i | ni1}

xi ,

cj xj + hf

{j | nj 1}

Sob,m

- i
{i | mi1}

xi ,

dj xj + hg

{j | mj 1}

Sob,m

= Y - i ij cj - i ij dj

 {i | ni1} {j | nj 1} 

= 

 -   0   

0

0

{i | mi1} {j | mj 1}
0    .



Analogically for n <   n + m.

(179) (180)
(181)

44

Finally, we have only to rewrite the constraints using (9) from Theorem 1.2:

f (x) =

x ,

cix i + hf

{i | ni1}

= (1)  Sob,m

 : n  1.

We analogically obtain

g(x) = (1)

f (x) = (2) 
g(x) = (2)


 : m  1,  : n  1,  : m  1.

(182)
(183) (184) (185)

45

References
Adams, R. A. (1975). Sobolev Spaces. New York: Academic Press. A®it-Sahalia, Y. and A. W. Lo (2000). Nonparametric risk management and implied risk aversion. Journal
of Econometrics 94 (1≠2), 9≠51. A®it-Sahalia, Y., Y. Wang, and F. Yared (2001). Do option markets correctly price the probabilities of
movement of the underlying asset? Journal of Econometrics 102 (1), 67≠110. Andel, J. (2002). Za¥klady matematick¥e statistiky. Prague: MatfyzPress. Preprint in Czech. Breeden, D. and R. Litzenberger (1978). Prices of state-contingent claims implicit in option prices. Journal
of Business 51, 621≠651. de Geer, S. V. (1990). Estimating a regression function. The Annals of Statistics 18 (2), 907≠924. Eubank, R. L. (1999). Nonparametric Regression and Spline Smoothing. New York: Marcel Dekker, Inc. Ha®rdle, W. (1990). Applied Nonparametric Regression. Cambridge: Cambridge University Press. Ha®rdle, W. and Z. Hl¥avka (2006). Dynamics of state price densities. submitted to Journal of Econometrics. Jackwerth, J. (1999). Option-implied risk-neutral distributions and implied binomial trees: a literature
review. Journal of Derivatives 7, 66≠82. Maz'ja, V. G. (1985). Sobolev Spaces. Berlin: Springer-Verlag. Pesta, M. (2006). Isotonic regression in sobolev spaces. Master's thesis, Charles University in Prague, Czech
Republic. Yatchew, A. and W. Ha®rdle (2005). Nonparametric state price density estimation using constrained least
squares and the bootstrap. Journal of Econometrics. doi:10.1016/j.jeconom.2005.06.031. Yatchew, A. J. and L. Bos (1997). Nonparametric least squares estimation and testing of economic models.
Journal of Quantitative Economics 13, 81≠131.
46

SFB 649 Discussion Paper Series 2006
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Calibration Risk for Exotic Options" by Kai Detlefsen and Wolfgang K. H‰rdle, January 2006.
002 "Calibration Design of Implied Volatility Surfaces" by Kai Detlefsen and Wolfgang K. H‰rdle, January 2006.
003 "On the Appropriateness of Inappropriate VaR Models" by Wolfgang H‰rdle, Zdenk Hl·vka and Gerhard Stahl, January 2006.
004 "Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification" by Harald Uhlig, January/February 2006.
005 "British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis" by Enzo Weber, January 2006.
006 "A Combined Approach for Segment-Specific Analysis of Market Basket Data" by Yasemin Boztu and Thomas Reutterer, January 2006.
007 "Robust utility maximization in a stochastic factor model" by Daniel Hern·ndez≠Hern·ndez and Alexander Schied, January 2006.
008 "Economic Growth of Agglomerations and Geographic Concentration of Industries - Evidence for Germany" by Kurt Geppert, Martin Gornig and Axel Werwatz, January 2006.
009 "Institutions, Bargaining Power and Labor Shares" by Benjamin Bental and Dominique Demougin, January 2006.
010 "Common Functional Principal Components" by Michal Benko, Wolfgang H‰rdle and Alois Kneip, Jauary 2006.
011 "VAR Modeling for Dynamic Semiparametric Factors of Volatility Strings" by Ralf Br¸ggemann, Wolfgang H‰rdle, Julius Mungo and Carsten Trenkler, February 2006.
012 "Bootstrapping Systems Cointegration Tests with a Prior Adjustment for Deterministic Terms" by Carsten Trenkler, February 2006.
013 "Penalties and Optimality in Financial Contracts: Taking Stock" by Michel A. Robe, Eva-Maria Steiger and Pierre-Armand Michel, February 2006.
014 "Core Labour Standards and FDI: Friends or Foes? The Case of Child Labour" by Sebastian Braun, February 2006.
015 "Graphical Data Representation in Bankruptcy Analysis" by Wolfgang H‰rdle, Rouslan Moro and Dorothea Sch‰fer, February 2006.
016 "Fiscal Policy Effects in the European Union" by Andreas Thams, February 2006.
017 "Estimation with the Nested Logit Model: Specifications and Software Particularities" by Nadja Silberhorn, Yasemin Boztu and Lutz Hildebrandt, March 2006.
018 "The Bologna Process: How student mobility affects multi-cultural skills and educational quality" by Lydia Mechtenberg and Roland Strausz, March 2006.
019 "Cheap Talk in the Classroom" by Lydia Mechtenberg, March 2006. 020 "Time Dependent Relative Risk Aversion" by Enzo Giacomini, Michael
Handel and Wolfgang H‰rdle, March 2006. 021 "Finite Sample Properties of Impulse Response Intervals in SVECMs with
Long-Run Identifying Restrictions" by Ralf Br¸ggemann, March 2006. 022 "Barrier Option Hedging under Constraints: A Viscosity Approach" by
Imen Bentahar and Bruno Bouchard, March 2006.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "How Far Are We From The Slippery Slope? The Laffer Curve Revisited" by Mathias Trabandt and Harald Uhlig, April 2006.
024 "e-Learning Statistics ≠ A Selective Review" by Wolfgang H‰rdle, Sigbert Klinke and Uwe Ziegenhagen, April 2006.
025 "Macroeconomic Regime Switches and Speculative Attacks" by Bartosz Makowiak, April 2006.
026 "External Shocks, U.S. Monetary Policy and Macroeconomic Fluctuations in Emerging Markets" by Bartosz Makowiak, April 2006.
027 "Institutional Competition, Political Process and Holdup" by Bruno Deffains and Dominique Demougin, April 2006.
028 "Technological Choice under Organizational Diseconomies of Scale" by Dominique Demougin and Anja Schˆttner, April 2006.
029 "Tail Conditional Expectation for vector-valued Risks" by Imen Bentahar, April 2006.
030 "Approximate Solutions to Dynamic Models ≠ Linear Methods" by Harald Uhlig, April 2006.
031 "Exploratory Graphics of a Financial Dataset" by Antony Unwin, Martin Theus and Wolfgang H‰rdle, April 2006.
032 "When did the 2001 recession really start?" by Jˆrg Polzehl, Vladimir Spokoiny and Ctlin Stric, April 2006.
033 "Varying coefficient GARCH versus local constant volatility modeling. Comparison of the predictive power" by Jˆrg Polzehl and Vladimir Spokoiny, April 2006.
034 "Spectral calibration of exponential LÈvy Models [1]" by Denis Belomestny and Markus Reiﬂ, April 2006.
035 "Spectral calibration of exponential LÈvy Models [2]" by Denis Belomestny and Markus Reiﬂ, April 2006.
036 "Spatial aggregation of local likelihood estimates with applications to classification" by Denis Belomestny and Vladimir Spokoiny, April 2006.
037 "A jump-diffusion Libor model and its robust calibration" by Denis Belomestny and John Schoenmakers, April 2006.
038 "Adaptive Simulation Algorithms for Pricing American and Bermudan Options by Local Analysis of Financial Market" by Denis Belomestny and Grigori N. Milstein, April 2006.
039 "Macroeconomic Integration in Asia Pacific: Common Stochastic Trends and Business Cycle Coherence" by Enzo Weber, May 2006.
040 "In Search of Non-Gaussian Components of a High-Dimensional Distribution" by Gilles Blanchard, Motoaki Kawanabe, Masashi Sugiyama, Vladimir Spokoiny and Klaus-Robert M¸ller, May 2006.
041 "Forward and reverse representations for Markov chains" by Grigori N. Milstein, John G. M. Schoenmakers and Vladimir Spokoiny, May 2006.
042 "Discussion of 'The Source of Historical Economic Fluctuations: An Analysis using Long-Run Restrictions' by Neville Francis and Valerie A. Ramey" by Harald Uhlig, May 2006.
043 "An Iteration Procedure for Solving Integral Equations Related to Optimal Stopping Problems" by Denis Belomestny and Pavel V. Gapeev, May 2006.
044 "East Germany's Wage Gap: A non-parametric decomposition based on establishment characteristics" by Bernd Gˆrzig, Martin Gornig and Axel Werwatz, May 2006.
045 "Firm Specific Wage Spread in Germany - Decomposition of regional differences in inter firm wage dispersion" by Bernd Gˆrzig, Martin Gornig and Axel Werwatz, May 2006.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

046 "Produktdiversifizierung: Haben die ostdeutschen Unternehmen den Anschluss an den Westen geschafft? ≠ Eine vergleichende Analyse mit Mikrodaten der amtlichen Statistik" by Bernd Gˆrzig, Martin Gornig and Axel Werwatz, May 2006.
047 "The Division of Ownership in New Ventures" by Dominique Demougin and Oliver Fabel, June 2006.
048 "The Anglo-German Industrial Productivity Paradox, 1895-1938: A Restatement and a Possible Resolution" by Albrecht Ritschl, May 2006.
049 "The Influence of Information Costs on the Integration of Financial Markets: Northern Europe, 1350-1560" by Oliver Volckart, May 2006.
050 "Robust Econometrics" by Pavel CÌzek and Wolfgang H‰rdle, June 2006. 051 "Regression methods in pricing American and Bermudan options using
consumption processes" by Denis Belomestny, Grigori N. Milstein and Vladimir Spokoiny, July 2006. 052 "Forecasting the Term Structure of Variance Swaps" by Kai Detlefsen and Wolfgang H‰rdle, July 2006. 053 "Governance: Who Controls Matters" by Bruno Deffains and Dominique Demougin, July 2006. 054 "On the Coexistence of Banks and Markets" by Hans Gersbach and Harald Uhlig, August 2006. 055 "Reassessing Intergenerational Mobility in Germany and the United States: The Impact of Differences in Lifecycle Earnings Patterns" by Thorsten Vogel, September 2006. 056 "The Euro and the Transatlantic Capital Market Leadership: A Recursive Cointegration Analysis" by Enzo Weber, September 2006. 057 "Discounted Optimal Stopping for Maxima in Diffusion Models with Finite Horizon" by Pavel V. Gapeev, September 2006. 058 "Perpetual Barrier Options in Jump-Diffusion Models" by Pavel V. Gapeev, September 2006. 059 "Discounted Optimal Stopping for Maxima of some Jump-Diffusion Processes" by Pavel V. Gapeev, September 2006. 060 "On Maximal Inequalities for some Jump Processes" by Pavel V. Gapeev, September 2006. 061 "A Control Approach to Robust Utility Maximization with Logarithmic Utility and Time-Consistent Penalties" by Daniel Hern·ndez≠Hern·ndez and Alexander Schied, September 2006. 062 "On the Difficulty to Design Arabic E-learning System in Statistics" by Taleb Ahmad, Wolfgang H‰rdle and Julius Mungo, September 2006. 063 "Robust Optimization of Consumption with Random Endowment" by Wiebke Wittm¸ﬂ, September 2006. 064 "Common and Uncommon Sources of Growth in Asia Pacific" by Enzo Weber, September 2006. 065 "Forecasting Euro-Area Variables with German Pre-EMU Data" by Ralf Br¸ggemann, Helmut L¸tkepohl and Massimiliano Marcellino, September 2006. 066 "Pension Systems and the Allocation of Macroeconomic Risk" by Lans Bovenberg and Harald Uhlig, September 2006. 067 "Testing for the Cointegrating Rank of a VAR Process with Level Shift and Trend Break" by Carsten Trenkler, Pentti Saikkonen and Helmut L¸tkepohl, September 2006. 068 "Integral Options in Models with Jumps" by Pavel V. Gapeev, September 2006. 069 "Constrained General Regression in Pseudo-Sobolev Spaces with Application to Option Pricing" by Zdenk Hl·vka and Michal Pesta, September 2006.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

