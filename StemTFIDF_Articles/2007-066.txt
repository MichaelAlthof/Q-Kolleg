BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2007-066
Modelling Financial High Frequency Data Using Point
Processes
Luc Bauwens* Nikolaus Hautsch**
* Université catholique de Louvain, Belgium ** Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Modelling Financial High Frequency Data Using Point Processes
Luc Bauwens Universit´e catholique de Louvain, CORE
Nikolaus Hautsch Humboldt-Universita¨t zu Berlin, CASE, CFS
November 2007
Abstract In this paper, we give an overview of the state-of-the-art in the econometric literature on the modeling of so-called financial point processes. The latter are associated with the random arrival of specific financial trading events, such as transactions, quote updates, limit orders or price changes observable based on financial high-frequency data. After discussing fundamental statistical concepts of point process theory, we review durationbased and intensity-based models of financial point processes. Whereas duration-based approaches are mostly preferable for univariate time series, intensity-based models provide powerful frameworks to model multivariate point processes in continuous time. We illustrate the most important properties of the individual models and discuss major empirical applications. Keywords: Financial point processes, dynamic duration models, dynamic intensity models. JEL Classification: C22, C32, C41
1 Introduction
Since the seminal papers by Hasbrouck (1991) and Engle and Russell (1998) the modelling of financial data at the transaction level is an ongoing topic in the area of financial econometrics. This has created a new body of literature which is often referred to as "the econometrics of (ultra-)high-frequency finance" or "high-frequency econometrics". The consideration of the peculiar properties of financial transaction data, such as the irregular spacing in time,
The paper is written as a contribution to the Handbook of Financial Time Series, Springer, 2008. This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
Universit´e catholique de Louvain and CORE. Address: Voie du Roman Pays 34, 1348, Louvain-la-Neuve, Belgium. Email: bauwens@core.ucl.ac.be
Institute for Statistics and Econometrics and CASE ­ Center for Applied Statistics and Economics, Humboldt-Universit¨at zu Berlin as well as Center for Financial Studies (CFS), Frankfurt. Address: Spandauer Str. 1, D-10178 Berlin, Germany. Email: Nikolaus.Hautsch@wiwi.hu-berlin.de.
1

the discreteness of price changes, the bid-ask bounce as well as the presence of serial dependence, provoked the surge of new econometric approaches. One important string of the literature deals with the irregular spacing of data in time. Taking into account the latter is indispensable whenever the full amount of information in financial transaction data has to be exploited and no loss of information due to fixed-interval aggregation schemes can be accepted. Moreover, it has been realized that the timing of trading events, such as the arrival of particular orders and trades, and the frequency in which the latter occur have information value for the state of the market and play an important role in market microstructure analysis, for the modelling of intraday volatility as well as the measurement of liquidity and implied liquidity risks.
Taking into account the irregular occurrence of transaction data requires to consider it as a point process, a so-called financial point process. Depending on the type of the financial "event" under consideration, we can distinguish between different types of financial point processes or processes of so-called financial durations. The most common types are trade durations and quote durations as defined by the time between two consecutive trade or quote arrivals, respectively. Price durations correspond to the time between absolute cumulative price changes of given size and can be used as an alternative volatility measure. Similarly, a volume duration is defined as the time until a cumulative order volume of given size is traded and captures an important dimension of market liquidity. For more details and illustrations, see Bauwens and Giot (2001) or Hautsch (2004).
One important property of transaction data is that market events are clustered over time implying that financial durations follow positively autocorrelated processes with a strong persistence. Actually, it turns out that the dynamic properties of financial durations are quite similar to those of daily volatilities. Taking into account these properties leads to different types of dynamic models on the basis of a duration representation, an intensity representation or a counting representation of a point process.
In this chapter, we review duration-based and intensity-based models of financial point processes. In Section 2, we introduce the fundamental concepts of point process theory and discuss major statistical tools. In Section 3, we review the class of dynamic duration models. Specifying a (dynamic) duration model is presumably the most intuitive way to characterize a point process in discrete time and has been suggested by Engle and Russell (1998), which was the starting point for a huge body of literature. Nevertheless, Russell (1999) realized that a continuous-time setting on the basis of the intensity function constitutes a more flexible framework which is particularly powerful for the modelling of multivariate processes. Different types of dynamic intensity models are presented in Section 4.
2

2 Fundamental Concepts of Point Process Theory
In this section, we discuss important concepts and relationships in point process theory which are needed throughout this chapter. In Section 2.1, we introduce the notation and basic definitions. The fundamental concepts of intensity functions, compensators and hazard rates are defined in Section 2.2, whereas in Section 2.3 different classes and representations of point processes are discussed. Finally, in Section 2.4, we present the random time change theorem which yields a powerful result for the construction of diagnostics for point process models. Most concepts discussed in this section are based upon Chapter 2 of Karr (1991).
2.1 Notation and Definitions
Let {ti}i{1,...,n} denote a random sequence of increasing event times 0 < t1 < . . . < tn associated with an orderly (simple) point process. Then, N (t) := i1 1l {tit} defines the right-continuous (ca`dl`ag) counting function. Throughout this chapter, we consider only point processes which are integrable, i.e. E[N (t)] <   t  0. Furthermore, {Wi}i{1,...,n} denotes a sequence of {1, . . . , K}-valued random variables representing K different types of events. Then, we call the process {ti, Wi}i{1,...,n} an K-variate marked point process on (0, ) as represented by the K sequences of event-specific arrival times {tki }i{1,...,nk}, k = 1, . . . , K, with counting functions N k(t) := i1 1l {tit}1l {Wi=k}.
The internal history of an K-dimensional point process N (t) is given by the filtration FtN with FtN = (N k(s) : 0  s  t, k  ), N k(s) = i1 1l {tis}1l {Wi}, where  denotes the -field of all subsets of {1, . . . , K}. More general filtrations, including e.g. also processes of explanatory variables (covariates) {zi}i{1,...,n} are denoted by Ft with FtN  Ft.
Define xi := ti - ti-1 with i = 1, . . . , n and t0 := 0 as the inter-event duration from ti-1 until ti. Furthermore, x(t) with x(t) := t - tN(t), with N (t) := i1 1l {ti<t} denoting the left-continuous counting function, is called the backward recurrence time. It is a leftcontinuous function that grows linearly through time with discrete jumps back to zero after each arrival time ti. Finally, let    denote model parameters.
2.2 Compensators, Intensities, and Hazard Rates
In martingale-based point process theory, the concept of compensators plays an important role. Using the property that an Ft-adapted point process N (t) is a submartingale1, it can be decomposed into a zero mean martingale M (t) and a (unique) Ft-predictable increasing process, ~(t), which is called the compensator of N (t) and can be interpreted as the local conditional mean of N (t) given the past. In statistical theory, this decomposition is typically
1An Ft-adpated ca`dl`ag process N (t) is a submartingale if E[|N (t)|] <  for each t and if s < t implies that E[N (t)|Fs]  N (s).
3

referred to as the Doob-Meyer decomposition. Define (t) as a scalar, positive Ft-predictable process, i.e. (t) is adapted to Ft, and
left-continuous with right hand limits. Then, (t) is called the (Ft-conditional) intensity of N (t) if

t
~(t) = (u)du,
0

(1)

where ~(t) is the (unique) compensator of N (t). This relationship emerges from the in-

terpretation of the compensator as integrated (conditional) hazard function. Consequently,

(t) can be also defined by the relation

s
E[N (s) - N (t)|Ft] = E (u)du Ft
t

(2)

which has to hold (almost surely) for all t, s with 0  t  s. Letting s  t leads to the

heuristic representation which is more familiar in classical duration analysis. Then, (t) is

obtained by

(t+)

:=

lim
0

1 

E

[

N

(t

+

)

-

N

(t)|

Ft]

,

(3)

where (t+) := lim0 (t + ). In case of a stationary point process, ¯ := E[dN (t)]/dt = E[(t)] is constant.
Equation (3) manifests the close analogy between the intensity function and the hazard function which is given by

1 h(x) := f (x)/S(x) = lim Pr[x  X < x + |X  x]
0 

(4)

with x denoting the (inter-event) duration as represented by the realization of a random variable X with probability density function f (x), survivor function S(x) = 1 - F (x), and cumulative distribution function (cdf) F (x) = Pr[X  x]. Whereas the intensity function is defined in (continuous) calendar time, the hazard rate is typically defined in terms of the length of a duration x and is a key concept in (cross-section) survival analysis.

2.3 Types and Representations of Point Processes
The simplest type of point process is the homogeneous Poisson process defined by

Pr [(N (t + ) - N (t)) = 1 |Ft ] =  + o(), Pr [(N (t + ) - N (t)) > 1 |Ft ] = o(),

(5) (6)

with   0. Then,  > 0 is called the Poisson rate corresponding to the (constant) intensity. Accordingly, equations (5) and (6) define the intensity representation of a Poisson process. A well-known property of homogenous Poisson processes is that the inter-event

4

waiting times xi = ti - ti-1 are independently exponentially distributed, leading to the duration representation. In this context,  is the hazard rate of the exponential distribution. Furthermore, it can be shown (see e.g. Lancaster (1997)) that the number of events in an interval (a, b], N (a, b) := N (b) - N (a) is Poisson distributed with Pr[N (a, b) = k] = exp[-(b - a)][(b - a)]k/k!, yielding the counting representation. All three representations of a Poisson process can be used as the starting point for the specification of a point process model.
Throughout this chapter we associate the term duration models to a model of the (discrete-time) duration process observable at the event-times {ti}i=1,...,n. Then, researchers parameterize the conditional distribution function F (xi|Fti-1) or, alternatively, the conditional hazard rate h(xi|Fti-1). Generally, such a model should aim, in particular, at fitting the dynamical and distributional properties of durations. The latter is often characterized by the excess dispersion, corresponding to the ratio between the standard deviation to the mean. In classical hazard rate models employed in traditional survival analysis, the hazard rate is typically parameterized in terms of covariates, see e.g. Kalbfleisch and Prentice (1980), Kiefer (1988) or Lancaster (1997). The most well-known hazard model is the proportional hazard model introduced by Cox (1972) and is given by

h(x|z; ) = h0(x|1)g(z, 2),

(7)

where  = (1, 2), h0(·) denotes the so-called baseline hazard rate and g(·) is a function of the covariates z and parameters 2. The baseline hazard rate may be parameterized in accordance with a certain distribution, like e.g., a Weibull distribution with parameters

, p > 0 implying

h0(x|1) = p(x)p-1.

(8)

For p = 1 we obtain the exponential case h0(x|1) = , implying a constant hazard rate. Alternatively, if p > 1, h0(x|1)/x > 0, i.e. the hazard rate is increasing with the length of the spell which is referred to as "positive duration dependence". In contrast, p < 1 implies

"negative duration dependence". Non-monotonic hazard rates can be obtained with more

flexible distributions, like the generalized F and particular cases thereof, including the

generalized gamma, Burr, Weibull and log-logistic distributions. We refer to the Appendix

to Chapter 3 of Bauwens and Giot (2001) and to the Appendix of Hautsch (2004) for

definitions and properties. Alternatively, the baseline hazard may be left unspecified and

can be estimated nonparametrically, see Cox (1975).

An alternative type of duration model is the class of accelerated failure time (AFT)

models given by

h(x|z; ) = h0[xg(z, 2)|1]g(z, 2).

(9)

5

Here, the effect of the exogenous variables is to accelerate or to decelerate the time scale on which the baseline hazard h0 is defined. As illustrated in Section 3.1, AFT-type models are particularly attractive to allow for autocorrelated duration processes.
Because of their discrete-time nature, duration models cannot be used whenever the information set has to be updated within a duration spell, e.g. caused by time-varying covariates or event arrivals in other point processes. For this reason, (discrete-time) duration models are typically used in a univariate framework.
Whenever a continuous-time modelling is preferential (as e.g. to account for the asynchronous event arrivals in a multivariate framework), it is more natural to specify the intensity function directly. This class of models is referred to as intensity models. One important extension of a homogenous Poisson process it to allow the intensity to be directed by a real-valued, non-negative (stationary) random process (t) with (internal) history Ft leading to the class of doubly stochastic Poisson processes (Cox processes). In particular, N (t) is called a Cox process directed by (t) if conditional on (t), N (t) is a Poisson process with mean (t), i.e. Pr[N (a, b) = k|Ft] = exp[-(t)] [(t)]k /k!. The doubly stochastic Poisson process yields a powerful class of probabilistic models with applications in seismology, biology and economics. For instance, specifying (t) in terms of an autoregressive process yields a dynamic intensity model which is particularly useful to capture the clustering in financial point processes. For a special type of doubly stochastic Poisson process see Section 4.2.
A different generalization of the Poisson process is obtained by specifying (t) as a (linear) self-exciting process given by

t
(t) =  + w(t - u)dN (u) =  + w(t - ti),
0 ti<t

(10)

where  is a constant, w(s) denotes a non-negative weight function, and

t 0

w(s)dN

(s)

is the stochastic Stieltjes integral of the process w with respect to the counting pro-

cess N (t). The process (10) was proposed by Hawkes (1971) and is therefore named a

Hawkes process. If w(s) declines with s, then, the process is self-exciting in the sense that

Cov[N (a, b), N (b, c)] > 0, where 0 < a  b < c. Different types of Hawkes processes and

their applications to financial point processes are presented in Section 4.1. A further type

of intensity models which is relevant in the literature of financial point processes is given

by a specification where the intensity itself is driven by an autoregressive process which is

updated at each point of the process. This leads to a special type of point process models

which does not originate from the classical point process literature but originates from the

autoregressive conditional duration (ACD) literature reviewed in Section 2 and brings time

series analysis into play. Such a process is called an autoregressive conditional intensity

6

model and is considered in Section 4.2. Finally, starting from the counting representation of a Poisson process leads to the
class of count data models. Dynamic extensions of Poisson processes in terms of counting representations are not surveyed in this chapter. Some references reflecting the diversity of approaches are Rydberg and Shephard (2003), Heinen and Rengifo (2003), Liesenfeld, Nolte, and Pohlmeier (2006), and Quoreshi (2006).

2.4 The Random Time Change Theorem
One fundamental result of martingale-based point process theory is the (multivariate) random time change theorem by Meyer (1971) which allows to transform a wide class of point processes to a homogeneous Poisson process:

Theorem (Meyer, 1971, Brown and Nair, 1988): Assume a multivariate point process (N 1(t), . . . , N K (t)) is formed from the event times {tik}i{1,...,nk}, k = 1, . . . , K, and has continuous compensators (~(t)1, . . ., ~(t)K) with ~k() =  for each k = 1, . . . , K, then the point processes formed from {~ k(tki )}{i=1,...,nk}, k = 1, . . . , K, are independent Poisson processes with unit intensity.
Proof: See Meyer (1971) or Brown and Nair (1988) for a more accessible and elegant
proof.

Define  k(t) as the (Ft-)stopping time obtained by the solution of

 0

k

(t)

k

(s)ds

=

t.

Applying the random time change theorem to (1) implies that the

point processes N~ k(t) with N~ k(t) := N k( k(t)) are independent Poisson processes with

unit intensity and event times {~ k(tik)}{i=1,...,nk} for k = 1, . . . , K. Then, the so-called

integrated intensities

k(tki-1, tik) := tik k(s)ds = ~ k(tik) - ~ k(tki-1)
tki-1

(11)

correspond to the increments of independent Poisson processes for k = 1, . . . , K. Consequently, they are independently standard exponentially distributed across i and k. For more details, see Bowsher (2006). The random time change theorem plays an important role in order to construct diagnostic tests for point process models (see Section 4.3) or to simulate point processes (see e.g. Giesecke and Tomecek (2005)).

3 Dynamic Duration Models

In this section, we discuss univariate dynamic models for the durations between consecutive (financial) events. In Section 3.1, we review in detail the class of ACD models, which is by far the most used class in the literature on financial point processes. In Section 3.2,

7

we briefly discuss statistical inference for ACD models. In Section 3.3, we present other dynamic duration models, and in the last section we review some applications.
3.1 ACD Models
The class of ACD models has been introduced by Engle and Russell (1997, 1998) and Engle (2000). In order to keep the notation simple, define xi in the following as the inter-event duration which is standardized by a seasonality function s(ti), i.e. xi := (ti - ti-1)/s(ti). The function s(ti) is typically parameterized according to a spline function capturing timeof-day or day-of-week effects. Time-of-day effects arise because of systematic changes of the market activity throughout the day and due to opening of other related markets. In most approaches s(ti) is specified according to a linear or cubic spline function and is estimated separately in a first step yielding seasonality adjusted durations xi. Alternatively, a nonparametric approach has been proposed by Veredas, Rodriguez-Poo, and Espasa (2002). For more details and examples regarding seasonality effects in financial duration processes, we refer the reader to Chapter 2 of Bauwens and Giot (2001) or to Chapter 3 of Hautsch (2004).
The key idea of the ACD model is to model the (seasonally adjusted) durations {xi}i=1,...,n in terms of a multiplicative error term model in the spirit of Engle (2002), i.e.

xi = i i,

(12)

where i denotes a function of the past durations (and possible covariates), and i defines an i.i.d. random variable for which it is assumed that

E[ i] = 1,

(13)

so that i corresponds to the conditional duration mean (the so-called "conditional duration") with i := E[xi|Fti-1]. The ACD model can be rewritten in terms of the intensity function as

(t|Ft) = 

x(t) 1 ,
N (t)+1 N (t)+1

(14)

where  (s) denotes the hazard function of the ACD error term. This formulation clearly demonstrates that the ACD model belongs to the class of AFT models. Assuming i to be standard exponentially distributed yields the so-called Exponential ACD model. More flexible specifications arise by assuming i to follow a more general distribution, see the discusssion after equation (8). It is evident that the ACD model is the counter-part to the GARCH model (Bollerslev (1986)) for duration processes. Not surprisingly, many results and specifications from the GARCH literature have been adapted to the ACD literature.

8

The conditional duration, i, is defined as a function  of the information set Fti-1 and provides therefore the vehicle for incorporating the dynamics of the duration process. In this respect it is convenient to use an ARMA-type structure of order (p, q), whereby

i = (i-1, . . . , i-p, xi-1, . . . , xi-q).

(15)

For simplicity, we limit the exposition in the sequel to the case p = q = 1.

The first model put forward in the literature is the linear ACD model, which specializes

(15) as

i =  + i-1 + xi-1.

(16)

Since i must be positive, the restrictions  > 0,   0 and   0 are usually imposed. It is also assumed that  = 0 if  = 0, otherwise  is a redundant parameter. The process defined by (12), (13) and (16) is known to be covariance-stationary if

( + )2 - 22 < 1,

(17)

where 2 := Var[ i] < , and to have the following moments and autocorrelations:

(1) E[xi] := µx = /(1 -  - ),

(2)

Var[xi]

:=

x2

=

µx2

2

1-2 1-(+

-2 )2-2

2

,

(3)

1

=

 (1-2- ) 1-2-2  

and

n

= ( + )n-1

(n  2).

The condition (17) ensures the existence of the variance. These results are akin to those for the GARCH(1,1) zero-mean process. They can be generalized to ACD(p,q) processes when p, q > 1. It is usually found empirically that the estimates of the parameters are such that  +  is in the interval (0.85,1) while  is in the interval (0.01,0.15). Since the ACD(1,1) model can be written as

xi =  + ( + )xi-1 + ui - ui-1,

(18)

where ui := xi - i is a martingale difference innovation, the resulting autocorrelation function (ACF) is that of an ARMA(1,1) process that has AR and MA roots close to each other. This type of parameter configuration generates the typical ACF shape of clustered data. Nevertheless, the ACF decreases at a geometric rate, though it is not uncommon to find duration series with an ACF that decreases at a hyperbolic rate. This tends to happen for long series and may be due to parameter changes that give the illusion of long memory in the process. In order to allow for long range dependence in financial duration processes, Jasiak (1998) extends the ACD model to a fractionally integrated ACD model. For alternative ways to specify long memory ACD models, see Koulikov (2002).

9

A drawback of the linear ACD model is that it is difficult to allow i to depend on functions of covariates without violating the non-negativity restriction. For this reason, Bauwens and Giot (2000) propose a class of logarithmic ACD models, where no parametric restrictions are needed to ensure positiveness of the process:

ln i =  +  ln i-1 + g( i-1),

(19)

where g( i-1) is either ln i-1 (log-ACD of type I) or i-1 (type II). Using this setting, it is convenient to augment i by functions of covariates, see e.g. Bauwens and Giot (2001). The stochastic process defined by (12), (13) and (19) is covariance-stationary if

 < 1, E [ i exp[g( i)]] , E [exp[2g( i)]] < .

(20)

Its mean, variance and autocorrelations are given in Section 3.2 in Bauwens and Giot (2001), see also Fernandes and Grammig (2006) and Bauwens, Galli, and Giot (2008). Drost and Werker (2004) propose to combine one of the previous ACD equations for the conditional duration mean with an unspecified distribution for i, yielding a class of semi-parametric ACD models.
The augmented ACD (AACD) model introduced by Fernandes and Grammig (2006) provides a more flexible specification of the conditional duration equation than the previous models. Here, i is specified in terms of a Box-Cox transformation yielding
i1 =  + i-1 1 + i-1 1[| i-1 - | - ( i-1 - )]2 ,
where 1 > 0, 2 > 0, , and  are parameters. The so-called news impact function [| i-1 - | - ( i-1 - )]2 allows for a wide variety of shapes of the curve tracing the impact of i-1 on i for a given value of i-1 and the remaining parameters. The parameter  is a shift parameter and the parameter  is a rotation parameter. If  =  = 0, the linear ACD model is obtained by setting 1 = 2 = 1, the type I logarithmic ACD model by letting 1 and 2 tend to 0, and the type II version by letting 1 tend to 0 and setting 2 = 1. Fernandes and Grammig (2006) compare different versions of the AACD model using IBM price durations arising from trading at the New York Stock Exchange. Their main finding is that "letting 1 free to vary and accounting for asymmetric effects (by letting  and  free) seem to operate as substitute sources of flexibility". Hautsch (2006) proposes an even more general augmented ACD model that nests in particular the so-called EXponential ACD model proposed by Dufour and Engle (2000) implying a kinked news impact function. As a counterpart to the semiparametric GARCH model proposed by Engle and Ng (1993), Hautsch (2006) suggests specifying the news impact function in terms of a linear spline function based on the support of i. He illustrates that the high flexibility of this model is needed in order to appropriately capture the dynamic properties of financial durations.

10

Another way to achieve flexibility in ACD models is to use the idea of mixtures. The mixture may apply to the error distribution alone, as in De Luca and Zuccolotto (2003), Hujer and Vuletic (2005), De Luca and Gallo (2004) and De Luca and Gallo (2006), or may involve the dynamic component as well. Zhang, Russell, and Tsay (2001) propose a threshold ACD model (TACD), wherein the ACD equation and the error distribution change according to a threshold variable such as the previous duration. For J regimes indexed by j = 1, . . . , J, the model is defined as

xi = (ij) i(j), (ij) = (j) + (j)i-1 + (j)xi-1

(21) (22)

when xi-1  [rj-1, rj), and 0 = r0 < r1 < . . . < rJ =  are the threshold parameters. The superscript (j) indicates that the distribution or the model parameters can vary with the regime operating at observation i. This model can be viewed as a mixture of J ACD models, where the probability to be in regime j at i is equal to 1 and the probabilities to be in each of the other regimes is equal to 0. Hujer, Vuletic, and Kokot (2002) extend this model to let the regime changes be governed by a hidden Markov chain.
While the TACD model implies discrete transitions between the individual regimes, Meitz and Tera¨svirta (2006) propose a class of smooth transition ACD (STACD) models which generalize the linear and logarithmic ACD models in a specific way. Conditions for strict stationarity, ergodicity, and existence of moments for this model and other ACD models are provided in Meitz and Saikkonen (2004) using the theory of Markov chains. A motivation for the STACD model is, like for the AACD, to allow for a nonlinear impact of the past duration on the next expected duration.

3.2 Statistical Inference

The estimation of most ACD models can be easily performed by maximum likelihood (ML). Engle (2000) demonstrates that the results by Bollerslev and Wooldridge (1992) on the

quasi-maximum likelihood (QML) property of the Gaussian GARCH(1,1) model extend to

the Exponential-ACD(1,1) model. Then, QML estimates are obtained by maximizing the

quasi-loglikelihood function given by

ln L ; {xi}{i=1,...,n}

n
=-
i=1

ln i

+

xi i

.

(23)

For more details we refer to Chapter 3 of Bauwens and Giot (2001), Chapter 5 of Hautsch (2004), and to the survey of Engle and Russell (2005).
Residual diagnostics and goodness-of-fit tests are straightforwardly performed by evaluating the stochastic properties of the ACD residuals ^i = xi/^ i. The dynamic properties

11

are easily analyzed based on Portmanteau statistics or tests against independence such as proposed by Brock, Scheinkman, Scheinkman, and LeBaron (1996). The distributional properties can be evaluated based on Engle and Russell's (1998) test for no excess dispersion using the asymptotically standard normal test statistic n/8 ^2, where ^2 denotes the empirical variance of the residual series. Dufour and Engle (2000) and Bauwens, Giot, Grammig, and Veredas (2004) evaluate the model's goodness-of-fit based on the evaluation of density forecasts using the probability integral transform as proposed by Diebold, Gunther, and Tay (1998). A nonparametric test against distributional misspecification is proposed by Fernandes and Grammig (2005) based on the work of A¨it-Sahalia (1996). Statistics that exclusively test for misspecifications of the conditional mean function i have been worked out by Meitz and Ter¨asvirta (2006) using the Lagrange Multiplier principle and by Hautsch (2006) using (integrated) conditional moment tests. A common result is that too simple ACD specifications, such as the ACD or Log-ACD model are not flexible enough to adequately capture the properties of observed financial durations.

3.3 Other Models

ACD models strongly resemble ARCH models. Therefore it is not surprising that Taylor's

(1986) stochastic volatility model for financial returns has been a source of inspiration of

similar duration models. Bauwens and Veredas (2004) propose the stochastic conditional

duration model (SCD) as an alternative to ACD-type models. The SCD model relates to

the logarithmic ACD model in the same way as the stochastic volatility model relates to

the exponential GARCH model of Nelson (1991). Thus the model is defined by equations

(12), (13), and

ln i =  +  ln i-1 +  i-1 + ui,

(24)

where ui is iid N(0, u2) distributed. The process {ui} is assumed to be independent of the process { i}. The set of possible distributions for the duration innovations i is the same as for ACD models. This model allows for a rich class of hazard functions for xi through the interplay of two distributions. The latent variable i may be interpreted as being inversely related to the information arrival process which triggers bursts of activity on financial markets. The "leverage" term  i-1 in (24) is added by Feng, Jiang, and Song (2004) to allow for an intertemporal correlation between the observable duration and the conditional duration, and the correlation is found to be positive. Bauwens and Veredas (2004) use a logarithmic transformation of (12) and employ QML estimation based on the Kalman filter. Knight and Ning (2005) use the empirical characteristic function and the method of generalized moments. Strickland, Forbes, and Martin (2003) use Bayesian estimation with a Markov chain Monte Carlo algorithm. For the model with leverage term,

12

Feng, Jiang, and Song (2004) use the Monte Carlo ML method of Durbin and Koopman (2004).
The ACD and SCD models reviewed above share the property that the dynamics of higher moments of the duration process are governed by the dynamics of the conditional mean. Ghysels, Gourieroux, and Jasiak (2004) argue that this feature is restrictive and introduce a nonlinear two factor model that disentangles the movements of the mean and of the variance of durations. Since the second factor is responsible for the variance heterogeneity, the model is named the stochastic volatility duration (SVD) model. The departure point for this model is a standard static duration model in which the durations are independently and exponentially distributed with a gamma heterogeneity, i.e.

xi

=

Ui aVi

=

H(1, F1i) , aH(b, F2i)

(25)

where Ui and Vi are two independent variables which are gamma(1,1) (i.e. exponential) and gamma(b, b) distributed, respectively. The last ratio in (25) uses two independent Gaussian factors F1i and F2i, and H(b, F ) = G(b, (F )), where G(b, .) is the quantile function of the gamma(b, b) distribution and (.) the cdf of the standard normal distribution. Ghysels, Gourieroux, and Jasiak (2004) extend this model to a dynamic setup through a VAR model for the two underlying Gaussian factors. Estimation is relatively difficult and requires simulation methods.

3.4 Applications

ACD models can be used to estimate and predict the intra-day volatility of returns from the intensity of price durations. As shown by Engle and Russell (1998), a price intensity is closely linked to the instantaneous price change volatility. The latter is given by

~2(t) :=

lim

1 E

0 

p(t + ) - p(t) p(t)

2
Ft

,

(26)

where p(t) denotes the price (or midquote) at t. By denoting the counting process associated with the event times of cumulated absolute price changes of size dp by N dp(t), we can

formulate (26) in terms of the intensity function of the process of dp-price changes. Then,

the dp-price change instantaneous volatility can be computed as

~(2dp)(t)

=

lim
0

1 

Pr

[|p(t

+

)

-

p(t)|



dp

|Ft

]

·

dp p(t)

2

=

1 lim Pr 0 

(N dp(t + ) - N dp(t)) > 0 |Ft

·

dp 2 p(t)

:= dp(t) ·

dp 2 ,

p(t)

(27)

13

where dp(t) denotes the corresponding dp-price change intensity. Hence, using (14), one can estimate or predict the instantaneous volatility of the price process p(t) at any time point. Giot (2005) compares these estimates with usual GARCH based estimates obtained by interpolating the prices on a grid of regularly spaced time points. He finds that GARCH based predictions are better measures of risk than ACD based ones in a Value-at-Risk (VaR) evaluation study.
ACD and related models have been typically used to test implications of asymmetric information models of price formation. For example, the model of Easley and O`Hara (1992) implies that the number of transactions influences the price process through information based clustering of transactions. Then, including lags as well as expectations of the trading intensity as explanatory variables in a model for the price process allows to test such theoretical predictions. For a variety of different applications in market microstructure research, see Engle and Russell (1998), Engle (2000), Bauwens and Giot (2000), Engle and Lunde (2003), and Hafner (2005) among others. Several authors have combined an ACD model with a model for the marks of a financial point process. The idea is generally to model the duration process by an ACD model, and conditionally on the durations, to model the process of marks. Bauwens and Giot (2003) model the direction of the price change between two consecutive trades by formulating a competing risks model, where the direction of the price movement is triggered by a Bernoulli process. Then, the parameters of the ACD process depend on the direction of the previous price change, leading to an asymmetric ACD model. A related type of competing risks model is specified by Bisi`ere and Kamionka (2000). Prigent, Renault, and Scaillet (2001) use a similar model for option pricing. Russell and Engle (2005) develop an autoregressive conditional multinomial model to simultaneously model the time between trades and the dynamic evolution of (discrete) price changes.
A related string of the literature studies the interaction between the trading intensity and the trade-to-trade return volatility. Engle (2000) augments a GARCH equation for returns per time by the impact of the inverse of the observed and expected durations (xi and i), and of the surprise xi/i. A decrease in xi or i has a positive impact on volatility while the surprise has the reverse impact. Dionne, Duchesne, and Pacurara (2005) use a related model to compute an intraday VaR. Ghysels and Jasiak (1998) and Grammig and Wellner (2002) study a GARCH process for trade-to-trade returns with time-varying parameters which are triggered by the trading intensity. Meddahi, Renault, and Werker (2006) derive a discrete time GARCH model for irregularly spaced data from a continuous time volatility process and compare it to the ACD-GARCH models by Engle (2000) and Ghysels and Jasiak (1998).
14

4 Dynamic Intensity Models

In this section, we review the most important types of dynamic intensity models which are applied to model financial point processes. The class of Hawkes models and extensions thereof are discussed in Section 4.1. In Section 4.2, we survey different autoregressive intensity models. Statistical inference for intensity models is presented in Section 4.3, whereas the most important applications in the recent literature are briefly discussed in Section 4.4.
4.1 Hawkes Processes
Hawkes processes originate from the statistical literature in seismology and are used to model the occurrence of earthquakes, see e.g. Vere-Jones (1970), Vere-Jones and Ozaki (1982), and Ogata (1988) among others. Bowsher (2006) was the first applying Hawkes models to financial point processes. As explained in Section 3.2, Hawkes processes belong to the class of self-exciting processes, where the intensity is driven by a weighted function of the time distance to previous points of the process. A general class of univariate Hawkes processes is given by

(t) =  µ(t) + ti<t w(t - ti) ,

(28)

where  denotes a possibly nonlinear function, µ(t) is a deterministic function of time, and w(s) denotes a weight function. If  : R  R+, we obtain the class of nonlinear Hawkes processes considered by Br´emaud and Massouli´e (1996). In this case, µ(t) and w(t) can take negative values since the transformation (·) preserves the non-negativity of the process. Such a specification is useful whenever the intensity may be negatively affected by the process history or covariates. For instance, in the context of financial duration processes, µ(t) can be parameterized as a function of covariates. Stability conditions for nonlinear Hawkes processes are derived by Br´emaud and Massouli´e (1996). For the special case where  is a linear function, we obtain the class of linear Hawkes processes originally considered by Hawkes (1971). They are analytically and computationally more tractable than their nonlinear counterparts, however, they require µ(t) > 0 and w(t) > 0 in order to ensure non-negativity.
As pointed out by Hawkes and Oakes (1974), linear self-exciting processes can be viewed as clusters of Poisson processes. Then, each event is one of two types: an immigrant process or an offspring process. The immigrants follow a Poisson process and define the centers of so-called Poisson clusters. If we condition on the arrival time, say ti, of an immigrant, then independently of the previous history, ti is the center of a Poisson process, (ti), of offspring on (ti, ) with intensity function i(t) = (t - ti), where  is a non-negative

15

function. The process (ti) defines the first generation offspring process with respect to ti. Furthermore, if we condition on the process (ti), then each of the events in (ti), say tj, generates a Poisson process with intensity j(t) = (t - tj). These independent Poisson processes build the second generation of offspring with respect to ti. Similarly, further generations arise. The set of all offspring points arising from one immigrant are called a Poisson cluster. Exploiting the branching and conditional independence structure of a (linear) Hawkes process, Møller and Rasmussen (2004) develop a simulation algorithm as an alternative to the Shedler-Lewis thinning algorithm or the modified thinning algorithm by Ogata (1981) (see e.g. Daley and Vere-Jones (2003)). The immigrants and offsprings can be referred to as "main shocks" and "after shocks" respectively. This admits an interesting interpretation which is useful not only in seismology but also in high-frequency finance. Bowsher (2006), Hautsch (2004) and Large (2007) illustrate that Hawkes processes capture the dynamics in financial point processes remarkably well. This indicates that the cluster structure implied by the self-exciting nature of Hawkes processes seem to be a reasonable description of the timing structure of events on financial markets.
The most common parameterization of w(t) has been suggested by Hawkes (1971) and is given by

P
w(t) = je-jt,
j=1

(29)

where j  0, j > 0 for j = 1, . . . , P are model parameters, and P denotes the order of the process and is selected exogenously (or by means of information criteria). The parameters

j are scale parameters, whereas j drive the strength of the time decay. For P > 1, the

intensity is driven by the superposition of differently parameterized exponentially decaying

weighted sums of the backward times to all previous points. In order to ensure identification

we impose the constraint 1 > . . . > P . It can be shown that the stationarity of the process

requires 0 <

 0

w(s)ds

<

1,

which

is

ensured

only

for

P j=1

j /j

<

1,

see

Hawkes

(1971).

While (29) implies an exponential decay, the alternative parameterization

H w(t) = (t + )p ,

(30)

with parameters H, , and p > 1 allows for a hyperbolic decay. Such weight functions are typically applied in seismology (see e.g. Vere-Jones and Ozaki (1982) and Ogata (1988)) and allow to capture long range dependence. Since financial duration processes also tend to reveal long memory behavior (see Jasiak (1998)), specification (30) might be an interesting specification in financial applications.
Multivariate Hawkes models are obtained by a generalization of (28). Then, (t) is given

16

by the (K × 1)-vector (t) = (1(t), . . . , K(t)) with

k(t) =  µk(t) +

K r=1

tir<t wrk(t - tir) ,

(31)

where wrk(s) is a k-type weight function of the backward time to all r-type events. Using an exponential decay function, Hawkes (1971) suggests to parameterize wrk(s) as

P

wrk(t) =

rk,j e-rk,j t,

j=1

(32)

where rk,j  0 and rk,1 > . . . > rk,P > 0 drive the influence of the time distance to past r-type events on the k-type intensity. Thus, in the multivariate case, k(t) depends not

only on the distance to all k-type points, but also on the distance to all other points of the

pooled process. Hawkes (1971) provides a set of linear parameter restrictions ensuring the

stationarity of the process.

Bowsher (2006) proposes a generalization of the Hawkes model which allows to model

point processes which are interrupted by time periods where no activity takes place. In

high-frequency financial time series these effects occur because of trading breaks due to

trading halts, nights, weekends or holidays. In order to account for such effects, Bowsher

proposes to remove all non-activity periods and to concatenate consecutive activity periods

by a spill-over function.

4.2 Autoregressive Intensity Processes

Hamilton and Jorda` (2002) establish a natural link between ACD models and intensity

models by extending the ACD model to allow for covariates which might change during a

duration spell (time-varying covariates). The key idea of their so-called autoregressive con-
ditional hazard (ACH) model is to rely on the fact that in the ACD model with exponential
error distribution, the intensity (or the hazard function, respectively) corresponds to the inverse of the conditional duration, i.e. (t) = N-1(t)+1. They extend this expression by a function of variables which are known at time t - 1,

1

(t) =

,

N (t)+1 + zt-1

(33)

where zt are time-varying covariates which are updated during a duration spell. An alternative model which can be seen as a combination of a duration model and an
intensity model is introduced by Gerhard and Hautsch (2007). They propose a dynamic extension of a Cox (1972) proportional intensity model, where the baseline intensity 0(t) is non-specified. Their key idea is to exploit the stochastic properties of the integrated intensity and to re-formulate the model in terms of a regression model with unknown lefthand variable and Gumbel distributed error terms ­ see Kiefer (1988) for a nice illustration

17

of this relation. To identify the unknown baseline intensity at discrete points, Gerhard and Hautsch follow the idea of Han and Hausman (1990) and formulate the model in terms of an ordered response model based on categorized durations. In order to allow for serial dependence in the duration process, the model is extended by an observation-driven ARMA dynamic based on generalized errors. As a result, the resulting semiparametric autoregressive conditional proportional intensity (ACPI) model allows to capture serial dependence in duration processes and to estimate conditional failure probabilities without requiring explicit distributional assumptions.
In autoregressive conditional intensity (ACI) models as introduced by Russell (1999), the intensity function is directly modeled in terms of an autoregressive process which is updated by past realizations of the integrated intensity. Let (t) = (1(t), . . . , K(t)) . Then, Russell (1999) proposes to specify k(t) in terms of a proportional intensity structure given by

k(t) = kN(t)+1k0(t)sk(t), k = 1, . . . K,

(34)

where N(t)+1 captures the dynamic structure, 0k(t) is a baseline intensity component capturing the (deterministic) evolution of the intensity between two consecutive points and sk(t) denotes a deterministic function of t capturing, for instance, possible seasonality ef-
fects. The function N(t) is indexed by the left-continuous counting function and is updated instantaneously after the arrival of a new point. Hence, i is constant for ti-1 < t  ti. Then, the evolution of the intensity function between two consecutive arrival times is governed by k0(t) and sk(t).
In order to ensure the non-negativity of the process, the dynamic component ki is specified in log-linear form, i.e.

ki = exp ~ ki + zi-1k ,

(35)

where zi denotes a vector of explanatory variables observed at arrival time ti and k the corresponding parameter vector. Define i as a (scalar) innovation term which is computed from the integrated intensity function associated with the most recently observed process,

i.e.


K
i = 1 -
k=1



tk
N

k

(ti

)

k(s; Fs)ds yik,

tk
N

k

(ti

)-1

(36)

where yik defines an indicator variable that takes the value 1 if the i-th point of the pooled process is of type k. Using the random time change argument presented in Section 2.4, i corresponds to a random mixture of i.i.d. centered standard exponential variates and thus

18

is itself an i.i.d. zero mean random variable. Then, the (K × 1) vector ~ i = ~ 1i , . . . , ~ Ki is parameterized as

K

~ i =

Aki-1 + Bk~ i-1 yik-1,

k=1

(37)

where Ak = {akj } denotes a (K ×1) innovation parameter vector and Bk = {bkij} is a (K ×K) matrix of persistence parameters. Hence, the fundamental principle of the ACI model is that

at each event ti all K processes are updated by the realization of the integrated intensity with respect to the most recent process, where the impact of the innovation on the K processes

can be different and also varies with the type of the most recent point. As suggested by

Bowsher (2006), an alternative specification of the ACI innovation term might be ~i =

1 - (ti-1, ti), where (ti-1, ti) :=

K k=1

k (ti-1 ,

ti)

denotes

the

integrated

intensity

of

the

pooled process computed between the two most recent points. Following the arguments

above, ~i is also a zero mean i.i.d. innovation term. Because of the regime-switching nature of the persistence matrix, the derivation of stationarity conditions is difficult. However, a sufficient (but not necessary) condition is that the eigenvalues of the matrices Bk for all

k = 1, . . . , K lie inside the unit circle. As proposed by Hautsch (2004), the baseline intensity function 0k(t) can be specified
as the product of K different Burr hazard rates, i.e.

k0 (t)

=

K
exp(k)
r=1

xr (t)prs -1 1 + rsxr(t)prs ,

(psr

>

0,

rs



0).

(38)

According to this specification k(t) is driven not only by the k-type backward recurrence

time but also by the time distance to the most recent point in all other processes r = 1, . . . , K with r = k. A special case occurs when psr = 1 and rs = 0,  r = s. Then, the k-th process is affected only by its own backward recurrence time.
Finally, sk(t) is typically specified as a spline function in order to capture intraday

seasonalities. A simple parameterization which is used in most studies is given by a linear

spline function of the form sk(t) = 1 +

S j=1

jk (t

-

j )

·

1l

{t>j },

where

j ,

j

=

1 . . . , S,

denote S nodes within a trading period and j the corresponding parameters. A more

flexible parameterization is e.g. given by a flexible Fourier form (Gallant (1981)) as used by

Andersen and Bollerslev (1998) or Gerhard and Hautsch (2002) among others. If K = 1 and 11 = 0, the ACI model and the ACD model coincide. Then, the ACI
model corresponds to a re-parameterized form of the Log-ACD model. If the ACI model is

extended to allow for time-varying covariates (see Hall and Hautsch (2007)), it generalizes

the approach by Hamilton and Jorda` (2002). In this case, all event times associated with

(discrete time) changes of time-varying covariates are treated as another point process that

19

is not explicitly modelled. Then, at each event time of the covariate process, the multivariate intensity process is updated, which requires a piecewise computation of the corresponding integrated intensities.
A generalization of the ACI model has been proposed by Bauwens and Hautsch (2006). The key idea is that the multivariate intensity function (t) = (1(t), . . . , K(t)) is driven not only by the observable history of the process but also by a common component. The latter may be considered as a way to capture the unobservable general information flow in a financial market. Such a setting turns out to be useful for the modelling of highdimensional point processes which are driven by an unobservable common random process. By assuming the existence of a common unobservable factor (t) following a pre-assigned structure in the spirit of a doubly stochastic Poisson process (see Section 2.3), we define the internal (unobservable) history of (t) as Ft. Then, we assume that (t) is adapted to the filtration Ft := (Fto  Ft), where Fto denotes some observable filtration. Then, the so-called stochastic conditional intensity (SCI) model is given by

k(t) = o,k(t) N (t)+1 k ,

(39)

where N(t)+1 := (tN(t)+1) denotes the common latent component which is updated at each point of the (pooled) process {ti}i{1,...,n}. The direction and magnitude of the process-specific impact of  is driven by the parameters k. The process-specific function o,k(t) := o,k(t|Fto) denotes a conditionally deterministic idiosyncratic k-type intensity component given the observable history, Fto.
Bauwens and Hautsch (2006) assume that i has left-continuous sample paths with
right-hand limits and in logarithm is the zero mean AR(1) process given by

ln i = a ln i-1 + ui ,

ui  i.i.d. N (0, 1).

(40)

Because of the symmetry of the distribution of ln i, Bauwens and Hautsch impose an identification assumption which restricts the sign of one of the scaling parameters k. The observation-driven component o,k(t) is specified in terms of an ACI parameterization as
described above. However, in contrast to the basic ACI model, in the SCI model, the
innovation term is computed based on the observable history of the process, i.e.

K
i = -
k=1

- ln o,k tkN k(ti)-1, tkN k(ti)

yik ,

(41)

20

where denotes Euler's constant, = 0.5772, and o,k tki-1, tki is given by

o,k tki-1, tki

N (tki )-1
:=

tj+1
o,k (u)du

j=N (tik-1) tj

= N(tki )-1 j -k k (tj , tj+1)

j=N (tik-1)

(42)

corresponding to the sum of (piecewise) integrated k-type intensities which are observed

through the duration spell and are standardized by the corresponding (scaled) realizations

of the latent component. This specification ensures that i can be computed exclusively based on past observables implying a distinct separation between the observation-driven

and the parameter-driven components of the model. Bauwens and Hautsch (2006) analyze

the probabilistic properties of the model and illustrate that the SCI model allows for a wide

range of (cross-)autocorrelation structures in multivariate point processes. In an application

to a multivariate process of price intensities, they find that the latent component captures

a substantial part of the cross-dependences between the individual processes resulting in a

quite parsimonious model. An extension of the SCI model to the case of multiple states

is proposed by Koopman, Lucas, and Monteiro (2005) and is applied to the modelling of

credit rating transitions.

4.3 Statistical Inference

Karr (1991) shows that valid statistical inference can be performed based on the intensity
function solely, see Theorem 5.2. in Karr (1991) or Bowsher (2006). Assume a K-variate point process N (t) = {N k(t)}kK=1 on (0, T ] with 0 < T < , and the existence of a Kvariate Ft-predictable process (t) that depends on the parameters . Then, it can be shown that a genuine log likelihood function is given by

K
ln L ; {N (t)}t(0,T ] =
k=1

T
(1 - k(s))ds +

ln k(s)dN k(s) ,

0 (0,T ]

which can be alternatively computed by

nK

ln L ; {N (t)}t(0,T ] =

(-k(ti-1, ti)) + yik ln k(ti) + T K.

i=1 k=1

(43)

Note that (43) differs from the standard log likelihood function of duration models by the

additive (integrating) constant T K which can be ignored for ML estimation. By applying the so-called exponential formula (Yashin and Arjas (1988)), the relation between the integrated intensity function and the conditional survivor function is given by

S(xi|Fti-1+xi ) = exp (-(ti-1, ti)) ,

(44)

21

which is the continuous counterpart to the well-known relation between the survivor function

and the hazard rate, S(xi) = exp(-

xi 0

h(u)du).

Hence, by ignoring the term T K, (43)

corresponds to the sum of the conditional survivor function and the conditional intensity

function. However, according to Yashin and Arjas (1988), the exponential formula (44)

is only valid if S(xi|Fti-1+xi) is absolutely continuous in xi, which excludes jumps of the conditional survivor function induced by changes of the information set during a spell.

Therefore, in a continuous, dynamic setting, the interpretation of exp (-(ti-1, ti)) as a

survivor function should be done with caution.

The evaluation of (43) for a Hawkes model is straightforward. In the case of an ex-

ponential decay function, the resulting log likelihood function can be even computed in a

recursive way (see e.g. Bowsher (2006)). An important advantage of Hawkes processes is

that the individual intensities k(t) do not have parameters in common and the parameter

vector can be expressed as  = 1, . . . , K , where k denotes the parameters associated

with the k-type intensity component. Given that the parameters are variation free, the

log likelihood function can be computed as ln L ; {N (t)}t(0,T ] =

K k=1

lk (k

)

and

can

be maximized by maximizing the individual k-type components lk(k) separately. This fa-

cilitates the estimation particularly when K is large. In contrast, ACI models require to

maximize the log likelihood function with respect to all the parameters jointly. This is due

to the fact that the ACI innovations are based on the integrated intensities which depend

on all individual parameters. The estimation of SCI models is computationally even more

demanding since the latent factor has to be integrated out resulting in a n-dimensional

integral. Bauwens and Hautsch (2006) suggest to evaluate the likelihood function numeri-

cally using the efficient importance sampling procedure introduced by Richard and Zhang

(2005). Regularity conditions for the maximum likelihood estimation of stationary simple

point processes are established by Ogata (1981). For more details, see Bowsher (2006).

Diagnostics for intensity based point process models can be performed by exploiting

the stochastic properties of compensators (see Bowsher (2006)) and integrated intensities

given in Section 2.4. The model goodness-of-fit can be straightforwardly evaluated through

the estimated integrated intensities of the K individual processes, eki,1 := ^ k(tki-1, tki ), the

integrated intensity of the pooled process ei,2 := ^ (ti-1, ti) =

K k=1

^ k

(ti-1,

ti

),

or

of

the

(non-centered) ACI residuals ei,3 :=

K k=1

^ k(tki-1, tik)

yik. Under correct model specifica-

tion, all three types of residuals must be i.i.d. standard exponential. Then, model evaluation

is done by testing the dynamic and distributional properties. The dynamic properties are

easily evaluated with Portmanteau statistics or tests against independence such as proposed

by Brock, Scheinkman, Scheinkman, and LeBaron (1996). The distributional properties can

be evaluated using Engle and Russell's (1998) test against excess dispersion (see Section

22

3.2). Other alternatives are goodness-of-fit tests based on the probability integral transform (PIT) as employed for diagnostics of ACD models by Bauwens, Giot, Grammig, and Veredas (2004).
4.4 Applications
For financial point processes, dynamic intensity models are primarily applied in multivariate frameworks or whenever a continuous-time setting is particularly required, like, for instance, in order to allow for time-varying covariates. One string of applications focusses on the modelling of trading intensities of different types of orders in limit order books. Hall and Hautsch (2007) apply a bivariate ACI model to study the intensities of buy and sell transactions in the electronic limit order book market of the Australian Stock Exchange (ASX). The buy and sell intensities are specified to depend on time-varying covariates capturing the state of the market. On the basis of the buy and sell intensities, denoted by B(t) and S(t), Hall and Hautsch (2007) propose a measure of the continuous net buy pressure defined by B(t) := ln B(t) - ln S(t). Because of the log-linear structure of the ACI model, the marginal change of B(t) induced by a change of the covariates is computed as B - S, where B and S denote the coefficients associated with covariates affecting the buy and sell intensity, respectively (see eq. (35)). Hall and Hautsch (2006) study the determinants of order aggressiveness and traders' order submission strategy at the ASX by applying a six-dimensional ACI model to study the arrival rates of aggressive market orders, limit orders as well as cancellations on both sides of the market. In a related paper, Large (2007) studies the resiliency of an electronic limit order book by modelling the processes of orders and cancellations on the London Stock Exchange using a ten-dimensional Hawkes process. Finally, Russell (1999) analyzes the dynamic interdependences between the supply and demand for liquidity by modelling transaction and limit order arrival times at the NYSE using a bivariate ACI model.
Another branch of the literature focusses on the modelling of the instantaneous price change volatility which is estimated on the basis of price durations, see (27) in Section 3.4. This relation is used by Bauwens and Hautsch (2006) to study the interdependence between instantaneous price change volatilities of several blue chip stocks traded at the New York Stock Exchange (NYSE) using a SCI model. In this setting, they find a strong evidence for the existence of a common latent component as a major driving force of the instantaneous volatilities on the market. In a different framework, Bowsher (2006) analyzes the two-way interaction of trades and quote changes using a two-dimensional generalized Hawkes process.
23

References
A¨it-Sahalia, Y. (1996): "Testing Continuous-Time Models of the Spot Interest Rate," Review of Financial Studies, 9, 385­426.
Andersen, T. G., and T. Bollerslev (1998): "Deutsche Mark-Dollar Volatility: Intraday Activity Patterns, Macroeconomic Announcements, and Longer Run Dependencies," Journal of Finance, 53, 219­265.
Bauwens, L., F. Galli, and P. Giot (2008): "The Moments of Log-ACD Models," Quantitative and Qualitative Analysis in Social Sciences, forthcoming.
Bauwens, L., and P. Giot (2000): "The Logarithmic ACD Model: An Application to the Bid/Ask Quote Process of two NYSE Stocks," Annales d'Economie et de Statistique, 60, 117­149.
(2001): Econometric Modelling of Stock Market Intraday Activity. Kluwer Academic Publishers, Boston, Dordrecht, London.
(2003): "Asymmetric ACD Models: Introducing Price Information in ACD Models with a Two State Transition Model," Empirical Economics, 28, 1­23.
Bauwens, L., P. Giot, J. Grammig, and D. Veredas (2004): "A Comparison of Financial Duration Models Via Density Forecasts," International Journal of Forecasting, 20, 589­609.
Bauwens, L., and N. Hautsch (2006): "Stochastic Conditional Intensity Processes," Journal of Financial Econometrics, 4, 450­493.
Bauwens, L., and D. Veredas (2004): "The Stochastic Conditional Duration Model: A Latent Factor Model for the Analysis of Financial Durations," Journal of Econometrics, 119, 381­412.
Bisie`re, C., and T. Kamionka (2000): "Timing of Orders, Order Aggressiveness and the Order Book at the Paris Bourse," Annales d'Economie et de Statistique, 60, 43­72.
Bollerslev, T. (1986): "Generalized Autoregressive Conditional Heteroskedasticity," Journal of Econometrics, 31, 307­327.
Bollerslev, T., and J. Wooldridge (1992): "Quasi-Maximum Likelihood Estimation and Inference in Dynamic Models with Time Varying Covariances," Econometric Reviews, 11, 143­172.
24

Bowsher, C. G. (2006): "Modelling Security Markets in Continuous Time: Intensity based, Multivariate Point Process Models," Journal of Econometrics, forthcoming.
Bre´maud, P., and L. Massoulie´ (1996): "Stability of Nonlinear Hawkes Processes," Annals of Probability, 24, 1563­1588.
Brock, W., W. Scheinkman, J. Scheinkman, and B. LeBaron (1996): "A Test for Independence Based on the Correlation Dimension," Econometric Reviews, 15, 197­235.
Brown, T. C., and M. G. Nair (1988): "A Simple Proof of the Multivariate Random Time Change Theorem for Point Processes," Journal of Applied Probability, 25, 210­214.
Cox, D. R. (1972): "Regression Models and Life Tables," Journal of the Royal Statistical Society, Series B, 34, 187­220.
(1975): "Partial Likelihood," Biometrika, 62, 269.
Daley, D., and D. Vere-Jones (2003): An Introduction to the Theory of Point Processes, vol. 1. Springer, New York.
De Luca, G., and G. Gallo (2004): "Mixture Processes for Financial Intradaily Durations," Studies in Nonlinear Dynamics and Econometrics, 8 (2), Downloadable under http://www.bepress.com/snde/vol8/iss2/art8.
(2006): "Time-Varying Mixing Weights in Mixture Autoregressive Conditional Duration Models," Manuscript, University of Florence.
De Luca, G., and P. Zuccolotto (2003): "Finite and Infinite Mixtures for Financial Durations," Metron, 61, 431­455.
Diebold, F. X., T. A. Gunther, and A. S. Tay (1998): "Evaluating Density Forecasts, with Applications to Financial Risk Management," International Economic Review, 39, 863­883.
Dionne, G., P. Duchesne, and M. Pacurara (2005): "Intraday Value at Risk (IVaR) Using Tick-by-Tick Data with Application to the Toronto Stock Exchange," Mimeo, HEC Montr´eal.
Drost, F. C., and B. J. M. Werker (2004): "Semiparametric Duration Models," Journal of Business and Economic Statistics, 22, 40­50.
Dufour, A., and R. F. Engle (2000): "The ACD Model: Predictability of the Time between Consecutive Trades," Working Paper, ISMA Centre, University of Reading.
25

Durbin, J., and S. Koopman (2004): "Monte Carlo Maximum Likelihood Estimation for Non-Gaussian State Space Models," Biometrika, 84, 669­684.
Easley, D., and M. O`Hara (1992): "Time and Process of Security Price Adjustment," The Journal of Finance, 47, 577­605.
Engle, R. F. (2000): "The Econometrics of Ultra-High-Frequency Data," Econometrica, 68, 1, 1­22.
(2002): "New Frontiers for ARCH Models," Journal of Applied Econometrics, 17, 425­446.
Engle, R. F., and A. Lunde (2003): "Trades and Quotes: A Bivariant Point Process," Journal of Financial Econometrics, 11, 159­188.
Engle, R. F., and V. K. Ng (1993): "Measuring and Testing the Impact of News on Volatility," Journal of Finance, 48, 1749­1778.
Engle, R. F., and J. R. Russell (1998): "Autoregressive Conditional Duration: A New Model for Irregularly Spaced Transaction Data," Econometrica, 66, 1127­1162.
(2005): Analysis of High Frequency Financial DataHandbook of Financial Econometrics. Yacine Ait-Sahalia and Lars Hansen (eds.), North-Holland.
Feng, D., G. J. Jiang, and P. X.-K. Song (2004): "Stochastic Conditional Duration Models with `Leverage Effect' for Financial Tranbsaction Data," Journal of Financial Econometrics, 2, 390­421.
Fernandes, M., and J. Grammig (2005): "Non-parametric Specification Tests for Conditional Duration Models," Journal of Econometrics, 127, 35­68.
Fernandes, M., and J. Grammig (2006): "A Family of Autoregressive Conditional Duration Models," Journal of Econometrics, 130, 1­23.
Gallant, R. A. (1981): "On the Bias in Flexible Functional Forms and an Essential Unbiased Form: The Fourier Flexible Form," Journal of Econometrics, 15, 211­245.
Gerhard, F., and N. Hautsch (2002): "Volatility Estimation on the Basis of Price Intensities," Journal of Empirical Finance, 9, 57­89.
(2007): "A Dynamic Semiparametric Proportional Hazard Model," Studies in Nonlinear Dynamics and Econometrics, 11(2), Downloadable under http://www.bepress.com/snde/vol11/iss2/art1.
26

Ghysels, E., C. Gourieroux, and J. Jasiak (2004): "Stochastic Volatility Duration Models," Journal of Econometrics, 119, 413­433.
Ghysels, E., and J. Jasiak (1998): "GARCH for Irregularly Spaced Financial Data: The ACD-GARCH Model," Studies in Nonlinear Dynamics and Econometrics, 2, 133­149.
Giesecke, K., and P. Tomecek (2005): "Dependent Events and Changes of Time," Working Paper, Cornell University.
Giot, P. (2005): "Market Risk Models for Intraday Data," European Journal of Finance, 11, 187­212.
Grammig, J., and M. Wellner (2002): "Modeling the Interdependence of Volatility and Inter-Transaction Duration Process," Journal of Econometrics, 106, 369­400.
Hafner, C. M. (2005): "Durations, Volume and the Prediction of Financial Returns in Transaction Time," Quantitative Finance, 5.
Hall, A. D., and N. Hautsch (2006): "Order Aggressiveness and Order Book Dynamics," Empirical Economics, 30, 973­1005.
(2007): "Modelling the Buy and Sell Intensity in a Limit Order Book Market," Journal of Financial Markets, 10, 249­286.
Hamilton, J. D., and O. Jorda` (2002): "A Model of the Federal Funds Rate Target," Journal of Political Economy, 110, 1135­1167.
Han, A., and J. A. Hausman (1990): "Flexible Parametric Estimation of Duration and Competing Risk Models," Journal of Applied Econometrics, 5, 1­28.
Hasbrouck, J. (1991): "Measuring the Information Content of Stock Trades," Journal of Finance, 46, 179­207.
Hautsch, N. (2004): Modelling Irregularly Spaced Financial Data. Springer, Berlin.
(2006): "Testing the Conditional Mean Function of Autoregressive Conditional Duration Models," Working Paper, Department of Economics, University of Copenhagen.
Hawkes, A. G. (1971): "Spectra of Some Self-Exciting and Mutually Exciting Point Processes," Biometrika, 58, 83­90.
Hawkes, A. G., and D. Oakes (1974): "A Cluster Process Representation of a SelfExciting Process," Journal of Applied Probability, 11, 493­503.
27

Heinen, A., and E. Rengifo (2003): "Multivariate Autoregressive Modelling of Time Series Count Data Using Copulas," Revision of CORE Discussion Paper 2003/25, forthcoming in Journal of Empirical Finance.
Hujer, R., and S. Vuletic (2005): "Econometric Analysis of Financial Trade Processes by Discrete Mixture Duration Models," Available at http://ssrn.com/abstract=766664.
Hujer, R., S. Vuletic, and S. Kokot (2002): "The Markov Switching ACD Model," Finance and Accounting Working Paper 90, Johann Wofgang Goethe-University, Frankfurt. Available at SSRN: http://ssrn.com/abstract=332381.
Jasiak, J. (1998): "Persistence in Intratrade Durations," Finance, 19, 166­195.
Kalbfleisch, J. D., and R. L. Prentice (1980): The Statistical Analysis of Failure Time Data. Wiley.
Karr, A. F. (1991): Point Processes and their Statistical Inference. Dekker, New York.
Kiefer, N. M. (1988): "Economic Duration Data and Hazard Functions," Journal of Economic Literature, 26, 646­679.
Knight, J., and C. Ning (2005): "Estimation of the Stochastic Conditional Duration Model via Alternative Methods ­ ECF and GMM," Mimeo, University of Western Ontario.
Koopman, S. J., A. Lucas, and A. Monteiro (2005): "The Multi-State Latent Factor Intensity Model for Credit Rating Transitions," Discussion Paper TI2005-071/4, Tinbergen Institute.
Koulikov, D. (2002): "Modeling Sequences of Long Memory Positive Weakly Stationary Random Variables," Discussion Paper 493, William Davidson Institute, University of Michigan Business School.
Lancaster, T. (1997): The Econometric Analysis of Transition Data. Cambridge University Press.
Large, J. (2007): "Measuring the Resiliency of an Electronic Limit Order Book," Journal of Financial Markets, 10, 1­25.
Liesenfeld, R., I. Nolte, and W. Pohlmeier (2006): "Modelling Financial Transaction Price Movements: a Dynamic Integer Count Model," Empirical Economics, 30, 795­825.
Meddahi, N., E. Renault, and B. J. Werker (2006): "GARCH and Irregularly Spaced Data," Economics Letters, 90, 200­204.
28

Meitz, M., and P. Saikkonen (2004): "Ergodicity, mixing, and existence of moments of a class of Markov models with applications to GARCH and ACD models," SSE/EFI Working Paper Series in Economics and Finance No. 573, Stockholm School of Economics.
Meitz, M., and T. Tera¨svirta (2006): "Evaluating Models of Autoregressive Conditional Duration," Journal of Business & Economic Statistics, 24, 104­124.
Meyer, P. A. (1971): "D´emonstration simplifi´ee d'un th´eor`eme Knight," in Lecture Notes in Mathematics, vol. 191, pp. 191­195. Springer.
Møller, J., and J. Rasmussen (2004): "Perfect Simulation of Hawkes Processes," Working Paper, Aalborg University.
Nelson, D. (1991): "Conditional Heteroskedasticity in Asset Returns: A New Approach," Journal of Econometrics, 43, 227­251.
Ogata, Y. (1981): "On Lewis' Simulation Method for Point Processes," IEEE Transactions of Information Theory, IT-27, 23­31.
Ogata, Y. (1988): "Statistical Models for Earthquake Occurrences and Residual Analysis for Point Processes," Journal of the American Statistical Association, 83, 9­27.
Prigent, J., O. Renault, and O. Scaillet (2001): An Autoregressive Conditional Binomial Option Pricing Model Selected Papers from the First World Congress of the Bachelier Finance Society. Geman and Madan and Pliska and Vorst (eds), Springer Verlag, Heidelberg.
Quoreshi, A. S. (2006): "Long Memory, Count Data, Time Series Modelling for Financial Application," Umea Economic Studies 673, Department of Economics, Umea University.
Richard, J.-F., and W. Zhang (2005): "Efficient High-Dimensional Importance Sampling," Working Paper Pittsburgh University.
Russell, J. R. (1999): "Econometric Modeling of Multivariate Irregularly-Spaced HighFrequency Data," Working Paper, University of Chicago.
Russell, J. R., and R. F. Engle (2005): "A Discrete-State Continuous-Time Model of Financial Transactions Prices and Times: The Autoregressive Conditional Multinomial Autoregressive Conditional Duration Model," Journal of Business and Economic Statistics, 23, 166­180.
Rydberg, T. H., and N. Shephard (2003): "Dynamics of Trade-by-Trade Price Movements: Decomposition and Models," Journal of Financial Econometrics, 1, 2­25.
29

Strickland, C. M., C. S. Forbes, and G. M. Martin (2003): "Bayesian Analysis of the Stochastic Conditional Duration Model," Monash Econometrics and Business Statistics Working Paper 14/03, Monash University.
Taylor, S. J. (1986): Modelling Financial Time Series. Wiley, New York. Vere-Jones, D. (1970): "Stochastic Models for Earthquake Occurrence," Journal of the
Royal Statistical Society, Series B, 32, 1­62. Vere-Jones, D., and T. Ozaki (1982): "Some Examples of Statistical Inference Applied
to Earthquake Data," Annals of the Institute of Statistical Mathematics, 34, 189­207. Veredas, D., J. Rodriguez-Poo, and A. Espasa (2002): "On the (Intradaily) Sea-
sonality, Dynamics and Durations Zero of a Financial Point Process," CORE Discussion Paper 2002/23, Louvain-La-Neuve. Yashin, A., and E. Arjas (1988): "A Note on Random Intensities and Conditional Survival Functions," Journal of Applied Probability, 25, 630­635. Zhang, M. Y., J. Russell, and R. S. Tsay (2001): "A Nonlinear Autoregressive Conditional Duration Model with Applications to Financial Transaction Data," Journal of Econometrics, 104, 179­207.
30

SFB 649 Discussion Paper Series 2007
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Trade Liberalisation, Process and Product Innovation, and Relative Skill Demand" by Sebastian Braun, January 2007.
002 "Robust Risk Management. Accounting for Nonstationarity and Heavy Tails" by Ying Chen and Vladimir Spokoiny, January 2007.
003 "Explaining Asset Prices with External Habits and Wage Rigidities in a DSGE Model." by Harald Uhlig, January 2007.
004 "Volatility and Causality in Asia Pacific Financial Markets" by Enzo Weber, January 2007.
005 "Quantile Sieve Estimates For Time Series" by Jürgen Franke, JeanPierre Stockis and Joseph Tadjuidje, February 2007.
006 "Real Origins of the Great Depression: Monopolistic Competition, Union Power, and the American Business Cycle in the 1920s" by Monique Ebell and Albrecht Ritschl, February 2007.
007 "Rules, Discretion or Reputation? Monetary Policies and the Efficiency of Financial Markets in Germany, 14th to 16th Centuries" by Oliver Volckart, February 2007.
008 "Sectoral Transformation, Turbulence, and Labour Market Dynamics in Germany" by Ronald Bachmann and Michael C. Burda, February 2007.
009 "Union Wage Compression in a Right-to-Manage Model" by Thorsten Vogel, February 2007.
010 "On -additive robust representation of convex risk measures for unbounded financial positions in the presence of uncertainty about the market model" by Volker Krätschmer, March 2007.
011 "Media Coverage and Macroeconomic Information Processing" by Alexandra Niessen, March 2007.
012 "Are Correlations Constant Over Time? Application of the CC-TRIGt-test to Return Series from Different Asset Classes." by Matthias Fischer, March 2007.
013 "Uncertain Paternity, Mating Market Failure, and the Institution of Marriage" by Dirk Bethmann and Michael Kvasnicka, March 2007.
014 "What Happened to the Transatlantic Capital Market Relations?" by Enzo Weber, March 2007.
015 "Who Leads Financial Markets?" by Enzo Weber, April 2007. 016 "Fiscal Policy Rules in Practice" by Andreas Thams, April 2007. 017 "Empirical Pricing Kernels and Investor Preferences" by Kai Detlefsen,
Wolfgang Härdle and Rouslan Moro, April 2007. 018 "Simultaneous Causality in International Trade" by Enzo Weber, April
2007. 019 "Regional and Outward Economic Integration in South-East Asia" by
Enzo Weber, April 2007. 020 "Computational Statistics and Data Visualization" by Antony Unwin,
Chun-houh Chen and Wolfgang Härdle, April 2007. 021 "Ideology Without Ideologists" by Lydia Mechtenberg, April 2007. 022 "A Generalized ARFIMA Process with Markov-Switching Fractional
Differencing Parameter" by Wen-Jen Tsay and Wolfgang Härdle, April 2007.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "Time Series Modelling with Semiparametric Factor Dynamics" by Szymon Borak, Wolfgang Härdle, Enno Mammen and Byeong U. Park, April 2007.
024 "From Animal Baits to Investors' Preference: Estimating and Demixing of the Weight Function in Semiparametric Models for Biased Samples" by Ya'acov Ritov and Wolfgang Härdle, May 2007.
025 "Statistics of Risk Aversion" by Enzo Giacomini and Wolfgang Härdle, May 2007.
026 "Robust Optimal Control for a Consumption-Investment Problem" by Alexander Schied, May 2007.
027 "Long Memory Persistence in the Factor of Implied Volatility Dynamics" by Wolfgang Härdle and Julius Mungo, May 2007.
028 "Macroeconomic Policy in a Heterogeneous Monetary Union" by Oliver Grimm and Stefan Ried, May 2007.
029 "Comparison of Panel Cointegration Tests" by Deniz Dilan Karaman Örsal, May 2007.
030 "Robust Maximization of Consumption with Logarithmic Utility" by Daniel Hernández-Hernández and Alexander Schied, May 2007.
031 "Using Wiki to Build an E-learning System in Statistics in Arabic Language" by Taleb Ahmad, Wolfgang Härdle and Sigbert Klinke, May 2007.
032 "Visualization of Competitive Market Structure by Means of Choice Data" by Werner Kunz, May 2007.
033 "Does International Outsourcing Depress Union Wages? by Sebastian Braun and Juliane Scheffel, May 2007.
034 "A Note on the Effect of Outsourcing on Union Wages" by Sebastian Braun and Juliane Scheffel, May 2007.
035 "Estimating Probabilities of Default With Support Vector Machines" by Wolfgang Härdle, Rouslan Moro and Dorothea Schäfer, June 2007.
036 "Yxilon ­ A Client/Server Based Statistical Environment" by Wolfgang Härdle, Sigbert Klinke and Uwe Ziegenhagen, June 2007.
037 "Calibrating CAT Bonds for Mexican Earthquakes" by Wolfgang Härdle and Brenda López Cabrera, June 2007.
038 "Economic Integration and the Foreign Exchange" by Enzo Weber, June 2007.
039 "Tracking Down the Business Cycle: A Dynamic Factor Model For Germany 1820-1913" by Samad Sarferaz and Martin Uebele, June 2007.
040 "Optimal Policy Under Model Uncertainty: A Structural-Bayesian Estimation Approach" by Alexander Kriwoluzky and Christian Stoltenberg, July 2007.
041 "QuantNet ­ A Database-Driven Online Repository of Scientific Information" by Anton Andriyashin and Wolfgang Härdle, July 2007.
042 "Exchange Rate Uncertainty and Trade Growth - A Comparison of Linear and Nonlinear (Forecasting) Models" by Helmut Herwartz and Henning Weber, July 2007.
043 "How do Rating Agencies Score in Predicting Firm Performance" by Gunter Löffler and Peter N. Posch, August 2007.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

044 "Ein Vergleich des binären Logit-Modells mit künstlichen neuronalen Netzen zur Insolvenzprognose anhand relativer Bilanzkennzahlen" by Ronald Franken, August 2007.
045 "Promotion Tournaments and Individual Performance Pay" by Anja Schöttner and Veikko Thiele, August 2007.
046 "Estimation with the Nested Logit Model: Specifications and Software Particularities" by Nadja Silberhorn, Yasemin Boztu and Lutz Hildebrandt, August 2007.
047 "Risiken infolge von Technologie-Outsourcing?" by Michael Stephan, August 2007.
048 "Sensitivities for Bermudan Options by Regression Methods" by Denis Belomestny, Grigori Milstein and John Schoenmakers, August 2007.
049 "Occupational Choice and the Spirit of Capitalism" by Matthias Doepke and Fabrizio Zilibotti, August 2007.
050 "On the Utility of E-Learning in Statistics" by Wolfgang Härdle, Sigbert Klinke and Uwe Ziegenhagen, August 2007.
051 "Mergers & Acquisitions and Innovation Performance in the Telecommunications Equipment Industry" by Tseveen Gantumur and Andreas Stephan, August 2007.
052 "Capturing Common Components in High-Frequency Financial Time Series: A Multivariate Stochastic Multiplicative Error Model" by Nikolaus Hautsch, September 2007.
053 "World War II, Missing Men, and Out-of-wedlock Childbearing" by Michael Kvasnicka and Dirk Bethmann, September 2007.
054 "The Drivers and Implications of Business Divestiture ­ An Application and Extension of Prior Findings" by Carolin Decker, September 2007.
055 "Why Managers Hold Shares of Their Firms: An Empirical Analysis" by Ulf von Lilienfeld-Toal and Stefan Ruenzi, September 2007.
056 "Auswirkungen der IFRS-Umstellung auf die Risikoprämie von Unternehmensanleihen - Eine empirische Studie für Deutschland, Österreich und die Schweiz" by Kerstin Kiefer and Philipp Schorn, September 2007.
057 "Conditional Complexity of Compression for Authorship Attribution" by Mikhail B. Malyutov, Chammi I. Wickramasinghe and Sufeng Li, September 2007.
058 "Total Work, Gender and Social Norms" by Michael Burda, Daniel S. Hamermesh and Philippe Weil, September 2007.
059 "Long-Term Orientation in Family and Non-Family Firms: a Bayesian Analysis" by Jörn Hendrich Block and Andreas Thams, October 2007
060 "Kombinierte Liquiditäts- und Solvenzkennzahlen und ein darauf basierendes Insolvenzprognosemodell für deutsche GmbHs" by Volodymyr Perederiy, October 2007
061 "Embedding R in the Mediawiki" by Sigbert Klinke and Olga ZlatkinTroitschanskaia, October 2007
062 "Das Hybride Wahlmodell und seine Anwendung im Marketing" by Till Dannewald, Henning Kreis and Nadja Silberhorn, November 2007
063 "Determinants of the Acquisition of Smaller Firms by Larger Incumbents in High-Tech Industries: Are they related to Innovation and Technology Sourcing? " by Marcus Wagner, November 2007
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

064 "Correlation vs. Causality in Stock Market Comovement" by Enzo Weber, October 2007
065 "Integrating latent variables in discrete choice models ­ How higherorder values and attitudes determine consumer choice" by Dirk Temme, Marcel Paulssen and Till Dannewald, December 2007
066 "Modelling Financial High Frequency Data Using Point Processes" by Luc Bauwens and Nikolaus Hautsch, November 2007
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

