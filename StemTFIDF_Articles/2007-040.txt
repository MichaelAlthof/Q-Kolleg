BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2007-040
Optimal Policy Under Model Uncertainty:
A Structural-Bayesian Estimation Approach
Alexander Kriwoluzky* Christian Stoltenberg*
* Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

OPTIMAL POLICY UNDER MODEL UNCERTAINTY: A STRUCTURAL-BAYESIAN ESTIMATION APPROACH
Alexander Kriwoluzky,Christian Stoltenberg July 9, 2007
Abstract In this paper we propose a novel methodology to analyze optimal policies under model uncertainty in micro-founded macroeconomic models. As an application we assess the relevant sources of uncertainty for the optimal conduct of monetary policy within (parameter uncertainty) and across models (specification uncertainty) using EU 13 data. Parameter uncertainty matters only if the zero bound on interest rates is explicitly taken into account. In any case, optimal monetary policy is highly sensitive with respect to specification uncertainty implying substantial welfare gains of a robustly-optimal rule that incorporates this risk. JEL classification: E32, C51, E52. Keywords: Optimal monetary policy, model uncertainty, Bayesian model estimation.
We are especially thankful to Harald Uhlig, Noah Williams, Patrick Kehoe, Ellen McGrattan and seminar participants at Humboldt-University Berlin. This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk". Alexander Kriwoluzky thanks the Deka Bank and the German Academic Exchange Service (DAAD) for funding.
Humboldt University Berlin, Department of Economics, D-10178 Berlin, Germany, email: kriwoluz@wiwi.hu-berlin.de, fax: +49 30 2093-5934, tel: +49/30/2093-5935.
Humboldt University Berlin, Department of Economics, D-10178 Berlin, Germany, email: stoltenb@wiwi.hu-berlin.de, fax: +49 30 2093-5934, tel: +49/30/2093-5935.
1

I have myself said several times that the Governing Council of the ECB has no intention of being the "prisoner" of a single system of equations. We both (Greenspan and Trichet) highly praise "robustness". There is no substitute for a comprehensive analysis of the risks to price stability that pays due attention to all relevant information. Jean-Claude Trichet, President of the ECB, 2005
Introduction
How should optimal policy be conducted if the policy maker is uncertain about the economic environment? Following Brainard (1967), numerous researchers consider theoretically the performance of (monetary) policy under parameter uncertainty and across various macroeconomic models.1 Not surprisingly, the policy recommendations are sensitive with respect to assumptions about the relevant model uncertainty. For example, optimal monetary policy can be either cautious or more aggressive compared to the case where the true model is known. On the other hand the empirical evaluation of micro-founded DSGE models employing Bayesian methods has made substantial progress (Smets and Wouters, 2003 and An and Schorfheide, 2006). Combining these lines of research we propose a novel methodology to analyze the optimal conduct of policy under various sources of model uncertainty for a wide range of micro-founded macroeconomic models.
In our framework we capture the optimal conduct of policy if the policy maker faces uncertainty about the true parameters of a model (parameter uncertainty), if the true model is not known (specification uncertainty) or a combination of both. We can analyze specification uncertainty in various ways: Omitted variables, additional lags in endogenous variables or shock processes and autocorrelation of error terms. We do not take a stand on the true data generating process.2 On the contrary, we assume that after some process of theorizing and data analysis the policy maker has arrived at a set of competing models. Our approach involves two steps. First, we estimate these models by Bayesian model estimation techniques to explain a set of macroeconomic time series. Then, the relevant source of parameter uncertainty is described by the joint posterior distribution
1For example McCallum (1998), Soderstrom (2002), Onatski and Stock (2002), Hansen and Sargent (2003) or more recently Küster and Wieland (2005).
2See e.g. Sargent (1999) for an analysis of that case for the US economy.
2

of the structural parameters and specification uncertainty is assessed by the marginal density of each model. Second, we determine the optimal policy under model uncertainty by maximizing the unconditional expectation of representative household's utility for each model. Notably, due to our strictly micro-founded approach this welfare measure and not only the structural equations are affected by moving along the parameter and/or the model space. The policy rule that performs best with respect to both sources of uncertainty is called the robustly-optimal rule.
We apply our method to determine optimal monetary policy under model uncertainty. Our benchmark model is a standard cashless New Keynesian economy with staggered price setting without indexation (Woodford, 2003). As examples for specification uncertainty we subsequently allow for more lags in endogenous variables (indexation and habit formation), for omitted variables (money) and for one large model that nests all features. While the cashless models are characterized by price stability as predominant principle, a demand for cash introduces the stabilization of the nominal interest rate as a conflicting aim. Estimating theses models using EU 13 data this bottom-up approach allows us to evaluate quantitatively the gain in explanatory power of each extension separately. I.e. ex post we can judge how reasonable the original choice of model features was. This approach marks a difference to the related literature that starts with a certain model (see Levin, Onatski, Williams and Williams, 2005 or Smets and Wouters, 2003) and then proceeds by including some extensions without quantifying their importance.
We differ from the existing literature in one further important aspect. Throughout our analysis we consider two cases: either policy faces an additional constraint that requires interest policy to be consistent with the zero bound (implementability) or not. We approximate the lower bound constraint in the following way. Choosing price stability as approximation point, we require the difference between the real interest rate and the zero bound to represent at least 2 standard deviations.
Our main findings can be summarized as follows:
· We find that the model that incorporates all features is not the best model to explain the data. Thus, setting up a large model cannot be recommended from a Bayesian-econometric perspective.
3

· If optimal policies are subject to the implementability constraint, then this induces sizeable welfare losses compared to the unconstrained case in models that are characterized by price stability as the predominant stabilization principle. Meeting this requirement is shown to be very sensitive with respect to uncertainty about the true parameters of each model.
· If policy is not constrained by implementability, optimal policies under parameter uncertainty and certainty are remarkably similar. In particular, the welfare losses that result from pursuing the latter instead of the former are neglectable.
· In any case, uncertainty about the true structure leads to high welfare losses over the model space. Pursuing the optimal policy from the model that incorporates all features cannot be recommended as a policy device. On the contrary, the optimal policy rule that is robust to specification uncertainty performs significantly better across models than any optimal rule of a particular model.
· We compute and recommend an optimal policy that is aware of both sources of uncertainty. Though different from a minmax strategy, it evades both high losses and violations of the zero bound over the model and parameter space.
Related Literature Our paper is related to Levin, Onatski, Williams and Williams (LOWW, 2005). They estimate a medium-scale New Keynesian Model with staggered price setting using US data and determine the optimal monetary policy in that model across the posterior distribution of the estimated parameters. Their optimal simple interest rate feedback rule is shown to be robust with respect to parameter uncertainty in structural parameters including the coefficients of the shock processes. However, as a side aspect, they find the optimal rule not to be robust to different extensions of the model including a demand for cash and different price and wage setting algorithms. The main differences to our approach are first that our estimation strategy allows us to quantify the importance of each model component for explaining the data and for the optimal conduct of monetary policy separately. As our main novel feature we analyze optimal policy across models using the posterior odds of all models under consideration. I.e. we give policy recommendations
4

in the presence of specification uncertainty. Additionally, we analyze the welfare effects of the zero bound restriction on interest rates under model uncertainty.
The remainder of the paper is organized as follows. In the next section we present our general framework to analyze the optimal conduct of policy in a broad range of micro-founded macroeconomic models. In the following sections we apply our methodology to optimal monetary policy under model uncertainty. The last section concludes.
1 Analyzing optimal policy under model uncertainty
In this section we describe the general framework and our novel methodology to analyze the optimal conduct of policy if the decision maker faces model uncertainty in micro-founded macroeconomic models. We assume that after some process of theorizing and data analysis the policy maker has arrived at a set of competing models. We propose to estimate these models by Bayesian model estimation techniques to explain a set of macroeconomic time series. Then the relevant source of parameter uncertainty is described by the joint posterior distribution of the structural parameters. The relevant degree of specification uncertainty enters our analysis by the marginal density of each model. After setting up the general framework, we show how to derive optimal simple feedback rules that are robust with respect to parameter uncertainty, specification uncertainty and a combination of both.
1.1 General framework
Consider a system of linear equations that represent log-linear approximations to the non-linear equilibrium conditions under rational expectations around a deterministic steady state of a particular model i . Let xt be the vector of state variables, zt the vector of structural shocks and yt the vector of observable variables. Furthermore, based on Bayesian model estimation techniques, let  denote the random vector of deep parameters and  a particular realization from the joint posterior distribution.3 Policy influences the equilibrium outcome by simple feedback rules. The link
3An and Schorfheide (2006) give an overview about Bayesian estimation procedures in DSGE models.
5

between the set of policy instruments as a subset of x is characterized by the vector of constant policy coefficients . I.e. per definition we consider steady state invariant policies. The state space form of the fundamental solution of model i is given by4:

xt = T (i , )xt-1 + R(i , )zt

(1)

yt = Gxt,

(2)

where T (i , ) and R(i , ) are matrices one obtains after solving a DSGE model with standard solution techniques. However, the entries of the matrixes T and R may differ across models since  varies. The benchmark model is the model that restricts some entries of  and correspondingly T to zero. Perturbations of the benchmark model are defined as dissolving zero restrictions. Suppose one perturbation of a benchmark model is characterized by converting the forward-looking variable consumption from the benchmark model into a history dependent variable by assuming habit persistence. Then, the column of T that is associated with lagged consumption has zeros in the benchmark model but at least one non-zero entry in the perturbation. In addition, the vector  exhibits a non-zero value for the habit parameter in the perturbation but a zero value in the benchmark model. This formulation implies that all perturbations and the benchmark are nested in the largest model. The matrix G is a picking matrix that equates observable and state variables. Using this equation and the solution given by (1) we estimate model i using data Y with Bayesian model estimation techniques.
Given a certain realization of i , we assess the performance of a particular policy  with two different criteria: A welfare and an implementability measure. As welfare measure we are using a particular quadratic loss function L(i , x) which represents the unconditional expectation of the steady state invariant part that belongs to a valid second order approximation to representative
4xt denotes the percentage deviation of the generic variable xt from a deterministic steady state x¯ chosen as approximation point.

6

households' utility in model i .5

E

 t =t0

tU (xt , i

)



U (x¯, i 1-

)

-

E

 t =t0

t

A(i

)xt

xt

=

U (x¯, i ) - L(i 1-

,

x)

(3)

We assume that the policy maker can credibly commit to a policy rule . Thus, the standard task of optimal policy is to find a particular steady state invariant policy i , such that the implied solution for xt minimizes the loss of the representative agent subject to (1). In any case, we only consider fundamental solutions and require the set of equilibrium sequences to be locally stable and unique. We now turn to the problem how to determine optimal policy under parameter and specification uncertainty.

1.2 Optimal policy under parameter uncertainty
We treat the mean vector E [i ] of the joint posterior distribution as the true value for the deep parameters of model i . Thus, we think of the joint posterior distribution in model i , f (i ), as the relevant uncertainty that a policy maker faces when he makes his decision about . The optimal policy problem under parameter uncertainty given the posterior distribution for  in model i can be stated as follows:

min


E

i

L

(i

,

x

)

=

min


L(i , x) f (i )d i
i

s.t . xt = T (i , )xt-1 + R(i , )zt , i .

(4)

Under parameter uncertainty the policy maker has to average the loss over all possible realization
of  with positive probability to find the optimal vector of constant policy coefficients in model i ,
i pu .6
5Benigno and Woodford show that a purely quadratic micro-founded loss function can be derived for a wide range of models (2006b). Applications of their method include monetary economics (2005) and the classical Ramsey optimal taxation problem (2006a).
6Note that we do not consider the perspective of a policy maker that uses Bayesian learning to update his beliefs about a model's parameters or the model itself (see e.g. Cogley and Sargent (2005)).
7

If i pu differs from i and pursuing the latter rather than the former leads to high average welfare losses across the parameter space, then parameter uncertainty matters for the optimal conduct of policy even if the true model is known.

1.3 Specification uncertainty
While under parameter uncertainty the policy maker believes that the given model is the true model, under specification uncertainty he has to deal with a situation where the parameter vector is known to him but the true model is not. We consider and estimate a discrete set of models M = {M1, ..., Mn}, where each is characterized by the number and position of zero restrictions in the vectors for deep parameters and variables. By subsequently adding zero restrictions to the largest model Mn, one finally arrives at the benchmark model M1. Since we estimate all models separately using the same data and the same number of shocks we can reasonably calculate marginal data densities p(Y |Mi ) and posterior odds to assess the probability that model i is the true model. The latter are defined as:

i =

0i p(Y

n j =1

0

j

p

|Mi ) (Y |M

j

)

,

(5)

where 0i denotes the prior probability for each model. Note that this formulation is not restrictive.
It captures the possibility of different assumptions about the policy maker's own beliefs ­ unless he
assigns equal prior weights for all models: Avoiding the worst outcome in a particular model i
(without ignoring the information how likely or unlikely this is) corresponds to setting a high prior probability for that model. 7
Suppose that the policy maker knows the exact values for all parameters, E [i ], but is uncertain 7We employ Geweke's(1999) harmonic mean estimator to compute the data likelihood in a certain model that takes into account both, how well the model fits the data and how many parameters are used to achieve this.

8

about the true model. Then the optimal policy problem can be formulated in the following way:

min


EM

L(E

[i

],

x

)

=

1L(E

[1],

x

)

+

...

+

n

L(E

[n

],

x

)

s.t . xt = T (E [i ], )xt-1 + R(E [i ], )zt , i = 1, ..., n,

(6)

where

n i =1

i

=

1.

The gains of this optimal policy rule su can be large if the optimal policy

prescriptions in the set of models 1 , ..., n differ substantially due to the presence of different and

possibly conflicting stabilization aims. Therefore the inclusion of a micro-founded loss function

in each model instead of using a single arbitrary quadratic function plays an important role to

account adequately for these differences across models.

1.4 Parameter and specification uncertainty
The more realistic case is that the decision maker does not know exactly neither the true model nor the exact values of all deep parameters. Uncertainty is then composed of parameter and specification uncertainty jointly. Then the optimal policy problem to determine the overall robustlyoptimal policy rule can be stated as follows:

min


E

M

E

L

(i

,

x

)

=

1

E

1

L

(1

,

x

t

)

+

...

+

n

E

n

L

(n

,

xt

)

s.t . xt = T (i , )xt-1 + R(i , )zt , i , i = 1, ..., n.

(7)

Comparing this optimal device for policy psu with the results of (6) allows us to examine the role of specification and parameter uncertainty jointly: should policy makers pay special attention to the interaction of these sources of uncertainty?
Throughout the paper we express the resulting business cycle costs as the percentage loss in certainty (steady state) equivalent consumption. First we compute the loss of a certain policy ~ given a particular parameter vector ~ in model i to derive overall utility:
U (c(~i ), x\c (~i ), ~i ) - L(~i , ~),

9

where the first term is steady state utility and x\c denotes the variables vector excluding consumption. Since we want to express utility as reduction in certainty consumption equivalents we set this

expression equal to:

Ui (c(~i )  (1 - ), x\c (~i ), ~i )

and solve for in percentage terms. Under parameter uncertainty this results in a distribution for (~i , ~) over i . Taking the expectation of this expression yields a measure for average losses in certainty consumption equivalents under a particular policy ~. In the absence of parameter uncertainty we set ~i = E [i ].
To apply our methodology we now analyze the optimal conduct of monetary policy under
model uncertainty.

2 Monetary policy: the economic environment
In this section we describe briefly the benchmark economy, a plain-vanilla cashless new keynesian economy (Woodford, 2003) and some reasonable deviations from this simple environment. Concretely, we allow for two different types of specification uncertainty: the case of omitted state variables or missing lags, habit formation and indexation, and the situation where one variable is not considered at all. To analyze that case we introduce a transaction friction that induces a demand for cash. In the last subsection we describe how we encounter the issue of the zero bound requirement for interest rates.
2.1 The benchmark economy
The benchmark economy consists of a continuum of infinitely lived households indexed with j  [0, 1]. It is assumed that households have identical initial asset endowments and identical preferences. Household j acts as a monopolistic supplier of labor services l j . Lower (upper) case letters denote real (nominal) variables. At the beginning of period t , households' financial wealth comprises a portfolio of state contingent claims on other households yielding a (random) payment Z j t ,
10

and one period nominally non-state contingent government bonds B j t-1 carried over from the previous period. Assuming complete financial markets let qt,t+1 denote the period t price of one unit of currency in a particular state of period t + 1 normalized by the probability of occurrence of that state, conditional on the information available in period t . Then, the price of a random payoff Zt+1 in period t + 1 is given by Et [qt,t+1Z j t+1]. The budget constraint of the representative household reads

1
B j t + Et [qt,t+1 Z j t+1] + Pt c j t  Rt-1B j t-1 + Z j t + Pt w j t l j t + D j i t d i - Pt Tt ,
0

(8)

where ct denotes a Dixit-Stiglitz aggregate of consumption with elasticity of substitution , Pt the aggregate price level, w j t the real wage rate for labor services l j t of type j , Tt a lump-sum tax, Rt the gross nominal interest rate on government bonds, and Di t dividends of monopolistically competitive firms. Further, households have to fulfill the no-Ponzi game condition, limi Et qt,t+i (B j t+i + Z j t+1+i )  0. The objective of the representative household is


Et0 t {u(c j t ) - v (l j t )},   (0, 1),
t =t0

(9)

where  denotes the subjective discount factor. The instantaneous utility function is assumed to

be non-decreasing in consumption, decreasing in labor time, strictly concave, twice continuously

differentiable, and to fulfill the Inada conditions.

Households are wage-setters supplying differentiated types of labor l j which are transformed

into

aggregate

labor

lt

with

l

( t

t -1)/

t

=

1 0

l

( j

t
t

-1)/

td

j.

We

assume

that

the

elasticity

of

substitution

between different types of labor, t > 1, varies exogenously over time. The time variation in this

markup parameter introduces a so called cost-push shock into the model that gives rise to a sta-

bilization problem for the central bank. Cost minimization implies that the demand for differenti-

ated labor services l j t , is given by l j t = (w j t /wt )- t lt , where the aggregate real wage rate wt is given

by

w

1- t

t

=

1 0

w

1- jt

td j.

Maximizing

(9)

subject

to

(8)

and

the

no-Ponzi

game

condition

for

given

initial values Z0, Bt0-1, and Rt0-1  0 leads to the following first order conditions for consumption,

11

the real wage rate for labor type j , government bonds, and contingent claims:

j t = uc (c j t ), vl (l j t ) = w j t j t /t ,

qt ,t +1

=

j t+1 t+1j t

,

jt

=

Rt Et

 j t+1 t +1

(10) (11)

where j t denotes a Lagrange multiplier, t the inflation rate t = Pt /Pt-1, and µt = t /( t - 1) the stochastic wage mark-up with mean µ¯ > 1. The first order condition for contingent claims holds for

each state in period t + 1, and determines the price of one unit of currency for a particular state at

time t +1 normalized by the conditional probability of occurrence of that state in units of currency

in period t . Arbitrage-freeness between government bonds and contingent claims requires Rt = 1/Et qt,t+1. The optimum is further characterized by the budget constraint (8) holding with equality and by the transversality condition limi Et i j t+i (B j t+i + Z j t+1+i )/P j t+i = 0.

The final consumption good Yt is an aggregate of differentiated goods produced by monopo-

-1 -1

listically competitive firms indexed with i  [0, 1] and defined as yt 

=

1 0

yi


t

d i , with  > 1. Let Pi t

and Pt denote the price of good i set by firm i and the price index for the final good. The demand

for each differentiated good is yidt = (Pi t /Pt )- yt , with Pt1- =

1 0

P

1- it

d

i

.

A

firm

i

produces

good

yi

using a technology that is linear in the labor bundle li t

=[

1 0

l

( j

t -1)/ it

td j]

t /(

t -1):

yi t

= at li t , where

lt =

1 0

li t d i

and

at

is

a

productivity

shock

with

mean

1.

Labor demand satisfies: mci t = wt /at , where mci t = mct denotes real marginal costs indepen-

dent of the quantity that is produced by the firm.

We allow for a nominal rigidity in form of a staggered price setting as developed by Calvo

(1983). Each period firms may reset their prices with the probability 1 -  independently of the

time elapsed since the last price setting. The fraction   [0, 1) of firms are assumed to keep

their previous period's prices, Pi t = Pi t-1, i.e. indexation is absent. Firms are assumed to maxi-

mize their market value, which equals the expected sum of discounted dividends Et

 T =t

q t ,T

DiT

,

where Di t  Pi t yi t (1 - ) - Pt mct yi t and we used that firms also have access to contingent claims.

Here,  denotes an exogenous sales tax introduced to offset the inefficiency of steady state out-

12

put due to markup pricing (Rotemberg and Woodford, 1999). In each period a measure 1 -  of

randomly selected firms set new prices Pi t as the solution to maxPit Et

 T =t

T

-t

q

t

,T

(Pi

t

yi

T

(1

-

) - PT mcT yiT ), s.t. yiT = (Pi t )-PT yT . The first order condition for the price of re-optimizing

producers is for  > 0 given by

Pit =  Ft , Pt -1 Kt

(12)

where Kt and Ft are given by the following expressions:


Ft = Et ()T -t uc (cT )yT
T =t

PT Pt


mcT

(13)

and


Kt = Et ()T -t uc (cT )(1 - )yT
T =t

PT Pt

-1
.

(14)

Aggregate output is given by yt = at lt /t , where t = 01(Pi t /Pt )-d i  1 and thus t = (1 - )(Pt /Pt )- + t t-1. The dispersion measure t captures the welfare decreasing effects of stag-

gered price setting. If prices are flexible,  = 0, then the first order condition for the optimal price

of

the

differentiated

good

reads:

mct

=

(1

-

)

-1 

.

The public sector consists of a fiscal and a monetary authority. The central bank as the mon-

etary authority is assumed to control the short-term interest rate Rt with a simple feedback rule

contingent on past interest rates, inflation and output:

Rt = f (Rt-1, t , yt ).

(15)

The fiscal authority issues risk-free one period bonds, has to finance exogenous government ex-

penditures PtGt , receives lump-sum taxes from households and tax-income from an exogenous

given constant sales tax , such that the consolidated budget constraint reads: Rt-1Bt-1 + PtGt =

Bt + Pt Tt +

1 0

Pi

t

y

i

t

d

i

.

The exogenous government expenditures Gt

evolve around a mean G¯,

which is restricted to be a constant fraction of output, G¯ = y¯(1 - sc). We assume that tax policy

guarantees government solvency, i.e., ensures limi (Bt+i )

i v

=1

R

-1 t +v

= 0.

Due to the existence

of the lump-sum tax, we consider only the demand effect of government expenditures and focus

13

exclusively on optimal monetary policy.

We collect the exogenous disturbances in the vector ut = [at ,Gt , µt ]. It is assumed that the percentage deviations of the first two elements of the vector from their means evolve according

to autonomous AR(1)-processes with autocorrelation coefficients a, G  [0, 1). The process for

log(µt /µ¯) as well as all innovations, zt = [

a t

,

g t

,

µt ], are assumed to be i.i.d..

The recursive equilibrium is defined as follows:

Definition 1 Given initial values Pt0-1 > 0 and t0-1  1, a monetary policy and a ricardian fis-

cal policy Tt  t  t0, a sales tax , a rational expectations equilibrium (REE) for Rt  1, is a set

of sequences {yt , ct , lt , mct , wt , t , Pt , Pi t ,Rt }t=t0 satisfying the firms' first order condition mct =

wt /at ,

(12)

with

Pi t

=

Pt ,

and

P t1-

=

P t1--1

+

(1

-

)P

1- t

,

the

households'

first

order

conditions

uc (yt - Gt )wt = vl (lt )µt , uc (yt - Gt )/Pt = Rt Et uc (yt+1 - Gt+1)/Pt+1, the aggregate resource con-

straint yt = at lt /t , where t = (1 - )(Pt /Pt )- + (Pt /Pt-1)t-1, clearing of the goods market

ct + Gt = yt and the transversality condition, for {ut }t=t0 .

In the next step, we seek to estimate the model employing Bayesian methods. To do so we loglinearize the structural equations around the deterministic steady state under zero inflation. Thus,

the dynamics in the benchmark economy are described by the following two structural equations:

(Et yt+1 - Et ytn+1) = (yt - ytn ) + Rt - Et t+1 - Rtn

(16)

t = Et t+1 + (yt - ytn),

(17)

where  = -ucc c/(uc sc),  = vll l /vl and  = (1 - )(1 - )( + )/. Further kt denotes the percentage deviation of a generic variable kt from its steady state value k. The natural rates of output and interest, i.e the values for output and real interest under flexible prices, are given by the follow-

ing expressions

y

n t

=

(1 +

)at 

+ gt +

-

µt

,

(18)

gt = (Gt - G)/y and

Rtn = [(gt - ytn ) - Et (gt+1 - ytn+1)].

(19)

14

The model is closed by a simple interest rate feedback rule

Rt = R Rt-1 + t + y yt .

(20)

The general system (1) in the benchmark model then is the fundamental locally stable and unique solution that satisfies (16)-(20) for a certain vector of constant policy coefficients  = (R , , y ).
Our welfare measure is the unconditional expectation of representative households' utility. Building on Woodford (2003), after averaging over all households the purely quadratic approximation to (9) in our benchmark model can be computed as8:

1

1 -



[u

-

uc

y

( 2

+

)

{v

ar

(t

)

+

d

v

ar

(yt

-

y

e t

)}],

(21)

where d = / and the efficient rate of output is given by

y

e t

=

y tn

+

µt

/(

+ ).

(22)

Note that from the perspective of the central bank the vector of structural parameters may be random. Therefore the objective of the central bank (for a given realization i ) reads:9

L(i

,

x)

=

uc

y ( 2

+

)

{v

ar

(t

)

+

d

v

ar

(yt

-

y te

)}.

(23)

In the next subsection we consider habit formation and indexation to past inflation as exam-
ples of missing lags in the nuclear variables output and inflation.
8Throughout we assume that the steady state is rendered efficient by an appropriate setting of the sales tax rate
9Note that the argument x does not depend on time since we compute unconditional variances, i.e. v ar (xt ) = v ar (x), t .

15

2.2 Missing lags in endogenous variables
One example for a missing lag in an endogenous variable is to allow for an internal habit (e.g. Boivin and Giannoni, 2006; Woodford, 2003) in households' total consumption. In that case, the marginal utility of consumption t is no longer exclusively a function of current consumption. Instead, the amount consumed last period affects households' behavior:

t = uc (ct - ct-1) - Et uc (ct+1 - ct ), 0    1,

(24)

with  as the habit parameter. Correspondingly, the constituting equations for (1) are the policy rule (20) and the modified versions of the Euler equation and the New Keynesian Philips curve:

[dt - dt-1] - Et [dt+1 - dt ]

=

E

t

t

+1

+

R

n t

-

Rt

...

+ Et [dt+1 - dt ] - Et [dt+2 - dt+1]

(25)

t = h[(dt - dt-1) - Et (dt+1 - dt )] + Et t+1

(26)

where

dt

=

y

t

-

y

n t

,

h

=

[(+)]-1,



=

/(1-)

and

the

natural

rate

of

output

follows10

[ + (1 + 2)]ytn - ytn-1 - Et ytn+1 = (1 + 2)gt - gt-1 - Et gt+1...

+ (1 + )at - µt .

(27)

Approximating households' utility to second order results in the following expression for the objective of the central bank:

L(2,

x)

=

(1 -

)uch 2h 

y

h



{v

ar

(t

)+

d,h v ar

(yt

-

y te

-

 ( y t -1

-

yte-1))},

(28)

10The parameter , 0    , is the smaller root of this quadratic equation: (1 + 2) = [ + (1 + 2)]. This root is assigned to past values of the natural and efficient rate of output in their stationary
solutions.

16

where d,h = h/ and the efficient rate of output is characterized by

[

+

(1

+

2)]

y

e t

-

y

e t -1

-

E

t

y

e t +1

=

(1 + 2)gt - gt-1 - Et gt+1...

+ (1 + )at .

(29)

Like habit formation, indexation of prices to past inflation induces the economy to evolve in a history-dependent way. We assume that the fraction of prices that are not reconsidered  adjusts according to the following simple rule:

log Pi t = log Pi t-1 +  log t-1,

(30)

with 0    1 as the degree of indexation. This implies that the dispersion measure is given by

t

=

(1

- )(

Pt Pt

)-

+

-t-1t-1t .

Correspondingly, the economy with indexation is characterized by a modified aggregate supply

curve

t - t-1 = Et (t+1 - t ) + (yt - ytn),

(31)

(16) and (20). The corresponding loss function of the central bank reads (Woodford, 2003):

L(3,

x)

=

uc

y

( 2

+

)

{v

ar

(t

-

t -1 )

+

d

v

ar

(yt

-

y te

)},

(32)

where d and the efficient rate of output are defined as in the benchmark economy.

2.3 Omitted variables
While the extensions discussed above modify preferences or technology, they do not add other frictions to the economy. The primary aim ­ in principle ­ is to stabilize inflation. In order to allow for the possibility of conflicting stabilization aims, we introduce a transaction friction by letting

17

real money balances entering households' utility in a separable way. Notably, this change does not affect the dynamics of inflation and output for a given monetary policy.11 More precisely, an euler equation or demand equation for real money balances enters the set of equilibrium conditions:

zm(mt ) t

=

R

t- Rt

1

.

(33)

We assume that z(mt ) implies satiation in real money balances at a finite positive level. The deriva-

tives zm, zmm have finite limiting values as m approaches the satiation level from below. In par-

ticular, the limiting value of zmm from below is negative (see Woodford, 2003a, Assumption 6.1).

Log-linearizing (33) results in:

mt

=

-1 m (R

- 1) Rt

-

1 m

t ,

(34)

where m = -zmmm/zm. In this case the stabilization loss that results is given by:

L(4,

x)

=

uc

y ( 2

+

)

{v

ar

(t

)

+

d

var

(yt

-

y te

)

+

1R

v

ar

(Rt

)},

(35)

where d = /, 1R = d [v( + )(1 - )m]-1 and v = y/m. The general form (1) has to satisfy the interest rate rule, (16)-(17) and (34).
If we combine the features habit formation, indexation and a demand for cash, the utility of the central bank's objective is:

L(5,

x)

=

(1 -

)uch 2h 

y

h



{v

ar

(t

-t-1)+d,h v ar

(yt

- y te

-(yt-1-yte-1))+2R v ar

(Rt

)},

(36)

with

2R

=

d,h  vm(1 - )(1 - )

11However, we assume that this does not lead to a optimal steady state that corresponds to Friedman's rule. I.e. the approximation point is still characterized by flexible prices and zero inflation (see Paustian and Stoltenberg, 2006). Note that our specification of utility is consistent with recent findings by Andrés, LópezSalido and Vallés (2006) for the Euro area. They estimate the role of money for the business cycle of the Euro area and the US and find that preferences are separable between consumption and real money balances.

18

The equilibrium conditions are (25),(34) and t - t-1 = Et (t+1 - t ) + h [(dt - dt-1) - Et (dt+1 - dt )]
for a monetary policy (20).

(37)

2.4 Monetary policy and the lower bound on interest
The question of implementability is of particular interest in monetary policy: the zero bound on interest rates, Rt  1, imposes a natural restriction on the set of feasible policies. We approximate this constraint by analyzing implementable policies.
Definition 2 Consider the locally stable and unique fundamental solution for model i under a particular policy  that satisfies (1) and the transversality condition. The policy  is implementable if the unconditional standard deviation of the net nominal interest rate i under the fundamental solution is at least k-times smaller than the steady state value for interest:
ki  R - 1, k > 0.
Notably, the higher k, the lower is the probability that the zero bound on interest rates becomes in fact binding. We stress that this approach ignores certain feedback channels.12 Nevertheless, computing the standard deviation of the nominal interest rate is one way to gauge the severity of the lower bound constraint in linear models (see Woodfor, 2003 or more recently Schmitt-Grohé and Uribe, 2005).
Throughout our optimal policy analysis we compute two vectors  : either requiring optimal policy to be implementable or not. This procedure allows us to examine the welfare costs of the zero bound requirement.
12Adam and Billi (2006) and Eggertsson and Woodford (2003) explicitly account for the non-linear zero bound constraint and show how the possibility of a binding constraint affects agents' decisions.

19

3 Results
3.1 Data and estimation results
We treat the variables real wage, output and consumer price inflation as observables. The data consists of HP filtered13 quarterly values of these variables for the EU 13 countries from 1970-2006 14.
We do not estimate the parameters  = 0.99, the fraction c/y = 0.8 and  = 6. While we assume the disturbances gt and at to follow stationary AR(1) processes, µt is supposed to be i .i .d .(see the appendix for the assumed prior distribution of the estimated parameters).
We approximate the joint posterior distribution of structural parameters by drawing 100, 000 times employing a MCMC algorithm. To insure convergence, we discard the first 20, 000 draws. In general, all parameters with the exception of the relative risk aversion with respect to real money balances m are identified (see Figure 1-5 and table 9 in the appendix). The fact that we are not able to identify m comes at no surprise since it only appears in equations that have no influence on predicting output, inflation or the real wage.
In order to assess the relevance of each perturbation we compute marginal likelihoods and corresponding posterior odds as their weighted average.15 The results are presented in table 1. Note that adding features to the model does not necessarily increase the marginal likelihood. This is supported by the following observations: First, enriching the benchmark model with a demand for cash lowers the marginal likelihood, since real money balances do not help to predict the observable variables. On the contrary the additional parameter to estimate (m) blurs the precision of the remaining parameters slightly (1677.9351 < 1677.2605). Second, while habit formation does modify the dynamics of the model it does not improve the fit of the model. This points to a well known problem in Bayesian model estimation, namely that the informative prior on the habit parameter introduces curvature into the posterior density surface that tilts it away from being zero at
13We set HP = 1600 as suggested by Ravn and Uhlig, 2002. 14The data-set we use was kindly provided by the Euro Area Business Cycle Network (EABCNF). For a description of how this data is constructed see Fagan, Henry and Mestre, 2001. 15We employ equal prior weights for each model.
20

the posterior, which would increase the predictive density. Third, history dependence in inflation seems to improve the predictive power of the model. Model 3 has the highest marginal likelihood. Thus, the largest model incorporates features that helps to predict the data (indexation) and others that do not (habit and money). It therefore exhibits a higher marginal likelihood than the benchmark model but a lower than the best one. This emphasizes our case: especially when conducting Bayesian model estimation one should consider carefully which features to include into the model and which not.

Table 1: Posterior odds and marginal data densities

p(Y |Mi ) i

M1 1677.9351
0.1995

M2 1675.2409
0.1992

M3 1691.6444
0.2011

M4 1677.2605
0.1994

M5 1687.8380
0.2007

3.2 Optimal policy at the posterior mean
As a benchmark we determine the optimal policy i = (R ,  , y )i at the posterior mean ¯ i for each model i , i = 1, 2, .., 5, i.e. we assume that the model and the corresponding parameters are known. Throughout our analysis we consider two cases: either optimal policies are assumed to be implementable or not. To be more precise, in the former case we require twice the unconditional standard deviation of the nominal interest rate not to exceed the difference between steady state interest and the zero bound (see definition 2 with k = 2). The optimal coefficients and the resulting business cycles costs (BCC ) expressed as equivalent reductions in steady state consumption are displayed in table 2.

21

Table 2: Optimal Policy at the Posterior Mean (i ) Rt = R Rt-1 + t + y yt

No IC

IC

M1 M2 M3 M4 M5 M1 M2 M3 M4 M5

R  y BCC (i )

1.02 3 0
0.39%

1.06 3 0
0.42%

1.08 3 0
0.45%

1.26 2.22
0 1.20 %

1.37 0.12
0 1.20%

1.23 0.19
0 2.38%

1.20 0.22
0 2.11%

1.70 0.32
0 1.25%

1.23 0.19
0 2.42%

1.37 0.12
0 1.20%

Evidently, optimal policies in both cases are characterized by drawing heavily on past interest rates. To put it differently, optimal policy is history-dependent (Woodford, 2003). In the first three models inflation stabilization is the predominant aim. Correspondingly, optimal policies feature a strong reaction on inflation. In models 4 and 5 households value real money balances as a medium for transactions. This introduces the stabilization of the nominal interest rate as an conflicting aim to price stability (see (35) and (36)). Therefore, optimal policies in these models exhibit a higher coefficient R to smooth interest rates and a less aggressive response to inflation.
Remarkably, requiring implementable policies leads to sizeable welfare losses in models 1-3 (compare table 2 and 3), where households' utility does not depend on variations in interest rates. In such an environment the implementability constraint implicitly introduces this by restricting the set of feasible policies. In particular, policies that aggressively fight inflation drop out since these policies result in a high variability of interest.16 To get intuition on this suppose that y is small and that the economy in model 1 is hit by a cost push shock. To fight inflationary tendencies the output gap must decrease according to the aggregate supply curve (17). This in turn requires a strong increase of the nominal interest rate to fulfill the euler equation (16) since the cost push shock affects the natural rate of interest.
Comparing the results for the two models that feature a demand for cash reveals that the imple16Note that even if  < 1 equilibrium sequences are uniquely determined and converge to the steady state. The condition for local stability and uniqueness of equilibrium sequences reads:  + (1 - )/y > 1 - R (Woodford, 2003).
22

mentability constraint affects the results in different ways. While the implementability constraint is a restriction on optimal policies in model 4 it is not in 5. Although both specifications incorporate stabilizing the nominal interest as a policy aim, it is dominated by inflation stabilization in model 4 (see table 3).

Table 3: The weights d and r at the posterior mean

Weights
d R

Model1 0.00690
-

Model2 0.03081
-

Model3 0.01048
-

Model4 0.00701 0.06811

Model5 0.04728 0.60396

We conclude that requiring policies to be implementable comes at a cost which can be substantial if inflation stabilization is the predominant aim according to preferences. However, ignoring the issue of the zero bound may cause credibility problems since agents form their expectations rationally.
3.3 Optimal policy under parameter uncertainty
In this section we consider the situation of a central bank that knows the true model but is uncertain about the corresponding parameters, i.e. we are solving the optimal policy problem under parameter uncertainty (see (5)). The optimal policy resulting of this procedure, i pu = (R ,  , y )i pu, maximizes representative households' utility over the posterior distribution of i , i = 1, 2...5. The main findings are: optimal policy under parameter uncertainty does not differ substantially if policy is not subject to the implementabiltiy constraint. But if the central bank is constrained in that way, following the optimal policy at the posterior mean is not a good policy recommendation: the central bank risks to violate the implementability requirement with high probability over the parameter space with one exception: the optimal policy of model 5 at the posterior mean, 5 , is unaffected by parameter uncertainty.
In table 4 we present our findings for optimal policy under parameter uncertainty. If we do not impose the implentability constraint (No IC ), the optimal rule in each model is very similar to
23

the optimal rule at the posterior mean. Business cycle cost computed over all draws are therefore alike.17

Table 4: Optimal Policy under Parameter Uncertainty (i pu) Rt = R Rt-1 + t + y yt

No IC

IC

M1 M2 M3 M4 M5 M1 M2 M3 M4 M5

R 1.02 1.07 1.10 1.24 1.33 1.20 1.17 1.66 1.22 1.33

 3 3 3 2.36 0.13 0.10 0.21 0.15 0.10 0.13

y 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00

BCC (i ) - BCC (i pu) 0.00 0.00 0.00 0.00 0.00 - - - - -

ICV (i )

- - - - - 40% 46% 43% 40% 0%

However, if we require optimal policy to be implementable (IC ) the picture changes. In models where optimal policy is characterized by a binding implentability constraint at the posterior mean (model 1,2,3 and 6), variations over the parameter vector lead easily to a violation of this constraint (in 40% to 46% of all draws). As one would expect, the optimal policy rules i pu in each model are characterized by a lower coefficient on inflation and a slightly higher one on past interest rate. This corresponds to the classical result of Brainard (1967): If policy makers are uncertain about the true parameters, then they act more cautious than if the parameters were certain. To sum up, uncertainty about the true parameters of a model plays an important role if the zero bound on interest is taken into account.
Our analysis so far indicates that parameter uncertainty does not affect optimal policy in the large model. This suggests that, despite the fact that model 5 does not have the highest marginal likelihood (see table 1), it can insulate the policy maker against the source of parameter uncertainty. We will put this statement on stand in the next section, where specification uncertainty is analyzed.
17LOWW (2005) find that parameter uncertainty plays a minor role as a source of uncertainty in a similar economic environment, too.
24

3.4 Optimal policy under specification uncertainty
Is there a need for a robust rule with respect to specification uncertainty? To put it differently, what would happen if we optimize in one model but the true model is a different one? To answer this question we compare social welfare of optimal rules across models. In table 5 we display how much steady state consumption agents in a particular model are willing to give up permanently to be indifferent between the optimal rule in that model and optimal rules of other models. These additional welfare costs are due to specification uncertainty.
Opposed to findings under parameter uncertainty the large model (M5) cannot be recommended as a policy device. Incorporating all model features does not in general guard against specification uncertainty. In other words, following a strategy similar to minmax to avoid possible high losses in M5 is not optimal. Furthermore, the welfare costs of not knowing the true model are at least 1% (see column 6) for all models under consideration. Taking into account that business cycle cost at the posterior mean are less than 1%, this can be referred to as substantial.

Table 5: Optimal Policy under Specification Uncertainty (su): NO IC

M1 M2 M3 M4 M5 WM

BCC (1 ) - BCC (i ) BCC (2 ) - BCC (i ) BCC (3 ) - BCC (i ) BCC (4 ) - BCC (i ) BCC (5 ) - BCC (i )
BCC (su) - BCC (i )

0 0.00% 0.00% 0.20% 2.52% 0.92%

0.00% 0
0.00% 0.21% 2.45% 0.87%

0.00% 0.00%
0 0.09% 0.98 0.51 %

0.16% 0.013% 0.11%
0 1.73% 0.33%

7.70% 7.48% 7.39% 4.72%
0 1.10%

1.58% 1.53% 1.51% 1.05% 1.53% 0.75%

W M indicates the weighted relative business cycle costs over the modelspace. For model j , j = 1, 2, .., 5, this

is defined as

5 i =1

[B

C

C

(

j

) - BCC (i

)]p (Y

|Mi ).

These outcomes indicate possible welfare gains for the optimal policy rule according to (6) by determining the policy rule su = (R ,  , y ) that minimizes the loss across models, each evaluated at its posterior mean. Indeed, the optimal policy under specification uncertainty cuts the additional welfare cost significantly (see the last column of table 5). The optimal rule (table 9, column 1 in the appendix) mirrors a compromise between the optimal policies in each model. It

25

reacts weaker on inflation than in model 1-4, where inflation stabilization is the primary aim to avoid high welfare losses in model 5. However, the higher coefficient on inflation compared to model 5 leads to a decrease of welfare losses across the majority of models (see line 5 of table 5).

Table 6: Optimal Policy under Specification Uncertainty (su): IC

M1 M2 M3 M4 M5

BCC (1 ) - BCC (i ) BCC (2 ) - BCC (i ) BCC (3 ) - BCC (i ) BCC (4 ) - BCC (i ) BCC (5 ) - BCC (i )
BCC (su) - BCC (i )

0 ICV 0.51% 0.00% 0.54% 0.04%

0.20% 0
0.91% 0.20% 0.75% 0.18%

ICV * ICV
0 ICV 0.17% 0.15%

ICV ICV 0.49%
0 0.51% 0.03%

0.07% ICV 0.10% 0.07%
0 0.07%

I V C in (i , j ) indicates that policy j violates the implementability constraint in model i .

Requiring policy to be implementable, the optimal policy rule derived from the large model is the only one that does not violate this constraint over the model space. This result is similar to the section above and comes at no surprise since the rule is characterized by a muted reaction and a high degree of interest rate inertia. However, the optimal policy rule abstains from violations of the zero bound and reduces additional welfare losses compared to the optimal rule from the largest model by approximately 0.5% (see line 5 of table 6), too. The optimal rule under specification uncertainty is similar to the unconstrained one, but the reaction on inflation is weaker in order to achieve the low variations of interest required by the implementability constraint (see second column 2 of table 9 in the appendix).
While the optimal policy device under parameter uncertainty is just to determine optimal policy in the largest model, this strategy leads to substantial welfare losses under specification uncertainty. Simply following policy advise from the largest model can therefore not be recommended. Which policy the central bank should follow in the presence of both sources of uncertainty will be investigated in the next section.

26

3.5 Optimal policy under parameter and specification uncertainty
In this section the central bank faces uncertainty about the true model as well as the true vector of deep parameters. The optimal policy rule psu = (R ,  , y ) is therefore determined by solving problem (7) 18.
In table 11 in the appendix and in table 7 we display the characteristics of the optimal policy in the presence of both sources of uncertainty. They reflect findings which were already mentioned earlier in the paper: First, faced with an increase in uncertainty, optimal policy reacts more cautious. Independent of whether policy is required to be implementable or not, optimal policy rules are characterized by lower reaction coefficients (see the optimal policy coefficients in table 11). Second, optimal policy determined only at the posterior mean leads to a high percentage of violations of the implementability constraint if the distribution of deep parameters is taken into account (table 7 row 5). To avoid violations of the IC constraint, the optimal rule features a very low coefficient on inflation (0.09) (see column 2 of table 11).
In table 7 we assess the welfare costs of each source of model uncertainty separately. If the policy maker is confronted with specification and parameter uncertainty, the optimal policy rule leads to weighted average welfare costs of 1.58% in terms of an equivalent reduction in steady state consumption. The difference between rows 2 and 3 is the reduction of welfare costs due to using the knowledge about the distribution of deep parameters. It is worth 0.01%. Notably, this gain is only due to a reduction of welfare costs in model 5, the model where the highest losses occur. In row 1 we display the welfare costs that arise under optimal policy in each model if the policy makers faces parameter uncertainty. Correspondingly, a comparison of rows 1 and 2 reveals the substantial costs of specification uncertainty. Knowing the true structure (the true model) of the economy would cut welfare costs in half on average. This difference in welfare costs that occur in any case can be interpreted as costs for insurance against different sources of model uncertainty. Since as we have demonstrated, welfare costs due to an deficient policy can be much higher, the
18For our large set of models, each associated with 80000 draws, solving problem (7)is a cumbersome task. We therefore considered only the last 20000 draws from each model. Since we insured convergence of our Markov chain earlier, restricting the number of draws, has no impact on the validity of the results in this section.
27

policy maker might however be willing to bear those costs. Table 7: Comparison of Optimal Policy under Parameter and Specification Uncertainty

No IC

IC

M1 M2 M3 M4 M5 WM M1 M2 M3 M4 M5

BCC (pu) BCC (su) BCC (psu)
IC V (su) I C V (psu)

0.36% 1.24% 1.25%
-

0.41% 1.35% 1.36%
-

0.46% 0.96 % 0.98 %
-

1.15% 1.48% 1.49%
-

1.31% 2.92% 2.82%
-

0.74% 1.59% 1.58%
-

2.72% -
2.74% 39% 0%

2.38% -
2.69% 1% 0%

1.37% -
1.6% 43% 0%

2.71% -
2.73% 28% 0%

1.31% -
1.36% 26% 0%

4 Conclusion
In this paper we propose a novel methodology to analyze optimal policies under model uncertainty in micro-founded macroeconomic models. To be more specific, it allows to capture specification uncertainty, parameter uncertainty and a combination of both in a unified framework. We suggest to start with a benchmark model where the basic intuition is well known and include step by step reasonable model extensions. This bottom-up approach allows us to evaluate quantitatively the explanatory power gain as well as the effect of each of these components on the optimal policy decision separately.
We apply our methodology to assess and quantify the relevant sources of uncertainty for the optimal conduct of monetary policy within (parameter uncertainty) and across models (specification uncertainty) using EU 13 data. Parameter uncertainty matters only if the zero bound on interest rates is explicitly taken into account. Then, optimal policy determined at the posterior mean violates the implementability constraint in about 40% of draws from the distributions of the parameters.
In either case, optimal monetary policy is highly sensitive with respect to specification uncertainty. Knowledge about the true structure of the economy would cut welfare costs in half.
28

However, incorporating this uncertainty into the decision about optimal policy avoids high welfare losses and renders policy implementable.
We recommend to follow an optimal policy that is aware of both sources of uncertainty. It evades high losses and violations of the zero bound constraint over the parameter and model distribution, jointly. Nevertheless, since insuring against model uncertainty comes at a cost, our results point towards the importance of improving the theoretical understanding of the fundamental mechanisms of the economies under consideration.
References
An, S. and F. Schorfheide, 2006, Bayesian Analysis of DSGE Models, forthcoming Econometrics Review.
Andrés, J., J.D. López-Salido and J. Vallés, 2006, Money in an Estimated Business Cycle Model of the Euro Area, Economic Journal, vol. 116, 457-477.
Benigno, P. and M. Woodford, 2005, Inflation Stabilization and Welfare: The Case of a Distorted Steady State, Journal of the European Economic Association, vol. 3, 1185-1236.
Benigno, P. and M. Woodford, 2006a, Optimal Taxation in an RBC Model: A Linear-Quadratic Approach, Journal of Economic Dynamics and Control 30, 1445-1489.
Benigno, P. and M. Woodford, 2006b, Linear-Quadratic Approximation of Optimal Policy Problems, Working Paper.
Boivin, J. and M.P. Giannoni, 2006, Has monetary policy become more effective?, Review of Economics and Statistics 88, vol. 3, 445 - 462.
Brainard, W., 1967, Uncertainty and the effectiveness of policy, American Economic Review 57, 411-425.
Calvo, G.A., 1983, Staggered prices in a Utility-Maximizing Framework, Journal of Monetary Economics, vol. 12, 383-398. 29

T. Cogley and T.J. Sargent, 2005, The conquest of US inflation: Learning and robustness to model uncertainty, Review of Economic Dynamics 8, 528U 563.
Fagan, G., Henry, J. and R. Mestre, 2001, An area-wide model (AWM) for the euro area, ECB Working Paper Series No. 42.
Geweke, J., 1999, Computational experiments and reality, University of Minnesota and Federal Reserve Bank of Minneapolis, June 1999.
Hansen, L. P. and T. J. Sargent, 2003, Robust Control and Econmic Model Uncertainty, Mimeo, University of Chicago.
Ireland, P.N., 2004, Money's Role in the Monetary Business Cycle, Journal of Money Credit and Banking, vol. 36, 969-983.
Levin, A., A. Onatski, J. Williams, and N. Williams, 2005, Monetary Policy under Uncertainty in Microfounded Macroeconometric Models. In: NBER Macroeconomics Annual 2005, Gertler, M., and K. Rogoff, eds. Cambridge, MA: MIT Press.
Lucas, R., 2000, Inflation and Welfare, Econometrica, vol. 68, 247 - 274.
McCallum, B.T. and E. Nelson, 2004, Timeless perspective vs. discretionary monetary policy in forward-looking models, Federal Reserve Bank of St. Louis Review, 86(2):43-56.
McCallum, B. T., 1988, Robustness properties of a rule for monetary policy, Carnegie- Rochester Conference Series on Public Policy 29, 175-203.
Onatski, A. and J. H. Stock, 2002, Robust monetary policy under model uncertainty in a small model of the US economy. Macroeconomic Dynamics 6, 85-110.
Ravn, M. and Uhlig, H, 2002, On Adjusting the HP-Filter for the Frequency of Observations, in Review of Economics and Statistics, 84(2):371-76.
Rotemberg, J.J. and M. Woodford, 1999, Interest-Rate-Rules in an Estimated Sticky-Price-Model, in J.B. Taylor, ed., Monetary Policy Rules, Chicago: University of Chicago Press. 30

Sargent, T.J., 1999, The Conquest of American Inflation. Princeton Univ. Press, Princeton, NJ. Schmitt-Grohé, S., and M. Uribe, 2006, Optimal Simple and Implementable Monetary and Fiscal
Rules, forthcoming Journal of Monetary Economics. Sidrauski, M., 1967, Rational Choice and Patterns of Growth in a Monetary Economy, American
Economic Review, vol. 57, 534-544. Smets, F. and R. Wouters, 2003, An Estimated Dynamic Stochastic General Equilibrium Model of
the Euro Area, Journal of the European Economic Association 1 (5) 1123-1175. Soderstrom, U., 2002, Monetary policy with uncertain parameters. Scandinavian Journal of Eco-
nomics 104, 125-145. Walsh, C., 2003, Monetary Theory and Policy, second edition, Cambridge: MIT Press. Woodford, M., 2003a, Interest and Prices: Foundations of a Theory of Monetary Policy, Princeton:
Princeton University Press. Woodford, M., 2003b, Optimal Interest Rate Smoothing, Review of Economic Studies, 70, 861-886.
31

5 Appendix

Estimation Results

Table 8: Prior distribution of the structural parameters

Parameter

 y 
c 


m g a g a µ

Prior distribution

distribution mean std

beta

0.8 0.1

normal

1.7 0.1

normal 0.125 0.05

normal

2 0.5

normal

1.5 0.375

beta

0.75 0.05

beta

0.7 0.1

beta

0.75 0.15

normal 1.25 0.375

beta

0.5 0.3

beta

0.5 0.1

invgamma 0.05 0.026

invgamma 0.05 0.026

invgamma 0.05 0.026

32

Table 9: Posterior estimates of the structural parameters in each model

33

Parameter

 y 
c 


m g a g a µ

distribution beta
normal normal normal normal
beta beta beta normal beta beta invgamma invgamma invgamma

Model1 mean std 0.4268 0.0704 1.7088 0.1007 0.0703 0.0345 0.5700 0.2217 1.4333 0.1756 0.8801 0.0164
---0.8777 0.0599 0.7975 0.0411 0.0099 0.0009 0.0153 0.0033 0.0092 0.0006

Model2 mean std 0.4377 0.0723 1.7534 0.0974 0.0551 0.0236 0.3270 0.0750 1.3698 0.1799 0.8977 0.0135 0.8816 0.0413
--0.9540 0.0728 0.8087 0.0398 0.0085 0.0005 0.0192 0.0034 0.0090 0.0006

Model3 mean std 0.3413 0.0802 1.6365 0.1018 0.1014 0.0384 0.4876 0.1328 1.3950 0.1723 0.8495 0.0221
-0.5367 0.0936
-0.8403 0.0587 0.6360 0.0714 0.0099 0.0007 0.0163 0.0029 0.0099 0.0007

Model4 mean std 0.4245 0.0729 1.7149 0.1029 0.0709 0.0368 0.5769 0.2313 1.4295 0.1739 0.8792 0.0167
--1.2570 0.3868 0.8789 0.0624 0.7951 0.0412 0.0100 0.0009 0.0153 0.0032 0.0092 0.0006

Model5 mean std 0.3669 0.0780 1.6945 0.1010 0.0695 0.0320 0.3427 0.0820 1.3531 0.1900 0.8663 0.0210 0.8658 0.0470 0.5253 0.0930 1.2347 0.3710 0.9260 0.0810 0.6528 0.0750 0.0086 0.0010 0.0190 0.0034 0.0097 0.0010

Additional tables and figures
Figure 1: Prior vs. posterior in model 1

2000 1500 1000
500 0 0

rho
0.5

3000 2000 1000
0 11

phi pi
2

4000 3000 2000 1000
0 30

phi y
0.5

3000 2000 1000
0 10

2500 2000 1500 1000
500 0 0

sigma c
12

10000 8000 6000 4000 2000 0
30

alpha
0.5

2500 2000 1500 1000
500 0
10

psi g
0.5

3000 2000 1000
0 10

x 104 sigma g
4 3 2 1 0
0 0.2 0.4

x 104 sigma a
4 3 2 1 0
0 0.2 0.4

x 104 sigma mu
6 4 2 0
0 0.2 0.4

omega
2
psi a
0.5

4 1

34

Figure 2: Prior vs. posterior in model 2

2000 1500 1000
500 0 0
2500 2000 1500 1000
500 0 0
4000 3000 2000 1000
0 0

rho
0.5
sigma c
12
psi a
0.5

3000 2000 1000

phi pi

6000 4000 2000

phi y

8000 6000 4000 2000

omega

000 1 1 2 3 0 0.5 1 0 2 4

10000 8000 6000 4000 2000 0
30

alpha
0.5

5000 4000 3000 2000 1000
0 10

eta
0.5

4
x 10

psi g

3 2 1 0 10

0.5

1

4
x 10

sigma g

6 4 2 0 10

0.2 0.4

4
x 10

sigma a

4

3

2

1

0 0 0.2 0.4

x

4
10

sigma

mu

6 4 2 0
0 0.2 0.4

35

Figure 3: Prior vs. posterior in model 3

rho

2000 1500 1000
500 0 0
2500 2000 1500 1000
500 0 0

0.5
sigma c
12
psi a

2000 1500 1000
500 0 0

0.5

3000 2000 1000
0 11

phi pi
2

4000 3000 2000 1000
0 30

phi y
0.5

5000 4000 3000 2000 1000
0 10

omega
2

4

6000 4000 2000
0 30

alpha
0.5

gamma

1500 1000
500 0
10

0.5

2500 2000 1500 1000
500 0
10

psi g
0.5

1

4
x 10

sigma g

4 3 2 1 0 10

0.2 0.4

4
x 10

sigma a

4

3

2

1

0 0 0.2 0.4

x

4
10

sigma

mu

4

3

2

1

0 0 0.2 0.4

36

Figure 4: Prior vs. posterior in model 4

2000 1500 1000
500 0 0
2500 2000 1500 1000
500 0 0
4000 3000 2000 1000
0 0

rho
0.5
sigma c
12
psi a
0.5

3000 2000 1000

phi pi

4000 3000 2000 1000

phi y

3000 2000 1000

omega

000 1 1 2 3 0 0.5 1 0 2 4

8000 6000 4000 2000
0 30

alpha
0.5

sigma m
1000 500 0
10 1 2

2500 2000 1500 1000
500 0
30

psi g
0.5

1

4
x 10

sigma g

4

3

2

1

4
x 10

sigma a

4

3

2

1

x

4
10

sigma

mu

6

4

2

0 10

0.2 0.4

0 0 0.2 0.4

0 0 0.2 0.4

37

Figure 5: Prior vs. posterior in model 5

2000

rho

1000

0 0 0.5
sigma c

2000 1000
0 0

12
sigma m

1000 500

3000 2000 1000
0 11
6000 4000 2000
0 30
10000
5000

phi pi
2
alpha
0.5
psi g

4000 2000
0 30
4000 2000
0 10
2000 1000

0 012

4
x 10

sigma a

4

2

00 3 0 0.5 1 0

x

4
10

sigma

mu

4 2

0 0 0.2 0.4

0 0 0.2 0.4

phi y
0.5
eta
0.5
psi a
0.5

omega

6000 4000 2000
0 10

2
gamma

4

1500

1000

500

0 10

0.5

4
x 10

sigma g

1

6 4 2 0 10

0.2 0.4

Table 10: The optimal robust policy rule at the posterior mean across models

Coefficients
R  y

No IC 1.2077 0.6706 0.0013

IC 1.2297 0.1797 0.0006

38

Table 11: The optimal robust policy rule over all draws across models

Coefficients
R  y

No IC 1.18 0.63 0.00

IC 1.19 0.09 0.00

39

SFB 649 Discussion Paper Series 2007
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Trade Liberalisation, Process and Product Innovation, and Relative Skill Demand" by Sebastian Braun, January 2007.
002 "Robust Risk Management. Accounting for Nonstationarity and Heavy Tails" by Ying Chen and Vladimir Spokoiny, January 2007.
003 "Explaining Asset Prices with External Habits and Wage Rigidities in a DSGE Model." by Harald Uhlig, January 2007.
004 "Volatility and Causality in Asia Pacific Financial Markets" by Enzo Weber, January 2007.
005 "Quantile Sieve Estimates For Time Series" by Jürgen Franke, JeanPierre Stockis and Joseph Tadjuidje, February 2007.
006 "Real Origins of the Great Depression: Monopolistic Competition, Union Power, and the American Business Cycle in the 1920s" by Monique Ebell and Albrecht Ritschl, February 2007.
007 "Rules, Discretion or Reputation? Monetary Policies and the Efficiency of Financial Markets in Germany, 14th to 16th Centuries" by Oliver Volckart, February 2007.
008 "Sectoral Transformation, Turbulence, and Labour Market Dynamics in Germany" by Ronald Bachmann and Michael C. Burda, February 2007.
009 "Union Wage Compression in a Right-to-Manage Model" by Thorsten Vogel, February 2007.
010 "On -additive robust representation of convex risk measures for unbounded financial positions in the presence of uncertainty about the market model" by Volker Krätschmer, March 2007.
011 "Media Coverage and Macroeconomic Information Processing" by Alexandra Niessen, March 2007.
012 "Are Correlations Constant Over Time? Application of the CC-TRIGt-test to Return Series from Different Asset Classes." by Matthias Fischer, March 2007.
013 "Uncertain Paternity, Mating Market Failure, and the Institution of Marriage" by Dirk Bethmann and Michael Kvasnicka, March 2007.
014 "What Happened to the Transatlantic Capital Market Relations?" by Enzo Weber, March 2007.
015 "Who Leads Financial Markets?" by Enzo Weber, April 2007. 016 "Fiscal Policy Rules in Practice" by Andreas Thams, April 2007. 017 "Empirical Pricing Kernels and Investor Preferences" by Kai Detlefsen,
Wolfgang Härdle and Rouslan Moro, April 2007. 018 "Simultaneous Causality in International Trade" by Enzo Weber, April
2007. 019 "Regional and Outward Economic Integration in South-East Asia" by
Enzo Weber, April 2007. 020 "Computational Statistics and Data Visualization" by Antony Unwin,
Chun-houh Chen and Wolfgang Härdle, April 2007. 021 "Ideology Without Ideologists" by Lydia Mechtenberg, April 2007. 022 "A Generalized ARFIMA Process with Markov-Switching Fractional
Differencing Parameter" by Wen-Jen Tsay and Wolfgang Härdle, April 2007.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "Time Series Modelling with Semiparametric Factor Dynamics" by Szymon Borak, Wolfgang Härdle, Enno Mammen and Byeong U. Park, April 2007.
024 "From Animal Baits to Investors' Preference: Estimating and Demixing of the Weight Function in Semiparametric Models for Biased Samples" by Ya'acov Ritov and Wolfgang Härdle, May 2007.
025 "Statistics of Risk Aversion" by Enzo Giacomini and Wolfgang Härdle, May 2007.
026 "Robust Optimal Control for a Consumption-Investment Problem" by Alexander Schied, May 2007.
027 "Long Memory Persistence in the Factor of Implied Volatility Dynamics" by Wolfgang Härdle and Julius Mungo, May 2007.
028 "Macroeconomic Policy in a Heterogeneous Monetary Union" by Oliver Grimm and Stefan Ried, May 2007.
029 "Comparison of Panel Cointegration Tests" by Deniz Dilan Karaman Örsal, May 2007.
030 "Robust Maximization of Consumption with Logarithmic Utility" by Daniel Hernández-Hernández and Alexander Schied, May 2007.
031 "Using Wiki to Build an E-learning System in Statistics in Arabic Language" by Taleb Ahmad, Wolfgang Härdle and Sigbert Klinke, May 2007.
032 "Visualization of Competitive Market Structure by Means of Choice Data" by Werner Kunz, May 2007.
033 "Does International Outsourcing Depress Union Wages? by Sebastian Braun and Juliane Scheffel, May 2007.
034 "A Note on the Effect of Outsourcing on Union Wages" by Sebastian Braun and Juliane Scheffel, May 2007.
035 "Estimating Probabilities of Default With Support Vector Machines" by Wolfgang Härdle, Rouslan Moro and Dorothea Schäfer, June 2007.
036 "Yxilon ­ A Client/Server Based Statistical Environment" by Wolfgang Härdle, Sigbert Klinke and Uwe Ziegenhagen, June 2007.
037 "Calibrating CAT Bonds for Mexican Earthquakes" by Wolfgang Härdle and Brenda López Cabrera, June 2007.
038 "Economic Integration and the Foreign Exchange" by Enzo Weber, June 2007.
039 "Tracking Down the Business Cycle: A Dynamic Factor Model For Germany 1820-1913" by Samad Sarferaz and Martin Uebele, June 2007.
040 "Optimal Policy Under Model Uncertainty: A Structural-Bayesian Estimation Approach" by Alexander Kriwoluzky and Christian Stoltenberg, July 2007.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

