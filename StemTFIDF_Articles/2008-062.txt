BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2008-062
Nonlinear Modeling of Target Leverage with Latent Determinant Variables - New Evidence on the Trade-off Theory
Ralf Sabiwalsky*
* Universität zu Köln, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Nonlinear Modeling of Target Leverage with Latent Determinant Variables ­ New Evidence on the Trade-off Theory
Abstract The trade-off theory on capital structure is tested by modelling the capital structure target as the solution to a maximization problem. This solution maps asset volatility and loss given default to optimal leverage. By applying nonlinear structural equation modelling, these unobservable variables are estimated based on observable indicator variables, and simultaneously, the speed of adjustment towards this leverage target is estimated. Linear specifications of the leverage target suffer from overlap between the predictions of various theories on capital structure about the sign and significance of determinants. In contrast, the framework applied here allows for a direct test: results confirm the trade-off theory for small and medium-sized firms, but not for large firms.
JEL: G32, G33, C61 Keywords: Capital Structure, Nonlinear, Latent Variables, Trade-off Theory
First draft: August 2008 Ralf Sabiwalsky*
University of Cologne
* I gratefully acknowledge support from Sonderforschungsbereich 649 ,,Economic Risk" (Deutsche Forschungsgemeinschaft) at Humboldt University, Berlin, for providing access to the data used in this study. I thank participants at the doctoral colloquium of Seminar für Wirtschafts- und Sozialstatistik, Cologne University, for helpful comments. Author: Ralf Sabiwalsky, University of Cologne, AlbertusMagnus-Platz, 50923 Cologne, Germany. Email: sabiwalsky@wiso.uni-koeln.de, Tel.: +49172 7134520, Fax: +49221 4705165.

Nonlinear Modelling of Target Leverage with Latent Determinant Variables ­ New Evidence on the Trade-off Theory
1. Introduction Explaining the level of and variation in corporate capital structure is the ongoing aim of a large body of empirical literature. Most commonly, linear regression - type models are applied to identify significant relationships between corporate characteristics and leverage. Independent variables are usually chosen among those that are related to company characteristics that, according to theory, exhibit a causal relationship to capital structure decisions. Among these theories is the trade-off-theory, which suggests that the optimal level of debt is reached just before the marginal tax advantage of debt is outweighed by the marginal cost of financial distress. Furthermore, agency benefits and cost of debt are suggested to play a role in determining corporate leverage, as well as market timing effects and information effects ­ such as signalling effects of the firm's choice between alternative sources of capital or the well-known pecking order theory. These theories provide causal relationships between corporate characteristics and leverage which can be used to form hypothesises on a relationship between company-specific observable variables and leverage, which themselves can easily be statistically tested by applying standard regression theory. However, it appears more difficult to directly test models of corporate capital. This is because first, the company-specific characteristics relevant for the theories cannot usually be perfectly measured. Second, the relationships between these characteristics and leverage are not necessarily linear. Third, the observable, current capital structure of companies differs from the optimal leverage due adjustment costs, market timing issues as well as exogenous shocks. And fourth, ex ante, the direction of the relationship between certain company characteristics and leverage is not unambiguous, due to contradictions between the causal relationships suggested by theory. It is difficult to find support for or to reject any of the theories, because empirical results often are consistent with more than one of them. This has the effect that a
1

considerable number of empirical studies concentrate on confirming significant determinant variables, rather than confirming theories. This study is an attempt to account for these difficulties and develops a model of the dynamics of corporate capital structure which is based on the maximization problem for company's wealth. It analyzes adjustments to corporate capital structure and explicitly models the capital structure target as a nonlinear function of company characteristics. There is a difference between searching for the nonlinear model that fits the data best and developing a nonlinear model based on a causal relationship between variables implied by corporate finance theory.
Here, the latter approach is taken by setting up the optimization problem for target leverage, based purely on the on the trade-off theory. Within that framework, the literature has developed model components that are robust enough to be applied to real world data. The relationships covered by the trade-off model are specified as follows: First, increasing leverage corresponds to a positive effect on firm value due to the tax shield, as long as the risk of default (and thus the risk of losing any tax advantage) does not outweigh this positive effect. Second, increasing leverage means, for a given level of business ("fundamental") risk, an increase in the probability of default and thus, an increase in the ex ante costs of financial distress resulting in a negative effect on firm value. The capital structure target is defined as that leverage where the marginal tax effect is just offset by the marginal distress costs effect. Company-specific variables relevant in this context are business risk and the amount of firm value lost in case of distress. To account for the unobservability of these characteristics, the concept of latent variables is applied and these are identified within a structural equation framework, based on a number of indicators for each characteristic. A simple approach is taken to model the trade-off: The tax shield is the expected net present value of all tax deductions induced by interest payments on debt, and the costs of financial distress are the expected net present value of losses of firm value that occur in the event of default. While
2

"initially", these losses are borne by the firm's creditors, they are indirectly borne by the company itself, because any stakeholder that enters into a contract with the firm will require an equivalent premium for bankruptcy risk. The expectation is taken over those states where the firm survives and those states where the firm defaults, and default probabilities are calculated within the framework of a structural model of credit risk which applies a jumpdiffusion process for firm value and differentiates between systematic and non-systematic business risk. While this framework allows for a straight-forward implementation, it reflects only a part of the causal relationships prevalent in reality. Agency costs, information effects and transaction costs have received much less attention in the literature for a long time and comprehensive models are far from being close to reality.
The main contribution of the paper is to present a method that is capable of testing the tradeoff model for (dynamic) capital structure choice directly and in isolation. Furthermore, it does provide a basis to incorporate any results from future research that allow an explicit specification of further value effects of leverage. From a methodological viewpoint, it contributes by applying the nonlinear structural equation framework, which combines the appealing idea of incorporating measurement error into the model itself and the flexibility of nonlinear modelling, into an econometric setting which, to our knowledge, has not yet been done before.
The rest of the paper is structured as follows. Section 2 briefly presents relevant results from the empirical literature on capital structure. Section 3 describes the model and its calibration and estimation, section 4 provides details on data sources and adjustments, section 5 presents and discusses the results, section 6 shows results of a goodness-of-fit comparison to a linear model, and section 7 concludes.
3

2. Capital Structure Theories and Empirical Tests Commonly suggested theories on capital structure choice are presented in standard treatments of Corporate Finance, and are therefore not repeated here. Predictions implied by the trade-off theory have been partially confirmed not only in manager survey studies such as Graham and Harvey (2001), but also by studies focussing on company data such as Wald (1999) or Rajan and Zingales (1995), who themselves argue that it is difficult to interpret their evidence with regards to causal theory. In the following, due to the vast literature on capital structure choice, only a brief overview will be given on issues closely related to the aim of this paper. First, examples where different theories imply the same empirical pattern will be mentioned. Second, recent results on capital structure determinants and relevant methodological advances will be presented. According to Fama and French (2002), both an advanced version of the pecking order theory and the trade-off theory predict that firms with more investments will have less leverage. Baker and Wurgler (2002) find that firms are more likely to issue equity when their market to book ratio is low. However, a low market to book ratio could either indicate that the prospects of the firm have deteriorated and equity is issued to reduce insolvency risk, or indicate that the market underprices the firm's equity and equity is issued to benefit from this undervaluation. Myers (1977) shows that tangible assets are more likely to be financed by debt than intangible assets are. While on the hand, tangible assets could be considered less risky and therefore debt would have less of an impact on the insolvency risk, it was also argued that the underinvestment problem is less prevalent in firms with less growth opportunities and more tangible assets and thus that these firms would take on more debt. An adjustment model for capital structure has recently been applied by Antoniou, Guney and Paudyal (2008) who find that leverage is significantly influenced by the economic environment of the country in which a firm operates. Lemmons, Roberts and Zender (2008) find that much of the variation in leverage ratio levels is caused by an unobserved factor which is stable over long time intervals. De Jong, Kabir and Nguyen (2008) show that
4

country-specific factors not only determine the level of debt directly, but also influence the importance of firm-specific factors, which is confirmed by Lopez-Iturriaga and RodriguezSanz (2008). Modelling company characteristics as latent variables to analyze capital structure determinants has previously been suggested by Titman and Wessels (1988) who use a linear structural equation model with 8 latent and 15 indicator variables. They find support for a number of theories suggested to explain capital structure decisions, but they cannot test these theories separately. Roberts (2002) has applied a state-space framework to capture measurement error of the determinants of a moving capital structure target. Pao and Chih (2005) found that artificial neural network methods increase the predictive power for Taiwanese high-tech companies' debt ratios, when compared to linear models. Fattouh, Harris and Scaramozzino (2008) capture nonlinearities in the relationship between determinants and leverage by dividing the sample into quantiles of the distribution of leverage and analyzing the linear regression coefficients for each quantile separately. That paper also presents a maximization model for the firm's optimal capital structure, which however does not focus on the quantification of marginal effects of debt, but which is rather presented to motivate the analysis of nonlinearities. Recently, Chang, Lee and Lee (2008) have applied linear structural equation modelling to capital structure choice.
3. Methodology 3.1. Adjustment-type Model for Corporate Capital Structure Dynamic capital structure effects are accounted for using the adjustment model in (1). Here, optimal capital structure is assumed identical to the capital structure target, i.e. companies aim at adjusting towards the optimal capital structure over time. The change in capital structure from time t-1 to time t is assumed to consist of a drift towards the optimal capital structure associated to time t and of a random exogenous shock captured by the error term  . The arguments of the lt* function will be explained in section 2.2.
5

lt - lt-1 =  (lt * ( S,t , lgdt , rt , bt ) - lt-1 ) +  where lt := leverage at time t  := adjustment speed of capital structure (towards l *) lt* () := optimal capital structure at t

(1)

3.2. Modelling the Capital Structure Target

Although most previous empirical studies on capital structure determinants apply a linear

regression model, the optimal capital structure is probably not a linear function of company

characteristics. Assuming that ex ante, the direction of the agency cost and benefit effects and

of the information effects of leverage on optimal capital structure is not unambiguous, I

suggest that two company-specific characteristics dominate the capital structure choice:

Business risk and expected losses in default. This is equivalent to the idea of the "trade-off"

theory on corporate capital structure choice. While the undiscounted debt-tax shield effect is, roughly, a linear function of leverage and the interest rate,1 the expected costs of financial

distress are a nonlinear function of leverage: leverage increases the probability of financial

distress, but in a simplified world, does not impact on the loss of firm value in case of distress.

Hence, the optimal capital structure lt* is modelled as in (2): lt* maximizes the expected net

present value of the tax shield of debt minus the expected net present value of the costs of

financial distress. Relevant company-specific variables are business risk, measured as the

volatility of changes in total assets, and loss given default, measured as the proportion of

assets lost in case of default:



 [ ]lt

* (rt , S,t , lgdt , bt )

=

arg

max l



e
=t +1

-rt

(

-t

)

EQ

l  S

 (rt

+ c) 

 (1 - C (S , l))

-

1 Titman and Wessels (1988) found that their company-specific non-debt-tax shield characteristic, suggested as a substitute for the debt-tax shield effect, is not significant in explaining capital structure. Hence, it suggests that the linearity of the debt-tax-shield effect is not disturbed by non-debt-tax shields.
6

 [ ]
e -rt ( -t) EQ lgd t  S  M  (S , l) ,
 =t +1
where

(2)

rt := riskfree interest rate in year t  := tax rate on corporate income

c := spread above the riskfree rate to be paid on debt

S := firm value at 

C

:=

1 0

if the firm has defaulted between t and  else

M

:= 1 0

if the firm has defaulted between - 1and  else

lgdt := fraction of asset value which is lost in default  S,t := asset volatility

bt := fraction of asset volatility entailed by systematic risk.

and where EQ is the expectation with respect to the risk-neutral probability measure Q. The

latter two variables  S,t and bt determine the distribution of S, the value of the firm's assets,

which is modelled as a stochastic process under Q as described in section 2.3. The future life of the firm is divided into subperiods, where  stands for the end of a subperiod. Tax shield is the product of the interest rate paid on debt (rt+c), the tax rate  and the amount of debt (leverage l · value of total assets S ). The corporate tax rate is set equal to the average U.S. combined corporate taxrate (39%). 2The tax shield is realized each period provided the firm has not defaulted before. Because the tax shield in the period followed by  is zero if the firm has defaulted, the expectation of the tax shield needs to be taken over those states where the firm has survived until  and those states where the firm has defaulted until  , which is

2 Graham (2000) studies the debt tax shield effect in detail.
7

captured by the "cumulative" default indicator C . As a guess on c, the average spread between the riskfree rate and the current yield on bonds with a leverage ratio comparable to l is used.
The costs of financial distress are the amount of firm value lost due to default, and these are indirectly borne by the company in the form of worse conditions in any contract with a stakeholder into which the firm enters. The costs of financial distress associated to  are equal to the wealth loss borne by any creditors who hold claims against the assets of the firm, which is represented by the product of loss given default (lgd) and the value of the firm's assets at  , S . The expected loss in case of default is only realized once, at the time of default. Thus, the expectation is taken over those states where the firm has survived until  and those states where the firm has survived until  -1 , but defaulted between  -1 and  . This is captured by the "marginal" default indicator M .
3.3. Estimating the Probability of Default This expression for lt* requires a specification of the probabilities of default for each subperiod of the future life of the firm, beginning in t, as a function of leverage. In order to find the marginal and cumulative default probabilities, the value of the firm's assets St and the level of corporate debt Dt are modelled as stochastic processes, and it is assumed that default happens as soon as St is equal or lower than Dt. This type of model is known as a structural model of credit risk, see Uhrig-Homburg (2002) for a survey of various alternative specifications. A promising type of model employs a jump-diffusion process for the company's assets, recognizing that the firm's value is subject to (mostly firm-specific) jumps related to rare, but high-impact effects such as new products or the loss of a major customer, and subject to (mostly economy-wide) diffusion effects such as a decline of overall demand.
8

Zhou (2001) suggests such a model, where the value of debt (i.e. the default barrier) is kept

constant over time. The model used in this study follows his basic idea but applies some

advancements. The value of assets is assumed to follow the following process:

dS = (r - qvq )Sd +  D S d dW + (J q -1)dY .

(3)

r := riskfree rate

vq

:=

E[J q

- 1]

=

E[µq

+

0,5

2 

]

-

1

 D := volatility of assets implied caused by the diffusion process d := marginal unit of time

J

:=

jump

size,

ln(J)

~

N(µq

,

2 

)

  := volatility of assets caused by the jump process dW := Brownian motion, dW ~ N (0,1)

dY := Poisson - process with parameter q

It is assumed that jumps represent firm-specific, i.e. unsystematic variations in asset value, i.e.

the proportion of asset volatility caused by the diffusion process is equal to the ratio of the

volatility of an appropriate stock index - as a measure of the amount of systematic risk - to the

volatility of the firm's share. Equity volatility is partially determined by companies' leverage,

so an adjustment is applied to the estimate of the proportion of systematic risk by multiplying

by the ratio of average leverage of index components to company leverage:

 D = b S

where

b =  X ,index  lindex

 X ,ges

l

(4)

 X ,index := volatility of stock index  X ,ges := volatility of the company's share price  S := volatility of the firm's assets lindex := average leverage of index component firms l := leverage of firm

Knowing the total asset volatility and the amount of systematic risk allows to derive the jump

process

volatility



2 

(see

Zhou

(2001),

p.

2023):

9



2 S

=

2 D

+

q

2 

.

(5)

Payouts in the form of dividends are modelled implicitly. As dividend payouts change the

capital structure, it is assumed that these payments are either set off by the subsequent

adjustment, if they lead to a higher deviation from the target structure, or that they form part

of the adjustment, if they decrease the deviation from the target. While Zhou (2001) assumes

a constant level of debt, I argue that firms adjust their capital structure towards a target

leverage. Therefore, the amount of debt needs to be modelled explicitly. The value of debt D is modelled as follows, where  is equivalent to the adjustment parameter in (1):

D0 = l*  S0

D +1

= D

+





D S +1

- l *   S +1 .

(6) (7)

For each l* , the solution to this model provides an estimate of the marginal and cumulative

default probabilities as a function of the parameters riskfree rate, asset volatility, systematic

portion of asset volatility and loss given default. The solution to the model is found by

employing a Monte-Carlo-simulation (for details refer to Zhou(2001)).

3.4. Calibrating the Structural Model of Credit Risk To apply the structural model, the debt level adjustment parameter  and the jump process parameters µq and q need to be specified exogenously. The probabilities of default from the jump-diffusion model can be used to calculate the value of a risky bond, and thus, the implied credit spread. The bond value is calculated under the risk-neutral measure, i.e. the expected return on the firms' assets is set equal to zero, and the payoffs are discounted with the riskfree rate ­ a standard result from the derivative pricing literature. This, at the same time, implies that the jump process parameters µq and q must as well be specified under the risk-neutral measure. Calibration of the model with respect to the parameters  , µq and q
10

is achieved by searching for those values where the distance between spreads implied by the model and empirically observed average credit spreads is minimized. The solution is  = 0.125, µq = -0.5 and q = 0.15. Figure 1 illustrates the fit between model-implied spreads and empirical spreads. For each rating category, based on a total of 100 representative companies, average parameter values for asset volatility, the systematic portion of total risk and leverage were isolated. Representative firms where chosen by randomly selecting from all firms with assets of more than 1 billion USD for which an issuer rating could be obtained. Altman and Kishore (1996) have undertaken an extensive study on recovery rates of corporate bonds. Recognizing that empirically, rating seems not be a major determinant of loss given default (see Altman and Kishore (1996), table 6), we use their average recovery rate of about 40% to derive an lgd estimate of 0.6. For each maturity between 1 and 10 years and each rating category, representative parameter values are used to calculate model-implied credit spreads. Empirically observed credit spreads by rating and maturity are taken from Almeida and Philippon (2007) who study corporate bond spreads during the period 1985-2004. As credit spread, the difference between the observed credit spread for each rating and each maturity and the credit spread for one-year AAA bonds is used (following Almeida and Philippon's idea who correspondingly calculate market-implied risk-adjusted costs of financial distress). If unsystematic default risk could easily be diversified and if jumps are (mostly) firm-specific, then creditors would not require a risk premium for jumps, and if jump risk is not priced, those parameter values will be identical to the values under the real probability measure. However, my model also allows for priced jumps, which is a reasonable assumption given transaction costs and other restrictions in diversifying credit risk.
3.5. Solving the Structural Model by Simulation Solving the structural model of credit risk for finding estimates of C and M requires a computationally intensive Monte-Carlo-type simulation. The simulation is constructed
11

following Zhou (2001). The time horizon (10 years) is discretized (into n = 120 periods which correspond to months), the asset value process is simulated by sampling from the diffusion process and from the jump process both specified in a risk-neutral world (see Zhou (2001), p. 2021 for details), and risk-neutral default probabilities are calculated by counting defaults (i.e. when the asset value process is "stopped") against the total number of samples. To account for the discretization bias, the asset value process is stopped between two time-points with a frequency equivalent to the probability of default for the interim period, calculated using the concept of a Brownian bridge as described in Baldi, Caramellino and Iovino (1999). For combinations of possible parameter values, the optimal capital structure is found by searching that l* where the expected net present value of the tax shield of debt minus the expected net present value of the costs of financial distress is maximized. Maximization is achieved by applying a the simple idea of a simplex initially suggested by Nelder and Mead (1965).
3.6. Interpolating the Optimal Capital Structure Function As the computation of marginal and cumulative default probabilities is costly regarding computation time, I discretize the range of reasonable values of the arguments of the optimal capital structure function into a number of 4 (riskfree rate) × 24 (asset volatility) × 9 (systematic portion of risk) × 11 (loss given default) = 9,504 datapoints and evaluate the function for each of those datapoints. Then, an algorithm for multidimensional spline interpolation on equidistant grids is used to calculate the function lt* (rt , S,t , lgdt ,bt ) . Because of the multidimensionality of the problem, the interpolation itself is costly, too, thus I ex ante determine the optimal capital structure for 6,044,876 datapoints. lt* is then determined by finding the optimal capital structure for that datapoint which is closest to the company's parameter vector. For interpolation, I follow the idea of Habermann and Kindermann (2007)
12

who suggest a simplified interpolation algorithm for multidimensional problems that exploits the presence of an equidistant grid of observed datapoints.

3.7. Measurement Model

Unfortunately, the determinants of optimal capital structure used in this model, business risk

and losses given default, cannot be observed directly, however, indicators of these

determinants can. I model asset volatility (business risk) and the loss given default as latent

variables, indicated by four respectively three observable variables:

Y = µ y + x + 

(8)

where

 AVOL

 1

 SVOL 

 

CVOL

 

  

2 3

Y= 

RD

, 



=

 

4

 INT 

0

 

MTB

 

 RD 

 

0 0

0   0 

0

0

 

  

µ µ

2 3

  

0

 

,

µ

y

=

 

µ

4

 

.

1 0

6 7

 

 

µ µ

6 7

 

x is a (2×1) vector of two latent variables distributed according to

N(µx,) where

(9)

µx

=



µ pd µ lg d



,

 y = 112

12 2



and Y is a vector of observable variables suggested as indicators of the latent variables, four

of which associated to business risk, and three of which associated to loss given default.

AVOL is asset return volatility. It is taken from the solution ( S^ , AVOL) to the following

13

system, i.e. the Black/Scholes pricing relation for the equity, which is modelled as an option on firm value with the level of debt as the strike price3:

MC = S  N (d1) - K  e-rt  N (d2 )

AVOL

=



MC



S

MC  N (d1)

(10)

where

MC := market capitalization

S := value of assets

K := level of debt

 MC : equity volatility

SVOL is the volatility of sales divided by the average volume of sales. CVOL is the volatility

of the ratio of costs to sales, and RD is the ratio of research and development costs over

assets. Observed volatilities are calculated from the last three years before the observation

date. These variables are suggested to be closely related to asset volatility. While AVOL is a

direct estimate of this figure, the combination of SVOL and CVOL disaggregates the risk of

changes in the company's profit: Variation in sales and variation in the ratio of profit to sales.

Firms with higher research and development expenditure are suggested to be more risky

because the success of these activities is predictable only to a small extent. INT is the inverse

of the ratio of tangible assets over total assets and MTB is the market to book ­ ratio. These

variables are suggested to be closely associated to the loss given default, because intangible

assets often become useless once the firm cannot continue to operate, and furthermore, the

market ­ to book ratio measures future profits ("growth options") that do not fulfil the

definition of assets and most probably will be lost in case of insolvency. Research and

development spending is as well interpreted as an option on future returns which will be lost

in case of cessation of the company's operation. This procedure is similar to the structural

3 d1 and d2 are the well known arguments of the cumulative normal distribution function N(·) in the Black Scholes plain vanilla call price function.
14

equation model employed by Titman and Wessels (1988) to identify significant determinants of corporate capital structure. AVOL and INT determine the level of the latent variables x1 and x2 in order to achieve identification, i.e. the appropriate elements in  respectively in µy are set equal to 1 respectively 0.

3.8. Estimation

Because the adjustment-type model is of non-linear form, a "conditional expectation ­

maximization" (ECM) technique is used for estimation of the complete model. It is based on

maximization of the conditional loglikelihood for the observed data Y = (y1, y2, ..., yn) and

the latent population variables X = (x1, x2, ..., xn):



L(Y, X | )

=

-0.5q * n * ln( 2 ) + 

n ln



+ n ln  

+

n ln 

n +  (xi
i =1

- µ x )T

 - 1(xi

-µx)

+

i

n  (y i =1

-µ

y

-

x i )T

 -1(y i

-µ

y

-

x i )

+

i

n  =

[ 1

(l

*

(

S

,t ,i

,

lgd

t

,i

,

rt

,

bt ,i

)

-

lt -1,i

)]2





-

1

 



(11)

where
q := number of stochastic variables n := number of observations  := variance - covariance matrix of   := variance - covariance matrix of x  := Var( ) li,t := leverage of firm - year i li,t-1 := leverage of firm - year i in the previous year

This approach has previously been employed in psychological statistics for iteratively finding

the solution to nonlinear structural equation models similar to the type presented here. The

idea is simple: At the r-th iteration, using the Metropolis-Hastings algorithm (see Liu and Liu

(2001)) and a guess ^(r) on the parameter vector  = (, , , µx, µy, , ), a sample of the

15

latent variables is generated and subsequently used to find an improved estimate ^(r+1) for . The latent variables are sampled from
exp - 0.5(x i - µ x )T  - 1 (x i - µ x ) - 0.5(y i - µ y - x i )T  - 1 (y i - µ y - x i )

- 0.5[ (li*( S, t, i , lgdt, i , rt , bt, i ) - li, t - 1)]2 - 1 .

(12)

The improved estimate is found by conditional maximization of the likelihood separately for

each element of : keeping all other parameters but one constant, the likelihood function is

analytically maximized with respect to the one parameter not being held constant. At the next iteration, the improved estimate (r+1) is used for generating a new sample of the latent

variables. That means, at each iteration (r), the following system of equations is solved (see

Lee and Zhu (2002):

 E



L(Y,

X

|

)

Y,



(r

)

 

=

0

 

(13)

The standard errors of parameter estimates are calculated based on the standard method of

inverting the information matrix of the log-likelihood function. However, as some variables

cannot be observed, the following identity is used (see Louis (1982))

-

2L(Y |  ) T

=

 E-


2L(Y, X T

|



)

 



 - Var-

L(Y, X 

|

)

 

,

(14)

and expectations are calculated with respect to the conditional distribution of the latent variables given the indicator variables and the parameter vector from the last iteration, which corresponds to the procedure presented in Lee and Zhu (2002).

4. Data Each observation i represents a firm-year. Observations are taken from those firms for which a dataset exists on both Compustat North America and on CRSP. Data on market capitalization are taken from CRSP, financial statement data are from Compustat. For each
16

year between 1990 and 2006 where all relevant data are available for a firm, including its leverage in the previous year and stock data and sales data are available for the last three years, one firm-year is included in the dataset. 1990 is chosen as the starting point as that implies that no data is used from 1987, the year of a significant stock market crash, or before. Firms with a GICS Code of 4010, 4020 or 4030 (financial institutions) and foreign companies with ADR listed in the U.S. are excluded. This results in a final sample size of 13.778 firmyears. The riskfree rate and data on S&P 500 stock index returns are taken from Thomson Datastream. Firms have been assigned to industry groups according to the first two digits of their GICS code, and have been assigned to size groups by taking the natural logarithm of their total asset value (measured as total assets) to account for the skewness of the size distribution, rounded to a number without decimal spaces.
Insert Table 1 about here
5. Results 5.1. Optimal capital structure The model of optimal capital structure results in a mapping that relates asset volatility and loss given default to a leverage target, given the riskfree rate and the overall market volatility as a measure of systematic risk. Table 2 reports results of the optimization model for different sets of input data. The optimal capital structure is monotonously decreasing in both default probability and default loss intensity, reaching 55.9% for firms with average asset volatility and low lgd, and approaching as low a value as 5.7% for firms with a lgd of 0.5 and a high asset volatility of 0.6.
Insert Table 2 about here
17

5.2. Convergence Behaviour The iterative approach to estimating the parameters means that it is not possible to determine with certainty whether the estimates in the current iteration are optimal. Here, the iteration is stopped when the change of parameter estimates from one iteration to another is sufficiently small. Convergence of parameters can be visualized by observing their value at each iteration, as presented in Figure 1. The convergence behaviour indicates that 150 iterations are sufficient for reliable estimates.
Insert Figure 1 about here
5.3. Parameter Estimates The estimation procedure results in a simultaneous solution to the complete set of 24 parameters. The linear relation between firm characteristics and latent variables is provided by the  estimates ('factor loadings'), which are all positive. µ and µlgd are the means of the distribution of the latent variables and their values are reasonable; the loss given default estimate of 0.62 corresponds to the empirical result of an overall average recovery rate of 0.4 observed by Altman and Kishore (1996). Asset volatility varied considerably over time; its 1990 mean4 was 0.27, in 2000 in was about a third higher (0.36). lgd values varied, too, but to a lesser extent: mean lgd was 0.56 in 1990, but 0.63 in 2000. The adjustment speed amounts to 0.16, which means, on average, it would take a firm six years to reach its capital structure target albeit any unexpected developments.
Insert Table 3 about here
4 To calculate this mean, for each firm-year, the expectation of the latent variables over a sample of 300 simulated values using the parameter values of the last iteration was calculated, and then, these means were averaged over all firms observed in 1990 respectively 2000.
18

The simultaneous estimation procedure renders it less straightforward to interpret the significance of coefficients compared to OLS regression coefficients. The significant  coefficients demonstrate that the indicator variables exhibit a significant relation to the latent variable, where, at the same time, the significant adjustment speed parameter indicates that the latent variable is informative in determining the target capital structure. The adjustment speed parameter is 0.16, and this is within the range of previous empirical results from linear type models applied to large panel datasets, such as Fama and French (2002), who report measures between 7% and 17%, or e.g. Flannery and Rangan (2006), who report an adjustment speed of 30%. The dispersion of these results indicates that for measuring the adjustment speed, specification of the target is crucial. This finding is also supported by the results from D'Mello and Farhat (2008) who find that results of regression models for capital structure adjustments are sensitive to the proxy chosen for optimal capital structure. Therefore, both for this study and previous approaches, inference about the true adjustment speed is limited because neither approach can be considered a close-to-perfect specification of the target. A linear specification does not reflect economic causal relations appropriately; our nonlinear approach is restricted to the effects predicted by the trade-off theory. However, it can be argued that specifying and solving the optimization problem inherent in capital structure decisions is a first step towards a better approximation of the true target. Our results are different to those from Titman and Wessels (1995) who applied a linear structural equation framework to the determinants of the level of leverage. There, neither volatility nor future growth were significant determinants of the debt ratio. Future growth and volatility, on the other hand, are main drivers of our latent variables asset volatility and loss given default.
5.4. Robustness: Parameter Estimates for Industry / Size Subgroups Estimates for subgroups have been obtained by reestimating the complete model for subsamples. Adjustment speed varies considerably across industry subsamples. Financial
19

companies (excluding banks and insurances) exhibit the lowest adjustment speed. The highest adjustment speed is observed for the health care and IT businesses, which at the same time exhibit the highest asset volatility estimates: high leverage implies that it is necessary to adjust considerably fast in order to ensure a reasonable level of leverage. The adjustment speed for all other industries lies between 0.13 and 0.19; which roughly corresponds to reaching the target in between 5 and 7 years.
Insert Table 4 about here
Industry-specific estimates provide the unsurprising result that asset volatility is highest among healthcare firms which heavily rely on risky research and development activity, and IT firms, whose business is technology driven and highly competitive. For other industries, it can be seen that asset volatility is moderate, i.e. between 0.14 and 0.2, except for utility companies, where it is lower than 0.1. This is in line with the intuition that both production and sales risk in this industry is rather low. High lgd values around 0.75 are prevalent in R&D intensive healthcare and IT businesses, where insolvency triggers noticeable impairment of intangible assets and growth options. Furthermore, it is surprising to see an lgd value for firms in the financial services industry, except for banks and insurances, of as high as 0.88. In other industries, lgd values are moderate and lie between 0.4 and 0.56.
Insert Table 5 about here
Looking at the adjustment speed and asset volatility estimates by size, it becomes apparent that both measures decrease remarkably robust with firm size. This finding is consistent with the idea that larger firms are more diversified and thus, less risky. While asset volatility amounts to nearly 0.58 for the smallest firms in the sample, it is around 0.15 for the largest
20

firms. The interpretation of the lower adjustment speed for large firms is less straightforward. Three alternative explanations are plausible. First, smaller firms incur lower adjustment costs. This could be the case when smaller firms rely on short-term bank debt, whereas large firms tend to issue long-term bonds which are more difficult to redeem. Furthermore, large firms are likely to pay dividends, even if profits are low, in order to uphold the image of providing a steady dividend stream to equityholders. At the same time, it might be easier for small firms to withhold dividends when equity needs to be preserved in the company. Second, firms with higher risk are required to adjust faster towards reasonable levels of capital structure just because for those firms, deviations from the target are more expensive. If a deviation from the target for a low-risk firm for a year would mean a moderately higher cost of capital, for a high-risk firm it might imply a remarkable threat to its survival. Third, the trade-off model presented here might work reasonably well for small firms, but not so for large firms. This would be consistent with the idea that other determinants apart from the tax shield and costs of financial distress are more important to large firms, such as agency costs and signalling as well as market-timing effects. If ownership is separated from management, and if ownership is dispersed as in large firms, under- and overinvestment problems become worse, and signalling and market timing is important mainly for firms that regularly issue capital on the market, which also mainly applies to large firms. When looking at the average lgd estimates for different size groups, no significant pattern can be observed. Variation in lgd across size groups is moderate compared to variation across industries.
6. Goodness of Fit It has to be acknowledged that it is difficult to use goodness-of-fit measures to assess the quality of statistical models. On the other hand, suggesting a nonlinear model to explain capital structure adjustments requires at least a rough assessment of how well the model fits the observed data, i.e. whether the nonlinearly estimated capital structure target allows for a
21

reasonable guess on firms' adjustments. Moreover, it would be desirable to have a goodness-

of-fit measure that, at least in a rough manner, can be compared to the fit of a linear-type

model which employs the same range of variables as determinants of target leverage.

However, a standard goodness of fit ­ measure for nonlinear SEM has not yet been found.

This problem has previously been identified by Mazanec (2007). Therefore, I use two

attempts to measure goodness of fit, which provide close to identical results. The goodness of

fit measure for the nonlinear SEM is calculated as 1 minus the ratio of the sum of the squared

differences between the observed capital structure adjustment and the modelled capital

structure adjustment:

 [ ]n (li,t - li,t-1 ) - (li*,t - li,t-1 ) 2

nonlinear SEM fit = 1 - i=1 n

n 2

 i=1 (li,t - li,t-1 ) - i=1 (li,t - li,t-1 )

(14)

The modelled capital structure adjustment is the product of the estimated adjustment speed

and the adjustment towards the optimal capital structure, where the latter is calculated using

the expectation of the two latent variables. This expectation is calculated, separately for each

firm-year, by drawing 300 samples from the distribution (12) using the parameter vector

obtained in the last iteration. Another goodness-of-fit statistic is obtained by using the

modelled capital structure target, calculated by using the expectations of the latent variables as arguments for the l* function in least squares-regression (I) and calculating the R2 measure.

In order to compare the fit of the nonlinear structural model to using a linear combination of company characteristics as target leverage, nonlinear5 least-squares regression (II) is

calculated.

Regression (I): dlt = k (lt*( E[ S,t ], E[lgdt ], rt , bt ) - lt-1)

5 The regression itself is nonlinear, because products of coefficients are estimated. However, the target leverage is specified as a linear combination of company characteristics, that is, +w. In the literature, regression II is usually transformed into a regression where lt is the dependent variable and lt-1 ocurs only on the RHS. This, however, implies that it is not possible to measure goodness of fit with regard to adjustments to leverage. I estimated the transformed model, which results in an R2 of 80.2: the determinants and lt-1 explain 80.2% of the variation in lt.
22

Regression (II): dlt = k (( + w) ­ lt-1) where dlt = lt ­ lt-1 lt = leverage at time t k = adjustment speed lt* = optimal leverage ,  = regression coefficients w = vector of company characteristics, w := (AVOL, SVOL, CVOL, RDRATE, INTAN, MTB)
Insert Table 6 about here
When comparing goodness-of-fit, it needs to be kept in mind that regression (I) captures variation in capital structure adjustments based on the trade-off between debt tax shield and costs of insolvency, whereas regression (II) captures any relationship between firm characteristics and target leverage, including relationships implied by e.g. agency cost effects, signalling effects and market timing effects. Comparison of the fit of the nonlinear model and the linear model shows that the trade-off model is capable of explaining nearly as much variation in capital structure adjustments as the atheoretic linear model. That means, either, the trade-off idea dominates capital structure decisions, or, the linear model does not capture the relationship between determinants and target leverage in an appropriate way. This supports the idea that the trade-off between the tax shield effect and insolvency risk does have a significant impact on dynamic capital structure decisions.
Insert Table 7 about here
23

There is considerable difference between the fit of the nonlinear SEM and the fit of regression (II) for two industries, namely the IT business and utilities. Causal theories different from the trade-off theory seem to dominate capital structure decisions for these industries, and these theories apparently are consistent with a linear relationship between firm characteristics included in w and target leverage, which can be seen from the high R2 measures; whatever these theories will be.
Insert Table 8 about here
While the fit of the trade-off model is just as good as the fit of the linear model for small firms, this pattern changes when considering large firms. While the linear model for the target provides a moderate fit, the trade-off model is not capable of explaining any of the adjustments to leverage of large firms. This implies that there must be more than transaction costs that could prevent large firms from actively managing their capital structure. Rather, large firms seem to adjust their leverage, too, but seem to follow rules different from the trade-off theory when setting their target leverage. However, it can still be observed that the adjustment speed and the fit decreases with firm size, when modelling the target in a linear way. This also supports the idea that large firms in general adjust slower, be it for higher transaction costs or for reasons associated to their lower risk profile.
7. Conclusion Modelling the capital structure target as a linear combination of company characteristics has the result that any theory which implies a relation between such a company characteristic and the capital structure target can receive support by observing a significant coefficient. This, however, means that linear regression models will not allow rejecting a theory except if any other theory would imply an insignificant relation or a relation with a different sign. This
24

paper sets up the optimization problem for capital structure choice based on the trade-off between the debt tax shield and expected costs of insolvency, and solves for optimal leverage as a function of two company characteristics: asset volatility and losses in case of corporate default. Due to the unobservability of these, a nonlinear structural equation model is developed to simultaneously measure these latent variables and estimate an adjustment-type model for corporate capital structure, which allows testing the trade-off theory in isolation. The nonlinear approach provides strong evidence that capital structure decisions are based on the trade-off theory in small and medium-sized firms, whereas for large firms, other causal effects seem to dominate. By comparing the goodness of fit of an a-theoretical specification of target leverage as a linear combination of company characteristics to the nonlinear model for small and medium-sized firms, we see that the nonlinear trade-off model explains virtually as much of the variation in adjustments to leverage as the linear model. The latter approach additionally captures various other effects beyond the trade-off such as agency cost effects, signalling and market timing effects. However, we still cannot learn how much of the variation in capital structure is truly determined by the trade-off concept. If, for example, agency cost effects would be incorporated into the nonlinear model explicitly by estimating the marginal effect of debt on agency costs, an even better proxy could be found compared to the proxy used here. Hence, future work might bring about advancements with respect to an explicit specification of target leverage, rather than purely statistical linear specifications. The results illustrate that applying nonlinear techniques is essential for testing capital structure theories in corporate finance, rather than testing the significance of determinants, because the decision-making processes of individuals respectively firms usually do not follow linear rules.
25

7. Literature Almeida, Heitor and Philippon, Thomas: The Risk-Adjusted Cost of Financial Distress, The Journal of Finance 62 (2007), pp. 2557 ­ 2586.
Baldi, Paolo; Caramellino, Lucia and Iovino, Maria Gabriella: Pricing General Barrier Options: A Numerical Approach Using Sharp Large Deviations, Mathematical Finance 9 (1999), pp. 293 ­ 322.
Chang, Chingfu; Lee, Alice C. and Lee, Cheng F.: Determinants of capital structure choice: A structural equation modelling approach, The Quarterly Review of Economics and Finance 47 (2008), in press
De Jong, Abe; Kabir, Rezaul and Nguyen, Thuy T.: Capital structure around the world: The roles of firm- and country-specific determinants, Journal of Banking & Finance 32 (2008), pp. 1954 ­ 1969.
D'Mello, Ranjan and Farhat, Joseph: A comparative analysis of proxies for an optimal leverage ratio, Review of Financial Economics 17 (2008), pp. 213 ­ 227.
Fama, Eugene F. and French, Kenneth R.: Testing Trade-off and Pecking Order Predictions about Dividends and Debt, Review of Financial Studies 15 (2002), pp. 1 ­ 33.
Fattouh, Bassam; Harris, Lawrence and Scaramozzino, Pasquale: Non-linearity in the determinants of capital structure: evidence from UK firms, Empirical Economics 34 (2008), pp. 417 ­ 438.
26

Flannery, Mark J. and Rangan, Kasturi P.: Partial adjustment toward target capital structures, Journal of Financial Economics 79 (2006), pp. 469 ­ 506.
Graham, John R.: How Big Are the Tax Benefits of Debt?, Journal of Finance 55 (2000), pp. 1901 ­ 1941.
Graham, John R. and Harvey, Campbell R.: The theory and practice of corporate finance: Evidence from the field, Journal of Financial Economics 60 (2001), pp. 187 ­ 243.
Habermann, Christian and Kindermann, Fabian: Multidimensional Spline Interpolation: Theory and Applications, Computational Economics 30 (2007), pp. 153 ­ 169.
Lee, Sik-Yum and Zhu, Hong-Tu: Maximum Likelihood Estimation of Nonlinear Structural Equation Models, Psychometrika 67 (2002), pp. 189 ­ 210.
Lemmon, Michael L.; Roberts, Michael R. and Zender, Jaime F.: Back to the Beginning: Persistence and the Cross-Section of Corporate Capital Structure, Journal of Finance 63 (2008), pp. 1575 ­ 1608.
Liu, Jun S.: Monte Carlo Strategies in Scientific Computing, New York 2001.
Lopez-Iturriaga, Felix and Rodriguez-Sanz, Juan Antonio: Capital structure and institutional setting: a decompositional and international analysis, Applied Economics 40 (2008), pp. 1851 ­ 1864.
27

Mazanec, Josef A., Exploring Tourist Satisfaction with Nonlinear Structural Equation Modeling and Inferred Causation Analysis, Journal of Travel and Tourism Marketing 21 (2007), pp. 73 ­ 90.
Myers, Stewart C.: Determinants of Corporate Borrowing, Journal of Financial Economics 5 (1977), pp. 147 ­ 175.
Nelder, John A. and Mead, Roger: A simplex method for function maximization, Computer Journal 7 (1965), pp. 308-313.
Pao, Hsiao-Tien and Chih, Yao-Yu: Comparison of Linear and Nonlinear Models for Panel Data Forecasting: Debt Policy in Taiwan, Review of Pacific Basin Financial Markets and Policies 8 (2005), pp. 525 ­ 541.
Paudyal, Krishna, Antoniou, Antonios and Guney, Yilmaz: The Determinants of Capital Structure: Capital Market Oriented versus Bank Oriented Institutions, forthcoming in Journal of Financial and Quantitative Analysis 2008.
Rajan, Raghuram G. and Zingales, Luigi: What Do We Know about Capital Structure? Some Evidence from International Data, Journal of Finance 50 (1995), pp. 1421 ­ 1460.
Roberts, Michael R.: The Dynamics of Capital Structure: An Empirical Analysis of a Partially Observable System, Working Paper, Fuqua School of Business, 2002.
Titman, Sheridan and Wessels, Roberto: The Determinants of Corporate Capital Structure Choice, Journal of Finance 43 (1988), pp. 1 ­ 19.
28

Uhrig-Homburg, Marliese: Valuation of Defaultable Claims - A Survey, 2002, in: Schmalenbach Business Review, Vol. 54, pp 24-57. Wald, John K.: How Firm Characteristics Affect Capital Structure: An International Comparison, Journal of Financial Research 22, pp. 161 ­ 187. Zhou, Chunsheng: The term structure of credit spreads with jump risk, Journal of Banking and Finance 25 (2001), pp. 2015 ­ 2040.
29

Table 1: Summary Statistics Mean, median and standard deviation of firm-specific variables for the complete sample over all years (13778 observations, from 1990 to 2006). An observation is defined as a firm-year, i.e. an observation of a specific firm in a specific year.

leverage asset volatility std. dev. of sales / total assets std. dev. of cost to sales ratio research & development cost / sales intangible portion of assets market to book ratio total assets

mean

median std. dev.

0.27 0.22

0.21

0.35 0.28

0.28

0.21 0.15

0.21

0.16 0.02

0.56

0.08 0.04

0.15

0.63 0.65

0.24

3.63 2.47

15.82

3,167.98 324.70 21,100.73

30

Table 2: Optimal Capital Structure Solutions to the optimal capital structure problem, by asset volatility respectively by loss given default. The figures represent the optimal debt to equity ratio. (The systematic portion of asset volatility is set equal to
0.75; r = 0.05,  = 0.125, µq = -0.5 and q = = 0.15)

asset volatility

optimal capital structure when lgd = 0.5

loss given default (lgd)

optimal capital structure when asset volatility = 0.3

0,1 42,2% 0,2 30,8% 0,3 20,0% 0,4 13,0% 0,5 7,3% 0,6 5,7%

0,1 55,9% 0,25 37,1%
0,4 24,3% 0,55 19,9%
0,7 18,7% 0,85 18,1%

31

12 10
8 6 4 2 0
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99 106 113 120 127 134 141 148 my6 l6 l7 eps6
Figure 1: Some examples of slowly converging parameter estimates by number of iteration
32

Table 3: Parameter Estimates Estimates of parameters of the nonlinear structural equation model (11) for annual adjustments to corporate leverage. The adjustment speed is . Estimation is accomplished by iteratively simulating the latent variables as implied by (1), (8) and (9), based on the parameter estimates of the current iteration using the MetropolisHastings algorithm and updating the estimates by conditional maximization of the likelihood using the simulated latent variables. The number of iterations is 150; for each firm-year (n = 13778), 100 simulated values are drawn, and std. errors are estimated by inverting the information matrix obtained by using 300 simulated values of the latent variables based on the final parameter estimates to calculate the Hessian matrix and the gradient vector.

std. estimate error

t-stat.

std. estimate error

t-stat.

estimate std. error

µ2 0.0519 0.0001

711 2

µ3 -0.8487 0.0027

-309 3

µ4 -0.0683 <0.0001 -6,635 4

µ6 0.7950 0.0072

111 6

µ7 -0.2292 <0.0001 -14,911 7

µ 0.3459 0.0001
µlgd 0.6245 <0.0001

6,915 71,711

0.4701 0.0002 3.3656 0.0083 0.4298 <0.0001
3.7616 0.0177 0.4962 <0.0001

1,897 404
16,979
213 12,695

1 2 3 4 5 6 7

1 2 12

0.0561 0.0321 1.4098 0.0062 0.0514 7.7593 0.0063
0.0621 0.0441 0.0446

<0.0001 <0.0001
0.0011 <0.0001 <0.0001
0.0092 <0.0001
<0.0001 <0.0001 <0.0001

 0.1602 <0.0001 6,720  0.0097 <0.0001

33

Table 4: Parameter Estimates by Industry Estimates and standard errors of the adjustment speed and the means of the distributions of the latent variables
probability of default (µ) and loss given default (µlgd) separately estimated for different industries. Firms are assigned to industry groups according to the first two digits of their GICS code.

industry
Energy Materials Industrials Consumer Discretionary Consumer Staples Health Care Financials (excluding banks & insurances) Information Technology Telecommunication Svcs Utilities

std. t-

std.

std.

n  error stat. µ error t-stat. µlgd error t-stat.

277 0.1534 859 0.1707 2066 0.1348 2488 0.1960 512 0.1499 3101 0.2411

0.0010 155 0.2695 0.0003 498 0.1434 0.0002 591 0.2089 0.0002 1,136 0.2498 0.0015 102 0.1825 0.0003 894 0.4669

0.0004 720 0.4825 <0.0001 3,580 0.4407 <0.0001 5,903 0.5684 <0.0001 11,486 0.4210
0.0001 1,760 0.4588 0.0001 5,870 0.7487

0.0001 5,553 0.0003 1,425 <0.0001 22,811 <0.0001 9,949 0.0006 731 <0.0001 39,596

266 0.0679 4127 0.2281
59 0.1608 23 0.1625

0.0012 59 0.1620 0.0001 1,556 0.4794 0.0046 35 0.1671 0.0756 2 0.0942

0.0001 1,269 0.8837 <0.0001 17,315 0.7501
0.0003 512 0.5156 0.0007 126 0.4070

0.0010 930 <0.0001 54,276
0.0009 567 0.0043 96

34

Table 5: Parameter Estimates by Size Estimates and standard errors of the adjustment speed and the means of the distributions of the latent variables
probability of default (µ) and loss given default (µlgd) separately estimated for different firm sizes. Each firm is assigned to a size category by using the natural logaritm of its total assets figure, rounded to obtain a natural number. All firms with a number > 10 are assigned to the last group.

std. std.

std.

size group n



error t-stat. µ

error t-stat.

µlgd

error

t-stat.

1 10 0.7382 2 1226 0.3055 3 1871 0.3555 4 2899 0.2290 5 2940 0.1807 6 2235 0.1350 7 1320 0.0932 8 750 0.0768 9 350 0.0647 10 177 0.0094

0.0004 1,989 0.0005 616 0.0014 262 <0.0001 30,753 0.0001 1,454 0.0001 1,993 0.0001 884 0.0001 657 0.0002 296 0.0002 38

0.5801 0.5860 0.4630 0.4022 0.3243 0.2709 0.2323 0.1773 0.1552 0.1505

0.0157 0.0004 0.0002 <0.0001 0.0001 <0.0001 <0.0001 <0.0001 0.0001 0.0002

37 1,484 2,675 19,042 5,823 29,963 5,548 16,570 1,750
628

0.5011 0.7041 0.6668 0.6501 0.6080 0.5887 0.5832 0.5588 0.5747 0.6026

0.0338 0.0001 0.0002 <0.0001 <0.0001 <0.0001 0.0001 0.0001 0.0008 0.0253

15 5,997 3,260 20,435 16,040 200,336 8,478 8,939
700 24

35

Table 6: Complete Sample Goodness of Fit The nonlinear SEM fit denotes the goodness of fit measure as presented in (14). For regressions (I) and (II), the estimated adjustment speed k, its associated tstatistic and the goodness of fit measure R2 is presented. Both regressions use the observed adjustment to the capital structure as the dependent and a modelled adjustment as the independent variable; regression (I) is based on the nonlinearly estimated optimal capital structure, regression (II) is based on a linear combination of determinants.

nonlinear SEM fit
k t-stat. R2

13.1%

regression (I)

regression (II)

0.1785

0.2242

45.0050

46.8682

12.8%

14.5%

36

Table 7: Goodness of Fit for Industry Subsamples The nonlinear SEM fit denotes the goodness of fit measure as presented in (14). For regressions (I) and (II), the estimated adjustment speed k, its associated t-statistic and the goodness of fit measure R2 are
presented. Both regressions use the observed adjustment to the capital structure as the dependent and a
modelled adjustment as the independent variable; regression (I) is based on the nonlinearly estimated
optimal capital structure, regression (II) is based on a linear combination of determinants.

industry

nonlinear SEM regression (I) fit k t-stat. R2

regression (II) k t-stat. R2

Energy Materials Industrials Consumer Discretionary Consumer Staples Health Care Financials Information Technology Telecommunication Svcs Utilities

14.0% 14.2% 10.6% 17.2% 11.5% 14.2%
4.6% 13.9% 10.1%
6.9%

0.2002 0.1880 0.1487 0.2109 0.1576 0.2497 0.0743 0.2140 0.1591 0.1978

7.0285 14.8% 12.0561 14.3% 15.9608 10.7% 22.8827 17.3%
8.2226 11.5% 22.7092 14.2%
3.6416 4.6% 25.9031 14.0%
2.6005 10.1% 1.3375 7.1%

0.1827 0.2878 0.2102 0.2330 0.2007 0.2986 0.1445 0.3241 0.4869 0.4675

5.4663 16.7% 13.8915 19.8% 17.0856 12.7% 21.5285 17.7%
8.7965 16.3% 24.8563 17.9%
4.7802 9.9% 31.6244 21.3%
6.1094 60.8% 1.6034 68.6%

37

Table 8: Goodness of Fit for Size Subsamples The nonlinear SEM fit denotes the goodness of fit measure as presented in (14). For regressions (I) and (II), the estimated adjustment speed k, its associated t-statistic and the goodness of fit measure R2 are presented. Both regressions use the observed adjustment to the capital structure as the dependent and a modelled adjustment as the independent variable; regression (I) is based on the nonlinearly estimated optimal capital structure, regression (II) is based on a linear combination of determinants. Results are presented for different sizes of firms: each firm is assigned to a size category by using the natural logarithm of its total assets figure, rounded to obtain a natural number. All firms with a number > 10 are assigned to the last group.

nonlinear SEM regression (I)

regression (II)

size fit k t-stat. R2 k t-stat. R2

1 23.9% 0.4897 2.7022 37.0% 0.0006 0.0003 85.5% 2 24.4% 0.3513 20.3773 24.8% 0.3428 18.8367 24.9% 3 26.8% 0.3361 26.9288 26.9% 0.3720 25.1833 26.4% 4 14.7% 0.2229 22.5100 14.7% 0.2648 23.4435 17.5% 5 12.1% 0.1820 20.1051 12.1% 0.2242 20.9252 13.8% 6 8.0% 0.1353 14.1102 8.0% 0.1923 16.5943 13.6% 7 4.4% 0.0918 8.0454 4.4% 0.1527 10.9043 10.8% 8 3.9% 0.0766 5.5721 3.9% 0.1580 8.3430 11.9% 9 4.1% 0.0688 3.8883 4.2% 0.1346 5.0504 9.9% 10 0.1% 0.0099 0.6947 0.1% 0.0593 2.4185 19.4%

38

SFB 649 Discussion Paper Series 2008
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001 "Testing Monotonicity of Pricing Kernels" by Yuri Golubev, Wolfgang

Härdle and Roman Timonfeev, January 2008.

002 "Adaptive pointwise estimation in time-inhomogeneous time-series

models" by Pavel Cizek, Wolfgang Härdle and Vladimir Spokoiny,

January 2008.

003 "The Bayesian Additive Classification Tree Applied to Credit Risk

Modelling" by Junni L. Zhang and Wolfgang Härdle, January 2008.

004 "Independent Component Analysis Via Copula Techniques" by Ray-Bing

Chen, Meihui Guo, Wolfgang Härdle and Shih-Feng

Huang, January

2008.

005 "The Default Risk of Firms Examined with Smooth Support Vector

Machines" by Wolfgang Härdle, Yuh-Jye Lee, Dorothea Schäfer

and Yi-Ren Yeh, January 2008.

006 "Value-at-Risk and Expected Shortfall when there is long range

dependence" by Wolfgang Härdle and Julius Mungo, Januray 2008.

007 "A Consistent Nonparametric Test for Causality in Quantile" by

Kiho Jeong and Wolfgang Härdle, January 2008.

008 "Do Legal Standards Affect Ethical Concerns of Consumers?" by Dirk

Engelmann and Dorothea Kübler, January 2008.

009 "Recursive Portfolio Selection with Decision Trees" by Anton Andriyashin,

Wolfgang Härdle and Roman Timofeev, January 2008.

010 "Do Public Banks have a Competitive Advantage?" by Astrid Matthey,

January 2008.

011 "Don't aim too high: the potential costs of high aspirations" by Astrid

Matthey and Nadja Dwenger, January 2008.

012 "Visualizing exploratory factor analysis models" by Sigbert Klinke and

Cornelia Wagner, January 2008.

013 "House Prices and Replacement Cost: A Micro-Level Analysis" by Rainer

Schulz and Axel Werwatz, January 2008.

014 "Support Vector Regression Based GARCH Model with Application to

Forecasting Volatility of Financial Returns" by Shiyi Chen, Kiho Jeong and

Wolfgang Härdle, January 2008.

015 "Structural Constant Conditional Correlation" by Enzo Weber, January

2008.

016 "Estimating Investment Equations in Imperfect Capital Markets" by Silke

Hüttel, Oliver Mußhoff, Martin Odening and Nataliya Zinych, January

2008.

017 "Adaptive Forecasting of the EURIBOR Swap Term Structure" by Oliver

Blaskowitz and Helmut Herwatz, January 2008.

018 "Solving, Estimating and Selecting Nonlinear Dynamic Models without

the Curse of Dimensionality" by Viktor Winschel and Markus Krätzig,

February 2008.

019 "The Accuracy of Long-term Real Estate Valuations" by Rainer Schulz,

Markus Staiber, Martin Wersing and Axel Werwatz, February 2008.

020 "The Impact of International Outsourcing on Labour Market Dynamics in

Germany" by Ronald Bachmann and Sebastian Braun, February 2008.

021 "Preferences for Collective versus Individualised Wage Setting" by Tito

Boeri and Michael C. Burda, February 2008.

SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

022 "Lumpy Labor Adjustment as a Propagation Mechanism of Business Cycles" by Fang Yao, February 2008.
023 "Family Management, Family Ownership and Downsizing: Evidence from S&P 500 Firms" by Jörn Hendrich Block, February 2008.
024 "Skill Specific Unemployment with Imperfect Substitution of Skills" by Runli Xie, March 2008.
025 "Price Adjustment to News with Uncertain Precision" by Nikolaus Hautsch, Dieter Hess and Christoph Müller, March 2008.
026 "Information and Beliefs in a Repeated Normal-form Game" by Dietmar Fehr, Dorothea Kübler and David Danz, March 2008.
027 "The Stochastic Fluctuation of the Quantile Regression Curve" by Wolfgang Härdle and Song Song, March 2008.
028 "Are stewardship and valuation usefulness compatible or alternative objectives of financial accounting?" by Joachim Gassen, March 2008.
029 "Genetic Codes of Mergers, Post Merger Technology Evolution and Why Mergers Fail" by Alexander Cuntz, April 2008.
030 "Using R, LaTeX and Wiki for an Arabic e-learning platform" by Taleb Ahmad, Wolfgang Härdle, Sigbert Klinke and Shafeeqah Al Awadhi, April 2008.
031 "Beyond the business cycle ­ factors driving aggregate mortality rates" by Katja Hanewald, April 2008.
032 "Against All Odds? National Sentiment and Wagering on European Football" by Sebastian Braun and Michael Kvasnicka, April 2008.
033 "Are CEOs in Family Firms Paid Like Bureaucrats? Evidence from Bayesian and Frequentist Analyses" by Jörn Hendrich Block, April 2008.
034 "JBendge: An Object-Oriented System for Solving, Estimating and Selecting Nonlinear Dynamic Models" by Viktor Winschel and Markus Krätzig, April 2008.
035 "Stock Picking via Nonsymmetrically Pruned Binary Decision Trees" by Anton Andriyashin, May 2008.
036 "Expected Inflation, Expected Stock Returns, and Money Illusion: What can we learn from Survey Expectations?" by Maik Schmeling and Andreas Schrimpf, May 2008.
037 "The Impact of Individual Investment Behavior for Retirement Welfare: Evidence from the United States and Germany" by Thomas Post, Helmut Gründl, Joan T. Schmit and Anja Zimmer, May 2008.
038 "Dynamic Semiparametric Factor Models in Risk Neutral Density Estimation" by Enzo Giacomini, Wolfgang Härdle and Volker Krätschmer, May 2008.
039 "Can Education Save Europe From High Unemployment?" by Nicole Walter and Runli Xie, June 2008.
040 "Solow Residuals without Capital Stocks" by Michael C. Burda and Battista Severgnini, August 2008.
041 "Unionization, Stochastic Dominance, and Compression of the Wage Distribution: Evidence from Germany" by Michael C. Burda, Bernd Fitzenberger, Alexander Lembcke and Thorsten Vogel, March 2008
042 "Gruppenvergleiche bei hypothetischen Konstrukten ­ Die Prüfung der Übereinstimmung von Messmodellen mit der Strukturgleichungsmethodik" by Dirk Temme and Lutz Hildebrandt, June 2008.
043 "Modeling Dependencies in Finance using Copulae" by Wolfgang Härdle, Ostap Okhrin and Yarema Okhrin, June 2008.
044 "Numerics of Implied Binomial Trees" by Wolfgang Härdle and Alena Mysickova, June 2008.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

045 "Measuring and Modeling Risk Using High-Frequency Data" by Wolfgang Härdle, Nikolaus Hautsch and Uta Pigorsch, June 2008.
046 "Links between sustainability-related innovation and sustainability management" by Marcus Wagner, June 2008.
047 "Modelling High-Frequency Volatility and Liquidity Using Multiplicative Error Models" by Nikolaus Hautsch and Vahidin Jeleskovic, July 2008.
048 "Macro Wine in Financial Skins: The Oil-FX Interdependence" by Enzo Weber, July 2008.
049 "Simultaneous Stochastic Volatility Transmission Across American Equity Markets" by Enzo Weber, July 2008.
050 "A semiparametric factor model for electricity forward curve dynamics" by Szymon Borak and Rafal Weron, July 2008.
051 "Recurrent Support Vector Regreson for a Nonlinear ARMA Model with Applications to Forecasting Financial Returns" by Shiyi Chen, Kiho Jeong and Wolfgang K. Härdle, July 2008.
052 "Bayesian Demographic Modeling and Forecasting: An Application to U.S. Mortality" by Wolfgang Reichmuth and Samad Sarferaz, July 2008.
053 "Yield Curve Factors, Term Structure Volatility, and Bond Risk Premia" by Nikolaus Hautsch and Yangguoyi Ou, July 2008.
054 "The Natural Rate Hypothesis and Real Determinacy" by Alexander MeyerGohde, July 2008.
055 "Technology sourcing by large incumbents through acquisition of small firms" by Marcus Wagner, July 2008.
056 "Lumpy Labor Adjustment as a Propagation Mechanism of Business Cycle" by Fang Yao, August 2008.
057 "Measuring changes in preferences and perception due to the entry of a new brand with choice data" by Lutz Hildebrandt and Lea Kalweit, August 2008.
058 "Statistics E-learning Platforms: Evaluation Case Studies" by Taleb Ahmad and Wolfgang Härdle, August 2008.
059 "The Influence of the Business Cycle on Mortality" by Wolfgang H. Reichmuth and Samad Sarferaz, September 2008.
060 "Matching Theory and Data: Bayesian Vector Autoregression and Dynamic Stochastic General Equilibrium Models" by Alexander Kriwoluzky, September 2008.
061 "Eine Analyse der Dimensionen des Fortune-Reputationsindex" by Lutz Hildebrandt, Henning Kreis and Joachim Schwalbach, September 2008.
062 "Nonlinear Modeling of Target Leverage with Latent Determinant Variables ­ New Evidence on the Trade-off Theory" by Ralf Sabiwalsky, September 2008.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

