BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2006-010
Common Functional Principal Components
Michal Benko* Wolfgang H‰rdle*
Alois Kneip**
* CASE - Center for Applied Statistics and Economics, Humboldt-Universit‰t zu Berlin, Germany
** Institute of Statistics, Department of Economics, Universit‰t Bonn, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

COMMON FUNCTIONAL PRINCIPAL COMPONENTS
By Michal Benko, Wolfgang Ha®rdle and Alois Kneip
Humboldt-Universita®t and Bonn Universita®t
Functional principal component analysis (FPCA) based on the Karhunen-Lo`eve decomposition has been successfully applied in many applications, mainly for one sample problems. In this paper we consider common functional principal components for two sample problems. Our research is motivated not only by the theoretical challenge of this data situation but also by the actual question of dynamics of implied volatility (IV) functions. For different maturities the logreturns of IVs are samples of (smooth) random functions and the methods proposed here study the similarities of their stochastic behavior. Firstly we present a new method for estimation of functional principal components from discrete noisy data. Next we present the two sample inference for FPCA and develop two sample theory. We propose bootstrap tests for testing the equality of eigenvalues, eigenfunctions, and mean functions of two functional samples, illustrate the test-properties by simulation study and apply the method to the IV analysis.

1. Introduction. In many applications in biometrics, chemometrics, economet-

rics, etc., the data come from the observation of continuous phenomenons of time or

space and can be assumed to represent a sample of i.i.d. smooth random functions

X1(t), . . . , Xn(t)  L2[0, 1]. Functional data analysis has received considerable at-

tention in the statistical literature during the last decade. In this context functional

principal component analysis (FPCA) has proved to be a key technique. An early

reference is Rao (1958), and some important methodological contributions are, for ex-

ample, given in Besse & Ramsay (1986) or Rice & Silverman (1991). For an overview

of FPCA applications studied by various authors see Ramsay & Silverman (2002) or

Ramsay & Silverman (2005).

The well-known Karhunen-Lo`eve (KL) expansion provides a basic tool to describe

the distribution of the random functions Xi and can be seen as the theoretical basis

of FPCA. For v, w  L2[0, 1] let

v, w

=

1 0

v(t)w(t)dt,

and

let

∑ = ∑, ∑ 1/2 denote

the usual L2-norm. With 1  2  . . . and 1, 2, . . . denoting eigenvalues and

We gratefully acknowledge financial support by the Deutsche Forschungsgemeinschaft and the Sonderforschungsbereich 649 "O® konomisches Risiko".
AMS 2000 subject classifications: Primary 62H25,62G08; secondary 62P05 JEL classifications: C14, G19 Keywords and phrases: Functional Principal Components, Nonparametric Regression, Bootstrap, Two Sample Problem

1

2 M. BENKO, W. HA® RDLE AND A.KNEIP
corresponding orthonormal eigenfunctions of the covariance operator  of Xi we obtain

Xi = µ + rir, i = 1, . . . , n, where µ = E(Xi) is the mean function and ri =
r=1
Xi-µ, r are (scalar) factor loadings with E(r2i) = r. Structure and dynamics of the random functions can be assessed by analyzing the "functional principal components" r as well as the distribution of the factor loadings. For a given functional sample, the unknown characteristics r, r are estimated by the eigenvalues and eigenfunctions of the empirical covariance operator ^n of X1, . . . , Xn.
In many important applications a small number of functional principal components will suffice to approximate the functions Xi with a high degree of accuracy. Indeed, FPCA plays a much more central role in functional data analysis than its well-known analogue in multivariate analysis. There are two major reasons. First, distributions on function spaces are complex objects, and the Karhunen-Lo`eve expansion seems to be the only practically feasible way to access their structure. Secondly, in multivariate analysis a substantial interpretation of principal components is often difficult and has to be based on vague arguments concerning the correlation of principal components with original variables. Such a problem does not at all exists in the functional context, where 1(t), 2(t), . . . are functions representing the major modes of variation of Xi(t) over t.
In this paper we consider inference and tests of hypotheses on the structure of functional principal components. Motivated by an application to implied volatility analysis we will concentrate on the two sample case. A central point is the use of bootstrap procedures. We will show that the bootstrap methodology can also be applied to functional data.
In Section 2 we start by discussing one-sample inference for FPCA. Basic results on asymptotic distributions have already been derived by Dauxois, Pousse & Romain (1982) in situations where the functions are directly observable. However, in practice the functions of interest are often not directly observed but are regression curves which have to be reconstructed from discrete, noisy data. Section 2.1 therefore presents a new method for estimation of functional principal components in such situations. It consists in an adaptation of a technique introduced by Kneip & Utikal (2001) for the case of density functions. The key-idea is to represent the components of the Karhunen-Lo`eve expansion in terms of an (L2) scalar-product matrix of the sample. We investigate the asymptotic properties of the proposed method. It is shown that under mild conditions the additional error caused by estimation from discrete, noisy data is first-order asymptotically negligible, and inference may proceed "as if" the functions were directly observed. Generalizing the results of Dauxois, Pousse & Romain (1982), we then present a theorem on the asymptotic distributions of the empirical eigenvalues and eigenfunctions. The structure of the asymptotic expansion derived in the theorem provides a basis to show consistency of bootstrap procedures.

COMMON FUNCTIONAL PC

3

Section 3 deals with two-sample inference. We consider two independent samples of functions {Xi(1)}ni=11 and {Xi(2)}ni=21. The problem of interest is to test in how far the distributions of these random functions coincide. The structure of the different

distributions in function space can be accessed by means of the respective Karhunen-

Lo`eve expansions


Xi(p) = µ(p) + r(pi )r(p), p = 1, 2.

r=1

Differences in the distribution of these random functions will correspond to differences in the components of the respective KL expansions above. Two sample inference for

FPCA in general has not been considered in the literature so far. In Section 3 we define bootstrap procedures for testing the equality of mean functions, eigenvalues, eigenfunctions, and eigenspaces. Consistency of the bootstrap is derived in Section

3.1, while Section 3.2 contains a simulation study providing insight into the finite
sample performance of our tests.
It is of particular interest to compare the functional components characterizing the two samples. If these factors are "common", this means r := r(1) = r(2), then only the factor loadings r(pi ) may vary across samples. This situation may be seen as a functional generalization of the concept of "common principal components" as introduced
by Flury (1988) in multivariate analysis. A weaker hypothesis may only require equality of the eigenspaces spanned by the first L  IN functional principal components.
If for both samples the common L-dimensional eigenspaces suffice to approximate
the functions with high accuracy, then the distributions in function space are well

represented by a low dimensional factor model, and subsequent analysis may rely on comparing the multivariate distributions of the random vectors (r(p1), . . . , r(pL)) .
The idea of "common functional principal components" is of considerable impor-
tance in implied volatility (IV) dynamics. This application is discussed in detail in
Section 4. Implied volatility is obtained from the pricing model proposed by Black &
Scholes (1973) and is a key parameter for quoting options prices. Our aim is to con-
struct low dimensional factor models for the log-returns of the IV functions of options
with different maturities. In our application the first group of functional observations ≠ {Xi(1)}in=11, are log-returns on the maturity "1 month" (1M group) and second group ≠ {Xi(2)}ni=21, are log-returns on the maturity "3 months" (3M group).
The first three eigenfunctions (ordered with respect to the corresponding eigenval-
ues), estimated by the method described in Section 2.1, are plotted in Figure 1. The
estimated eigenfunctions for both groups are of similar structure which motivates a

common FPCA approach. Based on discretized vectors of functional values, a (multivariate) common principal components analysis of implied volatilities has already been considered by Fengler, H®ardle & Villa (2003). They rely on the methodology

introduced by Flury (1988) which is based on maximum likelihood estimation under

4 M. BENKO, W. HA® RDLE AND A.KNEIP

Estimated Eigenfunctions, 1M
0.85 0.90 0.95 ATM 1.05 4.00 3.00 2.00 1.00 0.00 -1.00 -2.00

4.00 3.00 2.00 1.00 0.00 -1.00 -2.00

Estimated Eigenfunctions, 3M
0.85 0.90 0.95 ATM 1.05 4.00 3.00 2.00 1.00 0.00 -1.00 -2.00

4.00 3.00 2.00 1.00 0.00 -1.00 -2.00

0.85 0.90 0.95 ATM 1.05

0.85 0.90 0.95 ATM 1.05

Fig 1. Estimated eigenfunctions for 1M group in the left plot and 3M group in the right plot, blue solid ≠ first function, red dashed ≠ second function, black finely dashed ≠ third function.

the assumption of multivariate normality. Our analysis overcomes the limitations of this approach by providing specific hypothesis tests in a fully functional setup. It will be shown in Section 4 that for both groups L = 3 components suffice to explain 98.2% of the variability of the sample functions. An application of the tests developed in Section 3 does not reject the equality of the corresponding eigenspaces.
2. Functional Principal Components and one sample inference. In this section we will focus on one sample of i.i.d. smooth random functions X1, . . . , Xn  L2[0, 1]. We will assume a well-defined mean function µ = E(Xi) as well as the existence of a continuous covariance function (t, s) = E[{Xi(t)-µ(t)}{Xi(s)-µ(s)}]. Then E( Xi - µ 2) = (t, t)dt < , and the covariance operator  of Xi is given by
(v)(t) = (t, s)v(s)ds, v  L2[0, 1].

The Karhunen-Lo`eve decomposition provides a basic tool to describe the distribution of the random functions Xi. With 1  2  . . . and 1, 2, . . . denoting eigenvalues and a corresponding orthonormal basis of eigenfunctions of  we obtain


Xi = µ + rir, i = 1, . . . , n,
r=1

(1)

where ri = Xi - µ, r are uncorrelated (scalar) factor loadings with E(ri) = 0, E(r2i) = r, and E(riki) = 0 for r = k. Structure and dynamics of the random

COMMON FUNCTIONAL PC

5

functions can be assessed by analyzing the "functional principal components" r as

well as the distribution of the factor loadings.

A discussion of basic properties of (1) can, for example, be found in Gihman and

Skorohod (1973). Under our assumptions, the infinite sums in (1) converge with prob-

ability 1, and

 r=1

r

=

E(

Xi - µ

2)

<

.

Smoothness

of

Xi

carries

over

to

a

corresponding degree of smoothness of (t, s) and r. If, with probability 1, Xi(t) is

twice continuously differentiable, then  as well as r are also twice continuously dif-

ferentiable. The particular case of a Gaussian random function Xi implies that the ri

are independent N (0, r)-distributed random variables.

An important property of (1) consists in the known fact that the first L principal

components provide a "best basis" for approximating the sample functions in terms of

the integrated square error. For any choice of L orthonormal basis functions v1, . . . , vL
L
the mean integrated square error: (v1, . . . , vL) = E( Xi - µ - Xi - µ, vr vr 2)
r=1
is minimized by vr = r.

2.1. Estimation of Functional Principal Components. For a given sample an em-
pirical analog of (1) can be constructed by using eigenvalues ^1  ^2  . . . and orthonormal eigenfunctions ^1, ^2, . . . of the empirical covariance operator ^n, where

(^nv)(t) = ^(t, s)v(s)ds

nn
with XØ = n-1 Xi and ^(t, s) = n-1 {Xi(t) - XØ (t)}{Xi(s) - XØ (s)} denoting
i=1 i=1
sample mean and covariance function. Then

n
Xi = XØ + ^ri^r, i = 1, . . . , n,
r=1

(2)

where ^ri =< ^r, Xi - XØ >. We necessarily obtain n-1 i ^ri = 0, n-1 i ^ri^si = 0 for r = s, and n-1 i ^r2i = ^r. Obviously, ^r and ^r estimate r and r for r = 1, 2, . . . . The results of Dauxois, Pousse & Romain (1982) imply that under regularity conditions ^r - r = Op(n-1/2), |^r - r| = Op(n-1/2), as well as |^ri - ri| = Op(n-1/2).
However, in practice, the sample functions Xi are often not directly observed, but
have to be reconstructed from noisy observations Yij at discrete design points tik:

Yik = Xi(tik) + ik, k = 1, . . . , Ti,

(3)

where ik are independent noise terms with E(ik) = 0, Var(ik) = i2. In this context the standard approach to estimate functional principal components
is to first estimate individual functions nonparametrically (e.g. by B-Splines) and then

6 M. BENKO, W. HA® RDLE AND A.KNEIP

to determine eigenfunctions of the resulting estimated empirical covariance operator ≠ see, e.g., Ramsay & Silverman (2005).
We propose an approach motivated by the well known duality relation between row and column spaces of a data matrix, see Ha®rdle, & Simar (2003) chapter 8, among others. In a first step this approach relies on estimating the elements of the matrix:

Mlk = Xl - XØ , Xk - XØ , l, k = 1, . . . , n.

(4)

Some simple linear algebra shows that all nonzero eigenvalues ^1  ^2 . . . of ^n and l1  l2 . . . of M are related by ^r = lr/n, r = 1, 2, . . . . When using the correspondienmgpoirritchaolneoigrmenafluneicgteionnvsec^trorasrep1o,bpt2a,i.n.e.dobf yM^,rith=eemlrppiirriacanldscores ^ri as well as the

^r

=

1 lr

n i=1

pir

Xi - XØ

=

1 lr

n
pir Xi .
i=1

(5)

The elements of M are functionals which can be estimated with asympotically negligible bias and a parametric rate of convergence Ti-1/2. If the data in (3) is generated from a balanced, equidistant design, then it is easily seen that for i = j this rate of
convergence is achieved by the estimator:

T
Mij = T -1 (Yik - YØ∑k)(Yjk - YØ∑k), i = j,
k=1

and
T
Mii = T -1 (Yik - YØ∑k)2 - ^i2.

k=1

Where ^i2 denotes some nonparametric estimator of variance and YØ∑k = n-1

n j=1

Yjk

.

In the case of a random design some adjustment is necessary: Define the ordered

sample ti(1)  ti(2)  ∑ ∑ ∑  ti(Ti) of design points, and for j = 1, . . . , Ti let Yi(j) denote the observation belonging to ti(j). With ti(0) = -ti(1) and ti(Ti+1) = 2 - ti(Ti) set

Ti
i(t) = Yi(j)I

t

ti(j-1) + ti(j) , ti(j) + ti(j+1) 22

j=1

, t  [0, 1],

where I(∑) denotes the indicator function, and for i = j define the estimate of Mij by

1
Mij = {i(t) - Ø(t)} {j(t) - Ø(t)} dt,
0

COMMON FUNCTIONAL PC

7

where Ø(t) = n-1

n i=1

i

(t).

Finally,

by

redefining

ti(1)

=

-ti(2)

and

ti(Ti+1)

=

2 - ti(Ti), set i (t) =

Ti j=2

Yi(j-1)

I

t  [ , ]ti(j-1)+ti(j) ti(j)+ti(j+1)
22

,

t



[0, 1].

Then

construct estimators of the diagonal terms Mii by

1
Mii = {i(t) - Ø(t)} {i (t) - Ø(t)} dt.
0

(6)

The aim of using the estimator (6) for the diagonal terms is to avoid the additional bias implied by E(Yi2k) = Xi(tij)2 + i2. Here E denotes conditional expectation given tij, Xi. Alternatively we can construct a bias corrected estimator using some nonparametric estimation of variance i2, e.g. the difference based model-free variance estimators studied in Hall, Kay & Titterington (1990) can be employed.
The eigenvalues ^l1  ^l2 . . . and eigenvectors p^1, p^2, . . . of the resulting matrix M then provide estimates ^r;T = ^lr/n and ^ri;T = ^lrp^ir of ^r and ^ri. Estimates
^r;T of the empirical functional principal component ^r can be determined from (5) when replacing the unknown true functions Xi by nonparametric estimates X^i (as, for
example, local polynomial estimates) with smoothing parameter (bandwidth) b:

^r;T =

1 ^lr

n
p^ir X^ i .
i=1

(7)

When considering (7), it is important to note that ^r;T is defined as a weighted average of all estimated sample functions. Averaging reduces variance, and efficient

estimation of ^r therefore requires undersmoothing of individual function estimates X^i. Theoretical results are given in Theorem 1 below. Indeed, if for example n and
T = mini Ti are of the same order of magnitude, then under suitable additional regu-

larity conditions it will be shown that for an optimal choice of a smoothing parameter
b  (nT )-1/5 and twice continuously differentiable Xi, we obtain the rate of convergence ^r - ^r;T = Op{(nT )-2/5}. Note, however, that the bias corrected estimator

(6) may yield negative eigenvalues. In practice these values will be small and will have

to be interpreted as zero. Furthermore, the eigenfunctions determined by (7) may not

be exactly orthogonal. Again, when using reasonable bandwidths, this effect will be

small, but of course (7) may by followed by suitable orthogonalization procedure.

It is of interest to compare our procedure to more standard methods for estimating ^r and ^r as mentioned above. When evaluating eigenvalues and eigenfunctions of the empirical covariance operator of nonparametrically estimated curves X^i, then for
fixed r  {1, 2, . . . } the above rate of convergence for the estimated eigenfunctions

may well be achieved for a suitable choice of smoothing parameters (e.g. number

of basis functions). But as will be seen from Theorem 1 our approach also implies

that

|^r

-

^lr n

|

=

Op(T -1

+

n-1).

When

using

standard

methods

it

does

not

seem

to

8 M. BENKO, W. HA® RDLE AND A.KNEIP

be possible to obtain a corresponding rate of convergence, since any smoothing bias |E[X^i(t)] - Xi(t)| will invariably affect the quality of the corresponding estimate of ^r.
Note that in addition to (7) our final estimate of the empirical mean function µ^ = XØ will be given by µ^T = n-1 i X^i. A straightforward approach to determine a suitable bandwidth b consists in a "leave-one-individual-out" cross-validation. For a fixed s 
IN let µ^T,-i and ^r;T,-i, r = 1, . . . , s denote the estimates of µ^ and ^r obtained from the
data (Ylj, tlj), l = 1, . . . , i - 1, i + 1, . . . , n, j = 1, . . . , Tk. By (7) these estimates depend
on b, and one may approximate an optimal smoothing parameter by minimizing

ij

s2
Yij - µ^T,-i(tij ) - ^ri^r;T,-i(tij )
r=1

over b, where ^ri denote ordinary least squares estimates of ^ri. A more sophisticated version of this method may even allow to select different bandwidths br when estimating different functional principal components by (7). Although, under certain
regularity conditions, the same qualitative rates of convergence hold for any arbitrary
fixed r, convergence is not uniform over r = 1, 2, . . . . Due to < s, r >= 0 for s < r, the number of zero crossings, peaks and valleys of r has to increase with r. Hence, in tendency r will be less and less smooth as r increases. At the same time, r  0 which means that for large r the r-th eigenfunctions will only possess a very small
influence on the structure of Xi. This in turn means that the relative importance of the error terms ik in (3) on the structure of ^r;T will increase with r.

2.2. One sample inference. Clearly, in the framework described by (1) - (3) we

are faced with two sources of variability of estimated functional principal components.

Due to sampling variation ^r will differ from the true component r, and due to (3)

there will exist an additional estimation error when approximating ^r by ^r;T .

The following theorems quantify the order of magnitude of these different types of

error. Our theoretical results are based on the following assumptions on the structure

of the random functions Xi.

Assumption 1.

X1, . . . , Xn  L2[0, 1] is an i.i.d. sample of random functions with mean µ and contin-

uous covariance function (t, s), and (1) holds for a system of eigenfunctions satisfying

supr supt[0,1] r(t) < .

Furthermore,

 r=1

 s=1

E[r2is2i]

<



and

r = 1, 2, . . . .

 q=1

 s=1

E[r2iqisi]

<



for

all

Recall that E[ri] = 0 and E[risi] = 0 for r = s. Note that the assumption on

the factor loadings is necessarily fulfilled if Xi are Gaussian random functions. Then

ri and si are independent for r = s, all moments of moments ri are finite, and hence E[r2iqisi] = 0 for q = s as well as E[r2is2i] = rs for r = s, see Gihman and
Skorohod (1973).

COMMON FUNCTIONAL PC

9

We need some further assumptions concerning smoothness of Xi and the structure of the discrete model (3).
Assumption 2.
a) Xi is a.s. twice continuously differentiable. There exists a constant D1 <  such that the derivatives are bounded by supt E[Xi (t)4]  D1 as well as supt E[Xi (t)4]  D1.
b) The design points tik, i = 1, . . . , n, k = 1, . . . , Ti are i.i.d. random variables which are independent of Xi and ik. The corresponding design density f is continuous on [0, 1] and satisfies inft[0,1] f (t) > 0.
c) For any i the error terms ik are i.i.d. zero mean random variables with Var(ik) = i2. Furthermore, ik is independent of Xi, and there exists a constant D2 such that E(i8k) < D2 for all i, k.
d) The estimates X^i used in (7) are determined by either a local linear or a NadarayaWatson kernel estimator with smoothing parameter b and kernel function K. K is a continuous probability density which is symmetric at 0.
The following theorems provide asymptotic results as n, T  , where T = minni=1{Ti}. Note that eigenfunctions and eigenvectors are only unique up to sign changes. In the following we will always assume that the right "versions" are used. This will go without saying.

Theorem 1: In addition to Assumptions 1 and 2 assume that infs=r |r - s| > 0 holds for some r = 1, 2, . . . . Then

i) n-1

n i=1

(^ri

-

^ri;T )2

=

Op(T -1)

and

|^r

-

^lr n

|

=

Op(T

-1

+

n-1).

(8)

ii) If additionally (T b2)-1  0 as n, T  , then for all t  [0, 1]

|^r(t) - ^r;T (t)| = Op{b2 + (nT b)-1/2 + (T b1/2)-1 + n-1}.

(9)

A proof is given in the appendix. Theorem 2: Under Assumption 1 we obtain:
i) For all t  [0, 1]

n{XØ (t) - µ(t)} =
r

1 n

n

ri

r(t) L N

0,

rr(t)2 ,

i=1 r

10 M. BENKO, W. HA® RDLE AND A.KNEIP

If, furthermore, r-1 > r > r+1 holds for some fixed r  {1, 2, . . . }, then ii)

n(^r

-

r )

=

1 n

n

r2i - r + Op(n-1/2) L N (0, r),

i=1

(10)

where r = E[(r2i - r)2], iii) and for all t  [0, 1]

^r(t) - r(t) =
s=r

1n n(r - s) i=1 siri

s(t) + Rr(t), where Rr = Op(n-1). (11)

Moreover,

 n
s=r

1n n(r - s) i=1 siri



s(t) L N 0,

q=r

s=r

(q

E[r2iqisi] - r)(s -

r

)

q

(t)s(t)

A proof can be found in the appendix. The theorem provides a generalization of the

results of Dauxois, Pousse & Romain (1982) who derive explicit asymptotic distrib-

utions by assuming Gaussian random functions Xi. Note that in this case r = 2r2,

and

q=r

s=r

E[r2iqisi]
(q -r )(s -r

)

q

(t)s(t)

=

s=r

r s (s-r

)2

s

(t)2

.

Theoretical work in functional data analysis is usually based on the implicit as-

sumption that the additional error due to (3) is negligible, and that one can proceed

"as if" the functions Xi were directly observed. In view of Theorems 1 and 2 this

approach is justified in the following situations:

1) T is much larger than n, i.e. n/T 4/5  0, and the smoothing parameter b in (7) is of order T -1/5 (optimal smoothing of individual functions).
2) T is smaller than n but n/T 2  0, and an undersmoothing bandwidth b  (nT )-1/5 is used.

In

both

cases

1)

and

2)

the

above

theorems

imply

that

|^r

-

^lr n

|

=

Op(|^r

-

r |)

as

well as ^r - ^r;T = Op( ^r - r ). Inference about functional principal components

will then be first order equivalent to an inference based on known functions Xi.

In such situations Theorem 2 suggests bootstrap procedures as tools for one sam-

ple inference. For example, the distribution of ^r - r may by approximated by the bootstrap distribution of ^r - ^r , where ^r are estimates to be obtained from i.i.d. bootstrap resamples X1, X2, . . . , Xn of {X1, X2, . . . , Xn}. This means that X1 =

COMMON FUNCTIONAL PC

11

Xi1, . . . , Xn = Xin for some indices i1, . . . , in drawn independently and with replacement from {1, . . . , n} and, in practice, ^r may thus be approximated from corresponding discrete data (Yi1j , ti1j )j=1,....Ti1 , . . . , (Yinj , tinj )j=1,....Tin . The additional error is negligible if either 1) or 2) are satisfied.
One may wonder about the validity of such a bootstrap. Functions are complex
objects and there is no established result in bootstrap theory which readily generalizes to samples of random functions. But by (1) i.i.d. bootstrap resamples {Xi}i=1,...,n may be equivalently represented by corresponding i.i.d. resamples {1i, 2i, . . . }i=1,...,n of factor loadings. Standard multivariate bootstrap theorems imply that for any q  IN
the distribution of moments of the random vectors (1i, . . . , qi) may be consistently approximated by the bootstrap distribution of corresponding moments of (1i, . . . , qi). Together with some straightforward limit arguments as q  , the structure of the
first order terms in the asymptotic expansions (10) and (11) then allows to establish
consistency of the functional bootstrap. These arguments will be made precise in the
proof of Theorem 3 below, which concerns related bootstrap statistics in two sample
problems.

2.3. Example. For the illustration purposes, we use a simulated functional data set

of random linear combinations of two Fourier functions:

 Xi(tik) = 1i 2 sin(2tik) + 2i 2 cos(2tik) + ik

(12)

where the factor loadings are normally distributed with 1i  N (0, 6), 2i  N (0, 4), the error terms ik  N (0, 0.25) (all of them i.i.d. over i and k). The functions are generated ("observed") on the uniformly i.i.d. grid tik  U [0, 1], k = 1, . . . , T = 150, i = 1, . . . , n = 40. The estimators X^i are obtained by the local constant (NadarayaWatson) estimator with Epanechnikov kernel and bandwidth b = 0.07.
Estimators X^i of the simulated functional data set and estimator of the first eigenfunction are displayed in the Figure 2. The Figure 3 gives another insight in to the finite
sample behavior. Here we have repeated the simulations 50 times, with 1i  N (0, 6), 2i  N (0, 4), ik  N (0, 0.25). We can see that the variation of the sample generated by the scheme (12) is essentially reflected in some shift of the estimated eigenfunction.

3. Two sample inference. The comparison of functional components across groups leads naturally to two sample problems. Thus let
X1(1), X2(1), . . . , Xn(11) and X1(2), X2(2), . . . , Xn(22)
denote two independent samples of smooth functions. The problem of interest is to test in how far the distributions of these random functions coincide. The structure of the different distributions in function space can be accessed by means of the respective

12 M. BENKO, W. HA® RDLE AND A.KNEIP

Simulated functions

Simulated example

Y -5 0 5
Y -1 -0.5 0 0.5 1

0 0.5 1 0 0.5 1 XX
Fig 2. Simulated example, in the left picture the Nadaraya-Watson estimators of simulated functions are plotted (b=0.07). Estimated mean functions (black thick), in the right picture the estimated first (blue) and second (red) eigenfunction, true eigenfunctions: (first blue, second red dashed).

Karhunen-Lo`eve decompositions. The problem to be considered then translates into testing equality of the different components of these decompositions given by



Xi(p) = µ(p) +

r(pi )r(p),

r=1

p = 1, 2,

(13)

where again r(p) are the eigenfunctions of the respective covariance operator (p) corresponding to the eigenvalues 1(p) = E{(1(pi ))2}  (2p) = E{(2(pi ))2}  . . . .
It is of great interest to detect possible variations in the functional components char-
acterizing the two samples in (13). Significant difference may give rise to substantial
interpretation. Important hypotheses to be considered thus are:

H01 : µ(1) = µ(2) and H02,r : r(1) = r(2), r = 1, 2, . . .

Hypothesis H02,r is of particular importance. Then r(1) = r(2) and only the factor loadings ri may vary across samples. This assumption has been used in the work of Fengler, Ha®rdle & Villa (2003) and Benko & Ha®rdle (2005) in modeling implied
volatilities. It can be seen as a functional generalization of the concept of "common
principal components" as introduced by Flury (1988) in multivariate analysis.
If, for example, H02,r is accepted one may additionally want to test hypotheses about the distributions of r(pi ), p = 1, 2. Recall that necessarily E{r(pi )} = 0, E{r(pi )}2 = (rp),

COMMON FUNCTIONAL PC Monte Carlo Simulation

13

Y -1.5 -1 -0.5 0 0.5 1

0 0.5 1 X
Fig 3. Monte Carlo Simulation, 50 replications, thin lines are estimated first eigenfunctions, the bold black line is the true eigenfunction

and s(pi ) is uncorrelated with r(pi ) if r = s. If the Xi(p) are Gaussian random variables, the r(pi ) are independent N (0, r) random variables. A natural hypothesis to be tested then refers to the equality of variances:

H03,r : (r1) = (r2), r = 1, 2, . . .

Let

µ^(p)(t)

=

1 np

i Xi(p)(t), and let ^1(p)  ^(2p)  . . . and ^1(p), ^2(p)  . . . denote

eigenvalues and corresponding eigenfunctions of the empirical covariance operator ^(npp)

of X1(p), X2(p)(t), . . . , Xn(pp). The following test statistics are defined in terms of µ^(p), ^(rp)

and ^r(p). As discussed in the proceeding section, all curves in both samples are usually

not directly observed, but have to be reconstructed from noisy observations according

to (3). In this situation, the "true" empirical eigenvalues and eigenfunctions have to

be replaced by their discrete sample estimates. Bootstrap estimates are obtained by resampling the observations corresponding to the unknown curves Xi(p). As discussed in Section 2.2, the validity of our test procedures is then based on the assumption

that T is sufficiently large such that the additional estimation error is asymptotically

negligible.

14 M. BENKO, W. HA® RDLE AND A.KNEIP
Our tests of the hypotheses H01, H02,r and H03,r rely on the statistics
D1 d=ef µ^(1) - µ^(2) 2,
D2,r d=ef ^r(1) - ^r(2) 2, D3,r d=ef |^(r1) - ^r(2)|2.
The respective null-hypothesis has to be rejected if D1  1;1-, D2,r  2,r;1- or D3,r  3,r;1-, where 1;1-, 2,r;1- and 3,r;1- denote the critical values of the distributions of
1 d=ef µ^(1) - µ(1) - (µ^(2) - µ(2)) 2,
2,r d=ef ^r(1) - r(1) - (^r(2) - r(2)) 2, 3,r d=ef |^(r1) - r(1) - (^r(2) - r(2))|2.
Of course, the distributions of the different 's cannot be accessed directly, since they depend on the unknown true population mean, eigenvalues and eigenfunctions. However, it will be shown below that these distributions and hence their critical values are approximated by the bootstrap distribution of
1 d=ef µ^(1) - µ^(1) - (µ^(2) - µ^(2)) 2, 2,r d=ef ^r(1) - ^r(1) - (^r(2) - ^r(2)) 2, 3,r d=ef |^(r1) - ^(r1) - (^(r2) - ^(r2))|2.
where µ^(1), ^r(1), ^r(1) as well as µ^(2), ^r(2), ^(r2) are estimates to be obtained from independent bootstrap samples X11(t), X21(t), . . . , Xn11 (t) as well as X12(t), X22(t), . . . , Xn22 (t).
This test procedure is motivated by the following insights: 1) Under each of our null-hypotheses the respective test statistics D is equal to the corresponding . The test will thus asymptotically possess the correct level: P (D > 1-)  . 2) If the null hypothesis is false, then D = . Compared to the distribution of  the distribution of D is shifted by the difference in the true means, eigenfunctions, or eigenvalues. In tendency D will be larger than 1-. Even if for r  L the equality of eigenfunctions is rejected, we may be interested in the question whether at least the L-dimensional eigenspaces generated by the first L eigenfunctions are identical. Therefore, let EL(1) as well as EL(2) denote the L-dimensional linear function spaces generated by the eigenfunctions 1(1), . . . , L(1) and 1(2), . . . , L(2), respectively. We then aim to test the null hypothesis:
H04,L : EL(1) = EL(2).

COMMON FUNCTIONAL PC

15

Of course, H04,L corresponds to the hypothesis that the operators projecting into EL(1) and EL(2) are identical. This in turn translates into the condition that
LL
r(1)(t)r(1)(s) = r(2)(t)r(2)(s) for all t, s  [0, 1].
r=1 r=1
Similar to above, a suitable test statistics is given by

D4,L d=ef

L L2
^r(1)(t)^r(1)(s) - ^r(2)(t)^r(2)(s) dtds
r=1 r=1

and the null hypothesis is rejected if D4,L  4,L;1-, where 4,L;1- denotes the critical value of the distribution of

4,L d=ef

L
{^r(1)(t)^r(1)(s) - r(1)(t)r(1)(s)}

r=1

L2

- {^r(2)(t)^r(2)(s) - r(2)(t)r(2)(s)} dtds.

r=1

The distribution of 4,L and hence its critical values are approximated by the bootstrap distribution of

4,L d=ef

L
{^r(1)(t)^r(1)(s) - ^r(1)(t)^r(1)(s)}

r=1

L2
- {^r(2)(t)^r(2)(s) - ^r(2)(t)^r(2)(s)} dtds.

r=1

It will be shown in Theorem 2 below that under the null hypothesis as well as under the
alternative the distributions of n1, n2,r, n3,r, n4,L converge to continuous limit distributions which can be consistently approximated by the bootstrap distributions of n1, n2,r, n3,r, n4,L.

3.1. Theoretical Results. Let n = (n1 + n2)/2. We will assume that asymptotically
n1 = n ∑ q1 and n2 = n ∑ q2 for some fixed proportions q1 and q2. We will then study
the asymptotic behavior of our statistics as n  . We will use X1 = {X1(1), . . . , Xn(11)} and X2 = {X1(2), . . . , Xn(22)} to denote the observed
samples of random functions. Theorem 3: Assume that {X1(1), . . . , Xn(11)} and {X1(2), . . . , Xn(22)} are two indepen-
dent samples of random functions each of which satisfies Assumption 1.

16 M. BENKO, W. HA® RDLE AND A.KNEIP
As n   we then obtain n1 L F1, n2,r L F2,r, n3,r L F3,r, and n4,L L F4,L, where F1, F2,r, F3,r, F4,L are non-degenerated, continuous probability distributions. Furthermore, for any  > 0
i) |P (n1  ) - P (n1  | X1, X2) | = Op(1)
as n  . ii) If, furthermore, (r1-)1 > r(1) > r(1+)1 and r(2-)1 > (r2) > r(2+)1 hold for some fixed
r = 1, 2, . . . , then
|P (nk,r  ) - P nk,r  | X1, X2 | = Op(1), k = 2, 3
as n  . iii) If r(1) > r(1+)1 and (r2) > (r2+)1 holds for all r = 1, . . . , L, then
|P (n4,L  ) - P n4,L  | X1, X2 | = Op(1)
as n  .
The structures of the distributions F1, F2,r, F3,r, F4,L are derived in the proof of the theorem which can be found in the appendix. They are obtained as limits of distributions of quadratic forms.

3.2. Simulation study. In this paragraph we illustrate the finite behavior of the

proposed test. We make use of the findings of the Example 2.3 and focus here on the

test of common eigenfunctions. Looking at the Figure 3 we observe that the error of

the estimation of the eigenfunctions simulated by (12) is manifested by some shift of

the estimated eigenfunctions. This motivates the basic simulation-setup (setup "a"),

where the first sample is generated by the random combination of orthonormalized

sine and cosine functions (Fourier functions) and the second sample is generated by

the random combination of the same but shifted factor functions:

Xi(1) (tik ) Xi(2) (tik )

= =

11((21ii ))22

sin(2tik) sin{2(tik

+ +

2)(1i}) +2c2(o2i )s(22ctoiks){2(tik

+

)}.

The factor loadings are i.i.d. random variables with 1(pi)  N (0, 1(p)) and 2(pi)  N (0, 2(p)). The functions are generated on the equidistant grid tik = tk = k/T, k = 1, . . . T = 100, i = 1, . . . , n = 70. For the presentation of results in the Table 1, we use the following notation: "a) (11), 2(1), 2(2), 2(2)". The shift parameter  is changing from 0 to 0.25 with the step 0.05. It should be mentioned that the shift  = 0 yields
the simulation of level and setup with shift " = 0.25" yields the simulation of the
alternative, where the two factor functions are exchanged.

COMMON FUNCTIONAL PC

17

In the second setup (setup "b") the first factor functions are same and the second

factor functions differ:

Xi(1) (tik ) Xi(2) (tik )

= =

11((21ii ))22

sin(2tik) sin{2(tik

+ +

2)(1i}) +2c2(o2i )s(22stiink ){4(tik

+

)}.

In the Table 1 we use the notation "b) (11), (21), (22), 2(2), Dr". Dr means the test for the equality of the r-th eigenfunction. In the bootstrap tests we used 500 bootstrap
replications. The critical level in this simulation is  = 0.1. The number of simulations
is 250.

setup/shift

0 0.05 0.1 0.15 0.2 0.25

a) 10, 5, 8, 4

0.13 0.41 0.85 0.96 1 1

a) 4, 2, 2, 1

0.12 0.48 0.87 0.96 1 1

a) 2, 1,1.5, 2

0.14 0.372 0.704 0.872 0.92 0.9

b) 10, 5, 8, 4 D1 0.10 0.44 0.86 0.95 1 1

b) 10, 5, 8, 4 D2

1

1

1

111

Table 1 The results of the simulations for  = 0.1, n = 70, T = 100, number of simulations 250.

We can interpret the Table 1 in the following way: In power simulations ( = 0)

test behaves as expected: less powerful if the functions are "hardly distinguishable"

(small shift, small difference in eigenvalues). The level approximation seems to be less
precise if the difference in the eingenvalues ((1p) - (2p)) becomes smaller, this can be explained by relative small sample-size n, small number of bootstrap-replications and

increasing estimation-error as argued in the Theorem 2, assertion (iii).

In comparison to our general setup (3) we used an equidistant and common design

for all functions. This simplification is necessary, it simplifies and speeds-up the simu-

lations, in particular using general random and observation-specific design makes the

simulation computationally untractable.

Secondly we omitted the additional observation error, this corresponds to the stan-

dard assumptions in the functional principal components theory. As argued in Section

2.2 the inference based on the directly observed functions and estimated functions Xi is first order equivalent under mild conditions implied by Theorems 1 and 2. In order

to illustrate this theoretical result in the simulation we used the following setup:

Xi(1) (tik ) Xi(2) (tik )

= =

11((21ii ))22

sin(2tik) sin{2(tik

+ +

2)(1i})+2c2(o2i )s(22ctoiks){+2((itk1ik)

+

)}

+

(ik2).

where (ikp)  N (0, 0.25), p = 1, 2 all other parameters remain same as in the simulation setup "a". Using this setup we recalculate the simulation presented in the second "line"

18 M. BENKO, W. HA® RDLE AND A.KNEIP
of the Table 1, for estimation of the functions Xi(p), p = 1, 2 we used Nadaraya-Watson estimation with Epanechnikov kernel and bandwidth b = 0.05. We run the simulations with various bandwidths, the choice of the bandwidth doesn't have strong influence on results except by oversmoothing (large bandwidths). The results are printed in the

setup/shift

0 0.05 0.1 0.15 0.2 0.25

a)10,5,8,4 0.09 0.35 0.64 0.92 0.94 0.97

Table 2 The results of the simulation for  = 0.1, n = 70, T = 100 with additional error in observation.

Table 2. As we can see the difference of the simulation results using estimated functions
are not significantly different in comparison to the results printed in the second line
of the Table 1 ≠ directly observed functional values.
The last limitation of this simulation study is the choice of particular alternative.
A more general setup of this simulation study might be based on the following model: Xi(1)(t) = 1(1i )1(1)(t)+2(1i )2(1)(t), Xi(2)(t) = 1(2i )1(2)(t)+2(2i )2(2)(t) where 1(1), 1(2), 2(1) and g are mutually orthogonal functions on L2[0, 1] and 2(2) = (1 + 2)-1/2{2(1) + g}. Basically we create the alternative by the contamination of one of the "eigenfunctions" (in our case the second one) in the direction g and ensure ||2(2)|| = 1. The amount of the contamination is controlled by the parameter . Note that the exact squared integral difference ||2(1)-2(2)||2 does not depend on function g. Thus in the "functional sense" particular "direction of the alternative hypothesis" represented by the function
g has no impact on the power of the test. However, since we are using nonparametric
estimation technique, we might expect that rough (highly fluctuating) functions g will
yield higher error of estimation and hence decrease the precision (and power) of the
test. Finally, higher number of factor functions (L) in simulation may cause less precise
approximation of critical values and more bootstrap replications and larger sample-
size may be needed. This can also be expected from the Theorem 2 in Section 2.2 ≠ the
variance of the estimated eigenfunctions depends on all eigenfunctions corresponding
to non-zero eingenvalues.

4. Implied Volatility Analysis. In this section we present an application of the method discussed in previous sections to the implied volatilities of european options on the German stock index (ODAX). Implied volatilities are derived from the Black-Scholes (BS) pricing formula for European options, see Black & Scholes (1973). European call and put options are derivatives written on an underlying asset with price process Si, which yield the pay-off max(SI - K, 0) and max(K - SI , 0). Here i denotes the current day, I the expiration day and K the strike price. Define  = I - i, time to maturity. The BS pricing formula is:

Ci(Si, K, , r, ) = Si(d1) - Ke-r (d2)

(14)

COMMON FUNCTIONAL PC

19

where

d1

=

ln(Si /K )+(r+ 2 

/2)

,

d2

=

 d1 -   ,

r

is

the

riskless

interest

rate,

and

 is the (unknown and constant) volatility parameter. In (14) we assume the zero-

dividend case. The Put option price Pi can be obtained from the put-call parity Pi = Ci - Si + e-rK.

The implied volatility ~ is defined as the volatility , for which the BS price Ci in (14) equals the price C~i observed on the market. For a single asset, we obtain at each time point (day i) and each maturity  a IV function ~i (K). Practitioners often
rescale the strike dimension by plotting this surface in terms of (futures) moneyness

 = K/Fi( ), where Fi( ) = Sier .

Clearly, for given parameters Si, r, K,  the mapping from prices to IVs is a one-to-

one mapping. In the financial practice the IV is often used for quoting the European

options since it reflects the "uncertainity" of the financial market better then the

prices it self. For the purpose of this application we will understand the BS-IV as a

individual financial variable.

Fengler, Ha®rdle & Villa (2003) studied the dynamics of the IV via PCA on dis-

cretized IV functions for different maturity groups and tested the Common Principal

Components (CPC) hypotheses (equality of eigenvectors and eigenspaces for differ-

ent groups). Their method rely on the CPC methodology introduced by Flury (1988)

which is based on maximum likelihood estimation under the assumption of multivari-

ate normality. The main aim of this application is to verify their results in a func-

tional sense. Doing so, we overcome two basic weaknesses of their approach. Firstly,

the factor model proposed by Fengler, H®ardle & Villa (2003) is just performed on a

sparse design of moneyness. However, in practice, e.g. in Monte-Carlo pricing meth-

ods evaluation on a fine grid is needed. Using the functional PCA approach we may

overcome this difficulty and evaluate the factor model on an arbitrary fine grid. A

second difficulty of the procedure proposed by Fengler, H®ardle & Villa (2003) comes

from the data design ≠ on the exchange we cannot observe the option with desired

maturity on each day and we need to estimate them from the IV-functions with ma-

turities observed on the particular day. Consequently the two-dimensional Nadaraya-

Watson estimator proposed by Fengler, Ha®rdle & Villa (2003) results essentially in

the (weighted) average of the IVs (with closest maturities) observed on particular day,

which may affect the test of the common eigenfunction hypothesis. We use the linear

interpolation scheme in the total variance T2 OT,i(,  ) d=ef (i ())2, in order to recover the IV functions with fixed maturity (on day i). This interpolation scheme is based

on the arbitrage arguments originally proposed by Kahale (2004) for zero-divident

and zero-interest rate case and generalized for deterministic interest rate by Fengler

(2005). More precisely, having IVs with maturities observed on a particular day i: ~iji (), ji = 1, . . . , pi, we calculate the corresponding total variance ~T OT,i(, ji). From these total variances we linearly interpolate the total variance with the desired

maturity from the nearest maturities observed on day i. The total variance can easily

20

M. BENKO, W. HA® RDLE AND A.KNEIP

log IV returns, 1M
0.85 0.90 0.95 ATM 1.05

log IV returns, 3M
0.85 0.90 0.95 ATM 1.05

0.15 0.10 0.05 0.00 -0.05 -0.10 -0.15

0.15 0.10 0.05 0.00 -0.05 -0.10 -0.15

0.15 0.10 0.05 0.00 -0.05 -0.10 -0.15

0.15 0.10 0.05 0.00 -0.05 -0.10 -0.15

0.85 0.90 0.95 ATM 1.05

0.85 0.90 0.95 ATM 1.05

Fig 4. Nadaraya-Watson estimator of the log-IV-returns for maturity 1M in left figure and 3M in right figure. The bold line is the sample mean of the corresponding group.

be transformed to corresponding IV ~i (). As the last step we calculate the log-returns log ~i () d=ef log ~i+1() - log ~i (). The log-IV-returns are observed for each ma-
turity  on a discrete grid ik. We assume that observed log-IV-return log ~i (ik) consists of true log-return of the IV function denoted by log i (ik) and possibly of some additional error ik. By setting Yik := log ~i (ik), Xi () := log i () we
obtain analogue of the model (3) with the argument :

Yik = Xi (ik) + ik, i = 1, . . . , n .

(15)

In order to simplify the notation and make the connection with the theoretical part clear we will use the notation in form of (15).
For our analysis we use a recent data set containing the daily data from January 2004 to June 2004 taken from the German-Swiss exchange (EUREX). The violations of the arbitrage-free assumptions were corrected using procedure proposed by Fengler (2005). Similar to Fengler, Ha®rdle & Villa (2003) we excluded options with maturity smaller then 10 days, these option-prices are known to be very noisy, partially because of a special and arbitrary setup in the pricing systems of the dealers. Using the interpolation scheme described above we calculate the log-IV-returns for two maturity groups  = 0.12 (measured in years), we denote it as "1M" group. and  = 0.36 ("3M" group) and denote them by Yi1kM , k = 1, . . . , Ki1M , Yi3kM , k = 1, . . . , Ki3M . Since we ensured that for each i, the interpolation procedure does not use data with same maturity for both groups, this procedure has no impact on the independence of both samples.

COMMON FUNCTIONAL PC

21

The underlying models, based on the truncated version of (2) are:

L1M
Xi1M () = XØi1M () + ^r1iM r1M (), i = 1, . . . , n1M
r=1
L3M
Xi3M () = XØi3M () + ^r3iM r3M (), i = 1, . . . , n3M .
r=1

(16) (17)

Model (16) and (17) can serve e.g. in a Monte Carlo pricing tool in the risk management

for pricing exotic options where the whole path of implied volatilities is needed to

determine the price. Estimating the factor functions in (16) and (17) by eigenfunctions displayed in Figure 1 we only need to fit the (estimated) factor loadings ^j1iM and ^j3iM . The pillar of the model is the dimension reduction. Keeping the factor function fixed

for a certain time period we need to analyze (two) multivariate random processes of

the factor loadings. For the purposes of this paper we will concentrate on comparing

the factors of the models (16) and (17) and the technical details of the analysis of the

factor loading will not be discussed here, we refer to Fengler, Ha®rdle & Villa (2003),

who proposed to fit the factor loadings by centered normal distributions with diagonal

variance matrix containing the corresponding eigenvalues. For a deeper discussion of

the fitting of factor loadings using a more sophisticated approach, basically based on

(possibly multivariate) GARCH models, see Fengler (2005b).

From our data set we obtained 88 functional observations for the 1M group (n1M )

and 125 observations for the 3M group (n3M ). We will estimate the model on the interval for futures moneyness   [0.8, 1.1]. In comparison to Fengler, H®ardle & Villa

(2003) we may estimate the models (16) and (17) on arbitrary fine grid (we used an

equidistant grid of 500 points on the interval [0.8, 1.1]). For illustration, the Nadaraya-

Watson (NW) estimator of resulting log-returns is plotted in Figure 4. The smoothing

parameters have been chosen in accordance with the requirements in Section 2.2. As

argued in the Section 2.2, we should use small smoothing parameters in order to avoid

a possible bias in the estimated eigenfunctions. Thus we use for each i essentially the smallest bandwidth bi that guarantees that estimator X^i is defined on the whole

support [0.8, 1.1].

Using the procedures described in Section 2.1 we first estimate the eigenfunctions

of the both maturity groups. The estimated eigenfunctions are plotted in Figure 1.

The structure of the eigenfunctions is in accordance with other empirical studies on

IV-surfaces, for a deeper discussion and economical interpretation see for example

Fengler, H®ardle & Mammen (2005) or Fengler, H®ardle & Villa (2003).

Clearly, the ratio of the variance explained by the k-th factor function is given by

the quantity ^k1M = ^1kM /

n1M j=1

^ 1j M

for

the

1M

group,

correspondingly

^k3M

for

the

3M group. In Table 3 we list the contributions of the factor functions. Looking at the

22 M. BENKO, W. HA® RDLE AND A.KNEIP
Table 3 we can see, that the 4-th factor functions explain less than 1% of the variation, this number was the "threshold" for the choice of the L1M and L2M .
var. explained 1M var. explained 3M ^1 89.9% 93.0% ^2 7.7% 4.2% ^3 1.7% 1.0% ^4 0.6% 0.4%
Table 3 Variance explained by the eigenfunctions.
We can observe, see Figure 1, that the factor functions for both groups are similar. Thus, in the next step we use the bootstrap test for testing the equality of the factor functions. We use 2000 bootstrap replications. The test of equality of the eigenfunctions was rejected for the first eigenfunction for the analyzed time period (January 2004 ≠ June 2004) at a significance level  = 0.05 (P-value 0.01). We may conclude that the (first) factor functions are not exactly same in the factor model for both maturity groups. However from a practical point of view we are more interested in the checking the appropriateness of the whole models for fixed number of factors: L = 2 or L = 3 in (16) and (17), this turns into testing the equality of eigenspaces. Thus, in the next step we test with the same setup (2000 bootstrap replications) the hypotheses that first two and first three eigenfunctions span the same eigenspaces EL1M and EL3M . Both hypotheses L = 2 and L = 3 are not rejected at the significance level  = 0.05 (Pvalue 0.61 for L = 2 and 0.09 for L = 3). Summarizing, even in the functional sense we have no significant reason to reject the hypothesis of common eigenspaces for these two maturity groups. Using this hypothesis the factors governing the movement of the returns of IV surface are invariant to time to maturity, just their relative importance can change. This leads to the common factor model: Xi () = XØ  ()+ L ^rir(), i =
r=1
1, . . . , n ,  = 1M, 3M. Where r := r1M = r3M . Besides the contribution to the understanding the structure of the IV function dynamics, in the sense of dimension reduction, using the common factor model we reduce the number of functional factors by half comparing to models (16) and (17). Furthermore, from the technical point of view, we also obtain an additional dimension reduction and higher estimation precision, since under this hypothesis we may estimate the eigenfunctions from the (individually centered) pooled sample Xi()1M , i = 1, . . . , n1M , Xi3M (), i = 1, . . . , n3M }. The main improvement in comparison to the multivariate study by Fengler, H®ardle & Villa (2003) is that our test is performed in the functional sense, doesn't depend on particular discretization and our factor model can be evaluated on an arbitrary fine grid.

COMMON FUNCTIONAL PC

23

5.

Appendix: Mathematical Proofs.

In the following,

v

=

(

1 0

v(t)2dt)1/2

will denote the L2-norm for any square integrable function v. At the same time, a =

(

1 k

k i=1

ai2)1/2

will

indicate

the

Euclidean

norm,

whenever

a



Rk

is

a

k-vector

for

some k  IN .

In the proof of Theorem 1, E and Var denote expectation and variance with re-

spect to  only (i.e. conditional on tij and Xi).

Proof of Theorem 1. Recall the definition of the i(t) and note that i(t) = iX (t) + i (t), where

Ti
i (t) = i(j)I

t

ti(j-1) + ti(j) , ti(j) + ti(j+1) 22

j=1

as well as

Ti
iX (t) = Xi(ti(j))I

t

ti(j-1) + ti(j) , ti(j) + ti(j+1) 22

j=1

for t  [0, 1], ti(0) = -ti(1) and ti(Ti+1) = 2 - ti(Ti). Similarly, i (t) = Xi  (t) + i (t). By Assumption 2, E |ti(j) - ti(j-1)|s = O(T -s) for s = 1, . . . , 4, and the conver-
gence is uniform in j < n. Our assumptions on the structure of Xi together with some straightforward Taylor expansions then lead to

< i, j >=< Xi, Xj > +Op(1/T )

and Moreover,

< i, i >= Xi 2 + Op(1/T ).

E(< i, Xj >) = 0, E( i 2) = i2, E(< i, i >) = 0, E(< i , i  >2) = Op(1/T ), E(< i, jX >2) = Op(1/T ), E(< i , jX >< k, lX >) = 0 for i = k, E(< i, j >< i, k >) = 0 for j = k and E( i 4) = Op(1)

hold (uniformly) for all i, j = 1, . . . , n. Consequently, E( Ø 2 - XØ 2) = Op(T -1 + n-1).
When using these relations, it is easily seen that for all i, j = 1, . . . , n

Mij - Mij = Op(T -1/2 + n-1) and tr{(M - M )2}1/2 = Op(1 + nT -1/2). (18)

24 M. BENKO, W. HA® RDLE AND A.KNEIP

Since the orthonormal eigenvectors pq of M satisfy pq = 1, we furthermore obtain for any i = 1, . . . , n and all q = 1, 2, . . .

n1
pjq Mij - Mij - i (t)Xj (t)dt = Op(T -1/2 + n-1/2)
j=1 0

(19)

as well as

n1
pjq i(t)Xj (t)dt = Op
j=1 0

n1/2 T 1/2

(20)

and

nn

1

ai pjq i (t)Xj (t)dt = Op

i=1 j=1

0

n1/2 T 1/2

(21)

for any further vector a with a = 1.

Recall that the j-th largest eigenvalue lj satisfies n^j = lj. Since by assumption

infs=r |r - s| > 0, the results of Dauxois, Pousse & Romain (1982) imply that ^r

converges

to

r

as

n



,

and

sups=r

1 |^ r -^ s |

=

Op(1),

which

leads

to

sups=r

1 |lr -ls |

=

Op(1/n). Assertion a) of Lemma A of Kneip & Utikal (2001) together with (18) - (21)

then implies that

^r

-

^lr n

= n-1|lr - ^lr| = n-1|pr (M - M )pr| + Op(T -1 + n-1) = Op{(nT )-1/2 + T -1 + n-1}.

(22)

When analyzing the difference between the estimated and true eigenvectors p^r and pr, assertion b) of Lemma A of Kneip & Utikal (2001) together with (18) lead to

p^r - pr = -Sr(M - M )pr + Rr, with Rr = Op(T -1 + n-1)

(23)

and Sr =

s=r

ls

1 -lr

ps

ps

.

Since

sup

a

=1 a

Sr a



sups=r

1 |lr -ls |

= Op(1/n), we can

conclude that

p^r - pr = Op(T -1/2 + n-1),

(24)

and our assertion on the sequence n-1 i(^ri - ^ri;T )2 is an immediate consequence. Let us now consider assertion ii). The well-known properties of local linear estima-
tors imply that |E{X^i(t) - Xi(t)}| = Op(b2) as well as Var{X^i(t)} = Op{(T b)-1/2},
and the convergence is uniform for all i, n. Furthermore, due to the independence of the error term ij, Cov{X^i(t), X^j(t)} = 0 for i = j. Therefore,

|^r(t) -

1 lr

n i=1

pir X^ i (t)|

=

Op(b2

+

 1 ). nT b

COMMON FUNCTIONAL PC

25

On the other hand, (18) - (24) imply that with X^ (t) = (X^1(t), . . . , X^n(t))

|^r;T (t) -

1 lr

n
pir X^ i (t)|
i=1

=

| 1 lr

n
(p^ir
i=1

- pir)Xi(t) +

1 lr

n
(p^ir
i=1

- pir){X^i(t) - Xi(t)}| + Op(T -1 + n-1)

=

SrX (t) lr

|pr (M^ - M )Sr

X (t) Sr X (t)

| + Op(b2T -1/2 + T -1b-1/2 + n-1)

= Op(n-1/2T -1/2 + b2T -1/2 + T -1b-1/2 + n-1).

This proves the theorem.

Proof of Theorem 2: First consider assertion i). By definition,

nn
XØ (t) - µ(t) = n-1 {Xi(t) - µ(t)} = (n-1 ri)r(t).
i=1 r i=1

Recall that, by assumption, ri are independent, zero mean random variables with

variance r, and that the above series converges with probability 1. When defining the

truncated series

qn
V (q) = (n-1 ri)r(t),

r=1 i=1



standard central limit theorems therefore imply that nV (q) is asymptotically

N (0, The

q r=1

r

r

(t)2)

assertion of a

distributed for any possible q  IN .

N (0,

 r=1

r r (t)2 )

limiting

distribution

now

is

a

consequence

of the fact that for P {| nV (q)- n

all 1, r (n-1

2

> 0 there exists a q

n i=1

ri )r (t)|

>

1}

<

such that 2 for all q



q

and

all

n

sufficiently

large.

In order to prove assertions i) and ii), consider some fixed r  {1, 2, . . . } with

r-1 > r > r+1. Note that  as well as ^n are nuclear, self-adjoint and non-negative

linear operators with v = (t, s)v(s)ds and ^nv = ^(t, s)v(s)ds, v  L2[0, 1]. For

m  IN let m denote the orthogonal projector from L2[0, 1] into the m-dimensional

linear space spanned by {1, . . . , m}, i.e. mv =

m j=1

<

v, j

>

j ,

v



L2[0, 1].

Now consider the operator m^nm as well as its eigenvalues and corresponding

eigenfunctions denoted by ^1,m  ^2,m  . . . and ^1,m, ^2,m, . . . , respectively. It follows

from well-known results in Hilbert space theory that m^nm converges strongly to

^n as m   . Furthermore, we obtain (Rayleigh-Ritz theorem)

lim
m

^r,m

=

r ,

and

lim
m

^r - ^r,m

= 0 if ^r-1 > ^r > ^r+1.

(25)

26 M. BENKO, W. HA® RDLE AND A.KNEIP

Note that under the above condition ^r is uniquely determined up to sign, and recall

that we always implicitly assume that the right "versions" (with respect to sign) are

used when comparing eigenfunctions. By definition ji = j(t){Xi(t) - µ(t)}dt, and

therefore j(t){Xi(t) - XØ (t)}dt = ji - Øj as well as Xi - XØ = j(ji - Øj)j,

where

Øj

=

1 n

n i=1

ji

.

When

analyzing

the

structure

of

m ^ n m

more

deeply,

we

can verify that m^nmv = ^m(t, s)v(s)ds, v  L2[0, 1], with

^m(t, s) = gm(t) ^ mgm(s),

where gm(t) = (1(t), . . . , m(t)) , and where ^ m is the m ◊ m matrix with ele-

ments

{

1 n

in=1(ji - Øj)(ki - Øk)}j,k=1,...,m. Let 1(^ m)  2(^ m)  ∑ ∑ ∑  m(^ m)

and ^1,m, . . . , ^m,m denote eigenvalues and corresponding eigenvectors of ^ m. Some

straightforward algebra then shows that

^r,m = r(^ m), ^r,m = gm(t) ^r,m.

(26)

We will use m to represent the m◊m diagonal matrix with diagonal entries 1  ∑ ∑ ∑ 
m. Obviously, the corresponding eigenvectors are given by the m-dimensional unit
vectors denoted by e1,m, . . . , em,m. Lemma A of Kneip & Utikal (2001) now implies that the differences between eigenvalues and eigenvectors of m and ^ m can be bounded
by

^r,m - r = tr{er,mer,m(^ m - m)} + R~r,m,

with

R~r,m



6 sup

a =1 a (^ m - m)2a , mins |s - r|

(27)

^r,m-er,m = -Sr,m(^ m-m)er,m+Rr,m,

with

Rr,m



6 sup

a =1 a (^ m - m)2 mins |s - r|2

a

,

(28)

where Sr,m =

s=r

1 s-r

es,m

es,m.

Assumption

1

implies

E(Ør )

=

0,

Var(Ør)

=

r n

,

and

with

ii

=

1

as

well

as

ij

=

0

for i = j we obtain

E{ sup a
a =1

(^ m - m)2a}  E{tr[(^ m - m)2]} = E{

m

1 [ n

n
(ji - Øj )(ki - Øk) - jkj ]2}

j,k=1 i=1

 E{



1 [ n

n

(ji

- Øj )(ki

- Øk) - jkj ]2} =

1 (
n

j,k=1 i=1

j

E{j2ik2i}) + O(n-1) = O(n-1)
k

(29)

COMMON FUNCTIONAL PC

27

for

all

m.

Since

tr{er,mer,m(^ m

-

m)}

=

1 n

ni=1(ri - Ør)2 - r, (25), (26), (27),

and (29) together with standard central limit theorems imply that

n(^r

-

r )

=

1 n

n
(ri - Ør)2 - r + Op(n-1/2)

i=1

= 1 n n

(ri)2 - E{(ri)2} + Op(n-1/2) L N (0, r).

i=1

(30)

It remains to prove assertion iii). Relations (26) and (28) lead to

^r,m(t) - r(t) = gm(t) (^r,m - er,m)

m
=-
s=r

1 n(s -

r )

n
(si
i=1

-

Øs)(ri

-

Ør )

s(t) + gm(t) Rr,m,

(31)

where due to (29) the function gm(t) Rr,m satisfies



E(

gmRr,m

) = E(

Rr,m

)



n mins

6 |s

-

r |2



j


E j2ik2i  + O n-1
k

for all m. By Assumption 1 the series in (31) converge with probability 1 as m  . Obviously, the event ^r-1 > ^r > ^r+1 occurs with probability 1. Since m is arbi-
trary, we can therefore conclude from (25) and (31) that

^r(t) - r(t) = -
s=r
=-
s=r

1 n(s -

r )

n
(si
i=1

-

Øs)(ri

-

Ør )

s(t) + Rr(t)

1n n(s - r) i=1 siri s(t) + Rr(t),

(32)

where 

Rr

n s=r

= Op(n-1) as well as Rr

1 n(s -r )

n i=1

siri

s(t)

= Op(n-1). Moreover, is a zero mean random variable

with

variance

q=r

s=r

E[r2iqisi]
(q -r )(s -r

)

q

(t)s(t)

<

.

By

Assumption

1

it

follows

from

standard

central limit arguments that for any q  IN the truncated series

 nW (q)

d=ef

 n

q s=1,s=r

[

1 n(s-r

)

n i=1

siri]s(t)

is

asymptotically

normal

distrib-

uted. The asserted asymptotic normality of the complete series then follows from an

argument similar to the one used in the proof of Assertion i).

28 M. BENKO, W. HA® RDLE AND A.KNEIP

Proof of Theorem 3: The results of Theorem 2 imply that

n1 =

r

1 
q1n1

n1 i=1

r(1i )r(1)(t) -

r

1 
q2n2

n2 i=1

r(2i )r(2)(t)

2
dt.

(33)

Furthermore, independence of Xi(1) and Xi(2) together with (30) imply that

n[^(r1) - (r1) - {^r(2) - (r2)}] L N

0, r(1) + (r2) q1 q2

,

and

r(1) q1

n +

r(2) q2

3,r

L

12.

(34)

Furthermore, (32) leads to

n2,r =
s=r

1 q1n1(s(1)

- (r1))

n1 i=1

s(1i )r(1i )

s(1)

-
s=r

1

 q2n2

((s2)

- r(2))

n2 i=1

s(2i )r(2i )

2
s(2) + Op(n-1/2)

(35)

and

n4,L = n

L
r(1)(t){^r(1)(u) - r(1)(u)} + r(1)(u){^r(1)(t) - r(1)(t)}

r=1

L2
- r(2)(t){^r(2)(u) - r(2)(u)} + r(2)(u){^r(2)(t) - r(2)(t)} dtdu + Op(n-1/2)

r=1



=

L

r=1

s=r

{

 q1

n1

1 (s(1)

-

r(1))

n1 i=1

s(1i )r(1i )}{r(1)(t)s(1)(u)

+

r(1)(u)s(1)(t)}

2

-

L1

r=1

s=r

{

 q2n2

((s2)

- (r2))

n2 i=1

s(2i )r(2i )}{r(2)(t)s(2)(u)

+

r(2)(u)s(2)(t)}

dtdu + Op(n-1/2)

(36)

It is clear from our assumptions that all sums involved converge with probability 1.

Recall that E(r(pi )s(pi )) = 0, p = 1, 2 for r = s.

It follows that X~r(p) := q1pnp

s=r

 ,np s(pi )r(pi )
i=1 s(p)-r(p)

(p) s

p

=

1, 2,

is

a

continuous,

zero mean random function on L2[0, 1], and, by assumption, E( X~r(p) 2) < . By

COMMON FUNCTIONAL PC

29

Hilbert space central limit theorems (see, e.g, Araujo & Gin¥e (1980)) X~r(p) thus con-
verges in distribution to a Gaussian random function r(p) as n  . Obviously, r(1) is independent of r(2). We can conclude that n4,L possesses a continuous limit dis-

tribution F4,L defined by the distribution of

L {r(1)(t)r(1)(u) + r(1)(u)r(1)(t)}

r=1

-

L r=1

{r(2)(t)r(2)(u)

+

r(2)(u)r(2)(t)}

2
dtdu.

Similar

arguments

show

the

existence

of continuous limit distributions F1 and F2,r of n1 and n2,r.

For given q  IN define vectors b(i1p) = (1(pi ), . . . , q(pi ), )  Rq,

b(i2p) = (1(pi )r(pi ), . . . , r(p-)1,ir(pi ), r(p+)1,ir(pi ), . . . , q(pi )r(pi ))  Rq-1, and bi3 = (1(pi )2(pi ),

. . . , q(pi )L(pi))  R(q-1)L. When the infinite sums over r in (33) respectively s = r in (35) and (36) are restricted to q  IN components (i.e. r and s=r are replaced

by rq and s=r,sq), then the above relations can generally be presented as limits

n = lim n(q) of quadratic forms
q

n1(q) =

1 n1
1 n2

n1 i=1

b(i11)

n2 i=1

bi(12)

n2,r(q) =

1 n1
1 n2

n1 i=1

b(i21)

n2 i=1

b(i22)

n4,L(q) =

1 n1
1 n2

n1 i=1

bi(31)

n2 i=1

b(i32)

Qq1

1 n1
1

n2

n1 i=1

b(i11)

n2 i=1

bi(12)

,

1

Q2q

n1 1

n2

n1 i=1

bi(21)

n2 i=1

b(i22)

,

1

Q3q

n1 1

n2

n1 i=1

b(i31)

n2 i=1

b(i32)

,

(37)

where the elements of the 2q ◊2q, 2(q -1)◊2(q -1) and 2L(q -1)◊2L(q -1) matrices Q1q, Qq2 and Qq3 can be computed from the respective (q-element) version of (33) - (36). Assumption 1 implies that all series converge with probability 1 as q  , and by
(33) - (36) it is easily seen that for all ,  > 0 there exist some q( , ), n( , )  IN
such that

P (|n1 - n1(q)| > ) < , P (|n2,r - n2,r(q)| > ) < , P (|n4,L - n4,L(q)| > ) < ,

(38)

hold for all q  q( , ) and all n  n( , ). For any given q, we have E(bi1) = E(bi2) =

E(bi3) = 0, and it follows from Assumption 1 that the respective covariance structures

can be represented by finite covariance matrices 1,q, 2,q, and 3,q. It therefore fol-

lows from our assumptions together with standard multivariate central limit theorems

that

the

vectors

{

1 n1

n1 i=1

(bi(k1)

)

,

1 n2

n2 i=1

(bi(k2))

}

, k = 1, 2, 3, are asymptotically

normal with zero means and covariance matrices 1,q, 2,q, and 3,q. One can thus

30 M. BENKO, W. HA® RDLE AND A.KNEIP

conclude that as n   n1(q) L F1,q, n2,r(q) L F2,r,q, n4,L(q) L F4,L,q,

(39)

where F1,q, F2,r,q, F4,L,q denote the continuous distributions of the quadratic forms z1 Qq1z1, z2 Qq2z2, z3 Q3qz3 with z1  N (0, 1,q), z2  N (0, 2,q), z3  N (0, 3,q). Since ,  are arbitrary, (38) implies

lim
q

F1,q

=

F1,

lim
q

F2,r,q

=

F2,r ,

lim
q

F4,L,q

=

F4,L.

(40)

We now have to consider the asymptotic properties of bootstrapped eigenvalues

and

eigenfunctions.

Let

XØ (p)

=

1 np

np i=1

Xi(p),

r(pi )

=

r(p)(t){Xi(p)(t) - µ(t)},

Ør(p)

=

1 np

np i=1

r(pi ),

and

note

that

r(p)(t){Xi(p)(t) - XØ (p)(t)} = r(pi ) - Ør(p).

When considering unconditional expectations, our assumptions imply that for p = 1, 2

E[r(pi )] = 0, E[(r(pi ))2] = (rp),

E[(Ør(p))2] =

r(p) , np

E{[(r(pi ))2 - (rp)]2} = r(p),

E{  [ 1 l,k=1 np

np
(l(ip)
i=1

- Øl(p))(k(pi)

- Øk(p)) - lkl(p)]2}

1 =(
np l

l(p) +

l(p)k(p)) + O(np-1).

l=k

(41)

One can infer from(41) that the arguments used to prove Theorem 1 can be gener-
alized to approximate the difference between the bootstrap eigenvalues and eigenfunctions ^(rp), ^r(p) and the true eigenvalues r(p), r(p). All infinite sums involved converge with probability 1. Relation (30) then generalizes to

np(^(rp) - ^(rp)) = np(^(rp) - r(p)) - np(^r(p) - (rp))

1 np = np i=1

r(pi ) - Ør(p)

2 1 np - np i=1

r(pi ) - Ør(p) 2 + Op(np-1/2)

1 np = np i=1

(r(pi ))2

-

1 np

np
(r(pk))2
k=1

+ Op(np-1/2).

(42)

COMMON FUNCTIONAL PC

31

Similarly, (32) becomes

^r(p) - ^r(p) = ^r(p) - r(p) - (^r(p) - r(p))

=-
s=r

s(p)

1 - r(p)

1 np

np
(s(pi ) - Øs(p))(r(pi )
i=1

- Ør(p))

- (sp)

1 - r(p)

1 np

np
(s(pi )
i=1

- Øs(p))(r(pi )

- Ør(p))

s(p)(t) + Rr(p)(t)

=-
s=r

1 1 np (sp) - r(p) np i=1

s(pi )r(pi )

-

1 np

np
s(pk)r(pk)
k=1

s(p)(t) + R~r(p)(t) (43)

where due to (28), (29), and (41) the remainder term satisfies Rr(p) = Op(np-1). We are now ready to analyze the bootstrap versions  of the different . First

consider 3,r and note that {(r(pi ))2} are i.i.d. bootstrap resamples from {(r(pi ))2}.

It therefore follows from basic bootstrap results that the conditional distribution of

1np

np i=1

[(r(pi )

)2

-

1 np

nk=p 1(r(pk))2] given Xp converges to the same N (0, r(p)) limit

distribution as 1np

np i=1

[(r(pi )

)2

-

E{(r(pi ))2}].

Together

with

the

independence

of

(r(1i ))2 and (r(2i ))2 the assertion of the theorem is an immediate consequence. Let us turn to 1, 2,r and 4,L. Using (41) - (43) it is then easily seen that
n1, n2,r and n4,L admit expansions similar to (33), (35) and (36), when replacing

there 1np

np i=1

r(pi )

by

1np

np i=1

(r(pi )

-

1 np

np k=1

r(pk))

as

well

as

1np

np i=1

s(pi )r(pi )

by 1np

np i=1

(s(pi )r(pi )

-

1 np

np k=1

s(pk)

r(pk)).

Replacing r(pi ), s(pi ) by r(pi ), s(pi ) leads to bootstrap analogs b(ikp) of the vectors b(ikp), k = 1, 2, 3. For any q  IN define bootstrap versions n1(q), n3,r(q) and n4,L(q) of n1(q), n3,r(q) and n4,L(q) by using

1 n1

n1 i=1

(bi(k1)

-

1 n1

n1 k=1

bi(k1))

,

1 n2

n2 i=1

(b(ik2)

-

1 n2

n2 k=1

bi(k2))

instead of

1 n1

n1 i=1

(b(ik1)

)

,

1 n2

n2 i=1

(bi(k2))

, k = 1, 2, 3, in (37). Applying again (41) - (43)

one can conclude that for any > 0 there exists some q( ) such that as n  

P (|n1 - n1(q)| < )  1, P (|n2,r - n2,r(q)| < )  1,
P (|n4,L - n4,L(q)| < )  1,

(44)

hold for all q  q( ). Of course, (44) generalizes to the conditional probabilities given X1, X2.

32 M. BENKO, W. HA® RDLE AND A.KNEIP

In order to prove the theorem it thus only remains to show that for any given q and

all 

|P (n(q)  ) - P (n(q)  | X1, X2) | = Op(1)

(45)

hold for either (q) = 1(q) and (q) = 1(q), (q) = 2,r(q) and (q) = 2,r(q),

or (q) = 4,L(q) and (q) = 4,L(q). But note that for k = 1, 2, 3, E(bik) = 0,

{b(ikj)}

are

i.i.d.

bootstrap

resamples

from

{bi(kp)},

and

E(b(ikp)|

X1, X2)

=

1 np

np k=1

b(ikp)

are the corresponding conditional means. It therefore follows from basic bootstrap re-

sults that as n   the conditional distribution of

1 n1

n1 i=1

(bi(k1)

-

1 n1

n1 k=1

bi(k1))

,

1 n2

n2 i=1

(bi(k2)

-

1 n2

n2 k=1

bi(k2))

given X1, X2 con-

verges to the same N (0, k,q)- limit distribution as

1 n1

n1 i=1

(b(ik1))

1 n2

n2 i=1

,

(bi(k2)

)

.

This obviously holds for all q  IN , and (45) is an immediate consequence. The theorem

then follows from (38), (39), (40), (44) and (45).

REFERENCES
ARAUJO, A. & GINE¥, E. (1980). The Central Limit Theorem for Real and Banach Valued Random Variables, Wiley, New York.
BENKO, M. & HA® RDLE, W. (2005). Common Functional IV Surface Analysis Statistical Tools for Finance and Insurance, edited by C¥izek, P., Ha®rdle, W., Weron, R., Springer, Berlin.
BESSE, P. & RAMSAY, J.(1986). Principal Components of Sampled Functions, Psychometrika, 51: 285-311.
BLACK, F. & SCHOLES, M. (1973). The Pricing of Options and Corporate Liabilities, Journal of Political Economy, 81: 637-654.
DAUXOIS, J., POUSSE, A. & ROMAIN, Y. (1982). Asymptotic Theory for the Principal Component Analysis of a Vector Random Function: Some Applications to Statistical Inference, Journal of Multivariate Analysis 12: 136-154.
GIHMAN, I.I. & SKOROHOD, A.V. (1973). The Theory of Stochastic Processes II., Springer, New York.
HALL, P., KAY, J.W. & TITTERINGTON, D.M. (1990). Asymptotically Optimal Difference-based Estimation of Variance in Nonparametric Regression, Biometrika 77: 520-528.
HA® RDLE, W. & SIMAR, L. (2003). Applied Multivariate Statistical Analysis, Springer, Berlin.
FAN, J. & HUANG, L. (1999). Nonparametric Estimation of Quadratic Regression Functionals, Bernoulli 5: 927-949.
FENGLER, M. (2005). Arbitrage-free Smoothing of the Implied Volatility Surface SFB 649 Discussion Paper No. 2005-019, SFB 649, Humboldt-Universita®t zu Berlin.
FENGLER, M., HA® RDLE, W. & VILLA, P. (2003). The Dynamics of Implied Volatilities: A Common Principle Components Approach, Review of Derivative Research 6: 179-202.
FENGLER, M., HA® RDLE, W. & MAMMEN, E. (2005). A Dynamic Semiparametric Factor Model for Implied Volatility String Dynamics, SFB 649 Discussion Paper No. 2005-20, SFB 649 Humboldt-Univestit®at zu Berlin.
FENGLER, M. (2005b). Semiparametric Modeling of Implied Volatility, Springer, Berlin.
FLURY, B. (1988). Common Principal Components and Related Models, Wiley, New York.
KAHALE, N. (2004). An Arbitrage-free Interpolation of Volatilities, RISK 17: 102-106.

COMMON FUNCTIONAL PC

33

KNEIP, A. & UTIKAL, K. (2001). Inference for Density Families Using Functional Principal Components Analysis, Journal of the American Statistical Association 96: 519-531.
RAMSAY, J. & SILVERMAN, B. (2002). Applied Functional Data Analysis, Springer, New York. RAMSAY, J. & SILVERMAN, B. (2005). Functional Data Analysis, Springer, New York. RICE, J. & SILVERMAN, B. (1991). Estimating the Mean and Covariance Structure Nonparametri-
cally when the Data are Curves, Journal of Royal Statistical Society Ser. B 53: 233-243. RAO, C. (1958). Some Statistical Methods for Comparision of Growth Curves, Biometrics 14: 434-471.

CASE - Center for Applied Statistics and Economics Humboldt-Universita®t zu Berlin
Spandauerstr 1
D 10178 Berlin, Germany E-mail: {benko,haerdle}@wiwi.hu-berlin.de url: http://www.case.hu-berlin.de/

Statistische Abteilung Department of Economics Universita®t Bonn Adenauerallee 24-26 D-53113 Bonn, Germany e-mail: akneip@uni-bonn.de

SFB 649 Discussion Paper Series 2006
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Calibration Risk for Exotic Options" by Kai Detlefsen and Wolfgang K. H‰rdle, January 2006.
002 "Calibration Design of Implied Volatility Surfaces" by Kai Detlefsen and Wolfgang K. H‰rdle, January 2006.
003 "On the Appropriateness of Inappropriate VaR Models" by Wolfgang H‰rdle, Zdenk Hl·vka and Gerhard Stahl, January 2006.
004 "Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification" by Harald Uhlig, January/February 2006.
005 "British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis" by Enzo Weber, January 2006.
006 "A Combined Approach for Segment-Specific Analysis of Market Basket Data" by Yasemin Boztu and Thomas Reutterer, January 2006.
007 "Robust utility maximization in a stochastic factor model" by Daniel Hern·ndez≠Hern·ndez and Alexander Schied, January 2006.
008 "Economic Growth of Agglomerations and Geographic Concentration of Industries - Evidence for Germany" by Kurt Geppert, Martin Gornig and Axel Werwatz, January 2006.
009 "Institutions, Bargaining Power and Labor Shares" by Benjamin Bental and Dominique Demougin, January 2006.
010 "Common Functional Principal Components" by Michal Benko, Wolfgang H‰rdle and Alois Kneip, Jauary 2006.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

