BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2011-014
Difference based Ridge and Liu type Estimators
in Semiparametric Regression Models
Esra Akdeniz Duran* Wolfgang Karl Härdle**
Maria Osipenko**
* Gazi University Ankara, Turkey ** Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Difference based Ridge and Liu type Estimators in Semiparametric Regression
Models
Esra Akdeniz Duran Wolfgang Karl Ha¨rdle§
Maria Osipenko ¶
Abstract
We consider a difference based ridge regression estimator and a Liu type estimator of the regression parameters in the partial linear semiparametric regression model, y = X + f + . Both estimators are analysed and compared in the sense of mean-squared error. We consider the case of independent errors with equal variance and give conditions under which the proposed estimators are superior to the unbiased difference based estimation technique. We extend the results to account for heteroscedasticity and autocovariance in the error terms. Finally, we illustrate the performance of these estimators with an application to the determinants of electricity consumption in Germany.
Keywords: Difference based estimator; Differencing estimator, Differencing matrix, Liu estimator, Liu type estimator, Multicollinearity, Ridge regression estimator, Semiparametric model.
JEL classification: C14; C51
This research was supported by Deutsche Forschungsgemeinschaft through the SFB 649 'Economic Risk'.
Department of Statistics, Gazi University, Turkey. Email: esraakdeniz@gmail.com C.A.S.E. - Center for Applied Statistics & Economics, Humboldt-Universita¨t zu Berlin, Germany §Graduate Institute for Statistics, National Central University, Jhongli, Taiwan ¶Corresponding author. Email: maria.osipenko@wiwi.hu-berlin.de, Address: CASE, School of Business and Economics, Humboldt-Universi¨at zu Berlin, Unter den Linden 6, 10099, Germany.
1

1 Introduction

Semiparametric partial linear models have received considerable attention in statistics and econometrics. They have a wide range of applications, from biomedical studies to economics. In these models, some explanatory variables have a linear effect on the response while others are entering nonparametrically. Consider the semiparametric regression model:

yi = xi  + f (ti) + i,

i = 1, . . . , n

(1)

where yi's are observations at ti, 0  t1  t2  . . .  tn  1 and xi = (xi1, xi2, . . . , xip) are known p-dimensional vectors with p  n. In many applications ti's are values of an extra univariate "time" variable at which responses yi are observed. Here  = (1, . . . , p) is an unknown p-dimensional parameter vector, f (·) is an unknown smooth function and 's are inde-
pendent and identically distributed random errors with E(|x, t) = 0 and Var(|x, t) = 2. We shall call f (t) the smooth part of the model and as-
sume that it represents a smooth unparametrized functional relationship.

The goal is to estimate the unknown parameter vector  and the nonpara-

metric function f (t) from the data {yi, xi, ti}in=1. In vector/matrix notation, (1) is written as

y = X + f + 

(2)

where y = (y1, . . . , yn) , X = (x1, . . . , xn), f = {f (t1), . . . , f (tn)} ,  = (1, . . . , n) .

Semiparametric models are by design more flexible than standard linear regression models since they combine both parametric and nonparametric components. Estimation techniques for semiparametric partially linear models are based on different nonparametric regression procedures. The most important approaches to estimate  and f are given in Green et al. (1985), Engle et al. (1986), Eubank et al. (1998), Eubank et al. (1988), Eubank (1999), Ruppert et al. (2003), Ha¨rdle et al. (2004) and Ha¨rdle et al. (2000).

In practice, researchers often encounter the problem of multicollinearity. In case of multicollinearity we know that the (p × p) matrix X X has one or more small eigenvalues, the estimates of the regression coefficients can therefore have large variances: the least squares estimator performs poorly in this case. Hoerl and Kennard (1970) proposed the ridge regression estimator and it has become the most common method to overcome this particular weakness of the least squares estimator. For the purpose of this paper we will

2

employ the biased estimator that was proposed by Liu (1993) to combat the multicollinearity. The Liu estimator combines the Stein (1956) estimator with the ridge regression estimator, see also Akdeniz and Ka¸ciranlar (1995); Gruber (1985).
The condition number is a measure of multicollinearity. If X X is illconditioned with a large condition number, the ridge regression estimator or Liu estimator can be used to estimate , Liu (2003). We consider difference based ridge and Liu type estimators in comparison to the unbiased difference based approach. We give theoretical conditions that determine superiority among the estimation techniques in the mean squared error matrix sense.
We use data on monthly electricity consumption and its determinants (income, electricity and gas prices, temperature) for Germany. The purpose is to understand electricity consumption as a linear function of income and price and a nonlinear function of temperature: semiparametric approach is therefore necessary here. The data reveal a high condition number of 20.5, we therefore expect a more precise estimation with Ridge or Liu type estimators. We show how our theoretically derived conditions can be implemented for a given data set and be used to determine the appropriate biased estimation technique.
The paper is organised as follows. In Section 2, the model and the differencing estimator is defined. We introduce difference based ridge and Liu type estimators in Section 3. In Section 4, the differencing estimator proposed by Yatchew (1997) and the difference based Liu type estimator are compared in terms of the mean squared error. In Section 5, both biased regression methodologies in semiparametric regression models are compared in terms of the mean squared error. Section 6 relaxes the assumption of iid errors and replicates the results of the previous sections in the presence of heteroscedasticity and autocorrelation. Section 7 gives a real data example to show the performance of the proposed estimators.
2 The Model and Differencing Estimator
In this section we use a difference based technique to estimate the linear regression coefficient vector. This technique has been used to remove the nonparametric component in the partially linear model by various authors (e.g. Yatchew (1997), Yatchew (2003), Klipple and Eubank (2007), Brown
3

and Levine (2007)).

Consider the semiparametric regression model (2). Let d = (d0, d1, . . . , dm) be an m + 1 vector where m is the order of differencing and d0, d1, . . . , dm are differencing weights that minimise

m m-k

2

djdk+j ,

k=1 j=1

such that are satisfied.

m
dj = 0 and
j=0

m
dj2 = 1
j=0

(3)

Let us define the (n - m) × n differencing matrix D to have first and last
rows (d , 0n-m-1), (0n-m-1, d ) respectively, with i-th row (0i, d , 0n-m-i-1), i = 1, . . . , (n - m - 1), where 0r indicates an r-vector of all zero elements

d0 d1 d2 · · · dm 0 · · · · · · 0 

 0 d0 d1 d2 · · · dm 0 · · · 0 

D

=

  

...

...

  

 

0

···

···

d0

d1

d2 · · · dm

0

 

0 0 · · · · · · d0 d1 d2 · · · dm

Applying the differencing matrix to (2) permits direct estimation of the parametric effect. Eubank et al. (1988) show that the parameter vector in (2) can be estimated with parametric efficiency. If f is an unknown function with bounded first derivative, then Df is essentially 0, so that applying the differencing matrix we have

Dy = DX + Df + D  DX + D y  X + 

(4)

where y = Dy, X = DX and  = D. The constraints (3) ensure that the nonparametric effect is removed and Var() = Var() = 2. With (4) a simple differencing estimator of the parameter  in the semiparametric regression model results:

(0) = =

(DX) (DX) -1 (DX) Dy
-1
XX Xy

(5)

4

Thus, differencing allows one to perform inferences on  as if there were no nonparametric component f in the model (2), Fan and Wu (2008). We will also use the modified estimator of 2 proposed by Eubank et al. (1998)

2

=

y (I - P )y tr{D (I - P )D}

(6)

with P  = X(X X)-1X , I (p × p) identity matrix and tr(·) denoting the trace function for a square matrix.

3 Difference based ridge and Liu type estimator

As an alternative to (0) in (5), Tabakan and Akdeniz (2010) propose:
(1)(k) = (X X + kI)-1X y, k  0
Here k is the ridge-biasing parameter selected by the researcher. We call (1)(k) a difference based ridge regression estimator of the semiparametric regression model.

From the least squares perspective, the coefficients  are choosen to min-

imise

(y - X) (y - X)

(7)

Adding to the least squares objective (7) a penalising function of the squared
2
norm (0) -  for the vector of regression coefficients, yields a conditional
objective:

L = (y - X) (y - X) + ((0) - ) ((0) - )

(8)

Minimising (8) with respect to  , we obtain the estimator (2)() an alternative to (0) in (5):

(2)() = (X X + I)-1(X y + (0))

(9)

where , 0    1 is a biasing parameter and when  = 1, (2)() = (0). The formal resemblence between (9) and the Liu estimator motivated Akdeniz and Kac¸iranlar (1995), Hubert and Wijekoon (2006) and Yang and Xu (2009) to call it the difference based Liu type estimator of the semiparametric regression model.

5

4 Mean Squared Error Matrix (MSEM) Comparison of (0) with (2)()

In this section the objective is to examine the difference of the mean square error matrices of (0) and (2)(). We note that for any estimator  of ,
its mean squared error matrix (MSEM) is defined as MSEM() = Cov() +
Bias() Bias() , where Cov() denotes the variance-covariance matrix,
Bias() = E() -  is the bias vector. The expected value of (2)() can be written as
E{(2)()} =  - (1 - )(X X + I)-1

The bias of the (2)() is given as

Bias{(2)()} = -(1 - )(X X + I)-1.

(10)

Denoting F = (X X + I)-1(X X + I) and observing F and (X X)-1 are commutative, we may write (2)() as

(2)() = F(0) = F(X X)-1X y = (X X)-1FX y.

Setting S = (D X) (D X) and U = (X X)-1 we may write Cov{(2)()} as

Cov{(2)()} = 2FU SU F , Cov((0)) = 2U SU.

(11) (12)

Using (11) and (12) the difference 1 = Cov((0)) - Cov{(2)()} can be expressed as

1 = 2 U SU - FU SU F

(13)

= 2F{F-1U SU (F )-1 - U SU }F

= 2(1 - 2)(U -1 + I)-1 1 (U S + SU ) + U SU (U -1 + I)-1. 1+

Let 

=

1 1+

>

0,

M

=

USU,

N

=

US + SU.

Since M = L L and

rank(L) = p < n - m, then M is a (p × p) positive definite matrix, where

L = D X(X X)-1 and N = U S + SU is a symmetric matrix. Thus, we

may write (13) as

1 = 2(1 - 2)H(M +  N )H = 2(1 - 2)H(Q )-1(Q M Q +  Q N Q)Q-1H = 2(1 - 2)H(Q )-1(I +  E)Q-1H,

6

where I +  E = diag(1 +  e11, . . . , 1 +  epp) and H = (U -1 + I)-1. Since M is a positive definite and N is a symmetric matrix, a nonsingular matrix

Q exists such that Q M Q = I and Q N Q = E, here E is a diagonal

matrix and its diagonal elements are the roots of the polynomial equation |M -1N - eI| = 0 (see Graybill (1983), pp. 408 and Haville (1997), pp. 563)

and since N = U S + SU = 0 there is at least one diagonal element of E that

is nonzero. Let eii < 0 for at least one i, then positive definiteness of I +  E is guaranteed by

1 0 <  < min
eeii<0 ii

(14)

Hence 1 +  eii > 0 for all i = 1, . . . , p and therefore I +  E is a positive definite matrix. Consequently 1 becomes a positive definite matrix, as well. It is now evident that the estimator (2)() has a smaller variance compared

with the estimator (0) if and only if (14) is satisfied.

Next, we give necessary and sufficient conditions for the difference based Liu type estimator (2)() to be superior to (0) in the mean squared error matrix (MSEM) sense.

The proof of the next theorem requires the following

LEMMA 4.1 Farebrother (1976). Let A be a positive definite (p×p) matrix,
b a (p × 1) nonzero vector and  is a positive scalar. Then A - bb is nonnegative if and only if b A-1b  .

Let us compare the performance of (2)() with the differencing estimator (0) with respect to the MSEM criterion. In order to do that define 2 = MSEM((0)) - MSEM{(2)()}. Observe that:

MSEM((0)) = Cov((0)) = 2U SU

(15)

and

MSEM{(2)()} = 2FU SU F + (1 - )2(U -1 + I)-1 (U -1 + I)-1 (16)

Then from (15) and (16) one derives:

2 = 2F{F-1U SU (F )-1 - U SU }F -(1 - )2(U -1 + I)-1 (U -1 + I)-1,
= H 2(1 - 2)(M +  N ) - (1 - )2

H,

=

(1 - )2H

2

1 1

+ -

 (M


+

N)

-



H.

7

Applying Lemma 4.1 and assuming condition (14) to be satisfied, we see 2 is positive definite if and only if



(M

+

 N )-1



2

1

+

 ,

0<<1

1-

Now we may state the following theorem.

THEOREM 4.1 Consider the two estimators (2)() and (0) of . Let

W

=

1+ 1-

(M

+ N)

be

a

positive

definite

matrix.

Then

the

biased

estimator

(2)() is MSEM superior to (0) if and only if

 W -1  2.

5 MSEM Comparison of (1)(k) and (2)()
Let us now compare the MSEM performance of

(1)(k) = (X X + kI)-1X y = SkX Dy = A1y

(17)

with

(2)() = (X X + I)-1(X y + (0)) = (X X)-1(X X + I)-1(X X + I)X y
= U FX Dy = A2y

(18)

The MSEM of the difference based ridge regression estimator (1)(k) is given by

MSEM{(1)(k)} = Cov{(1)(k)} + Bias{(1)(k)} Bias{(1)(k)} = Sk(2S + k2 )Sk = 2(A1A1 ) + d1d1 ,
where Sk = (X X + kI)-1 and d1 = Bias{(1)(k)} = -kSk, see Tabakan and Akdeniz (2010). The MSEM in (16) may be written as

MSEM{(2)()} = 2(A2A2 ) + d2d2 ,

8

with d2 = Bias{(2)()} = -(1 - )(U -1 + I)-1. Define
3 = MSEM{(1)(k)}-MSEM{(2)()} = 2(A1A1 -A2A2 )+(d1d1 -d2d2 ) (19)
For the following proofs we employ:
LEMMA 5.1 (Trenkler and Toutenburg (1990)) Let (j) = Ajy, j = 1, 2 be the two linear estimators of . Suppose the difference Cov((1)) - Cov((2)) of the covariance matrices of the estimators (1) and (2) is positive definite. Then MSEM((1)) - MSEM((2)) is positive definite if and only if
-1
d2 Cov((1)) - Cov((2)) + d1d1 d2 < 1.

THEOREM 5.1 The sampling variance of (2)() is smaller than that of (1)(k), if and only if min(G2-1G1) > 1, where min is the minimum eigenvalue of G2-1G1 and Gj = 2AjAj , j = 1, 2.
Proof. Consider the difference
 = Cov{(1)(k)} - Cov{(2)()} = 2(A1A1 - A2A2 ), = G1 - G2

with G1 = (D XWkU ) = V V , Wk = I+kU and G2 = (XF U ) (XF U ). Since rank(V ) = p < n - m, G1 is a (p × p) positive definite matrix and
G2 is a symmetric matrix. Hence, a nonsingular matrix O exists such
that O G1O = I and O G2O = , with  diagonal matrix with diag-
onal elements roots  of the polynomial equation |G1 - G2| = 0 (see Haville (1997), p.563 or Schott (2005), p.160). Thus, we may write  = (O )-1(O G1O - O G2O)O-1 = (O )-1( - I)O-1 or O O =  - I.
If G1 - G2 is positive definite, then O G1O - O G2O =  - I is positive definite. Hence i - 1 > 0, i = 1, 2, . . . , p so we get min(G2-1G1) > 1.

Let now min(G2-1G1) > 1 hold. Furthermore, with G2 positive definite

and

G1

symmetric,

we

have

min

<

 

G1 G2

<

max

for

all

nonzero

(p × 1)

vectors , so G1 - G2 is positive definite, see Rao (1973),p.74. It is obvious

that Cov{(2)()} - Cov{(1)(k)} is positive definite for 0    1, k  0 if and only if min(G-2 1G1) > 1.

9

THEOREM 5.2 Consider (1)(k) = A1y and (2)() = A2y of . Suppose that the difference Cov{(1)(k)} - Cov{(2)()} is positive definite. Then
3 = MSEM{(1)(k)} - MSEM{(2)()} is positive definite if and only if
d2 2(A1A1 - A2A2 ) + d1d1 ) -1 d2 < 1
with A1 = SkX D, A2 = U FX D.
Proof. The difference between the MSEMs of (2)() and (1)(k) is given by
3 = MSEM{(1)(k)} - MSEM{(2)()} = 2(A1A1 - A2A2 ) + (d1d1 - d2d2 ) = Cov{(1)(k)} - Cov{(2)()} + (d1d1 - d2d2 )
Applying Lemma 5.1 yields the desired result.
It should be noted that all results reported above are based on the assumption that k and  are non-stochastic. The theoretical results indicate that the (2)() is not always better than the (1)(k), and vice versa. For practical purposes, we have to replace these unknown parameters by some suitable estimators.

6 The Heteroscedasticity and Correlated Error Case

Up to this point independent errors with equal variance were assumed. The error term might also exibit autocorrelation. To acccount for these effects we extend the results in this section and consider the more general case of heteroscedasticity and autocovariance in the error terms.

Consider now observations {yt, xt, tt}Tt=1 and the semiparametric partial linear model yt = xt  + f (tt) + t, t = 1, . . . , T . Let E( |x, t) =  not
necessarily diagonal. To keep the structure of the errors for later inference

we define an (n × n) permutation matrix P as in Yatchew (2003). Consider

a permutation:

1

 ...

 

i

 

...

t(1) 

... 

t(i)

 

...

 

n t(n)

10

where i = 1, . . . , n is the index of the ordered nonparametric variable and t(i) = 1, . . . , T corresponding time index of the observations. Then P is defined for i, j = 1, . . . , n:

Pij =

1, j = t(i) 0, otherwise

We can now rewrite the model after reordering and differencing:

DP y = DP X + DP f (x) + DP , E( |x, t) = 

(20)

Then, with X = DP X and y = DP y from (20) (0) is given: (0) = (X X)-1X y
with Cov((0)) = (X X)-1X DP D P X(X X)-1 = U X DP D P XU.

(21) (22)

We will use a heteroscedasticity and autocovariance consistent estimator described in Newey and West (1987) for the interior matrix of (22), which is in our case:

DP D P = {DP (DP ) }

L
1- L+1 H
=0

(23)

with DP  = y - X(0), denoting the elementwise matrix product, L maximum lag of non-zero autocorrelation in the errors and H0 identity matrix.

Let L be a matrix with ones on the th diagonal, then H , = 1, . . . L are

such that:

Hij =

0, if {DP (L + L )D P }ij = 0, 1, otherwise and i, j = 1, . . . , p.

Plugging (23) in (22) we obtain a consistent estimator for Cov((0)), see Yatchew (1999) for details.

Denoting S = X DP D P X we can write down Cov{(1)(k)} and Cov{(2)()} in the model (20).

Cov{(1)(k)} = SkSSk Cov{(2)()} = FU SU F

(24) (25)

11

Using (22) and (25) the difference 1 = Cov((0)) - Cov{(2)()} can be expressed as

1 = U SU - FU SU F

(26)

= F{F-1U SU (F )-1 - U SU }F

= (1 - 2)(U -1 + I)-1

1 (U S + SU ) + U SU

1+

(U -1 + I)-1,

with



=

1 1+

>

0,

M

=

USU,

N

= US

+ SU.

Since

M

is

a

(p × p)

positive

definite matrix and N is a symmetric matrix, a nonsingular matrix T exists

such that T M T = I and T N T = E, here E is a diagonal matrix and its

diagonal elements are the roots of the polynomial equation |M -1N - eI| = 0

(see Graybill (1983), pp. 408 and Haville (1997), pp. 563) and we may write

(26) as

1 = (1 - 2)H(M +  N )H = (1 - 2)H(T )-1(T M T +  T N T )T -1H = (1 - 2)H(T )-1(I +  E)T -1H,

where I +  E = diag(1 +  e11, . . . , 1 +  epp) and H = (U -1 + I)-1. Since

N = U S +SU = 0 there is at least one diagonal element of E that is nonzero.

Let eii < 0 for at least one i, then positive definiteness of I + E is guaranteed

by

1 0 <  < min
eeii<0 ii

(27)

Hence 1 +  eii > 0 for all i = 1, . . . , p and therefore I +  E is a positive definite matrix. Consequently 1 becomes a positive definite matrix, as well. It is now evident that the estimator (2)() has a smaller variance compared
with the estimator (0) if and only if (27) is satisfied.

With

1 = Cov((0)) - Cov{(1)(k)}

= k2Sk

1 (U S + SU ) + U SU
k

Sk

= k2Sk

1 N +M
k

Sk

12

and analogeous argumentation as above obtained for (1)(k): 11
0 < < min k eeii<0 ii

(28)

The next theorem extends the results of Theorem 3.1 in Tabakan and Akdeniz (2010) and Theorem 4.1 of Section 4 to the more general case of (20).

THEOREM 6.1 Consider the estimators (i)(x), i = {1, 2}; x = {k, }

and

(0)

of

.

Let

W1

= M + N,

W2 =

1+ 1-

(M

+

N)

be

positive

definite

(alternative: assume (27), (28) hold). Then the biased estimator (i)(x) is

MSEM superior to (0) if and only if

 Wi-1  1.

Proof. Consider the differences

2 = MSEM((0)) - MSEM{(2)()}

= Cov((0)) - Cov{(2)()} - Bias{(2)()} Bias{(2)()} = F{F-1U SU (F )-1 - U SU }F
-(1 - )2(U -1 + I)-1 (U -1 + I)-1

= (1 - )2H

1 1

+ -

 (M


+

N)

-



H

= (1 - )2H W2 -  H.

2 = MSEM((0)) - MSEM{(1)(k)}

= Cov((0)) - Cov{(1)(k)} - Bias{(1)(k)} Bias{(1)(k)}

= Sk{k(SU + U S) + k2U SU - k2 }Sk

= k2Sk

1N + M -  k

Sk

= k2Sk(W1 -  )Sk.

With Lemma 4.1 the assertation follows.

Theorem 6.1 gives conditions under which the biased estimator (i)(x), i =
{1, 2}; x = {k, } is superior to (0) in presence of heteroscedasticity and autocorrelation in the data.

13

Note, that for comparison of the biased estimators Theorem 5.1 can be extended straight forwardly to the general case by exchanging G1 and G2 by G1 = A1A1 and G2 = A2A2 correspondingly, with A1 = SkX DP, A2 = U FX DP . Hence, the sampling variance of (2)() is always smaller than
-1
that of (1)(k), if and only if min(G2 G1) > 1, where min is the minimum
-1
eigenvalue of G2 G1.
Now, we give a generalised version of Theorem 5.2.
THEOREM 6.2 Consider (1) = A1y and (2) = A2y of . Suppose that the difference Cov{(1)} - Cov{(2)} is positive definite. Then
3 = MSEM((1)) - MSEM((2)) is positive definite if and only if
-1
d2 A1A1 - A2A2 + d1d1 d2 < 1
Proof. The difference between the MSEMs of (2)() and (1)(k) is given by
3 = MSEM((1)) - MSEM((2)) = A1A1 - A2A2 + d1d1 - d2d2 = Cov((1)) - Cov((2)) + d1d1 - d2d2
Applying Lemma 5.1 yields the desired result.
We note that in order to use the criteria above one has to estimate the parameters. The estimation of  is thereby the most challenging. However, as long as the estimator (23) is available, all considered criteria can be evaluated on the real data and can be used for practical purposes.
7 Determinants of Electricity Demand
The empirical study example is motivated by the importance of explaining variation in electricity consumption. Since electricity is a non-storable good, electricity providers are interested in understanding and hedging demand fluctuations.
Electricity consumption is known to be influenced negatively by the price of electricity and positively by the income of the consumers. As electricity
14

is frequently used for heating and cooling the effect of the air temperature must also be present. Both heating by low temperatures and cooling by high temperatures result in higher electricity consumption and motivate the use of a nonparametric specification for the temperature effect. Thus we consider the semiparametric regression model defined in (1)

y = f (t) + 1x1 + 2x2 + 3x3 + . . . + 13x13 + 

(29)

where y is the log monthly electricity consumption per person (aggregated electricity consumption was devided by population interpolated linearly from quaterly data), t is cumulated average temperature index for the corresponding month taken as average of 20 German cities computed from the data of German weather service (Deutscher Wetterdienst), x1 is the log GDP per person interpolated linearly from quaterly data, detrended and deseasonalised and x2 is the log rate of electricity price to the gas price, detrended. The data for 199601-201009 comes from EUROSTAT. Reference prices for electricity were computed as an average of electricity tarifs for consumer groups IND-Ie and HH-Dc, for gas ­ IND-I3-2 and HH-D3 with reference period 2005S1. Time series of prices were obtained by scaling with electricity price or correspondingly gas price indices. x3, x4, . . . , x13 are dummy variables for the monthly effects.

The model in (29) includes both parametric effects and a nonparametric effect. The only nonparametric effect is implied by the temperature variable. From Figure 1 we can see that the effect of t on y is likely to be nonlinear, while the effects of other variables are roughly linear. The dummy variables enter into the linear part in the specification of the semiparametric regression as well.

We note that the condition number of X X of these explanatory variables is 20.5, which justifies the use of (1)(k) and (2)(), see Belsley et al. (1980).
Throughout the paper we use fifth-order differencing (m = 5). Results for other orders of differencing were similar. The admissible regions for the biasing parameters  and k for MSEM superiority were   0.923 and k  0.0085 determined using estimated values and Theorem 4.1 and Theorem 3.1 in Tabakan and Akdeniz (2010) respectively. Under more general assumptions on  and resulting heteroscedasticity and autocovariance consistent NeweyWest covariance estimator, the admissible region for  (Theorem 6.1 and restriction (27)) was shrinked to   0.927. For (1)(k) no admissible values of k were found, since admissible k  1.57 of (28) do not satisfy the condition of Theorem 6.1. Though scalar mean squared error (SMSE) superiority

15

y
6.1 6.2 6.3 6.4 6.5 6.6
y
6.1 6.2 6.3 6.4 6.5 6.6 y 6.1 6.2 6.3 6.4 6.5 6.6

0 200 400 600 800
t

-0.2 -0.1 0.0

0.1

x_2

0.2

-0.04

0.00
x_1

0.04

Figure 1: Plots of individual exp. variables vs. dependent variable, linear fit (green), local polynomial fit (red), 95 % confidence bands (black).

of (1)(k) and (1)() over (0) under general  is given for k  0.0267 and   0.384 compared to k  0.0123 and   0.708 by standard assumptions, see Figure 2 which depicts SMSE of the estimators computed as a trace of the MSEM and the corresponding  and k under standard and general assumptions. Thus the SMSE superiority intervals for  and k become even larger in the case of the general form of .
Our computations here are performed with R 2.10.1 and the codes are available on www.quantlet.org.

16

SMSE
0.1 0.2 0.3 SMSE
0.1 0.2 0.3

0.1 0.3 0.5 0.7 0.9
eta

0.1 0.3 0.5 0.7 0.9
k

Figure 2: SMSE of (2)() in dependence of  (left) and (1)(k) in dependence
of k (right) against that of (0) (dashed) under standard assumptions (black) and under generalized assumptions (red).

Results of different estimation procedures can be found in Table 1. We note that regardless of the estimator type, the effect of income is positive and the effect of relative price is negative as expected from an economic perspective, as in Engle et al. (1986). However, the R2 obtained by difference based methods is higher and SMSE lower for Liu type and ridge difference based estimator. The values of biasing parameters for which conditions of Theorem 5.1 and 5.2 are satisfied are given in Table 3. The superiority of (2)() over (1)(k) is assured for the zone of values marked by plus.
Returning to our semiparametric specification, we may now remove the estimated parametric effect from the dependent variable and analyse the nonparametric effect. We use a local linear estimator of f to model the nonparametric effect of temperature. The resulting plots are presented in figure 3 where we also include the linear effect. We notice that all differencing procedures result in similar estimators of f , regardless of notable differences in the coefficients of the linear part. The estimator of f is consistent with findings e.g. of Engle et al. (1986) for US electricity data.
In both specifications f is different from the linear effect and therefore including temperature as a linear effect is misleading.
17

OLS

(0) (1)(10-3) (2)(0.95)

x1

0.634

0.578

0.550

0.562

x2 -0.152 -0.160 -0.158 -0.161

x3

0.030 0.030

0.030

0.030

x4 -0.043 -0.040 -0.040 -0.040

x5

0.011

0.031

0.031

0.031

x6 -0.051 -0.014 -0.013 -0.014

x7 -0.054 -0.014 -0.013 -0.014

x8 -0.079 -0.065 -0.064 -0.065

x9 -0.036 -0.037 -0.036 -0.037

x10 -0.052 -0.044 -0.043 -0.044

x11 -0.049 -0.013 -0.012 -0.013

x12 -0.000

0.040

0.040

0.040

x13 -0.001 t -13 · 10-5

0.016 -

0.016 -

0.016 -

R2

0.729

0.749

0.749

0.749

Table 1: Results of OLS, difference based and Liu type difference based estimations.  indicates significance on 1%,  on 5 % and  on 10 %.


x1 x2 SMSE

(0) 2I NW 0.215 0.347 0.034 0.047 0.058 0.148

(1)(10-3) 2I NW 0.209 0.337 0.034 0.047 0.056 0.141

(2)(0.95) 2I NW 0.205 0.215 0.034 0.034 0.054 0.058

Table 2: Standard errors of the estimators in comparison to Newey-west standard errors for the effects of x1 (income) and x2 (relative price).

8 Conclusion
We proposed a difference based Liu type estimator and a difference based ridge regression estimator for the partial linear semiparametric regression model.
The results show that in case of multicollinearity the proposed estimator, (2)() is superior to the difference based estimator (0). We gave bounds on the value of  which ensure the superiority of the proposed estimator. The two biased estimators (2)() and (1)(k) for different values of  and k can
18

k · 104  · 102 1 2 3 4 5 6 7 8 9 10 11 12 13 9.23­9.23 ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.24­9.24 + ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.25­9.25 + + ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.26­9.26 + + + ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.27­9.27 + + + + ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.28­9.28 + + + + + ­ ­ ­ ­ ­ ­ ­ ­ 9.29­9.30 + + + + + + ­ ­ ­ ­ ­ ­ ­ 9.31­9.31 + + + + + + + ­ ­ ­ ­ ­ ­ 9.32­9.32 + + + + + + + + ­ ­ ­ ­ ­ 9.34­9.35 + + + + + + + + + ­ ­ ­ ­ 9.36­9.37 + + + + + + + + + + ­ ­ ­ 9.38­9.39 + + + + + + + + + + + ­ ­ 9.40­9.43 + + + + + + + + + + + + ­ 9.44­9.56 + + + + + + + + + + + + + 9.57­9.61 + + + + + + + + + + + + ­ 9.62­9.65 + + + + + + + + + + + ­ ­ 9.66­9.69 + + + + + + + + + + ­ ­ ­ 9.70­9.72 + + + + + + + + + ­ ­ ­ ­ 9.73­9.76 + + + + + + + + ­ ­ ­ ­ ­ 9.77­9.79 + + + + + + + ­ ­ ­ ­ ­ ­ 9.80­9.82 + + + + + + ­ ­ ­ ­ ­ ­ ­ 9.83­9.85 + + + + + ­ ­ ­ ­ ­ ­ ­ ­ 9.86­9.88 + + + + ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.89­9.91 + + + ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.92­9.94 + + ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.95­9.97 + ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ 9.98­9.99 ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­ ­
Table 3: Admissible biasing parameters  and k marked by plus if they satisfy conditions of Theorems 5.1 and 5.2, i.e. (2)() is superior to (1)(k).
be compared in terms of MSEM with the theoretical results above.
Finally, an application to electricity consumption has been provided to show properties of the proposed estimator based on the mean square error criterion. We could estimate the linear effects of the linear determinants as well as the nonparametric effect f of a cumulated average temperature index.
19

y
6.1 6.2 6.3 6.4 6.5
y
6.1 6.2 6.3 6.4 6.5 y 6.1 6.2 6.3 6.4 6.5

0 200 400 600 800
t

0 200 400 600 800
t

0 200 400 600 800
t
Figure 3: Estimated f nonlinear effect of t on y via differenced based (left), Liu-type differenced based (right) and difference-based ridge (center) approaches.
References
Akdeniz, F. and Ka¸ciranlar, S. (1995). On the almost unbiased generalized liu estimator and unbiased estimation of the bias and mse. Communications in Statistics Theory and Methods, 24(7):1789­1797.
Belsley, D., Kuh, E., and Welsch, R. (1980). Regression Diagnostics. Wiley, New York.
20

Brown, L. and Levine, M. (2007). Variance estimation in nonparametric regression via the difference sequence method. Annals of Statistics, 35:2219­ 2232.
Engle, R. F., Granger, C., Rice, J., and Weiss, A. (1986). Semiparametric estimates of the relation between weather and electricity sales. Journal of American Statistical Association, 81:310­320.
Eubank, R. (1999). Nonparametric Regression and Spline Smoothing. Marcel Dekker, New York.
Eubank, R., Kambour, E., Kim, J., Klipple, K., Reese, C., and Schimek, M. (1988). Kernel smoothing in partial linear models. Journal of Royal Statistical Society Series B, 50(3):413­436.
Eubank, R., Kambour, E., Kim, J., Klipple, K., Reese, C., and Schimek, M. (1998). Estimation in partially linear models. Computational Statistics and Data Analysis, 29:27­34.
Fan, J. and Wu, Y. (2008). Semiparametric estimation of covariance matrices for longitudinal data. Journal of American Statistical Association, 103:1520­1533.
Farebrother, R. (1976). Further results on the mean square error of ridge regression. Journal of Royal Statistical Society Series B, 38:248­250.
Graybill, F. (1983). Matrices with Applications in Statistics. Duxbury Classic.
Green, P., Jennison, C., and Seheult, A. (1985). Analysis of field experiments by least squares smoothing. Journal of Royal Statistical Society Series B, 47:299­315.
Gruber, M. (1985). Improving Efficiency by Shrinkage: The James-Stein and Ridge Regression Estimators. New York, Marcell Dekker, Inc.
Ha¨rdle, W., Liang, H., and Gao, J. (2000). Partially Linear Models. Physika Verlag, Heidelberg.
Ha¨rdle, W., Mu¨ller, M., Sperlich, S., and Werwatz, A. (2004). Nonparametric and Semiparametric Models. Springer Verlag, Heidelberg.
Haville, D. (1997). Matrix Algebra from a Statistician's Perspective. Springer Verlag,New York.
21

Hoerl, A. and Kennard, R. (1970). Ridge regression:biased estimation for orthogonal problems. Technometrics, 12:55­67.
Hubert, M. and Wijekoon, P. (2006). Improvement of the liu estimation in linear regression model. Statistical Papers, 47(3):471­479.
Klipple, K. and Eubank, R. (2007). Difference-based variance estimators for partially linear models. Festschrift in honor of Distinguished Professor Mir Masoom Ali on the occasion of his retirement, pages 313­323.
Liu, K. (1993). A new class of biased estimate in linear regression. Communications in Statistics Theory and Methods, 22:393­402.
Liu, K. (2003). Using liu type estimator to combat multicollinearity. Communications in Statistics Theory and Methods, 32(5):1009­1020.
Newey, W. and West, K. (1987). A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):pp. 703­708.
Rao, C. (1973). Linear Statistical Inference and Its Applications. Wiley, New York.
Ruppert, D., Wand, M., Carroll, R., and Gill, R. (2003). Semiparametric Regression. Cambridge University Press.
Schott, J. (2005). Matrix Analysis for Statistics (Second Ed.). Wiley Inc., New Jersey.
Stein, C. (1956). Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. Proc. Third Berkeley Symp. Math. Statist. Prob., 1:197­206.
Tabakan, G. and Akdeniz, F. (2010). Difference-based ridge estimator of parameters in partial linear model. Statistical Papers, 51:357­368.
Trenkler, G. and Toutenburg, H. (1990). Mean square matrix comparisons between two biased estimators-an overview of recent results. Statistical Papers, 31:165­179.
Yang, H. and Xu, J. (2009). An alternative stochastic restricted liu estimator in linear regression. Statistical Papers, 50:639­647.
Yatchew, A. (1997). An elementary estimator of the partial linear model. Economics Letters, 57:135­143.
22

Yatchew, A. (1999). Differencing methods in nonparametric regression: Simple techniques for the applied econometrician. http://www.economics.utoronto.ca/yatchew/.
Yatchew, A. (2003). Semiparametric Regression for the Applied Econometrican. Cambridge University Press.
23

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Localising temperature risk" by Wolfgang Karl Härdle, Brenda López Cabrera, Ostap Okhrin and Weining Wang, January 2011.
002 "A Confidence Corridor for Sparse Longitudinal Data Curves" by Shuzhuan Zheng, Lijian Yang and Wolfgang Karl Härdle, January 2011.
003 "Mean Volatility Regressions" by Lu Lin, Feng Li, Lixing Zhu and Wolfgang Karl Härdle, January 2011.
004 "A Confidence Corridor for Expectile Functions" by Esra Akdeniz Duran, Mengmeng Guo and Wolfgang Karl Härdle, January 2011.
005 "Local Quantile Regression" by Wolfgang Karl Härdle, Vladimir Spokoiny and Weining Wang, January 2011.
006 "Sticky Information and Determinacy" by Alexander Meyer-Gohde, January 2011.
007 "Mean-Variance Cointegration and the Expectations Hypothesis" by Till Strohsal and Enzo Weber, February 2011.
008 "Monetary Policy, Trend Inflation and Inflation Persistence" by Fang Yao, February 2011.
009 "Exclusion in the All-Pay Auction: An Experimental Investigation" by Dietmar Fehr and Julia Schmid, February 2011.
010 "Unwillingness to Pay for Privacy: A Field Experiment" by Alastair R. Beresford, Dorothea Kübler and Sören Preibusch, February 2011.
011 "Human Capital Formation on Skill-Specific Labor Markets" by Runli Xie, February 2011.
012 "A strategic mediator who is biased into the same direction as the expert can improve information transmission" by Lydia Mechtenberg and Johannes Münster, March 2011.
013 "Spatial Risk Premium on Weather Derivatives and Hedging Weather Exposure in Electricity" by Wolfgang Karl Härdle and Maria Osipenko, March 2011.
014 "Difference based Ridge and Liu type Estimators in Semiparametric Regression Models" by Esra Akdeniz Duran, Wolfgang Karl Härdle and Maria Osipenko, March 2011.
SFB 649, Ziegelstraße 13a, D-10117 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

