BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2013-017
Estimating the Quadratic Covariation
Matrix from Noisy Observations: Local Method of Moments
and Efficiency
Markus Bibinger* Nikolaus Hautsch*
Peter Malec* Markus Reiss*
* Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

ESTIMATING THE QUADRATIC COVARIATION MATRIX FROM NOISY OBSERVATIONS:
LOCAL METHOD OF MOMENTS AND EFFICIENCY
By Markus Bibinger, Nikolaus Hautsch, Peter Malec and Markus Reiß
Humboldt-Universit¨at zu Berlin
An efficient estimator is constructed for the quadratic covariation or integrated covolatility matrix of a multivariate continuous martingale based on noisy and non-synchronous observations under high-frequency asymptotics. Our approach relies on an asymptotically equivalent continuous-time observation model where a local generalised method of moments in the spectral domain turns out to be optimal. Asymptotic semiparametric efficiency is established in the Cram´er-Rao sense. Main findings are that non-synchronicity of observation times has no impact on the asymptotics and that major efficiency gains are possible under correlation. Simulations illustrate the finite-sample behaviour.
1. Introduction. The estimation of the quadratic variation or integrated volatility of a semi-martingale is a key question, both from a theoretical viewpoint as well as for applications, particularly in finance. Here we treat the multi-dimensional case, where the quadratic covariation or integrated covolatility matrix is the quantity of interest. It turns out that the richer geometry, e.g. due to non-commuting matrices, generates new effects and calls for a deeper mathematical understanding. Covariation estimates are particularly important in various financial applications, for instance, as inputs in portfolio allocation problems, quantification of risk, hedging or asset pricing. The availability of high-frequency data opens up new ways for inference. As the data is typically polluted by observational noise, e.g. by microstructure frictions, estimation in these models is far from obvious and furnishes unexpected results.
Financial support from the Deutsche Forschungsgemeinschaft via SFB 649 O¨konomisches Risiko and FOR 1735 Structural Inference in Statistics: Adaptation and Efficiency is gratefully acknowledged.
AMS 2000 subject classifications: Primary 62M10; secondary 62G05; JEL classes: C14, C32, C58, G10.
Keywords and phrases: adaptive estimation, asymptotic equivalence, asynchronous observations, integrated covolatility matrix, quadratic covariation, semiparametric efficiency, microstructure noise, spectral estimation
1

2 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

We focus on the fundamental statistical model where the d-dimensional discrete-time process

(E0)

Yi(l) = Xt((ill)) + (il), 0  i  nl, 1  l  d,

is observed with the d-dimensional continuous martingale

Zt
Xt = X0 + 1/2(s) dBs, t  [0, 1],
0

in terms of a d-dimensional standard Brownian motion B and the squared (instantaneous or spot) covolatility matrix

(t) = (lr(t))1l,rd  Rd×d.

The signal part X is assumed to be independent of the observation noise . The observation errors ((il)), 1  l  d, 1  i  nl, are assumed to be mutually independent and centered normal with variances l2. The observation times are given via quantile transformations as til = Fl-1(i/nl) for some distribution functions Fl. While this model is certainly an idealisation of many real data situations, its precise analysis delivers a profound understanding and thus serves as a basis for developing procedures in more complex models.
Covariation estimation is a core research topic in current financial econometrics and various approaches exist. Let us mention the quasi-maximumlikelihood method by A¨it-Sahalia et al. [1], realised kernels by BarndorffNielsen et al. [3], pre-averaging by Christensen et al. [5] and the local spectral estimator by Bibinger and Reiß [4]. In contrast to the univariate case, however, the asymptotic properties are very involved, difficult to compare and a lower efficiency bound was lacking as a benchmark.
Building on the idea of locally constant approximations, we propose a local method of moments (LMM) estimator in the spectral domain which is shown to be asymptotically efficient. We perform an asymptotic analysis where the sample sizes n1, . . . , nd tend to infinity. In Section 2 (strong) asymptotic equivalence in Le Cam's sense is established by Theorem 2.4 with the signal-in-white-noise model

(E1) dYt = Xt dt + diag(Hn,l(t))1ld dWt , t  [0, 1] ,

where W is a standard d-dimensional Brownian motion independent of B and the local noise level is given by

Hn,l(t) := l(nlFl (t))-1/2 .

(1.1)

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

3

The imposed regularity condition is that (t) is the sum of an L2-Sobolev function of regularity  and an L2-martingale and the size of  accommodates for asymptotically separating sample sizes (nl)1ld.
Let us recall that if two sequences of statistical experiments are asymptotically equivalent, then any statistical procedure in one experiment has a counterpart in the other experiment with the same asymptotic properties for bounded loss functions, see Le Cam and Yang [17] for a thorough introduction. Our proof is constructive such that the procedure that we shall develop for (E1) has a concrete counterpart in (E0) with the same asymptotic properties.
A remarkable theoretical consequence of this result is that under noise the asynchronicity of the data does not affect the asymptotically efficient procedures (it is of smaller asymptotic order). In model (E1) the distribution functions Fl only generate a varying local noise level Hn,l(t), but the shift between observation times of different processes does not matter. This is in sharp contrast to the noiseless setting where the variance of the HayashiYoshida estimator [12] suffers from errors due to asynchronicity, which carries over to the preaveraged version by Christensen et al. [5] designed for the noisy case.
In Section 3 we consider the continuous-time model (E1) and go over to a block-wise constant approximation. Empirical Fourier coefficients yield local spectral statistics (Sjk) in the spirit of Reiß [19]. On each block we apply locally a generalised method of moments, using a weighted sum of the empirical covariance matrices SjkSjk  Rd×d and a bias correction. The optimal weighting for estimating an entry of the covariation matrix combines in general all entries of SjkSjk. The non-commutativity of different Fisher information matrices then implies in particular that the volatility estimation for one coordinate process X(l) gains in efficiency when using data for all other potentially correlated processes X(r), see Sections 4.2 and 5 for details and the improvement with respect to the approach in Bibinger and Reiß [4]. Note the contrast with i.i.d. observations of a Gaussian vector where the empirical variance of one component is an efficient estimator and using the other entries cannot improve the variance estimator unless the correlation is known, cf. the classical Example 6.6.4 in Lehmann and Casella [18].
In Theorem 3.2 a multivariate central limit theorem (CLT) is provided for an oracle LMM (Local Method of Moments) estimator, using the unknown optimal weights and an information-type matrix for normalisation. Specifying to sample sizes of the same order n, Corollary (3.3) yields a CLT with rate n1/4 and a covariance structure between matrix entries, which is given explicitly by concise matrix algebra. Using pre-estimated weight matrices, a

4 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

fully adaptive version of the LMM-estimator is obtained, which by Theorem 3.4 shares the same asymptotic properties as the oracle estimator.
Another main result of this work is that the asymptotic covariance structure of the LMM-estimators is optimal in a semiparametric Cram´er-Rao sense. In Section 4 a lower bound proof is achieved by a combination of space-time transformations and advanced calculus for covariance operators. The concrete form of the asymptotic variance is discussed for some key settings, thus generalising the univariate optimality theory by Gloter and Jacod [10] and Reiß [19] to the multivariate case. The discretisation and implementation of the estimator for model (E0) is briefly described in Section 5 and presented together with some numerical results for a simple toy model and a more complex and realistic scenario. The finite sample behaviour of the LMM estimators is well predicted by the asymptotic theory (even in cases where it does not apply formally) and some comparison with competing procedures is provided.

2. From discrete to continuous-time observations.

2.1. Setting. First, let us specify different regularity assumptions. For functions f : [0, 1]  Rm, m  1 or also m = d × d for matrix values, we introduce the L2-Sobolev ball of order   (0, 1] and radius R > 0

H(R) = {f  H([0, 1], Rm)| f H  R} where

f

H

:=

max
1im

fi H ,

which for matrices means f H := max1i,jd fij H. We also consider Ho¨lder spaces C([0, 1]) and Besov spaces Bp,q([0, 1]) of such functions. Canonically, for matrices we use the spectral norm · and we set
f  := supt[0,1] f (t) .

In order to pursue asymptotic theory, we impose that the deterministic samplings in each component can be transferred to an equidistant scheme by respective quantile transformations independent of nl, 1  l  d.
Assumption 2.1.-() Suppose that there exist differentiable distribution functions Fl  C([0, 1]), 1  l  d, with Fl(0) = 0, Fl(1) = 1 and Fl > 0, such that the observation times in (E0) are generated by ti(l) = Fl-1(i/nl), 0  i  nl, 1  l  d.
We gather all assertions on the instantaneous covolatility matrix function (t), t  [0, 1], which we shall require at some point.

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

5

Assumption 2.2. Let  : [0, 1]  Rd×d be a possibly random function with values in the class of symmetric, positive semi-definite matrices, independent of X and the observational noise, satisfying:
(i-)   H([0, 1]) for  > 0. (ii-)  = B + M with B  B1,([0, 1]) for  > 0 and M a matrix-
valued L2-martingale. (iii-) (t)   for a strictly positive definite matrix  and all t  [0, 1].

Let us briefly discuss the different function spaces, see e.g. Cohen [7,
Section 3.2] for a survey. First, any -Ho¨lder-continuous function lies in the L2-Sobolev space H and any H-function lies in the Besov space B1,, where differentiability is measured in an L1-sense. The important class of
bounded variation functions (e.g., modeling jumps in the volatility) lies in B11,, but only in H for  < 1/2. In particular, part (ii-),   1, covers L2-semi-martingales by separate bounds on the drift (bounded variation)
and martingale part. Beyond classical theory in this area is the fact that also non-semi-martingales like fractional Brownian motion BH with hurst
parameter H > 1/2 give rise to feasible volatility functions in the results below, using BH  CH-  B1H, for any  > 0 from Ciesielski et al. [6].
In the sequel the potential randomness of  is often not discussed addi-
tionally because by independence we can always work conditionally on .
Finally, let us mention that we could also weaken the Ho¨lder-assumptions
on F1, . . . , Fd towards Sobolev or Besov regularity at the cost of tightening the assumptions on . For the sake of clarity this is not pursued here.
Throughout the article we write Zn = OP (n) and Zn = OP (n) for a sequence of random variables Zn and a sequence n, to express that n-1Zn is bounded or tends to zero in probability, respectively. Analogously O (or
equivalently ) and O refer to deterministic sequences. We write Zn Xn if Zn = OP (Yn) and Yn = OP (Zn) and the same for deterministic quantities.
Ed denotes the d-dimensional unit matrix and p,q = 1(p = q) equals 1 for
p = q and 0 otherwise.

2.2. Continuous-time experiment.

Definition 2.3. Let E0((nl)1ld, , R) with nl  N,   (0, 1], R > 0, be the statistical experiment generated by observations from (E0) with   H(R). Analogously, let E1((nl)1ld, , R) be the statistical experiment generated by observing (E1) with the same parameter class.
As we shall establish next, experiments E0 and E1 will be asymptotically

6 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

equivalent as nl  , 1  l  d, at a comparable speed, denoting

nmin

=

min
1ld

nl

and

nmax

=

max
1ld

nl

.

Theorem 2.4. Grant Assumption 2.1-() on the design. The statistical experiments E0((nl)1ld, , R) and E1((nl)1ld, , R) are asymptotically equivalent for any   (0, 1/2] and R > 0, provided
nmin  , nmax = O((nmin)1+).

More precisely, the Le Cam distance  is of order  (E0((nl)1ld, , R), E1((nl)1ld, , R)) = O R2

Xd

nl

!
/l2

n-m1in-

!

.

l=1

By inclusion, the result also applies for  > 1/2 when in the remain-
ing expressions  is replaced by min(, 1/2). A standard Sobolev smooth-
ness of  is  almost 1/2 for diffusions with finitely many or absolutely
summable jumps. In that case, the asymptotic equivalence result holds if nmax grows more slowly than nm3/i2n. Theorem 2.4 is proved in the appendix in a constructive way by warped linear interpolation, which yields a readily
implementable procedure, cf. Section 5 below.

3. Localisation and method of moments.

3.1. Construction. We partition the interval [0, 1] in blocks [kh, (k +1)h)

of length h. On each block a parametric MLE for a constant model could be

sought for. Its numerical determination, however, is difficult and unstable

due to the non-concavity of the ML objective function and its analysis is

quite involved. Yet, the likelihood equation leads to spectral statistics whose

empirical covariances estimate the quadratic covariation matrix. We there-

fore prefer a localised method of moments (LMM) for these spectral statistics

where for an adaptive version the theoretically optimal weights are deter-

mined in a pre-estimation step, in analogy with the classical (multi-step)

GMM (generalised method of moments) approach by Hansen [11]. As motivated in Reiß [19], consider the L2([0, 1])-orthonormal system

(jk) and its antiderivatives (jk):

r

j k (t)

=

2 h

cos


j

h-1

(t

-


kh) 1[kh,(k+1)h](t), j



1,

(3.1a)

jk(t) =

2h j

sin


j



h-1

(t

-


kh) 1[kh,(k+1)h](t),

j



1

.

(3.1b)

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

7

In each component local spectral statistics are defined by

Sj(kl)

=

Z (k+1)h

jh-1

j k (t)

kh

dY

(l)(t),

j



1,

k

= 0, .

.

.,

h-1-

1, 1



l



d, (3.2)

from the continuous-time experiment E1. In order to design our estimator, we consider a locally constant approximation of the general non-parametric model.

Definition 3.1.

Set

f¯h(t)

:=

h-1

R (k+1)h kh

f (s)ds

for

t



[kh, (k

+

1)h),

k  N0, a function f on [0, 1] and h  (0, 1). Assume h-1  N and let

Xth

=

X0

+

Rt 0

¯ h1/2(s)

dBs

with

a

d-dimensional

standard

Brownian

motion

B. Define the process

q



(E2)

dY~t = Xth dt + diag

H 2 n,l,h (t)

dWt , t  [0, 1] ,

1ld

where W is a standard Brownian motion independent of B and with noise
level (1.1). The statistical model generated by the observations from (E2) for   H(R) is denoted by E2((nl)1ld, h, , R).

In experiment E2 we thus observe a process with a covolatility matrix which is constant on each block [kh, (k + 1)h), k = 0, 1, . . . , h-1 - 1, and cor-
rupted by noise of block-wise constant magnitude. Our approach is founded
on the idea that for small block sizes h and sufficient regularity this piecewise constant approximation is close to E1.
A basic ingredient to derive the covariance structure are the joint moments for a d-dimensional random vector X  N (0, Q). For (l, r, p, q)  {1, . . . , d}4
we have by Isserlis [13]:


E X(l)X(r)X(p)X(q) = QlrQpq + QlpQrq + QlqQrp ,

(3.3)

in particular Var((X(l))2) = 2Q2ll, Var(X(l)X(p)) = QllQpp + Ql2p. The LMM estimator is built from the data in experiment E1, but designed
for the block-wise parametric model E2. In E2, the L2-orthogonality of (jk)
as well as that of (jk) imply (cf. Reiß [19] in the scalar case)

Sjk  N (0, Cjk) independent for all (j, k)

(3.4)

with covariance matrix

Cjk = kh+ 2j2h-2 diag(Hnk,hl)l2, kh = ¯ h(kh), Hnk,hl = (H2n,l,h(kh))1/2. (3.5)

8 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

In the multivariate central limit theorem we are facing covariances between

entries of the covariation matrix estimator, which we shall formalise by interpreting matrices as vectors: for a matrix A  Rd×d we consider the vector

of its entries


vec(A) := A11, A21, . . . , Ad1, A12, A22, . . . , Ad2, . . . , Ad(d-1), Add

 Rd2 .

The natural estimator for vec(Cjk) is the empirical covariance vec(SjkSjk). We employ Kronecker (tensor) product calculus, where A  B  Rd2×d2 for A, B  Rd×d is given by

(A  B)p(d-1)+q,p (d-1)+q = App Bqq , p, q, p , q = 1, . . . , d.

We evaluate the covariance matrix of vec(SjkSjk) in model E2 by introducing

Z = COV(vec(ZZ )) for Z  N(0, Ed)

(3.6)

and applying the rule vec(ABC) = (C  A)vec(B), see e.g. Fackler [9]:

COVE2 (vec(SjkSjk)) = COV(vec(Cj1k/2ZZ Cj1k/2)) = (Cjk  Cjk)Z. (3.7)
We have used the commutativity of Z with (Cjk  Cjk)1/2 = (Cj1k/2  Cj1k/2), which is easily checked using the actual form of Z derived from (3.3)

Zp(d-1)+q,p (d-1)+q = (1 + p,q){p,q},{p ,q }, p, q, p , q = 1, . . . , d,

or the equivalent property Zvec(A) = vec(A + A ) for all A  Rd×d. Let us further introduce the Fisher information-type matrices

Ijk = Cj-k1  Cj-k1,

X
Ik = Ijk,

j  1, k = 0, . . . , h-1 - 1.

j=1

Our local method of moments estimator with oracle weights LMMo(nr) uses that on each block a natural second moment estimator of kh is given as a
convex combination of the bias-corrected empirical covariances:

LMMo(nr)

:=

h-X1-1 X



h Wjkvec SjkSjk

-

2j2 h2


diag ((Hnk,hl)2)1ld .

(3.8)

k=0 j=1

The optimal weight matrices Wjk in the oracle case are obtained as

Wjk := Ik-1Ijk  Rd2×d2 .

(3.9)

Note

that

Cjk, Ijk, Ik

and

Wjk

all

depend

on

(nl

)1ld

and
P

h,

which

is

omit-

ted in the notation. Finally, observe that (3.5) and j Wjk = Ed2 imply

that LMM(onr) is unbiased under E2.

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

9

3.2. Asymptotic properties of the estimators. We formulate the main result that the oracle estimator (3.8) and also a fully adaptive version for the integrated volatility matrix satisfy central limit theorems.

Theorem 3.2. Let Assumptions 2.1-(), 2.2(ii-) and 2.2(iii-) with

> (3.8)

1/2 hold yields a

ctornuseisftoerntobessetirmvaattioornsfofrrovmec(mR0o1del(sE)1d.s)Thaes

oracle estimator nmin   and

h = h0n-m1in/2 with h0  . Moreover, if nmax = O(nm2in) and h = O(nm-1a/x4),

then a multivariate central limit theorem holds:

 Z
I1n/2 LMMo(nr) -vec

1

(s)


ds

-L

N

(0,

Z)

in

E1

(3.10)

0

with

Z

from

(3.6)

and

In-1

=

Ph-1 -1 k=0

h2Ik-1.

We give a typical illustration with convergence rate nm1/i4n and asymptotic covariance matrix.

Corollary 3.3. Under the assumptions of Theorem 3.2 suppose
nmin/np  p  (0, 1] for p = 1, . . . , d and introduce H(t) = diag(pp1/2Fp(t)-1/2)p  Rd×d and H1/2 := H(H-1H-1)1/2H. Then

 Z
n1m/i4n LMMo(nr) -vec

1

(s)


ds

-L

N


0,

I-1 Z 

in E1

0

(3.11)

with

I-1

=

Z
2

1
(



1H/2

+ H1/2



)(t) dt.

0

In particular, the entries satisfy for p, q = 1, . . . , d


n1m/i4n (LMM(onr))p(d-1)+q

Z
-

1
pq(s) ds

-L

(3.12)

0


N 0, 2(1 +

Z
p,q )

1(pp(H1/2)qq

+ qq(H1/2)pp


+ 2pq(H1/2)pq)(t) dt .

0

The variance (3.12) will coincide with the lower bound obtained in Section
4 below. The local noise level in H(t) depends on the observational noise level p and the local sample size p-1Fp(t), p = 1, . . . , d, after normalisation by nmin. It is easy to see that in the case nmin/np  0 the asymptotic variance vanishes for all entries (p, q), q = 1, . . . , d. In general, also all other
asymptotic variances are then reduced because the estimator profits from
correlation. In Section 4.2 the effect of H on H is discussed and illustrated

10 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

in Figure 1. We infer the structure of the asymptotic covariance matrix using

block-wise diagonalisation in Appendix B.
For a feasible estimator the optimal weight matrices Wjk = Wj(kh) and the information-type matrices Ijk = Ij(kh) are estimated in a preliminary
step from the same data. To reduce variability in the estimate, a coarser grid of r-1 equidistant intervals, r/h  N is employed for W^ jk. As derived in Bibinger and Reiß [4] for supremum norm loss and extended to L1-loss
and Besov regularity using the L1-modulus of continuity as in the case of wavelet estimators (Cor. 3.3.1 in Cohen [7]), a preliminary estimator ^ (t)

of the instantaneous covolatility matrix (t) exists with

^ -  L1 = OP nm-in/(4+2)

(3.13)

for   B1,([0, 1]). For k with kh  [mr, (m + 1)r) we set

W^ jk = Wj(^ mr), I^jk = Ij(^ kh) with ^ mr = ^ r(mr), ^ kh = ^ h(kh).

The quadratic covariation matrix estimator with adaptive weights is then given by

LMM(and)

=

h-X1 -1

h

X

W^ jk

vec


Sjk

Sjk

-

2j2 h2


diag ((Hnk,hl)2)1ld .

(3.14)

k=0 j=1

We estimate the total covariance matrix via

^In-1

=

h-X1 -1


h2

X I^jk-1

.

k=0 j=1

(3.15)

For j   the weights Wj() and the matrices Ij() decay like j-4 in norm, compare Lemma C.1 below, such that in practice a finite sum over frequencies j suffices. By a tight bound on the derivatives of   Wj() we show in Appendix C.4 the following general result.

Theorem 3.4. Suppose   B1,([0, 1]) for   (1/2, 1] satisfying /(2 + 1) > log(nmax)/ log(nmin) - 1. Choose h, r  0 such that h0 = hn1m/i2n log(nmin) and n-min/(2+1) r (nmin/nmax)1/2, h-1, r-1, r/h  N. If the pilot estimator ^ satisfies (3.13), then under the conditions of
Theorem 3.2 the adaptive estimator (3.14) satisfies

^I1n/2


LMMa(nd)

-vec

Z 1 0

(s)


ds

-L

N

(0,

Z)

,

(3.16)

with ^In from (3.15). Moreover, Corollary 3.3 applies equally to the adaptive estimator (3.14).

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

11

Since the estimated ^In appears in the CLT, we have obtained a feasible limit theorem and (asymptotic) inference statements are immediate.
Some assumptions of Theorem 3.4 are tighter than for the oracle estimator. To some extent this is for the sake of clarity. Here, we have restricted Assumption 2.2(ii-) to the Besov-regular part. A generalisation of the pilot estimator to martingales seems feasible, but is non-standard and might require additional conditions. We have also proposed a rather concrete choice of h and r, less is used in the proof, see e.g. (C.3) below.
The lower bound for  in terms of the sample-size ratio nmax/nmin is due to bounding norms of (estimated) information-type matrices separately. For  = 1 (bounded variation case) the restriction imposes nmax to be somewhat smaller than n4m/i3n. By the Sobolev embedding B11,  H for all  < 1/2 the restriction nmax = O(n1m+in) from Theorem 2.4 is clearly also satisfied in this case.
It is not clear whether a more elaborate analysis can avoid these restrictions. Still, to the best of our knowledge, a feasible CLT for asymptotically separating sample sizes has not been obtained before. The inclusion of possible jumps in  by measuring regularity only in the Besov scale is already in the scalar case a significant improvement over Reiß [19].

4. Semiparametric efficiency.

4.1. Semiparametric Cram´er-Rao bound. We shall derive an efficiency bound for the following basic case of observation model (E1):

dYt

=

Xt

dt

+

1 n

dWt,

Zt
Xt = (s)1/2dBs,
0

t  [0, 1], (4.1)

where

(t) = 0(t) + H(t), 0(t)1/2 = O(t) (t)O(t).

(4.2)

We assume 0(t) and H(t) to be known symmetric matrices, O(t) orthogonal matrices, (t) = diag(1(t), . . . , d(t)) diagonal and consider   [-1, 1] as unknown parameter. Furthermore, we require Assumption 2.2(iii-) for all
. Finally, we impose throughout this section the regularity assumption that
the matrix functions O(t), H(t), (t) are continuously differentiable. The key idea is to transform the observation of dYt in such a manner that
the white noise part remains invariant in law while for the central parameter
(t) = 0(t) the process X is transformed to a process with independent coordinates and constant volatility. It turns out that this can only be achieved

12 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

at the cost of an additional drift in the signal. The construction first ro-
tates the observations via O(t), which diagonalises 0(t), and then applies a coordinate-wise time-transformation, corrected by a multiplication term to ensure L2-isometry such that the white noise remains law-invariant.
We introduce the coordinate-wise time changes by

ri(t)

=

Rt 0

i

(s)ds

R1 0

i

(s)ds

and

(Trg)(t)

:=

(g1(r1(t)), . . . , gd(rd(t)))

for g = (g1, . . . , gd) : R  Rd. Moreover, we set
Z1
¯ := (s)ds, R (t) := ¯-1(t) = diag(r1(t), . . . , rd(t)).
0
Lemma 4.1. By transforming dY¯ = Tr-1M(R )-1/2OdY , the observation model (4.1), (4.2) is equivalent to observing

dY¯ (t) = S(t) dt + 1 dW¯ (t) n

with

(4.3)

S(t)

=


Tr-1 (R

Z
)-1

·
((R

)-

1 2

O)

Z
(s)X(s) ds+

·
(R

(s))-

1 2

O(s)

dX


(s) (t)

00

for t  [0, 1]. At  = 0 the observation dY¯ (t) reduces to



Zt 0

Tr-1((R

)-1((R

)-1/2O)

X )(s)

ds

+


¯ B¯ (t)

dt

+

1 dW¯ n

(t).

(4.4)

Here W¯ and B¯ are Brownian motions obtained from W and B, respectively, via rotation and time shift, as defined in (D.1) below.

If we may forget in (4.4) the first term, which is a drift term with respect to the martingale part ¯B¯(t), then the central observation is indeed a constant
volatility model in white noise. The lemma is proved in Appendix D.1. Let us introduce the multiplication operator MAg := Ag and the inte-
gration operator

Z1

Zt

Ig(t) = - g(s) ds and its adjoint Ig(t) = - g(s) ds.

t0

The covariance operator Cn, on L2([0, 1], Rd) obtained from observing the differential in (4.3) is then given by

Cn, = TrM(R )1/2OIM0+HIMO (R )1/2 Tr + n-1 Id .

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

13

The covariance operator Qn, when omitting the drift part is given by

Qn, = Qn,0 + ITrMM TrI with M (t) := ((R )-1/2OHO (R )-1/2)(t)

where for  = 0 the one-dimensional Brownian motion covariance operator CBM appears

Qn,0 = diag(¯iiCBM + n-1 Id)1id, CBM = II.

We set C 0 = (Cn, - Cn,0)/ and Q 0 = (Qn, - Qn,0)/. Standard Fisher information calculations for the finite-dimensional Gaus-
sian scale model, e.g. [18, Chapter 6.6], transfer one-to-one to the infinitedimensional case of observing N(0, Qn,) and yield as Fisher information for the parameter  at  = 0 the value

InQ

=

1 2

Q-n,10/2Q 0Q-n,10/2

2 HS

,

because Q-n,10/2Qn,Qn-,10/2 is differentiable at  = 0 in Hilbert-Schmidt norm. In Appendix D.2 we show by Hilbert-Schmidt calculus, the Feldman-Hajek
Theorem and the Girsanov Theorem that the models with and without drift
do not separate:

Lemma 4.2. We have

lim sup
n

Q-n,10/2Q 0Qn-,10/2 - Cn-,10/2C 0Cn-,10/2

HS < .

Lemma 4.2 implies that the drift only contributes the negligible order O(1) = O( n) to the Fisher information. By identifying the hardest parametric subproblem for observations N(0, Qn,) we thus establish in Appendix D.3 a semiparametric Cram´er-Rao bound for estimating any linear functional of the covolatility matrix. Further classical asymptotic statements like the local asymptotic minimax theorem would require the LAN-property of the parametric subproblem.

Theorem 4.3. For a continuous matrix-valued function A : [0, 1]  Rd×d consider the estimation of

Z1

Z 1 Xd

 := A(t), (t) HS dt =

Aij(t)ij(t) dt  R.

0 0 i,j=1

(4.5)

Then a hardest parametric subproblem in model (4.1), (4.2) is obtained for the perturbation of 0 by
H(t) = (0(A + A )01/2 + 01/2(A + A )0)(t).

14 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

There any estimator ^n of , which is asymptotically unbiased in the sense

d d

(E

[^n]

-

)



0,

satisfies

as

n





Var=0(^n)



(2 +O(1)) n

Z1 0

(001/2+01/20)Zvec(A), Zvec(A)

(t)dt.

4.2. Discussion. The Cram´er-Rao bound of Theorem 4.3 coincides with
the asymptotic variance obtained in Corollary 3.3 in the case H(t) = Ed. For different, but constant in time noise levels H, we can apply a rescaling argument and replace in the lower bound model (t) by H-1(t)H-1 and
A(t) by HA(t)H. This gives the more general Cram´er-Rao bound

Var=0(^n)



(2 +O(1)) n

Z1 0

(0 01/,H2 +01,/H2 0)Zvec(A), Zvec(A)

(t)dt

with 10/,H2 = H(H-10H-1)1/2H, as in Corollary 3.3. If H(t) depends on t, rescaling generates another drift term, but if it varies smoothly in t, we

expect to obtain again a lower bound that matches the asymptotic variance

of our estimator. Let us discuss the efficient asymptotic variance AVAR

further, concentrating on the homogeneous case H(t)

The

efficient

asymptotic

variance

for

estimating

R1 0

= Ed. pp(t)

dt

is

Z 1

 Z1

AVAR

pp(t) dt = 8 pp(t)(1/2(t))pp dt.

00

For

the

asymptotic

variance

of

estimating

R1 0

pq (t)

dt

we

obtain

Z 1



2 (1/2)ppqq + (1/2)qqpp + 2(1/2)pqpq (t) dt.

0

Let us calculate specific examples. First, in the case d = 1,  = 2 this

simplifies to

Z 1

 Z1

AVAR 2(t) dt = 8 3(t) dt,

00

which agrees with the efficiency bound in Reiß [19]. For p = q in the inde-

pendent case  = diag(p2)1pd we find

Z 1

 Z1

AVAR

pq(t) dt = 2 (p2q + pq2)(t) dt.

00

In the case d = 2 with spot volatilities 12(t) = 22(t) = 2(t) and general correlation (t), i.e. 12(t) = (12)(t), we obtain

Z 1

 Z1

È

È

AVAR

12(t) dt = 4 3(t) 1 + (t) + 1 - (t) dt,

00

Z 1

 Z1

AVAR

12(t) dt = 2 3(t)((1 + (t))3/2 + (1 - (t))3/2) dt.

00

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

15

Fig 1. Asymptotic variances of LMM for volatility 12 (left) and covolatility 12 (right) plotted against correlation  and noise level 2 (constant in time).

With time-constant parameters these bounds decay for 12 (resp. grow for 12) in || from 83 (resp. 43) at  = 0 to 4 23 at || = 1 for both cases.
All the preceding examples can be worked out for different noise levels in H, let us just highlight the bound for AVAR(R01 pq(t)dt):

Z
2

1 (1H/2)ppqq

+ (H1/2)qqpp

+

2(H1/2)pq

pq


(t)

dt,

0

which matches the asymptotic variances obtained in Corollary 3.3. In general all noise levels enter for a fixed entry (p, q) via the matrix root (H-1H-1)1/2 which only in the case of a diagonal covolatility matrix  = diag(p2)p decouples as diag(p-1p)p and where the bound simplifies to

p=q:

Z
2

1 ppq2 + qqp2(t) dt;

p=q:

Z
8

1 pp3(t) dt.

00

Figure 1 illustrates the general dependence of the asymptotic variance on
the noise level via H in the case d = 2. The two volatilities are 1 = 2 = 1, the covolatility is 12 =  (constant in time) and the first noise level is 1 = 1. The left plot shows the asymptotic variance of the estimator of 12 as a function of  and 2. We see the significant gain of using observations from the other process for larger values of . If the noise level 2 for the second process is small, then the asymptotic variance can even approach zero. The
plot on the right shows the same dependence for estimating the covolatility
12. For comparable size of 2 and 1 the asymptotic variance increases in  which is explained by the fact that also the value to be estimated
increases. For small values of 2, however, the efficiency gain by exploiting the correlation prevails.

16 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

 For larger dimension d the variance can even be of order O(1/ d); in the

concrete case where all volatilities and noise levels equal 1, the asymptotic

variance for estimating 12 can be reduced from 8 (using only observations from the first component or if  is diagonal) down to 8/ d (in case of perfect

correlation).

We can also investigate the estimation of the entire quadratic covariation

matrix

R1 0

(t)

dt

and

measure

its

loss

with

respect

to

the

squared

d

×

d-

Hilbert-Schmidt norm. Summing up the variances for each entry, we obtain

the asymptotic risk

4

Z 1



trace(1/2) trace() + trace(3/2) (t) dt.

n0

This can be compared with the corresponding Hilbert-Schmidt norm error

1 n

(trace()2

+

trace(2))

for

the

empirical

covariance

matrix

in

an

i.i.d.

Gaussian N(0, )-setting.

For nonlinear functionals the Cram´er-Rao bound is obtained by lin-

earisation. Consider the prominent example of estimating power varia-

tions

of

the

form

R1 0

(pq

)/2

(t)dt

for

some



>

0

(

=

4

yields

the

so-

called quarticity). Linearisation of the perturbation yields pq/2 = (0)pq/2 +

 2

(0)pq/2-1

Hpq

+

O()

provided

(0)pq

>

0.

We

thus

consider

A(t)

=

 2

(0(t))pq/2-1((p,q),(p

,q

))p

,q

and obtain the lower bound

2 2

Z 1(0)pq-2(10/2)pp(0)qq
0

+

(01/2 )qq (0 )pp

+

2(01/2

)pq

(0

)pq


(t)

dt.

For p = q this reduces to 22 R01((01/2)pp(0)pp-1)(t)dt, which is independent of 0 only if  = 1/2 and 0 is diagonal. In that case asymptotic equivalence with a homoskedastic Gaussian shift in terms of the mean function (t)1/4
holds, derived by independence of coordinates from Reiß [19], but for non-
diagonal 0 a variance-stabilising transform or even an equivalence result is not apparent.

5. Implementation and numerical results.

5.1. Discrete-time estimator. The construction to transfer discrete-time to continuous-time observations in the proof of Theorem 2.4 paves the way to the discrete approximation of the local spectral statistics (3.2). Using the interpolated process and integration by parts yields

Z
jk(t)dY (l)(t)

Xnl Z t(l)
-
=1 t(l-) 1

jk

(t)

Y(l) t(l)

- -

Y(-l)1 t(l-) 1

dt.

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

17

Fig 2. Variances of estimators of 12 (left) and 12 (right) in time-constant scenario (n = 30, 000).

Hence, for discrete-time observations from (E0) we use the local spectral statistics

Sjk

=


jh-1

Xnl


Y(l)

=1

-

Y(-l)1


jk



t(l-) 1

+ 2

t(l) 
1ld

.

(5.1)

The noise terms in (3.5) translate from E1 to E0 via substituting

n-l 1

Rk(hk+1)h(Fl

(s))-1

ds

by

P


:

kht(l)

(k+1)h

(t(l)

-

t(l-) 1)2.

The

discrete

sum

times h-1 can be understood as a block-wise quadratic variation of time in

the spirit of Zhang et al. [20]. The bias is discretised analogously.

For the adaptive estimator we are in need of local estimates of nlFl ,  and estimators for l2, 1  l  d. It is well known how to estimate noise variances with faster nl-rates, see e.g. Zhang et al. [20]. Local observation
densities can be estimated with block-wise quadratic variation of time as above, which then yield estimators H^nk,hl of Hn,l around time kh. Uniformly consistent estimators for (t), t  [0, 1], are feasible, e. g. averaging spectral

statistics for j = 1, . . . , J over a set Kt of K adjacent blocks containing t:

Ò(t) = K-1

X

J -1

XJ


Sj k Sj k

-

2j2h-2

diag((H^ nk,hl

)2l


)

,

kKt

j=1

(5.2)

We refer to Bibinger and Reiß [4] for details on the non-parametric pilot estimator with J = 1.

5.2. Simulations. We examine the finite-sample properties of the LMM for the case d = 2 in two scenarios. First, we compare the finite-sample vari-

18 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

ance with the asymptotic variances from Sections 3 and 4, i. e. for a parametric setup with 12 = 22 = 0.1, 1 = 2 = 1 and constant correlation . We simulate n1 = n2 = 30, 000 synchronous observations on [0, 1]. For estimating 12 and 12 = , Figure 2 displays the rescaled Monte-Carlo variance based on 20, 000 replications of the oracle and adaptive LMM (LMMor and LMMad), as well as the adaptive spectral estimator (SPECad) by Bibinger and Reiß [4], which relies on the same spectral approach, but uses only scalar
weighting instead of the full information matrix approach.
In practice the pilot estimator from (5.2) for J not too large performed well. As configuration we use h-1 = 10, J = 30 and K = 8, which turned
out to be an accurate choice, but the estimators are reasonably robust to alternative input choices. For the LMM of 12, we observe the already familiar variance reduction effect associated with a growing signal correlation , while
the simulation-based variances of both LMMor and LMMad are close to their theoretical asymptotic counterpart (Theor). The results for 12 underline the precision gains compared to SPECad with univariate weights when  increases.
Next, we consider a complex and realistic stochastic volatility setting that
relies on an extension of the widely-used Heston model, as e. g. employed by
A¨it-Sahalia et al. [1], accounting for both leverage effects and an intraday
seasonality of volatility. The signal process for l = 1, 2 evolves as

dXt(l) = l(t) l(t) dZt(l),

dl2(t)

=

l


µl

-


l2(t) dt

+

l

l(t)

dVt(l),

where Zt(l) and Vt(l) are standard Brownian motions with dZt(1)dZt(2) =  dt

aR0n1d2ld(Zt)t(ld)dtV=t(m1).

= l,m l dt. l(t) is a The unit time interval

non-stochastic can represent

seasonal factor with one trading day, e.g.

6.5 hours or 23,400 seconds at NYSE.

We initialise the variance process l2(t) by sampling from its stationary distribution  2 l µl/l2, l2/(2l) and vary the value of the instantaneous

signal which

correlation under the

, while setting (µl, l, l, l) = stationary distribution, implies

(E1, R60,10.32l ,(t-)0.l23()t,)ldt=

1, 2, = 1.

The seasonal factor l(t) is specified in terms of intraday volatility functions

estimated for S&P 500 equity data by the procedure in Anderson and Boller-

slev [2]. 1(t) and 2(t) are based on cross-sectional averages of the 50 most

and 50 least liquid stocks, respectively, which yields a pronounced L-shape

in both cases and mutually

(see Figure 3). We independent with

add l =

noise processes that 0.1(ER01 4l (t) l4(t)

are 

i.i.d. N

0, l2

dt )1/4, computed

under the stationary distribution of l2(t). Finally, asynchronicity effects are introduced by drawing observation times t(il), 1  i  nl, l = 1, 2, from two

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

19

FR0i1g213(.t)No12n(t-)stdotch(raisgthict)

volatility seasonality factors (left) in stochastic volatility scenario.

and

RMSE

for

estimators

of

independent Poisson processes with intensities 1 = 1 and 2 = 2/3 such

that, on average, n1 = 23, 400 and n2 = 15, 600.

As a representative example, Figure 3 depicts the root mean-squared er-

rors (RMSEs) based on 40, 000 replications of the following estimators of

R1 0

21(t)

12(t)

dt:

the

oracle

and

adaptive

LMM

using

h-1

=

20,

J

=

15

and

K = 8, the quasi-maximum likelihood (QML) estimator by A¨it-Sahalia et.

al. [1] as well as an oracle version of the widely-used multivariate realised

kernel (MRKor) by Barndorff-Nielsen et al. [3]. For the latter, we employ the

average univariate mean-squared error optimal bandwidth based on the true

value

of

R1 0

4l (t) l4(t) dt,

l

=

1, 2.

Further,

we

include

the

theoretical

vari-

ance from the asymptotic theory (Theor), which is computed as the variance

(3.12) averaged across all replications.

Three major results emerge. First, the LMM offers considerable precision

gains when compared to both benchmarks. Second, a rising instantaneous

signal correlation  is associated with a declining RMSE of the LMM, which

is due to the decreasing variance, and thus confirms the findings from Section

3 in a realistic setting. Finally, the adaptive LMM closely tracks its oracle

counterpart.

In summary, the simulation results show that the estimator has promising

properties even in settings which are more general than those assumed in

(E1), allowing, for instance, for random observation times, stochastic intra-

day volatility as well as leverage effects. Even if the latter effects are not yet

covered by our theory, the proposed estimator seems to be quite robust to

deviations from the idealised setting.

20 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

APPENDIX A: FROM DISCRETE TO CONTINUOUS EXPERIMENTS

Proof of Theorem 2.4. To establish Le Cam equivalence, we give a constructive proof to transfer observations in E0 to the continuous-time model E1 and the other way round. We bound the Le Cam distance by estimates for the squared Hellinger distance between Gaussian measures and refer to Section A.1 in [19] for information on Hellinger distances between Gaussian measures and bounds with the Hilbert-Schmidt norm. The crucial difference here is that linear interpolation is carried out for non-synchronous irregular observation schemes. Consider the linear B-splines or hat functions

bi,n(t)

=

1[

i-1 n

,

i+1 n

]

(t)

min


1

+

n


t

-

i n

,

1

-

n


t

-

i  n

.

Define bli(t) := bi,nl(Fl(t)), 1  i  nl, 1  l  d, which are warped spline functions satisfying bil1(t(i2l)) = i1,i2. A centered Gaussian process Y^ is de-
rived from linearly interpolating each component of Y :

Y^t(l)

=

Xnl Yi(l)bil(t)
i=1

=

Xnl i=1

Xt((ill))

bli

(t)

+

Xnl i(l)bli(t) .
i=1

(A.1)

The covariance matrix function E Y^tY^s  of the interpolated process Y^ is determined by

E

hi
Y^t(l)Y^s(r)

=

Xnl

Xnr

alr (ti(l)



t(r))bli(t)br

(s)

+

l,r l2

Xnl

bli(t)bli(s)

i=1 =1

i=1

Zt

with A(t) = (alr(t))l,r=1,...,d = (s) ds .
0

For any g = (g(1), . . . , g(d))  L2([0, 1], Rd) we have


E

g, Y^


2

=

h
E

Xd

g(v), Y^ (v)

2 i

=1

= Xd Xnl Xnr alr(t(il)  t(r)) g(l), bli
l,r=1 i=1 =1

g(r), br

Xd Xnl
+

g(l), bil

2l2.

l=1 i=1

The sum of the addends induced by the observation noise in diagonal terms is bounded from above by

Xd l=1

l2 nl


g(l)

È
/ Fl

2

L2

=

Xd


g

(l)

Hn,l

2

L2

l=1

,

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

21

since

by

virtue

of

0



P
i

bi,n



1,

R

bi,n

=

1/n

and

Jensen's

inequality:

Xnl i=1

g(l), bil

2



1 nl

Xnl i=1

Z1
((g(l)
0



Fl-1)

·

(Fl-1)

)2bi,nl



1 nl

Z 1
(g(l)
0

 Fl-1) · (Fl-1)

2

=

1 nl

Z1 0

(g(l))2 Fl

.

On the other hand, we have

E[ g, diag(Hn,l)l dW

Xd
]=

g(l)Hn,l

2 L2

l=1

for a d-dimensional standard Brownian motion W . Consequently, a process Y¯ with continuous-time white noise and the same signal part as Y^ can be
obtained by adding uninformative noise. Introduce the process

dY¯

=



Xnl i=1

Xti(l)


bil(t) 1ld

dt

+

diag(Hn,l(t))1ld

dWt

,

(A.2)

and its associated covariance operator C¯ : L2  L2, given by

C¯g(t) =  Xd Xnl Xnr alr(t(il)  t(r))
r=1 i=1 =1

g(r), br





+
1ld

Hn,l(t)2g(l)(t) 1ld

.

In to

foabcste,rivtaitsiopnosssoibf le(Ato.2t)rabnysfaedrdoibnsgerNvat0io, Cn¯s-froC^m-onuorisoer,igwihnearleexC^pe:rLim2 ent

E0 L2

is the covariance operator of Y^ . Now, consider the covariance operator

Z 1 Z tu





Cg(t) =

A(s) ds g(u) du +

l2


g(l)(t)

,

00

nlFl (t)

1ld

associated with the continuous-time experiment E1. We can bound C-1/2 on L2([0, 1], Rd) from below (by partial ordering of
operators) by a simple matrix multiplication operator:

C-1/2  Mdiag(Hn,l(t))l .

Denote the Hilbert-Schmidt norm by · HS. The asymptotic equivalence of observing Y¯ and Y in E1 is ensured by the Hellinger distance bound

H2


L

Y¯



,

L (Y


)



2

C -1/2

C¯

-


C

C -1/2

2 HS

22 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

Z 1Z 1
2

Xd Xd Hn,l(t)-2Hn,r(t)-2

0 0 l=1 r=1



Xnl

Xnr

alr (t(i l)



t(r))bil(t)br (s)

-

alr (t



!
2
s)

dt

ds

i=1 =1

Z 1Z 1
=2
00

Xd Xd nlnr l=1 r=1 l2r2



Xnl

Xnr

alr (t(i l)



t(r))bi,nl (u)b,nr (z)

-

alr (Fl-1 (u)



!

Fr-1

2
(z))

du

dz

i=1 =1

=


O R4

Xd

Xd


l-2r-2nlnrnm-2in-2 .

l=1 r=1

The estimate for the L2-distance between the function (t, s)  A(Fl-1(t)  Fr-1(s)), (l, r)  {1, . . . , d}2, and its coordinate-wise linear interpolation by O(n-m1in- n-m3in/2) relies on a standard approximation result on a rectangular grid of maximal width (nmin)-1 based on the fact that this function lies

in the Sobolev class H1+([0, 1]2) with corresponding norm bounded by

2R4. This follows immediately by the product rule from A =   H

and (Fl-1)  C, together with an L2-error bound at the skewed diagonal {(t, s) : Fl(t) = Fr(s)}.

Next, we explicitly show that E1 is at least as informative as E0. To this

end

we

discretise

in

each

component

on

the

intervals

Ii,l

=

[

i nl

-

1 2nl

,

i nl

+

1 2nl

]



[0,

1]

for

i

=

0,

.

.

.

,

nl.

Define


Yi

(l)

=

1 |Ii,l|

Z
Fl-1(Ii,l) Fl (t) dYt(l)

=

1 |Ii,l|

Z
Fl-1(Ii,l) Xt(l)Fl (t) dt

+ (il)

=

1 |Ii,l|

Z
Ii,l

XF(l-) 1(u) du + (il) ,

(A.3)

for 0  i  nl with i. i. d. random variables:

i(l)

=

1 |Ii,l|

Z
Fl-1(Ii,l) l(Fl /nl)1/2 dWt(l)

i.i. d.

N


0,

l2

.

(A.4)

The covariances are calculated as


E (Yi


)(l)(Y )(r) =

1 |Ii,l ||I,r |

Z
Ii,l

Z

alr
I,r

(Fl-1

(u)



Fr-1

(u

)) dudu

+ l,ri, l2.

We obtain for the squared Hellinger distance between the laws of observation





H2 L (Yi(l))l=1,...,d;i=0,...,nl

,

L


((Yi

)(l)

)l=1,...,d;i=0,...,nl



EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

23



Xd l,r=1

l-2r-2

Xnl i=0

Xnr =0

1 |Ii,l ||I,r |

Z
Ii,l

Z
I,r

alr (Fl-1 (u)



Fr-1(u

))

-

alr (Fl-1 (i/nl



 /nr ))

dudu

2
.

Write AFlr(u, u ) = alr(Fl-1(u)  Fr-1(u )) and note AlFr  H1+([0, 1]2) due to A =   H and Fl-1, Fr-1  C. For (i, ) / C := {(0, 0), (0, nr), (nl, 0), (nl, nr)} the rectangle Ii,l × I,r is symmetric around (i/nl, /nr) such that
the integral in the preceding display equals ( denotes the gradient)

Z
Ii,l ×I,r

Z 1¬AFlr
0



i nl


+ u-



i nl

,

 nr


+ u

 

-

 nr

,

u

-

i nl

,

u

-

¶
 nr

¬  

¶

- AFlr

i nl

,

 nr

,

u

-

i nl

,

u

-

 nr

ddudu .

Using Jensen's inequality we thus obtain further the bound for the squared Hellinger distance:

Xd l,r=1

l-2r-2

Xnl i=0

Xnr =0

(nl  nr)-2 |Ii,l ||I,r |

Z
Ii,l ×I,r

Z1 0

AFlr(i/nl + (u - i/nl),

/nr + (u - /nr)) - AFlr(i/nl, /nr)1((i, ) / C) 2 d dudu



=

Xd l-2r-2
l,r=1

nlnr (nl  nr)2

O

R4(nl



nr)-2 

=

O

R4

Xd

nl

/l2

!2 
nm-2in-2

l=1

where the order estimate is due to AFlr H  R2 and a standard L2approximation result for Sobolev spaces, observing that for the four corner
rectangles in C the boundedness of the respective integrals only adds the total order 4nm-2in < nlnrnm-2in-2.

APPENDIX B: ASYMPTOTICS IN THE BLOCK-WISE CONSTANT EXPERIMENT
Proof of Theorem 3.2. As we have seen, the estimator is unbiased in E2. For the covariance structure we use the independence between blocks and frequencies and the commutativity with Z to infer


COVE2 I1n/2


LMM(onr)

=

I1n/2

h-X1 -1

h2

X

Wj k CO VE2


vec

 
Sj k Sj k

Wj k In1/2

k=0 j=1

= I1n/2 h-X1-1 h2Ik-1I1n/2Z = Z .
k=0

(B.1)

24 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

Since the local Fisher-type informations are strictly positive definite and thus
invertible by Assumption 2.2(iii), the multivariate CLT (3.10) for the oracle
estimator follows by applying a standard CLT for triangular schemes as
Theorem 4. 12 from [14]. The Lindeberg condition is implied by the stronger
Lyapunov condition which is easily verified here by bounding moments of
order 4. In Appendix C below we prove that in experiment E1 the estimator
LMM(onr) has an additional bias of order O(n-min/2) + OP (h) and a difference in the covariance of order O(hnm-in/2) + OP (h2) under our Assumption 2.2(ii-),(iii-), which by Slutsky's lemma yields an asymptotically negligible term compared to the best attainable rate (in any entry) nm-1a/x4, cf. Theorem 4.3.

Proof of Corollary 3.3. An important property of our oracle estimator is its equivariance with respect to invertible linear transformations Ak on each block k in the sense that for observed statistics S~jk := AkSjk  N(0, C~jk) under E2 we obtain (A- := (A )-1 for short)
Cjk = Ak-1C~jkA-k , Ijk = (AkAk) I~jk(AkAk), Ik = (AkAk) I~k(AkAk)

and hence

LMM(onr) = h-X1-1 h(Ak  Ak)-1I~k-1 X I~jk(Ak  Ak)vec(SjkSjk)
k=0 j0

=

h-X1 -1
(Ak




Ak)-1 hI~k-1

X


I~jkvec(S~jkS~jk) .

k=0 j0

For the covariance we use commutativity with Z and obtain likewise

COVE2 (LMM(onr)) = h-X1-1 h2(Ak  Ak)-1I~k-1(Ak  Ak)- Z.
k=0

(B.2)

We use this property to diagonalise the problem on each block. In terms
of the noise level matrix Hk := diag(Hlk,n)l=1,...,d, let Ok be an orthogonal matrix such that

kh = OkHk-1khHk-1Ok

(B.3)

is diagonal. Note that kh grows with n, but we drop the dependence on n in the notation for all matrices kh, Ok and Hk. Use Ak = OkHk-1 to obtain the spectral statistics (3.2) transformed:

S~jk

= OkHk-1Sjk



N


0,

C~j k 

independent

for

all

(j, k)

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

25

which yields a simple-structured diagonal covariance matrix:

C~jk = OkHk-1CjkHk-1Ok

=

kh

+

2j2 h2

Ed.

A key point is that the covariance structure (3.7) in Rd2×d2 is for independent components S~jk also diagonal, up to symmetry in the volatility matrix entries. Summing I~jk over j is explicitly solvable and gives for p, q = 1, . . . , d

(hI~k-1)p,q

=


h-1

X (C~j-k1



C~j-k1 )p,q -1

j=1

=


h-1

X (pkph

+

2j2h-2)-1(kqqh

+

 2 j 2 h-2 )-1 -1

j=1

È ÈÈ È

= =

2(kpphkqÈqh cokqtqhh2(+ÈhkpkqpqhhkpÈphkq)qh-(pkph)qkqh1-pk+phcOkpopht)he-(h2hkqkpqhph)-qkqh2+hhpk1-ph1(kqqhkpph-1


qkqh)-1/2 ,

using kh  (minl,t nlFl (t)l-2) nminEd, h2nmin   and coth(x) = 1 + O(e-2x) for x  . We thus obtain uniformly over k
 hI~k-1 = (2 + O(1))(kh  kh + kh  kh).
By formula (B.2) we infer in terms of (kHh)1/2 := Hk(Hk-1khHk-1)1/2Hk
COVE2 (LMM(onr)) = (2 + O(1)) h-X1-1 h(kh  (Hkh)1/2 + (Hkh)1/2  kh)Z.
k=0
The final step consists in combining nm1/i2nHn,l(t)  Hl(t) uniformly in t together with a Riemann sum approximation to conclude

lim
nmin 

n1m/i2n

CO

VE2

(LMMo(nr)

)

Z 1



= 2   (H(H-1H-1)1/2H) + (H(H-1H-1)1/2H)   (t) dt Z.

0

APPENDIX C: PROOFS FOR CONTINUOUS MODELS
C.1. Weight matrix estimates. We shall often need general norm bounds on the weight matrices Wjk.

26 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

Lemma C.1. The oracle weight matrices satisfy uniformly over (j, k) and the matrices kh with kh  + (kh)-1  1
Wjk h0-1(1 + j4/h04)-1.

Proof. From the proof of Corollary 3.3 we infer

Wjk W~ jk

= =

(HkOk  HkOk )W~ jk(Ok (2 + O(1))h-1(khC~j-k1)

Hk-1  (OkHk-1)  ( khC~j-k1) +

with  ( khC~j-k1)



(khC~j-k1).

We evaluate one factor in Wjk using

HkOk khC~j-k1OkHk-1
By A  B  A B are diagonal), we infer

= kh(kh+2j2h-2Hk2)-1

(1+j2h-2nm-2in)-1.

 and khC~j-k1 = (khC~j-k1)(kh)-1/2 (the matrices

Wjk

h-1(1 + j2h0-2)-2 HkOk (kh)-1/2OkHk-1 .

To evaluate the last norm, despite matrix multiplication is non-commutative, we note


Ok

(kh

)-

1 2

Ok Hk-1 

Ok

(kh)-

1 2

Ok Hk-1

=

Hk-1Ok

(kh )-1 Ok Hk-1

=

(kh)-1,

whence by polar decomposition |Ok (kh)-1/2OkHk-1| = (kh)-1/2 implies

Ok

(kh)-

1 2

Ok Hk-1

=

(kh

)-

1 2

1.

Together with Hk nm-1in/2 this yields

Wjk

h-1(1 + j2h-0 2)-2nm-1in/2,

which gives the result.

Moreover, for the adaptive estimator we have to control the dependence of the weight matrices Wjk = Wj(kh) on kh. We use the notion of matrix
differentiation as introduced in [9]: define the derivative dA/dB of a matrixvalued function A(B)  Ro×p with respect to B  Rq×r as the Rop×qr matrix
with row vectors (d/dBab)vec(A), 1  a  q, 1  b  r.

Lemma C.2. For the derivatives of the oracle weight matrices Wj(kh), assuming kh  + (kh)-1  1, we have uniformly over (j, k):

   

d dkh

Wj


(kh)

h0-1(1 + j4h0-4)-1 .

(C.1)

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

27

Proof. Since the notion of matrix derivatives relies on vectorisation, the identities vec(Ik-1Ijk) = (Ed2  Ik-1)vec(Ijk) = (Ijk  Ed2)vec(Ik-1) give rise
to the matrix differentiation product rule

d dkh Wjk

=

(Ijk



Ed2 )

dIk-1 dkh

+


Ed2



Ik-1

dIjk dkh

.

(C.2)

Applying the mixed product rule (A  B)(C  D) = (AC  BD) repeatedly, and the differentiation product rule and chain rule to Ijk = Cj-k1  Cj-k1, we obtain

d dCjk

Cj-k1



Cj-k1 =

-

Cj-k1



Cj-k1



Cj-k1



Cj-k1


(Cjk



Ed



Ed2 )





+(Ed2  Ed  Cjk) (Ed  Cd,d  Ed) (vec(Ed)  Ed2 ) + (Ed2  vec(Ed)) ,

with the so-called commutation matrix Cd,d = Z - Ed2. By orthogonality of the last factors in both addends, A  B = A B , and the mixed
product rule, we infer for the norm of the second addend in (C.2)

 
(Ed2



Ik-1

)

dIjk dkh

   



2

Ed



Cj-k1



Ik-1(Cj-k1



Cj-k1)



= 2 Wjk Cj-k1

Wjk .

By virtue of

(Ik-1



Ed2

)

dIk dkh

=

-(Ed2



Ik

)

dIk-1 dkh

,

it follows with the mixed product rule that

dIk-1/dkh = -(Ik-1  Ik-1)(dIk/dkh).

This yields for the norm of the first addend in (C.2)


(Ijk



Ed2

)

dIk-1 dkh

  

=


(Wjk



Ik-1)

dIk dkh

  

Wjk


(Ed2



Ik-1)

X

dIj k dkh

  

j

X



Wjk

Wj k

Wjk

j

since we can differentiate inside the sum by the absolute convergence of
P
j Wj k . This proves our claim by Lemma C.1.

28 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

C.2. Bias bound. Using the formula 1 - 2 sin2(x) = cos(2x) and It^o

isometry, the (d×d)-matrix of (negative) biases (in the signal) of the addends in (3.8) as an estimator of kh in experiment E1 is given by

Z (k+1)h

Bj,k := 2h-1

(t) cos(2jh-1(t - kh)) dt,

kh

which has the structure of a j-th Fourier cosine coefficient. We introduce

the corresponding weighting function in the time domain:

Gk(u) = 2 X Wjk cos(2ju)  Rd2×d2, u  [0, 1].
j=1

Parseval's identity then shows for the d2-dimensional block-wise bias vector of (3.8):

X
Wj k v ec(Bj,k )

=

Z

(k+1)h
h-1Gk(h-1(t - kh))vec((t)) dt.

j=1 kh

The vector of total biases of (3.8) is then the linear functional of :

h-X1-1 X
h Wjkvec(Bjk)

=

Z

1
Gh(t)vec((t)) dt

k=0 j=1

0

where for t  [kh, (k + 1)h))

Gh(t) = Gk(h-1(t - kh)) = 2 X Wjk cos(2jh-1t) .
j=1

For  in the Besov space B1,([0, 1]), 0 <   1, the L1-modulus of continuity satisfies L1([0,1])(, )   B1,, see e.g. [7, Section 3.2]. We have for   (0, 1) and s  [0, 1 - ]

Z 





0

vec((t

+



s))

cos(

2t 

)

dt

=

1 

Z 

0

Z 0

vec((t +

s) - (u

+ s)) du

 Z

cos(

2t 

)

dt



sup

0v

|vec((t + s) - (t + v + s))| dt  L1([s,s+])(, ).
0

This shows for the total bias in estimation of the volatility in X by the bound on Wjk in Lemma C.1

Z  

1
Gh(t)vec((t)) dt



h-X1-1 X
2

Wjk

L1([kh,(k+1)h])(, h/j)

0 k=0 j=1

X h0-1(1 + (h0/j)4)-1(h/j) (h/h0) = n-min/2.

j=1

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

29

We thus have a bias of order O(n-min/2). Remark that it is quite surprising that this bias bound is independent of h, which is also at the heart of the
quasi-maximum likelihood method [1].
If vec() is a (vector-valued) square-integrable martingale, then we use
that martingale differences are uncorrelated and write for the total bias
Z1 Z1
Gh(t)vec((t)) dt = Gh(t)vec((t) - ( h-1t h)) dt,
00
R
using Gk = 0. This expression is centred with covariance matrix

h-X1-1 Z

Gk(h-1(t - kh))E[vec((t) - (kh))vec((s) - (kh)) ]

k=0 [kh,(k+1)h]2

Gk(h-1(s - kh)) dtds.

The expected value in the display is smaller than (in matrix ordering)
E[vec(((k+1)h)-(kh))vec(((k+1)h)-(kh)) ]. Because of Gk  1 the covariance matrix (in any norm) is of order O(h2E[ (1) - (0) 2]) = O(h2).
If  = B + M is the sum of a function B in B1,([0, 1]) and a squareintegrable martingale M , then the preceding estimations apply for each summand and the total bias has maximal order O(n-min/2) + OP (h).

C.3. Variance for general continuous-time model. The covariance
for the estimator under model E1 can be calculated as under model E2, but
we lose independence between different frequencies j, j on the same block.
For that we use the formula for Gaussian random vectors A, B

COV(vec(AA ), vec(BB )) = COV(B, B)  COV(A, B) + COV(A, A)

COV(A, B) + COV(A, B)  COV(A, A) + COV(A, B)  COV(B, B) Z/4,

obtained by polarisation. This implies

COVE1 (LMM(onr)) - COVE2 (LMM(onr)) h-X1-1 h2 X Wj k Wjk(COVE1 (Sjk, Sjk)  COVE1 (Sjk, Sj k)) .
k=0 j,j =1

From Lemma C.1 and A  B  A B for matrices A, B we infer that

the series over j, j is bounded in order by

X

h-0 2(1 + j

/h0)-4(1

+

Z

j/h0)-2

 

1
( - ¯ h)(t)

j,j =1

0

jk(t)j k(t) jk L2 j k L2


dt

Z

+

 

1
diag(Hn2,l

- H2n,l,h)(t) jk(t)j


k(t) dt .

0

30 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

The identities 2 cos(a) cos(b) = cos(a + b) + cos(a - b), 2 sin(a) sin(b) =
cos(a - b) - cos(a + b) and the same bound as in Section C.2 imply for , (F1)-1, . . . , (Fd)-1  B1,([0, 1]) (note that even (Fl )-1  C([0, 1]))

Z 1

 

( - ¯ h)(t)

0

jk(t)j k(t) jk L2 j k L2


dt


h-1

h

+ h(1 - j,j ) 

j+j

|j - j |

×  B1,([kh,(k+1)h])

and similarly the bound


h-1

h

j+j

+

h(1 - j,j |j - j |

)  jj

h0-2

max
l

(Fl )-1

B1,([kh,(k+1)h])

for the norm over Hn2,l. Putting all estimates together gives

COVE1 (LMM(onr)) - COVE2 (LMMo(nr)) h X h0-2(1 + j /h0)-4(1 + j/h0)-2h(1 + |j - j |)-(1 + jj h-0 2).
j,j =1

By comparison with the double integral (in terms of x  j/h0, y  j /h0)
Z Z 
(1 + y)-4(1 + x)-2|x - y|-(1 + xy) dxdy 1
00
we conclude

COVE1 (LMMo(nr)) - COVE2 (LMM(onr)) hnm-in/2.
Arguing exactly as in Section C.2 for the case of  being a sum of a B1,function and an L2-martingale, the difference of covariances is in general of order O(hn-min/2) + OP (h2).

C.4. Proof of Theorem 3.4. Let us denote the rate of convergence of ^ by n = nm-in/(4+2). For later use we note the order bounds

n

=


O r1/2

h-0 1/2(nmin

/nmax)1/4


,

n

=


O h-0 1(nmin

/nmax


)1/2 .

(C.3)

First, we show that LMMo(nr) - LMM(and) = OP (nm-1a/x4)

(C.4)

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

31

which by Slutsky's Lemma implies the CLT with normalisation matrix In.
This in turn is already sufficient for obtaining the result of Corollary 3.3 for LMM(and). Let us start with proving that

Tnm

:=

  

r-X1 -1

(m+X1)r/h-1
h

X


Wj

(^ mr

)

-

Wj

(mr


)

Zj

k

  

=

OP (nm-1a/x4)

m=0 k=mr/h j=1

where the random variables

Zjk = vec SjkSjk - 2j2h-2 diag ((Hnk,hl)2)1ld - kh
are independent, EE2[Zjk] = 0, COVE2 (Zjk) = Ij-k1Z. We have

Tnm



r-X1 -1

h

X


Wj

(^ mr

)

-

Wj

(mr


)

(m+X1)r/h-1

Zjk


,

m=0 j=1

k=mr/h

(C.5)

since the weight matrices do not depend on k on the same block of the coarse grid. Using Lemma C.2 and that ^ -  L1 = OP (n), we obtain


Wj (^ mr)

-


Wj (mr)



max
k

  

dWj

(kh

)

 

dkh 

^ mr - mr

= OP h-0 1  h30j-4 r-1

^ - 


L1([mr,(m+1)r]) .

For the second factor in (C.5) we employ COVE2(Zjk) = 2 Cjk 2 such that Zjk = OP ( Cjk ). Consequently, (C.3) implies for Tnm the bound

r-X1-1 X
h

OP

(h-0 1



h30j-4)r-1

^ - 

L1([mr,(m+1)r])(r/h)1/2(1  j2h0-2)

m=0 j=1

= OP r-1/2h1/2n = OP (n-m1a/x4) .

The asymptotics (C.4) follow if we can ensure that the coarse grid approximations of the weights induce a negligible error, i.e. if also

r-X1-1 (m+X1)r/h-1 h2

X


Wj (kh )

-


Wj (mr) Zjk

=

OP (n-m1a/x4)

m=0 k=mr/h

j=1

holds. The term is centred and its covariance matrix is bounded in norm by

r-X1 -1

(m+X1)r/h-1

h2

X


Wj (kh)

-

Wj (mr)2

Ij-k1 .

m=0 k=mr/h

j=1

32 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

From Lemma C.2, Ij-k1 = 2 Cjk 2 1 + j4h0-4 and   B1,([0, 1]) we derive the upper bound


O

h-X1 -1

h2

X

r2h0-2(1

+


j4h-0 4)-1

=

On-m1i/n2r2

=

O(nm-1a/x2)

k=0 j=1

by the choice of r and  > 1/2.
Another application of Slutsky's Lemma yields the CLT with normalisation matrix ^In provided I1n/2^In-1/2  Ed2 in probability. The proof of Lemma C.2, more specifically the bound on the last term in (C.2), yields also

  

d dkh

Ij

(kh


)

h0-1(1 + j4h-0 4)-1.

P
This implies k,j

I^jk - Ijk

= OP (h-1n). Using A^-1 - A-1 = A-1(A^ -

A)A^-1 and Ik-1 h-0 1, we infer

^I-n 1 - In-1



h-X1 -1



h2

 

X

I^j k -1

-



X

Ijk

-1

  

=

OP


hn

h-0 2


.

k=0 j=1

j=1

The smallest eigenvalue of I-n 1 equals In -1 which has order at least nm-1a/x2. The global Lipschitz constant Ln of f (x) = x1/2 for x  In -1 is therefore of order n1m/a4x. The perturbation result from [16] for functional calculus therefore implies

I1n/2^I-n 1/2 - Ed  Ln In1/2 I-n 1 - ^I-n 1 = OP nm1/a2xhnh0-2 .

The order is (nmax/nmin)1/2h-0 1n and tends to zero by (C.3).

APPENDIX D: PROOF OF THE LOWER BOUND

D.1. Proof of Lemma 4.1. Since M(R )1/2Tr is an isometry on L2([0, 1]; Rd), we obtain directly for the adjoint Tr = Tr-1M(R )-1. We ob-
serve in a formal differential notation:

TrM(R )1/2OdY

= Tr-1M(R )-1/2O(Xdt +

1 n

dW

)

=

-Tr-1 I  (M((R

)-1/2 O)

X dt

+

M(R

)-1/2OdX )

+

1 n

dW¯

=

-I  Tr (M((R

)-1/2 O)

X dt

+

M(R

)-1/2OdX )

+

1 n

dW¯

.

Here, we use that TrM(R )1/2O is an L2-isometry and we introduce the independent Brownian motions W¯ , B¯ via the differentials

dW¯ = TrM(R )1/2O dW, dB¯ = TrM(R )1/2O dB

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

33

or alternatively (apply -I) via their coordinates i = 1, . . . , d as

W¯ i(u) = Xd Z ri-1(u) Rii(s)1/2Oij (s) dWj (s),
j=1 0

(D.1)

and B¯i(u) analogously.
The formal derivations are made rigorous by duality, that is testing stochastic differentials with deterministic L2-functions. We infer from the coordinate-wise definition of W¯ for f  L2([0, 1]; Rd) (e.g., check via indica-
tor functions f )

Z1 Z1
O(t) R (t)1/2(Trf )(t), dWt = f (u), dW¯ (u)
00

and equally for B¯. Now consider for functions g  L2([0, 1]; Rd) the real observations

Z1

Z1

O(t) R (t)1/2(Trg)(t), dYt = O(t) R (t)-1/2(TrIg) (t), dYt

00

Z1
= (O(t) R (t)-1/2(TrIg)) - (O(t) R (t)-1/2) (TrIg)(t), Xt dt

0

+ 1

Z1
O(t)

n0

R (t)1/2(Trg)(t), dWt

Z1
= -(O(t) R (t)-1/2) (TrIg)(t), Xt dt

0

Z1
-
0

O(t)

R (t)-1/2(TrIg)(t), dXt

+

1 n

Z1 0

g(u), dW¯ u

Z1
= -(O(t) R (t)-1/2) (TrIg)(t), Xt dt

0

Z1
- (t)1/2O(t)
0

R (t)-1/2(TrIg)(t), dBt

+

1 n

Z1 0

g(u), dW¯ u

.

For  = 0 we use (R )-1/2(R )-1/2 = ¯ and evaluate the first two terms of the last display as

Z1

Z1

-(O(t) R (t)-1/2) (TrIg)(t), Xt dt - (Tr-1¯ (TrIg))(u), dB¯u .

00

As

¯

is

constant

in

time,

the

second

term

is

equal

to

-

R1 0

Ig, ¯dB¯

and the

formal derivations above are confirmed.

34 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

D.2. Proof of Lemma 4.2. In a first step note that for general operators A, B we have

AA - BB

2 HS

=

1 2

(2A + B - A)(A - B) + (2A + B - A)(A - B)

HS

2 A

A-B

HS +

A-B

2 HS

.

Hence, it suffices to show

Q-n,10/2Q1n/,12

1 and Qn-,10/2Qn1/,12 - Cn-,10/2Cn1/,12 HS 1.

A further reduction is achieved by splitting terms to obtain

Q-n,10/2Q1n/,12 - Cn-,10/2Cn1/,12 HS  Id -Cn-,10/2Qn1/,02 HS Qn-,10/2Qn1/,12 + Cn-,10/2Qn1/,02 Qn-,10/2Qn1/,12 Id -Qn-,11/2Cn1/,12 HS .

Owing to Cn-,10/2Q1n/,02  1 + Id -Cn-,10/2Qn1/,02 HS it remains to show

Qn-,10/2Q1n/,12

1, Id -Cn-,10/2Qn1/,02 HS 1 and Id -Qn-,10/2Cn1/,02 HS 1.

Finally, we can use Qn,1 - Qn,0 = Q,1 - Q,0, Qn,1  Q,1 in operator order (and similarly for Cn,) as well as |a - 1|  |a2 - 1| for a  0 implying
A - Id HS  AA - Id HS for positive operators A. We are thus left with
proving that the following three quantities are uniformly bounded

Qn-,10/2Q1n/,12 , C-1,0/2(Q,0-C,0)C-1,0/2 HS , Q-1,/12(C1/,21-Q1/,21)Q-1,1/2 HS .

By the Feldman-Hajek Theorem for Gaussian measures, see e.g. [8], the latter two quantities are finite iff the Gaussian laws N(0, C,) and N(0, Q,), are equivalent for   {0, 1}. Using again differential notation, these are the laws of

ZC := TrM(R )1/2OX, ZQ := -ITrM(R )-1/2OdX

where dX = 1/2dB for the  at hand. Both processes are images in C([0, 1], Rd) under the linear (and thus measurable) map Tr-1 = TrMR of the respective processes

Z~C := M(R )-1/2OX, Z~Q := -IM(R )-1/2OdX.

By the product rule we see

Z~C (t) = -I{M(R )-1/2OdX + M((R )-1/2O) X}(t)
Zt
= Z~Q(t) + ((R )-1/2O) (s)X(s) ds.
0

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

35

Hence, Z~C equals the Brownian martingale Z~Q plus an adapted linear drift
in X. By Girsanov's theorem, noting that all deterministic quantities are continuous and bounded away from zero, the laws of Z~C and Z~Q are equiv-
alent, e.g. use Thm. 3.5.1. together with Cor. 3.5.16 in [15]. Hence, so are the laws of their images ZC and ZQ, as required.
Let us finally consider Qn-,10/2Qn1/,12. Its squared norm equals

sup
f L2

Qn,1f, f Qn,0f, f

= sup
f L2

M(R )-1/2O1O (R )-1/2 TrI f, TrI f

If

2

+

1 n

f

2

 (1 +

M

) max
i=1,...,d

(ri-1)

.

+

1 n

f

2

This uniform bound is finite under our regularity assumptions.

D.3. Proof of Theorem 4.3. Without loss of generality we may as-
sume that A(t) is symmetric for all t because (t) is symmetric. Owing to Zvec(A) = 2vec(A) and (0  01/2)vec(A) = vec(01/2A0), we thus have to show in terms of the Hilbert-Schmidt scalar product

Var=0(^n)



(8 +O(1)) n

Z1 0

(0  10/2

+ 01/2

 0)A)(t), A(t)

H S dt.

Since CBM is a positive operator on L2([0, 1]; R), we can define the bounded self-adjoint operator

n

=

I (2 CBM

+

1 n

Id)-1 I 

=

(2

Id

+

1 n

CB-M1

)-1.

CBM is Hilbert-Schmidt and so is n. We identify its kernel n : [0, 1]2  R (or Green function) as

n(t, s)

=

2

 n

 sinh( n(1

cosh( n)

-

|s

-

t|))

+

 sinh( n(t

+

s

-


1)) .

This can be formally derived from the properties CB-M1 = -D2 on its domain, n in the domain (i.e. n(0, s) = 0, (n) (1, s) = 0) and n(t, s) = ns(t).
Alternatively use the eigenvalue-eigenfunction decomposition of CBM and apply functional calculus. The main observation is that n has all the properties of a smoothing kernel, which for n   concentrates on the diagonal

{t = s}, where it approximates the uniform law. This is best seen by the

approximation for large n

n(t, s)

 n



 exp(- n|s

-

t|)

+

sgn(t

+

s

-

1)

 exp( n(|t

+

s

-

1|

-


1)) ,

2

36 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

observing |t + s - 1| - 1  |s - t| + |2t - 1| - 1 such that the second

exponential asymptotically only contributes at the corners (0, 0) and (1, 1)

of the diagonal.

We shall see, however, that for the Hilbert-Schmidt norm evaluation we

face (n)2 as the on the diagonal,

operator kernel, but needs to be

which also behaves

rescaled by

n

2 L2

l=ike(1a+smOo(1o)t)hinng/k(4ern3e)l.

Consequently, in terms of n = diag(n¯ii)i=1,...,d and its kernel n(t, s) =

diag(n¯ii(t, s))i=1,...,d, the Fisher information evaluates as

InQ

=

1 2

Qn-,10/2Q 0Qn-,10/2

2 HS

=

1 2

trace(TrIQn-,10ITrMM TrIQ-n,10ITrMM )

=

1 2

trace((TrnTr)MM (TrnTr)MM )

1Z 1Z 1

= 20

traceRd×d(n(r(t), r(s))M (s)n(r(s), r(t))M (t)) dtds.
0

We

now

use

R1 0

M

(s)an

e-|t-s|an

ds

=

2M (t)(1

+

O(1))

uniformly

over

t



[bn, 1 - bn] whenever an  , anbn   and M (t) is continuously differ-

entiable. Together with the asymptotic behaviour of n we obtain

Z 1 n¯ii (ri(t), ri(s))Mij(s)n¯jj (rj(s), rj(t)) ds

0

=

(1

+

n O(1)) 4¯ ii¯ jj

Z1 0

exp(-n(¯ iiri(t) 

+

¯ jjrj(t))|t

-

s|)Mij (s)

ds

n

=

(1 + 

O(1))

2¯ ii¯ jj

(¯ ii

ri(t)

+

¯ jj

rj

(t))

Mij

(t)

=

nMij(t)(1 + O(1)) 2¯ ii¯ jj(i(t) + j(t))

with O(1) uniformly in n and t  [n-p, 1 - n-p] for any p  (0, 1/2) to infer

InQ

=

 n 4

Xd

Z
¯ -ii1¯ -jj1

1
(i(t)

i,j=1

0

+

j(t))-1Mij(t)2(1 + O(1))dt

 n(1

+

O(1))

Z

1

Xd

=

(OHO )i2j (t) dt.

4 0 i,j=1 i(i + j )j

Asymptotically for n   neglecting terms of smaller order, this bound is obtained by the worst parametric perturbation H(t) = 0A01/2 +

EFFICIENT QUADRATIC COVARIATION MATRIX ESTIMATION

37

R0110/P2Ai,jA0,ijw(th)iBchij

we evaluate (t)dt as

using

duality

with

respect

to

the

scalar

product

sup
H:H(t)=H(t)

(R01

Pd i,j

=1

Aij

(t)Hij

(t)

dt)2

 n 4

R1 0

Pd k,l=1

(OHO )2kl k (k +l )l

(t)

dt

=

(R01

Pd i,j=1

Aij

(t)Hij

(t)

dt)2

 n 4

R1 0

Pd k,l=1

(OHO )k2l k (k +l )l

(t)

dt

=

4 n

Z1 0

A(t), (A1/2 + 1/2A)(t)

HS

dt.

Finally, remark that the Cram´er-Rao inequality, e.g. [18, Thm. 2.5.10], is applicable since (N(0, Qn,)) forms an exponential family in (Q-n,1), which is differentiable at  = 0, and thus the models (N(0, Qn,)) as well as
(N(0, Cn,)) are regular.

REFERENCES
[1] A¨it-Sahalia, Y. and Fan, J. and Xiu, D. (2010). High frequency covariance estimates with noisy and asynchronous financial data. J. Amer. Statist. Assoc. 105 1504-1517. MR2796567
[2] Andersen, T. and Bollerslev, T. (1997). Intraday perdiodicity and volatility persistence in financial markets. J. Empir. Financ. 4 115-158.
[3] Barndorff-Nielsen, O. E. and Hansen, P. R. and Lunde, A. and Shephard, N. (2011). Multivariate realised kernels: consistent positive semi-definite estimators of the covariation of equity prices with noise and non-synchronous trading. J. Econometrics 162(2) 149­169.
[4] Bibinger, M. and Reiß, M. (2013). Spectral estimation of covolatility from noisy observations using local weights. Scand. J. Stat., to appear.
[5] Christensen, K. and Podolskij, M. and Vetter, M. (2012). On covariation estimation for multivariate continuous Ito^ semimartingales with noise in non-synchronous observation schemes. Preprint.
[6] Ciesielski, Z., Kerkyacharian, G. and Roynette, B. (1993). Quelques espaces fonctionnels associ´es `a des processus gaussiens, Studia Math. 107(2), 171­203. MR1244574
[7] Cohen, A. (2003). Numerical Analysis of Wavelet Methods, Studies in Math. Appl. 32, Elsevier. MR1990555
[8] Da Prato, G. and Zabczyk, J. (2008). Stochastic Equations in Infinite Dimensions, corr. ed., Cambridge University Press. MR1207136
[9] Fackler, P. L. (2005). Notes on matrix calculus, Lecture Notes, North Carolina State University. http://www4.ncsu.edu/~pfackler/MatCalc.pdf
[10] Gloter, A. and Jacod, J. (2001). Diffusions with measurement errors 1 and 2. ESAIM, Probab. Stat. 5 225­242. MR1875672 & 1875673
[11] Hansen, L. P. (1982). Large sample properties of generalized methods of moment estimation. Econometrica 50(4) 1029­1054.
[12] Hayashi, T. and Yoshida, N. (2011). Nonsynchronous covariation process and limit theorems. Stochastic Processes Appl. 121(10) 2416-2454. MR2822782
[13] Isserlis, L. (1918). On a formula for the product-moment coefficient of any order of a normal frequency distribution in any number of variables. Biometrika 12 134­139.

38 M. BIBINGER, N. HAUTSCH, P. MALEC & M. REISS

[14] Kallenberg, O. (2002), Foundations of Modern Probability, 2nd ed., Springer. MR1876169
[15] Karatzas, I. and Shreve, S. E. (1991). Brownian Motion and Stochastic Calculus, 2nd ed., Springer Graduate Texts 113. MR1121940
[16] Kittaneh, F. (1985). On Lipschitz functions of normal operators. Proc. Amer. Math. Soc. 94, 416­418. MR0787884
[17] Le Cam, L. and Yang, L. G. (2000). Asymptotics in Statistics. Some basic concepts, 2nd ed., Springer. MR1784901
[18] Lehmann, E. L. and Casella, G. (1998). Theory of Point Estimation, 2nd ed., Springer. MR1639875
[19] Reiß, M. (2011). Asymptotic equivalence for inference on the volatility from noisy observations. Ann. Stat. 2 772­802. MR2816338
[20] Zhang, L. and Mykland, P. A. and A¨it-Sahalia, Y. (2005). A Tale of Two Time Scales: Determining Integrated Volatility With Noisy High-Frequency Data. J. Amer. Statist. Assoc. 100 1394­1411. MR2236450

Institut fu¨r Mathematik Humboldt-Universita¨t zu Berlin Unter den Linden 6 10099 Berlin, Germany E-mail: bibinger@math.hu-berlin.de
School of Business and Economics Humboldt-Universita¨t zu Berlin Spandauer Str. 1 10178 Berlin, Germany E-mail: malecpet@hu-berlin.de

School of Business and Economics Humboldt-Universita¨t zu Berlin Spandauer Str. 1 10178 Berlin, Germany E-mail: nikolaus.hautsch@wiwi.hu-berlin.de
Institut fu¨r Mathematik Humboldt-Universita¨t zu Berlin Unter den Linden 6 10099 Berlin, Germany E-mail: mreiss@math.hu-berlin.de

SFB 649 Discussion Paper Series 2013
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001
002 003 004 005 006
007
008 009
010 011 012
013 014
015 016
017

"Functional Data Analysis of Generalized Quantile Regressions" by
Mengmeng Guo, Lhan Zhou, Jianhua Z. Huang and Wolfgang Karl Härdle, January 2013. "Statistical properties and stability of ratings in a subset of US firms" by
Alexander B. Matthies, January 2013. "Empirical Research on Corporate Credit-Ratings: A Literature Review" by Alexander B. Matthies, January 2013. "Preference for Randomization: Empirical and Experimental Evidence" by
Nadja Dwenger, Dorothea Kübler and Georg Weizsäcker, January 2013. "Pricing Rainfall Derivatives at the CME" by Brenda López Cabrera, Martin Odening and Matthias Ritter, January 2013. "Inference for Multi-Dimensional High-Frequency Data: Equivalence of
Methods, Central Limit Theorems, and an Application to Conditional Independence Testing" by Markus Bibinger and Per A. Mykland, January 2013.
"Crossing Network versus Dealer Market: Unique Equilibrium in the Allocation of Order Flow" by Jutta Dönges, Frank Heinemann and Tijmen R. Daniëls, January 2013. "Forecasting systemic impact in financial networks" by Nikolaus Hautsch,
Julia Schaumburg and Melanie Schienle, January 2013. "`I'll do it by myself as I knew it all along': On the failure of hindsightbiased principals to delegate optimally" by David Danz, Frank Hüber,
Dorothea Kübler, Lydia Mechtenberg and Julia Schmid, January 2013. "Composite Quantile Regression for the Single-Index Model" by Yan Fan, Wolfgang Karl Härdle, Weining Wang and Lixing Zhu, February 2013. "The Real Consequences of Financial Stress" by Stefan Mittnik and Willi
Semmler, February 2013. "Are There Bubbles in the Sterling-dollar Exchange Rate? New Evidence from Sequential ADF Tests" by Timo Bettendorf and Wenjuan Chen, February 2013.
"A Transfer Mechanism for a Monetary Union" by Philipp Engler and Simon Voigts, March 2013. "Do High-Frequency Data Improve High-Dimensional Portfolio
Allocations?" by Nikolaus Hautsch, Lada M. Kyj and Peter Malec, March 2013. "Cyclical Variation in Labor Hours and Productivity Using the ATUS" by Michael C. Burda, Daniel S. Hamermesh and Jay Stewart, March 2013.
"Quantitative forward guidance and the predictability of monetary policy ­ A wavelet based jump detection approach ­" by Lars Winkelmann, April 2013.
"Estimating the Quadratic Covariation Matrix from Noisy Observations: Local Method of Moments and Efficiency" by Markus Bibinger, Nikolaus Hautsch, Peter Malec and Markus Reiss, April 2013.

SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

