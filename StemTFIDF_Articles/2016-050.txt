BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2016-050
Network Quantile Autoregression
Xuening Zhu* Weining Wang *² Hangsheng Wang* Wolfgang K. Härdle*²
* Peking University, People's Republic of China; *² Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Network Quantile Autoregression
Xuening Zhu2, Weining Wang1,3, Hangsheng Wang2, and Wolfgang Karl Ha¨rdle1,4
1 Humboldt-Universit¨at zu Berlin, C.A.S.E.-Center of Applied Statistics and Economics, Unter den Linden 6, 10099 Berlin, Germany;
2 Guanghua School of Mangement, Peking University, Beijing, China;
3 Department of Economics, City University London;
4 Research fellow in Sim Kee Boon Institute for Financial Economics, Singapore Management University, 90 Stamford Road, 6th Level, School of Economics, Singapore
178903.
Abstract It is a challenging task to understand the complex dependency structures in an ultra-high dimensional network, especially when one concentrates on the tail dependency. To tackle this problem, we consider a network quantile autoregression model (NQAR) to characterize the dynamic quantile behavior in a complex system. In particular, we relate responses to its connected nodes and node specific characteristics in a quantile autoregression process. A minimum contrast estimation approach for the NQAR model is introduced, and the asymptotic properties are studied. Finally, we demonstrate the usage of our model by investigating the financial contagions in the Chinese stock market accounting for shared ownership of companies. KEY WORDS: Social Network; Quantile Regression; Autoregression; Systemic Risk; Financial Contagion; Shared Ownership.
This research is supported in part by National Natural Science Foundation of China (NSFC, 11131002, 11271032) and Center for Statistical Science at Peking University. This research is partly supported by DFG (WA3868/1-1) " quantile method for complexed financail system". We acknowledge the support from German Research Foundation DFG on SFB649.
1

1. INTRODUCTION
Quantile regression is an increasingly popular tool for modern statistical analysis. Instead of studying the conditional mean function of the response variable, quantile regression is concerned with estimating the conditional quantile function. It has been applied to a wide range of applications, such as labor economics (Koenker and Hallock, 2001; Fitzenberger et al., 2013), financial risk management (Gaglianone et al., 2012; Ha¨rdle et al., 2016), and environmental statistics (Sankarasubramanian and Lall, 2003; Wang et al., 2013). Particularly, the linear quantile model has been studied by a seminal work by Koenker and Bassett (1978), and the asymptotic theory has been developed by Portnoy (1991, 1997). See Koenker (2005) for a comprehensive summary of the methods and applications.
Following the well development of quantile method in the existing literature, the quantile regression in time series is of particular interest. An early stream of research such as Hallin et al. (1999), Hasan and Koenker (1997) deal with linear quantile autoregression model, which focus on independent identically distributed (iid) innovations in a relative restrictive location shift model. As another approach, Engle and Manganelli (2004) propose a set of autoregressive forms (referred to as CaViaR model) for valueat-risk (VaR), which all require to estimate VaR at first step and plug in a prescribed time series model. The framework is easy to apply but is quite difficult to directly infer the underlying process. As an alternative, Koenker and Xiao (2006) develop a quantile autoregressive method to model the conditional quantile function, which allows to study the asymptotic properties of the underlying process and does not assume an iid underlying process. However, it can only be employed in the univariate case. There have been a few efforts to develop multivariate quantile time series model. White et al. (2010) propose a generalization of the CaViaR method to a vector autoregression
2

model for VaR. Recently, Barun´ik and Kley (2015) consider the quantile cross-spectral analysis of quantile vector autoregression processes. However, to our best knowledge, the existing methods are not flexible to directly extend to high dimensional data, as the number of parameters becomes large. This makes the estimation not only suboptimal, but also with high computational complexity. Therefore, a tractable quantile autoregression model for analyzing high dimensional data needs to be developed.
In the meanwhile, the rapid development of modern computer science and technology has allowed us to approach large amount of data with network structure. On one hand, it poses challenges on analyzing the dynamic processes with high dimensions. On the other hand, this brings us a unique opportunity to develop network models with network information naturally embedded. To take advantage of this extremely valuable information (i.e., network structure), we established a network quantile autoregression model (NQAR), which allows us to both make inference on the underlying processes and handle high dimensional modeling issues.
In the existing literature, great efforts have been taken to incorporate the network information into the modeling framework. For instance, Sewell and Chen (2015) take advantage of the network information to study the dynamic social behavior of students in a Dutch class by a latent space model. The community detection and extraction methods are studied by Zhao et al. (2011), Amini et al. (2013), and Sewell et al. (2016) using the block network structure. Accordingly, the corresponding consistency properties are established (Bickel and Chen, 2009; Zhao et al., 2012). Recently, the autoregression models in large-scale social networks receive great attention, where the estimation and computation issues are intensively discussed (Zhang and Chen, 2013; Zhou et al., 2015; Huang and Wang, 2016; Zhu et al., 2016). For other related statistical methods and applications, see Carrington et al. (2005), Newman (2010), and Kolaczyk
3

and Cs´ardi (2014) for a comprehensive summary.
However, to our best knowledge, none of these aforementioned works have taken account the network information into a quantile regression framework. In this work, we provide an innovative network quantile autoregression model to better estimate and predict conditional quantiles in complex network systems (e.g. a network of stocks in a stock market). The model is based on a univariate quantile autoregression model. It is assumed the conditional quantile function of the response variable (e.g., volatility of stock returns) is related to several terms. The included terms are nodal specific variables (e.g., firm specific variables), the lagged response of the same node (e.g., volatility of the same stock in the previous time point), and the lagged responses of the other connected nodes (e.g., volatility of the related stocks in the previous time points). In our application, the connection between nodes are decided by the network information from the data, i.e., if two stocks share the same shareholder, then they are defined as connected.
Our paper contributes to the literature in three aspects. Firstly, we provide a network quantile model that characterizes the dynamic quantile behavior for high dimensional processes. Secondly, we propose a model framework to incorporate valuable network information from the data. Thirdly, the asymptotic theories are derived for both the underlying processes and estimated parameters.
The rest of the paper is organized as follows. Section 2 introduces the network quantile autoregression model, where the stationary results are established. Section 3 proposes a novel impulse analysis framework for the network quantile autoregression model. The parameter estimation method is given in Section 4, where the asymptotic properties are given. Extensive numerical studies and a real data analysis for stocks in Chinese financial markets are conducted in Section 5. Lastly, a brief conclusion is discussed in
4

Section 6. All technical details are delegated to the appendix.

2. NETWORK QUANTILE AUTOREGRESSION

2.1. Model and Notations

Let Uit (1  i  N , 1  t  T ) be a sequence of iid random variables, which follows the standard uniform distribution. Assume that a q-dimensional random nodal covariate vector Zi  Rq is collected for each node. To record the network relationship, we define A = (aij)  RN×N as the adjacency matrix, where aij = 1 if there is an edge from i to j, otherwise aij = 0. Following the convention, the nodes are assumed to be not self-related (i.e., aii = 0). Motivated by the univariate autoregression quantile model (Koenker and Xiao, 2006), we consider the network quantile autoregression (NQAR) model as

qN

Yit = 0(Uit) +

Zill(Uit) + 1(Uit)ni-1

aijYi(t-1) + 2(Uit)Yi(t-1) d=ef g(Uit), (2.1)

l=1 j=1

where js (0  j  2) and ls (1  l  q) are unknown coefficient functions from [0, 1] to R1, and ni = j=i aij is the out-degree for the ith node.
Denote QY ( |X) as the conditional quantile function of Y given X. By assuming the right side of (2.1) is monotonically increasing in Uit, we can write the conditional quantile function of Yit given (Zi, Yt-1) as:
qN
QYit( |Zi, Yt-1) = 0( ) + Zill( ) + 1( )ni-1 aijYj(t-1) + 2( )Yi(t-1).
l=1 j=1

It is worth mentioning that the varying coefficients are functions of  . Therefore, not only the location of the conditional density of Yit is determined by  , but also the
5

shape of the conditional density is allowed to be  -dependent. Specifically, 0( ) +

q l=1

Zill( )

reflects

the

nodal

impact

with

respect

to

node

i,

where

0( )

is

referred

to as the baseline function. The covariates Zil refer to node-specific variables, which are

invariant over t. For example, it can be stock related features (e.g., size, leverage ratio).

It is assumed the nodal covariates Zis are independent of the Uits. Next, the second

term ni-1

N j=1

aij

Yj(t-1)

characterizes

the

network

impact

from

the

connected

nodes

(e.g., stocks with same shareholders). The corresponding coefficient function 1( ) is

then referred to as the network function. Lastly, Yi(t-1) captures the impact from the response of the same node in the previous time point. Accordingly, the coefficient

function 2( ) is then referred to as the momentum function. To obtain more insights of the NQAR model, we discuss the model under the following three scenarios.

Scenario 1. (Tail Behavior) The NQAR model can capture the asymmetric effect between the responses at different quantile levels. In particular, it is of great interest to understand the tail dependency of the responses. For instance, to model the conditional VaR of the stock return, one could define Yit to be the return for the ith stock and fix  = 0.05 for analysis. In such a situation, an asymmetric pattern gives indication on whether the financial institutions tend to have closer connections in the lower tail (e.g. in the financial crisis) than at the other levels (e.g., median level and upper tail). Scenario 2. (Robust Estimation) In many occasions, the estimation can be seriously distorted by outliers in the dataset (Abello et al., 2013; Li et al., 2015). To obtain reliable results, robust estimation can be conducted. Compared to vector autoregression for the mean case (Lu¨tkepohl, 2005), the NQAR is insensitive extreme values. In particular, robust median estimation can be obtained by setting  = 0.5.

Scenario 3. (Constant Coefficient Function) Consider the case that the coefficient

6

functions exhibit constant forms, i.e. j( ) = j (j = 1, 2) and l( ) = l (l = 1, · · · , q). In such a situation, the conditional distribution of Yit is  -invariant. We further let 0(u) = -1(u), where (·) is the distribution function of the standard normal distribution. Then the NQAR model degenerates to the network autoregression (NAR) model in the mean case, see Zhu et al. (2016).

For convenience, define Yt = (Y1t, · · · , YNt)  RN , Z = (Z1, · · · , ZN )  RN×q.

Let B0t = 0(Uit) + l Zill(Uit), 1  i  N  RN , B1t = diag{1(Uit), 1  i 

N } RN×N , B2t = diag{2(Uit), 1  i  N }  RN×N . One easily verifies that  =

E(B0t) = c01N  RN , where c0 = b0 + cZ, b0 =

1 0

0(u)du

and

cZ

= E(Z1)

r with

r=

1 0

l(u)du,

1



l



q

 Rq. Without loss of generality, we set E(Z1) = 0. Then

the NQAR model (2.1) can be re-written in vector form as

Yt =  + GtYt-1 + Vt,

(2.2)

where Gt = B1tW +B2t  RN×N , W = (wij) = (n-i 1aij)  RN×N is the row-normalized

adjacency matrix, and Vt = B0t -   RN is iid with mean 0 and covariance V =

V2 IN  RN×N with V2 = b20 +E{

(U1t)Z (U1t)}, b20 =

1 0

02(u)du

-

{

1 0

0(u)du}2,

and Z = Cov(Z1)  Rq×q.

2.2. Covariance Stationarity

Given the NQAR model (2.2), it is then of great interest to check stationarity of Yt. A process {Yt}-+ is covariance stationary if (a) E(Yt) = µY for a constant vector µY  RN ; (b) Cov(Yt, Yt-h) = E{(Yt - µy)(Yt-h - µy) } = (h) with (h)  RN×N only related to h. For convenience, let b1 = E{1(Uit)}, b2 = E{2(Uit)}, b1 = {E 12(Uit)}1/2, b2 = {E 22(Uit)}1/2, G = E(Gt), and G = E(Gt  Gt). Then we have the following theorem.
7

THEOREM 2.1. Assume b1 + b2 < 1 and E |Vit| < C for some positive constant C. (a) There exists a unique covariance stationary solution of the NQAR model (2.2) with finite first order moment as


Yt = l + lVt-l,
l=0 l=0

where l =

l k=1

Gt-k+1

for

l



1

and

0

=

IN .

(b) Denote Y = (0). The stationary mean is µY = c1-1c01N and

(2.3)

vec(Y ) = (M1 - c-1 2c02)1N2 + c1-1c0(I - G)-1vec(bv) + (I - G)-1vec(V ), (2.4)

where c1 = (1 - b1 - b2)-1, M1 = c1-1c20(1 + b1 + b2)(I - G)-1, bv = bvIN , and bv = E[{1(Uit) + 2(Uit)}Vit]. Moreover, we have (h) = GhY for any integer h > 0, and (h) = Y (G )-h for h < 0.
The proof of Theorem 2.1 is given in Appendix A.1. By Theorem 2.1, unique covariance stationary solution (2.3) of the NQAR model is given. To obtain more insights of Theorem 2.1, we would like to add two remarks.
Remark 1. It is straightforward to verify b1 = (b21 + b21)1/2, where b21 = Var{1(Uit)}. Similarly one can define b22 = Var{2(Uit)}. Therefore the stationarity assumption in Theorem 2.1 essentially sets constraint on the expectation and variance of the network and momentum functions (i.e., 1(·) and 2(·)). It is noteworthy that the assumption does not require |1( )| + |2( )| to be strictly less than one for every  . Instead, it imposes an upper bound on average levels, which allows for some "explosive" cases in a specific quantile (i.e., |1( )| + |2( )| > 1 for some  ). Particularly, if the network and momentum functions take constant forms, i.e., 1( ) = b1 and 2( ) = b2 for some constants b1 and b2, then the stationary assumption reduces to |b1| + |b2| < 1, which
8

corroborates to the stationary condition in the mean case (Zhu et al., 2016).
Remark 2. Let us look at the stationary mean µy and covariance Y . First, note that µY = c-1 1c01N , is the same for all the nodes, and unrelated to the network structure. However, the situation for the covariance Y is more complicated. The network effect though tends to be very small (Chen et al., 2013; Zhu et al., 2016). This motivates us to assume that b1 = O(1), and then approximate the Y by the leading terms. For convenience, define b12 = E{1(Uit)2(Uit)}, b01 = E{1(Uit)Vit}, and b02 = E{2(Uit)Vit}. Then we have,

Var(Yit)  cb1c20 + (1 - b22)-1{2(1 - b2)-2{(1 - b2)bv + b1b02}c0 + V2 },

(2.5)

Cov(Yi1t, Yi2t)  cb2c02 + (1 - b22)-2{2(1 - b2)-1b02c0 + V2 }b1b2(wi1i2 + wi2i1), (2.6)

where cb1 = [(1-b22)-1{1-b22 +2b1 +2(1-b22)-1(1-b22)b12}-(1-b2)-1(1-b2 +2b1)](1- b2)-2 and cb2 = (1 - b2)-2(1 - b22)-2(1 - b22 + 2b1 + 2b1b2) - (1 - b2)-3(1 - b2 + 2b1). The verifications of (2.5) and (2.6) are given in Appendix A.2. It is worth noting that the variance of Yit is mainly determined by the momentum function 2(·) and the baseline function 0(·). Particularly, larger b2 will result in higher variance of Yit. Next, the covariance between Yi1t and Yi2t is not only related to 2(·), but is also related to the network function 1(·). Nodes have a higher correlation with each other if b1 is larger. Note that wi1i2 + wi2i1 = ni-11ai1i2 + n-i21ai2i1. Therefore, the correlation between node i1 and i2 is higher if (a) they connect to each other in the network (i.e., ai1i2 = ai2i1 = 1) and (b) they both have small out-degrees (i.e., small ni1 and ni2).
2.3. Asymptotic Stationary Distribution

Given the established covariance stationarity, it is then natural to derive the asymptotic stationary distribution. However, it is not straightforward to derive the asymptotic sta-
9

tionary distribution of Yt. Instead, we focus on the long run average of Yt. Accordingly,

let YT = T -1

T
t=1 Yt

be

the

average

of

the

responses

during

T

periods,

we

then

have

the following theorem.

THEOREM 2.2. Assume c < 1 and E(|Vit|4) < M , where c = 1 4 + 2 4 with j 4 = E{j(Uit)4}1/4 (j = 1, 2), and M is finite positive constant. Then the covariance stationary solution in (2.3) follows the central limit theorem as

 T (YT

-

µY

)

-L

N(0, Y

)

(2.7)

as T  .
The proof of Theorem 2.2 is given in Step 3 Appendix A.1. By (2.7), it can be seen 
that the asymptotic covariance of T (YT - µY ) is equivalent to Cov(Yt) = Y .
3. IMPULSE ANALYSIS

3.1. Measurements of Impulse Effect

In this section, we aim to investigate how a node in the network will react to an

exogenous shock imposed on the other nodes, which is referred to as an impulse analysis.

Particularly, consider a stimulus  = (1, · · · , N )  RN imposed on Vt, and shock it

to Vt + . Then, the response for the ith node at time point t (i.e., Yit) will grow to

Yit + i. Following the NQAR model (2.2), the response at time point (t + l), l  1

(i.e., Yt+l) is increased by

l-1
IEt,t+l = Gt+l-k,
k=0

(3.1)

where IEt,t+l is referred to as the impulse effect from time t to t + l. For instance, if

 = (1, 0, · · · , 0) , then the IEt,t+l is the first column of

l-1 k=0

Gt+l-k

.

It

can

be

noted

10

that the impulse effect is only related to the network and momentum functions, i.e., 1(·) and 2(·).

However, by this definition of (3.1), IEt,t+l is a random vector so that it cannot be directly calculated. To fix the problem, the following three measurements of impulse effects are considered.

1. Average Impulse Effect. The average impulse effect (AIE) is defined as the expectation of IEt,t+l as E(IEt,t+l) = Gl = (b1W + b2IN )l. Specifically, the AIE is only related to the average network and momentum effect b1 and b2. Furthermore, it is noteworthy that the AIE is no longer related to t but only depends on the time difference l. It can be further derived|1 E(IEt,t+l)|  N (b1 + b2)lC, where C = maxi |i|. Therefore, it can be concluded that the total network AIE will decrease to 0 as l  , if the stationary condition in Theorem 2.1 is satisfied.

2. Interval Impulse Effect. Although the AIE can characterize the impulse effect on an average level, it is hard to capture the asymmetric effect for different quantiles. To this end, we define the interval impulse effect (IIE) from t to t+l within the interval [1, 2], (0  1 < 2  1) as

l-1

IIEl,12 = E

Gt+l-k Uim  [1, 2], 1  i  N, t + 1  m  t + l

k=0

= (c1,12 W + c2,12 IN )l,

where c1,12 =

2 1

1(u)du

and

c2,12

=

2 1

2(u)du.

As one can see, the amount

of IIE is determined by the integration of 1(u) and 2(u) within the interval [1, 2].

Note that the interval [1, 2] can be any interested regions. For example, to measure

the asymmetric effects of the upper tail, median level, and the lower tail, one can split

(0, 1) equally into three intervals (i.e., [0, 1/3), [1/3, 2/3), [2/3, 1]) and compare the

11

IIEs for different intervals respectively.

3. Impulse Effect Intensity. By the IIE, the asymmetric effects can be readily quantified. However, due to the unknown function form of 1 and 2, the integration can still be hard to compute. On the other hand, note the fact that the IIE can be defined in arbitrary intervals in [0, 1]. Motivated by this, we consider a sufficiently small interval [,  + ], and define the impulse effect intensity (IEI) at  as

l-1

IEIl,

=

lim -lE
0

Gt+l-k Uim  [,  + ], 1  i  N, t + 1  m  t + l

k=0

= 1( )W + 2( )IN l,

where 1(u) and 2(u) are assumed to be continuous at  . By this definition, IEIl, could reflect impulse impact at the  th quantile. Moreover, the quantity IEIl, is easy to compute once the estimates of 1( ) and 2( ) are obtained.
Given the three types of impulse effect measurement, the cross-sectional impulse analysis can be conducted. Assume that one unit stimulus is imposed on the ith node, a cross-sectional impulse analysis is about analyzing its impact on the other nodes. This kind of analysis is interesting in a network of banks. It delivers an importance message on the systemic risk spillover of an institution. Take the IEI as an example and assume  = (i) with only i = 1 and i = 0 (for all i = i). The IEI from node i to j can be defined by the jth element of IEIl, , which is then denoted as IEI(l,i,j). Equivalently, IEI(l,i,j) is equal to the (j, i)th element of the matrix 1( )W + 2( )IN l. Then, if IEI(l,i,j) is sufficiently large and decays slowly as l  , then the jth risk factor can be seriously affected by the fluctuations of the ith risk factor. Lastly, it is worth mentioning that the cross-sectional impulse analysis can also be conducted by using the other two impulse effect measurements, which can be defined in the similar way with details

12

ignored here.

3.2. Influential Node Analysis

It is worth noting that the impulse effect can be interpreted as the instantaneous

impact from t to t + l. Moreover, we define the total network average impulse effect

(TNAIE) as the summation of AIE over all nodes and all horizons l as TNAIE() =

 l=0

1

E(IEt,t+l) =

 l=0

1

Gl

=

1

(IN

- G)-1.

It should be noted that the

definition of TNAIE can also be extended using the other two impulse effect measures

(IIE and IEI). For the sake of their similarities, we skip the details and use the AIE-

defined TNAIE in this section.

Write TNAIE() as TNAIE() =

N i=1

ii,

where

i

is

the

ith

element

of

the

vector

 = (I -G )-11. Consider that one unit stimulus is imposed on the node i, that i = 1

and j = 0 for all j = i. Then we have TNAIE() = i as a result. This reflects the amount that the whole network will react to the unit perturbation of the node i. For

convenience, we refer to i as the influential power of node i.

As one can see, the definition of i is straightforward but it can be hard to compute. This is because the calculation of  involves the inverse of a high dimensional matrix (I - G ). Following the idea of Remark 2 of Theorem 2.1, we approximate the ivalues by first order Taylor expansion: i  1/(1-b2)+(1-b2)-2b1 j nj-1aji. Assume b1 > 0, then the influential power of node i is mainly determined by the quantity j nj-1aji, which is referred to as the weighted degree of the node i. Consequently, nodes with larger weighted degrees tend to be followed by a large amount of nodes (i.e., j aji). Moreover, at the same time, the followers should have less out-degrees (i.e., small njs).

4. PARAMETER ESTIMATION
13

In this section, we provide an estimation method to the NQAR model (2.1). The

asymptotic properties are also established. Write ( ) to be a (q + 3)-dimensional

parameter vector as ( ) = [0( ),  ( ), 1( ), 2( )]  Rp+3. Define g,it( ) =

Xit ( ), and Vit = Yit - g,i(t-1)( ) where Xit = (1, Zi , n-i 1

N j=1

aij

Yjt,

Yit

)

 Rq+3.

Then the parameter vector ( ) can be estimated by

NT

( ) = arg min

 {yit - g,i(t-1)( )},

i=1 t=1

(4.1)

where  (u) = u{ - 1(u < 0)} is the check function for quantile regression.

Let the conditional density function of Yit given Ft-1 be fi(t-1)(·). To facilitate the study

of the asymptotic properties of our estimation, we define 0 = (N T )-1

N i=1

T t=1

XitXit and 1 = (N T )-1

N i=1

T t=1

fit

Xit ( )

XitXit

for any given 



(0, 1).

Specifically, fit Xit ( ) can be estimated by f^it Xit ^( ) = {Xit (^(l)-^(l-1))}-1(l-

l-1), where   [l-1, l] and {l} is an appropriately chosen sequence. Next, to prove

the asymptotic properties of the estimated parameters, the following assumptions are

required.

(C1) (Moment Assumption) Assume c < 1, where c is defined in Theorem 2.2. Further, assume that Zis are independent and identically distributed random vectors, with mean 0 and covariance z  Rp×p. Furthermore, its fourth order moment is finite. The same assumption is also needed for Vit across both 1  i  N and 0  t  T . Moreover, we need {Zi} and {Uit} to be mutually independent.
(C2) (Network Structure)
(C2.1) (Connectivity) Let the set of all the nodes {1, · · · , N } be the state space of a Markov chain, with the transition probability given by W . It is assumed the Markov chain is irreducible and aperiodic. In addition, define  =
14

(i)  RN to be the stationary distribution vector of the Markov chain

(i.e., i  0,

i  = 1, and W  = ). It is assumed that

N i=1

i2



0

as

N  .

(C2.2) (Sparsity) Assume |1(W )| = O(log N ), where W  is defined to be a symmetric matrix as W  = W + W .

(C3) (Convergence) Assume 1 p 1 as N  , where 1 = (1,ij)  RN×N is a positive definite matrix. In addition, assume the following limits exist. They are, respectively, 1 = limN N -1tr(Y ), 2 = limN N -1tr(W Y ), 3 = limN N -1tr(W Y W ), and 4 = limN N -1tr{(I - G)-1}, 5 = limN N -1tr{W (I - G)-1}. Here j (1  j  5) are fixed constants.

(C4) (Density) There exists positive constants 0 < c1 < c2 <  such that c1  fit(x)  c2 for all 1  i  N, 1  t  T with x  R.

(C5) (Monotonicity) It is assumed ( ) Xit (1  i  N, 1  t  T ) is monotone increasing functions with respect to  .

We now comment on the conditions. Condition (C1) are standard conditions on the noise term Vits, nodal covariates Zis and (Uit)s for the parameter consistency results. This condition can be relaxed to allow for the weak dependence or mixing case over time. Condition (C2) is set for the network structure. Specifically, condition (C2.1) ensures a structure on the network connectivity, as it will result to all nodes in the network connecting each other within a finite number of steps, which corresponds to the empirical phenomenon named as "six degrees of separation". (C2.2) assures that the network structure is sufficiently sparse,i.e. the divergence rate of 1(W ) is slower than log(N ). Condition (C3) is set on the design matrices. These are conditions needed to apply law of large number to the design matrices, which lead to a proper limit to
15

the asymptotic covariance matrix. Importantly, it restricts the dependency between nodal covariates such that the convergence is ensured. Finally, condition (C4) requires the density function of the response distribution to be bounded from up and below.
Remark 3. The monotonicity assumption is imposed by condition (C5) to ensure the validness of the quantile regression. As it is mentioned in Koenker and Xiao (2006), the monotonicity of Xit ( ) is likely to be violated in some regions of the covariate space. We therefore refer to Koenker and Xiao (2006) and Fan and Fan (2010) as a discussion and solutions on the relevant issue.
The following theorem provides the consistency of the parameter estimation.

THEOREM 4.1. Under condition (C1)­(C5), the following representation holds uniformly over   B (i.e., B is a compact set in (0, 1)),

NT

( ) - ( ) = (N T )-1( )-1

Xit (Vit ) + rNT ( ),

i=1 t=1

(4.2)

where ( ) = -1 101-1, Vit = Yit - g,i(t-1)( ), and the remainder term satisfies supB|rNT ( )| = Op (N T )-1/2 . This leads to the consistency result that ( ) p ( ) as min{N, T }   uniformly for   B.

The proof of Theorem 4.1 is given in Appendix B. With the consistency of the parameter, we then present the asymptotic distribution of the estimated parameter.

THEOREM 4.2. Under condition (C1)­(C5), we have

 N

T

- 1/2(

)

( ) - ( )

-L Bq+3( )

as min{N, T }  , where Bq+3( ) is a (q + 3)-dimensional Brownian bridge,  =

16

1-10-1 1 with



1





 0

0

=

 



 

cb





 cb

0 z 5 z 4 z

cb 5 z  3 + c2b 2 + c2b

cb 







4z 

 

,



2

+ c2b

 





 1 + cb2

cb = c1-1c0, and 1 is defined in condition (C3).

(4.3)

The proof of Theorem 4.2 is given in Appendix B. To better understand the convergence result given in Theorem 4.2, we consider the case that for any fixed  , that Bq+3( ) reduces to N(0,  (1 -  )Iq+3). Specifically, we have the following Corollary.

COROLLARY 4.1. Under condition (C1)­(C5), for any fixed   B we have the

 result N T

( )-( )

-L N(0,  (1- )( )) as min{N, T }  , where B  (0, 1)

is a compact set.

Corollary 4.1 is a direct implication from Theorem 4.2. By Corollary 4.1, the asymptotic normality can be obtained at aribitrary fixed  . This enables us to conduct pointwise (for a fixed  ) inference on the estimated parameters. Specifically, the numerical details are given in the next section.

5. NUMERICAL STUDIES

5.1. Simulation Models

We consider three simulation settings in this subsection to illustrate the finite sample performance of the proposed NQAR model. The main difference lies in the generating mechanism of the network structure (i.e, A). The baseline, network, and the
17

momentum function are set to be 0(u) = u, 1(u) = 0.1(u), 2(u) = 0.4{1 + exp(u)}-1 exp(u). In addition, we fix the dimension of nodal covariates (i.e., Zi) to be 5. The corresponding nodal functions are then set to be 1(u) = 0.5(u), 2(u) = 0.3G(u, 1, 2), 3(u) = 0.2G(u, 2, 2), 4(u) = 0.25G(u, 3, 2), and 5(u) = 0.2G(u, 2, 1), where G(·, a, b) is the Gamma distribution function with shape parameter a and scale parameter b.
To generate observations from the NQAR mechanism (2.1), the following procedures are performed. First, we generate uits (1  i  N, 1  t  T ) independently from a standard normal distribution N (0, 1) and t-distribution with 5 degrees of freedom. Then, the random coefficients are obtained by substituting uit into j(·) and l(·) functions for 0  j  2 and 1  l  5. Next, the nodal covariates Zi = (Zi1, · · · , Zi5)  R5 are sampled from a multivariate normal distribution N (0, z), where z = (j1j2) and j1j2 = 0.5|1-2|. Lastly, we fix Y0 = (1 - ^b1 - ^b2)-1^b01, where ^bj is the numerical mean of the j(·) function over a set of  s. Then Yts are generated according to (2.1). We adopt three kinds of the adjacency matrix structures that are well-known in the literature. The details are given in the following.
Example 1. (Dyad Independence Model) Holland and Leinhardt (1981) introduce a Dyad Indenpendece Model with Dyad defined as Dij = (aij, aji) for 1  i < j  N . It is assumed the different Dijs are independent. Specifically, we set the probability of mutually connect dyads to be P (Dij = (1, 1)) = 20N -1 to ensure the network sparsity. Besides, set P (Dij = (1, 0)) = P (Dij = (0, 1)) = 0.5N -0.8, which implies that the expected degree for each node is O(N 0.2). Accordingly, we have P (Dij = (0, 0)) = 1 - 20N -1 - N -0.8, which is close to 1 as N  .
Example 2. (Stochastic Block Model) We further consider the stochastic model, which is extensively studied in network analysis (Wang and Wong, 1987; Nowicki and
18

Snijders, 2001). In particular, it is important for community detection (Zhao et al., 2012). To generate the block network structure, we follow Nowicki and Snijders (2001) to randomly assign for each node a block label which is indexed from 1 to K, where K  {5, 10, 20}. We then set P (aij = 1) = 0.3N -0.3 if i and j are in the same block, and P (aij = 1) = 0.3N -1. This indicates that the nodes within the same block have higher probability to connect than between blocks.
Example 3. (Power-law Distribution Network) According to Baraba´si and Albert (1999), it is a common phenomenon that the majority nodes in the network have small links, but a small amount of nodes have large number of links. The degrees of nodes could then be characterized by the power-law distribution. To generate the network structure in the spirit of this phenomenon, we simulate A as follows according to Clauset et al. (2009). For each node, we generate the in-degree di = j aji according to the discrete power-law distribution as P (di = k) = ck-, where c is a normalizing constant and the exponent parameter  is set to be  = 2.5 by Clauset et al. (2009). Lastly, for the ith node, we randomly select di nodes as its followers.
5.2. Performance Measurements and Simulation Results
We consider different network sizes (i.e., N = 100, 500, 1000) and let T = N/10. we have considered the case of N > T , and the results are not significantly different. Moreover, for each example, the numerical performance is evaluated at  = 0.1, 0.2, · · · , 0.9. The experiment is randomly replicated R = 1000 times for a reliable evaluation. Specifically, use ^( ) = {^0(m)( ), ^1(m)( ), ^2(m)( ), ^(m) ( )} be the estimator from the mth replication. To evaluate the finite sample performance, the following measures are considered. First, for the given parameter j( ) (0  j  2), the root mean square error (RMSE) is calculated by RMSEj( ) = {R-1 r(^j(r)( ) - j( ))2}1/2. Besides, for the
19

nodal effect function , the RMSE is given by RMSE( ) = {(5R)-1 r ^(r)( ) -

( ) 2}1/2. Next, for each j( ), a 95% confidence interval is constructed as CI(jr)( ) =

(^j(r)(

)

-

(r)
z0.975SEj ( ),

^j(r)( )

+

(r)
z0.975SEj ( )),

where

(r)
SEj ( )

is

the

jth

diagonal

element of (N T )-1 (1 -  ), and z is the th quantile of the standard normal

distribution. Then, the coverage probability (CP) can be computed as CPj( ) =

R-1

R m=1

I

{j

(

)



CI(jr)( )},

where

I (·)

is

the

indicator

function.

Lastly, the net-

work density (ND) is given by {N (N - 1)}-1 a .i1,i2 i1i2

The three types of network structures are visualized in Figure 1. Besides, the detailed

results of the three simulation examples are given in Table 1 to 3. It can be found for a fixed  , the RMSE is decreased as N and T increased. For example, the RMSE of ^1( ) drops from 11.22 × 10-2 to 4.90 × 10-2 at  = 0.1 as N is increased from 100 to 500 in

Example 1 for the t- distribution. It can also be noted that the RMSE for t-distribution

of same network size N is slightly larger than standard normal distribution. At the

same time, the network is becoming sparser as N increases (e.g. ND drops from 2.4%

to 0.2% for the power-law distribution network from N = 100 to 1000). Moreover,

it can be concluded the computed coverage probabilities for j( )s are stable at the nomial level 95%, which corroborates with the theoretical results. Lastly, we plot the estimated ^j( ) with the 95% confidence interval against  in Figure 2. A monotonic increasing pattern can be detected.

5.3. Financial Contagion and Shared Ownership

In this study, we focus on studying the financial risk contagion mechanism accounting for the common shared ownership information using NQAR model. Specifically, we apply our NQAR model to the Chinese Stock Market in 2013. The dataset consists of 2, 442 stocks from the Chinese A share market, which are traded in Shanghai Stock
20

Exchange and Shenzhen Stock Exchange. Specifically, the corresponding response Yit is the weekly absolute return volatility. The time series of averaged stock volatility is plotted in the left panel of Figure 3, where a relatively higher volatility level can be captured during May and July. To construct the network structure, the top ten shareholders' information for each stock are collected. For the ith and jth stock, aij = 1 if they share at least one common shareholder, otherwise aij = 0. The network structures of stocks with top 100 market values are visualized in the right panel of Figure 3. The resulting network density is 3.9%.
Besides the shared ownership information, the firm specific variables are also taken into consideration. Motivated by Fama and French (2015), we consider the following K = 6 covariates to represent stocks' fundamentals. They are, SIZE (measured by the logarithm of market value), BM (book to market ratio), PR (increased profit ratio compared to the last year), AR (increased asset ratio compared to the last year), LEV (leverage ratio), and Cash (cash flow of the firm). Lastly, all covariates are re-scaled within the interval [0, 1].
We then launch the network analysis using the NQAR model. Particularly, the estimation results of our NQAR model are given in Table 4 for  = 0.05, 0.50, and 0.95 respectively. It is noteworthy that the stocks have stronger network effect and momentum effect in the upper tail (i.e.,  = 0.95) than the median and lower tail case (i.e.,  = 0.5 and 0.05). This indicates that stocks tend to have higher correlation through the network when higher volatility level is exposed in the market. Besides, the size (i.e., CAP), the book to market ratio (BM), and the leverage ratios (LEV) are shown to have a negative correlation with the conditional quantile level of the volatility at  = 0.95 and  = 0.5. However, this phenomena does not appear in the lower quantile case. In addition, stocks with higher liquidity (i.e., cash flow), profit ratio (PR), and
21

asset increasing ratio (AR) tend to have lower volatility in the median case.

Lastly, we conduct an impulse analysis in Section 3. Particularly, the influential power

can be calculated by ^ = {(1-^b2)IN -^b1W }-11, where ^b1 = 10-1

9 m=0

^1(0.05

+

0.1m)

and ^b2 = 10-1

9 m=0

^2

(0.05

+ 0.1m)

are

the

numerical

approximations

for

b1

and

b2.

Specifically, the influential power is found to have a linear pattern with the weighted

degrees in the left panel of Figure 4, and the histogram of the weighted degrees is given

in the right panel of Figure 4. Then, the cross-sectional impulse analysis is conducted.

Particularly, IEIl, ( = 0.05, 0.5, 0.95) is computed within 5 banks at l = 1, · · · , 10, which is visualized in Figure 5. Note that the impulse direction is from column to row.

The banks include, Bank of China (BOC), China Merchants Bank (CMB), Industrial

and Commercial Bank of China (ICBC), Ping An Bank (PAB), and Shanghai Pudong

Development Bank (SPDB). We observe significant asymmetric effects across different

quantiles. It can be seen the IEI is higher in the upper tail ( = 0.95). Moreover, the

impulse impacts between the BOC, CMB, and ICBC are much stronger than with the

other two banks.

APPENDIX A

In Appendix A, we are going to prove the stationarity result (Theorem 2.1) in Section A.1. the proof of Theorem 2.2 are given in Section A.2, and the verification of (2.5) and (2.6) are given in Section A.3.

Appendix A.1: Proof of Theorem 2.1

First of all, by iteration, we obtain the solution of NQAR model (2.1) as

L-1

L-1



Yt =

l + LYt-L + lVt-l = l + lVt-l,

l=0 l l=0 l=0

22

(A.1)

where l d=ef GtGt-1 · · · Gt-l+1 for l > 0 and 0 = 1. We then prove Theorem 2.1 in two steps. In the first step, we prove the covariance stationarity of the solution (A.1). Next, we prove the uniqueness of the stationary solution (A.1).
Step 1. (Proof of Covariance Stationarity)
In this step, we show the covariance stationarity by calculating E(Yt) and Cov(Yt, Yt-h) respectively. Denote i(M ) as the ith eigenvalue of any arbitrary matrix M  RN×N such that |1(M )| > |2(M )| > · · · > |N (M )|. Recall that E(Gt) = G = b1W + b2I. We firstly verify that E(Yt) = µY for 1  t  T . To this end, note that |1(W )| = 1 by Banerjee et al. (2014), we have

|1(G)|  |b1||1(W )| + |b2| < 1

(A.2)

due to the stationarity condition in Theorem 2.1. Then it could be computed that

E(Yt) =

 l=0

Gl

=

(I

-

G)-1

due

to

the

independence

of

Gts

over

t,

and

E(Vt-l)

=

0 for l  0. Recall that we have  = c01N . Then it is straightforward to have

µY = c-1 1c01N , where c1 = (1 - b1 - b2)-1 is defined in Theorem 2.1.

We next calculate the covariance of Yt. Specifically, it can be expressed as

 

Cov(Yt) = Cov( l) + Cov( l1, l2Vt-l2)

l=0 

l1=0 

l2=0



+ Cov( l2Vt-l2, l1) + Cov( lVt-l).

l2=0

l1=0

l=0

(A.3)

Recall that G = E(Gt Gt) = E{B1(Ut)B1(Ut)}(W W )+E{B1(Ut)B2(Ut)}(W  I) + E{B2(Ut)  B1(Ut)}(I  W ) + E{B2(Ut)  B2(Ut)}(I  I), b1 = {E 12(Uit)}1/2,

23

and b2 = {E 22(Uit)}1/2. Then we have

|1(G)|  b12|1(W )|2 + 2b1b2|1(W )| + b22  (b1 + b2)2 < 1,

(A.4)

by the fact that |1(W )| < 1 and the stationarity condition in Theorem 2.1. Note the matrix G can be represented in Jordan canonical form as P P -1, where  is a matrix of the Jordan block diagonal form with diagonal elements being i(G) (1  i  N ) and P is an invertible matrix. Then by (A.4), (G)l converges to zero at a geometric rate as l   and therefore we have


(G)l = (I - G)-1.
l=0

(A.5)

Similarly, by (A.4) we have Cov(Yt) in (A.3) one by one.

 l=0

Gl

=

(I

- G)-1.

We then calculate the terms of

For the first term it can be calculated Cov(

 l=0

l)

=

E{(

 l1

l1

)(

 l2



l2)} -

µY µY =

 l1,l2=0

E(l1



l2) - µY µY .

Firstly we have vec(l1 l2) = (l2 

l1)vec( ). Without loss of generality, we assume l1  l2. Then it can be obtained

E(l2  l1) = (G)l2(IN  G)l1-l2 and

E(l2  l1)vec( ) = (G)l2(b1 + b2)l1-l2 c021N2

(A.6)

due to that vec( ) = c021N2, (IN  G)1 = (b1 + b2)1. Therefore, by (A.4), (A.5),

and (A.6) we have

 l1,l2

=0

E{vec(l1



l2)} = {

 l2=0

(G

)l2

l1>l2 (b1 + b2)l1-l2 +

 l1=0

(G

)l1

l1l2(b1 + b2)l2-l1}c201 = M11N2, where M1 = c-1 1c20(1 + b1 + b2)(I - G)-1.

As a result, we have vec{Cov(

 l=0

l)}

=

M1 1N 2

-

c-1 2c201N2 .

Next, for the second term, we have Cov(

 l1=0

l1

,

 l2=0

l2 Vt-l2 )

=

 l1,l2=0

E(l1

Vt-l2 l2

),

24

due to that E(

 l2=0

l2

Vt-l2

)

=

0.

It is straightforward to verify that for l2  l1,

we have E(l1Vt-l1 l2) = 0. Therefore, by (A.4) and (A.5), one could verify

 l1,l2=0

E{vec(l1

Vt-l1



l2)} =

 l1=0

 l2=l1

+1

(G

)l1

E{(Gt-l1



I )(G



I

)l2-l1-1(I



Vt-l1 )} =

 l1=0

l2=l1+1(G)l1 (b1 + b2)l2-l1-1vec(bv) = c-1 1c0(I - G)-1vec(bv).

Similarly, one could verify that the third term Cov(

 l1=0

l1

Vt-l1

,

 l2=0

l2 )

is

also

equivalent to c1-1c0(I - G)-1vec(bv).

For the last term, we have Cov(

 l=0

lVt-l)

=

 l=0

E(l

Vt-lVt-ll

)

due

to

that

E(l1Vt-l1Vt-l2l2) = 0 for any l1 = l2. Then, note that E{vec(lVt-lVt-ll )} =

E{(l l)vec(Vt-lVt-l)} = (G)lvec(V ). Then by (A.5) we have Cov(

 l=0

lVt-l)

=

l=0(G)lvec(V ) =

 l=0

P

lP

-1vec(V

)

=

P

(I -)-1 P

-1vec(V

)

=

(I -G )-1 vec(V

).

Consequently, by (A.4) one obtains that vec{Cov(

 l=0

lVt-l)}

=

(I

-

G)-1vec(V

).

From the above, we have vec(Y ) takes the form (I - G)-1vec(V ) + 2c-1 1c0(1 - G)-1vec(bv)+(M1-c-1 2c02)1N2. To prove the covariance stationary, it suffices to prove

that Cov(Yt, Yt-h) only depends on h. As we can see that for h  1, Cov(Yt, Yt-h) =

E(YtYt-h) - E(Yt) E(Yt-h) = E{E(Yt|Ft-h)Yt-h} - µyµy . Further one can obtain that

E(Yt|Ft-h) =

h-1 l=0

Gl

+

GhYt-h

.

So

it

is

straightforward

to

conclude

Cov(Yt, Yt-h)

=

GhY , which is only related to h. This completes the proof of Step 1.

Step 2. (Uniqueness of the Solution)

Assume that Yt is another covariance stationary solution to the NQAR model. Define

|M |a = (|mij|)  Rm×n for any arbitrary matrix M  Rm×n. In addition, for any two

arbitrary matrices M1 = (m1,ij)  Rm×n and M2 = (m2,ij)  Rm×n, define M1 M2

as m1,ij  m2,ij for 1  i  m and 1  j  n. Then we know that E |Yt |a C11N for

some constant C1. Similarly we have Yt =

m-1 l=0

l

(

+

Vt-l)

+

mYt-m

.

To

calculate

the difference between Yt and Yt , we have E |Yt-Yt|a = E |(

 l=m

l+

 l=m

l

Vt-l

)-

mYt-m|a

C2(

 l=m

E

|l|a

+

E

|m|a)1N

,

where

C2

=

max{C1, c0, E |Vit|}.

It

can

be

25

verified that E |l|a1N = E |1(Uit)W + 2(Uit)IN |la1N (b1 + b2)l1N . Therefore we

have (

 l=m

E

|l|a

+

E

|m|a)1N

C3(b1 + b2)m1N for some positive constant C3. As

this holds for any m, we can then prove that Yt = Yt by the stationary condition that

b1 + b2 < 1 with probability 1. This completes the proof.

Appendix A.2: Proof of Theorem 2.2

In this subsection, we establish the asymptotic normality of Yt. Define Yt = Yt -µY = (Y1t, · · · , YNt)  RN . We then adopt the dependent Lindeberg central limit theorem (theorem 2) in Bardet et al. (2008) on (N T )-1/2Yt. We verify the two conditions in the following two parts. Step 1 is concerning moments bounds, and Step 2 is regarding the time dependency.

Step 1. (Bounding Moments) First, it suffices to show that there exists 0 <   1

satisfying

T

ST = (N T )-(2+)/21N

E |Yt|2a+  0

t=0

(A.7)

as T  , where |Y |2a+ denotes (|Y1|2+, · · · , |Yp|2+)  Rp for a p-dimensional vec-

tor Y = (Y1, · · · , Yp)  Rp. Then, one can verify that E |Yt|2a+ = E | l=0(l +

lVt-l)-µY |a2+. Further by the Jensen's inequality ST = (N T )-(2+)/21N

T t=0

E |Yt|a2+



(N T )-(2+)/21N

tT=0{E |

 l=0

lVt-l|a2+

+

E|

l l|2a+ + E |µY |a2+}(32+/3). It is

not hard to see that (N T )-(2+)/21N

T t=1

|µY

|2a+

 0.

Let



=1

and define ST v

=

N -3/2T -1/21N E |

 l=0

l Vt-l |a3 .

Then it suffices to show ST v

 0.

It can be derived

26

ST v· = ST v1 + ST v2 + ST v3 + ST v4, where

ST v1 = N -3/2T -1/21N E

|lVt-l|a3 ,
l

ST v2 = 3N -3/2T -1/2

E |l1 Vt-l1 |2a  |l2 Vt-l2 |a ,

l1 l2>l1

ST v3 = 3N -3/2T -1/2

E |l1 Vt-l1 |a  |l2 Vt-l2 |a2 ,

l1 l2>l1

ST v4 = 6N -3/2T -1/2

E |l1 Vt-l1 |a  |l2 Vt-l2 |a  |l3 Vt-l3 |a ,

l1 l2>l1 l3>l2>l1

and  means point wise product. We then verify the terms ST vj  0 for j = 1, · · · , 4 as follows.

Firstly we have ST v1  N -3/2T -1/2

 l=0

E

|l

(Vt-1)|a3

N -3/2T -1/2

 l=0

C3

E

|(|l|a1N

)|a3 ,

where C3 = maxi E |Vit|3 is finite by Theorem 2.2. Further, the above term is element-

wisely bounded by C3N -3/2T -1/2

 l=0

Cbl1N ,

where

Cb

=

E(|1(Uit)|

+

|2(Uit)|)3

<

1

by Theorem 2.2. Consequently we have ST v1  0. Next we look at the second

term in ST v. It can be firstly verified that E(3 l1 l2>l1 |l1Vt-l1|a2  |l2Vt-l2|a)

3Cv l1 l2>l1 E(|l1 Vt-l1 |2a  |l2 1N |a)

3Cv(b1 + b2)l2-l1 l1 l2>l1 E(|l1 Vt-l1 |a2 

|l1-11N |a) due to the independence of l1-1 and

l2-l1 k=1

Gt-l1 -k ,

and

the

inequality

E(

l2-l1 k=1

|Gt-l1-k|a1N )

(b1 + b2)l2-l11N , where Cv = maxi E(|Vit|). Further it can be

derived that 1N E(|l1Vt-l1|a2  |l1-11N |a) =

E(|l1 Vt-l1 |2a |l1-11N |a)  Cbv E(|l1 1N |a2 |l1 1N |a)  N CbvCbl1 ,

(A.8)

where Cbv = (E{Vi4t })1/2[E{(|1(Uit)| + |2(Uit)|)2}]1/2. As a result, we have ST v2  0. Then, by iteratively applying (A.8), one could obtain ST v3  0 and ST v4  0, where the details are omitted here. As a result, (A.7) can be obtained.

27

Step 2. (Time Dependency) Next, we verify conditions imposed on the dependency structure of Yt. To this end, we show the definition of  dependency as in Bardet et al. (2008).
DEFINITION 5.1. ( dependency) A process Xt in Rd is said to be  dependent if there exists a sequence {r} such that r  0 when r   satisfying

Cov{f (Xm1, Xm2, · · · , Xmv ), g(Xs1, Xs2, · · · , Xsu)}  (uvLf Lg + vLf + uLg)r,

for all v, u  N  × N  (N  denotes the natural number space), Lf and Lg as constants, where v, u are two integers corresponding to support of f and g respectively.

Next we prove that T -1/2Yt is  dependent with a satisfactory rate. For this propose,

we rewrite the NQAR model to be Yt = GtYt-1 + Vt , and Vt = Vt + (Gt - G)µY .

Then we have Yt =

 l

lVt-l.

For convenience, we define YtL =

L l

lVt-l

as

the

truncated form of Yt.

First of all define

v = {Ym1, Ym2, · · · , Ymv } and

u = {Ys1, Ys2, · · · , Ysu}. And

L v

=

{YLm1, YmL 2, · · · , YmL v } and

L u

=

{YLs1, YLs2, · · ·

, YLsu}.

We

than

have

Cov

f(

v), g(

u)

=

Cov

f(

v) - f(

vL), g(

u)

+ Cov

f(

L v

),

g(

u) - g(

Lu )

+ Cov

f(

Lv ), g(

uL) . De-

fine f~(X) = f (X) - E(f (X)). Without loss of generality, we set L = r - 1. Then

Cov(f (

Lv ), g(

L u

))

=

0,

and

| Cov(f (

v), g(

u))| can be bounded by

g  E |f (

v) -

f ( Lv )|+ f  E |g( u)-g( Lu )|  c(vLf +uLg)1N E |Yms-YmL s|a, where ms = m1m2,

c is a constant and ·  is the uniform norm of a function, which takes the supremum

of the absolute value of a function on its support. Then it can be verified that E |Yms -

YmL s|a = E |

 l=L+1

lVt-l|a



 l=L+1

Cv

E |l1N |a

 l=L+1

Cv

[E{|1(Uit)|+|2(Uit)|}]l1N



Cv (b1 + b2)L+1(1 - b1 - b2)-1, where Cv = E(|Vit|) + 2(b1 + b2). As a result, it can be

concluded Cov f ( v), g( u)  0 as r  . This completes the proof.

28

Appendix A.3: Verification of (2.5) and (2.6)

Assume b1 = |

1 0

1(u)2du|1/2

=

O(1).

Recall

b22

=

1 0

2(u)2du,

b12

=

1 0

1

(u)2(u)du.

By the stationary condition we have b22 < 1, then by the Cauchy's inequality we

have |b12|  b1|b22|1/2 = O(1). Recall that vec(Y ) = S1 + S2 + S3, where S1 =

M11N2 - c1-2c021N2 (M1 = c1-1c20(1 + b1 + b2)(I - G)-1), S2 = 2c-1 1c0(1 - G)-1vec(bv),

and S3 = (I - G)-1vec(V ). Next, we approximate Y by neglecting higher order

terms of b1, b12, b1. To this end, we first approximate (I - G)-1 and c1-1 as follows

(I - G)-1  (I - B22)-1(I + M12), c-1 1  (1 - b2)-1 1 + (1 - b2)-1b1 , c-1 2  (1 - b2)-2 1 + 2(1 - b2)-1b1 ,

(A.9) (A.10) (A.11)

where M12 = (I - B22)-1{B12(W  I) + B21(I  W )}, B22 = E{B2(Ut)  B2(Ut)}, B12 = E{B1(Ut)  B2(Ut)}, and B21 = E{B2(Ut)  B1(Ut)}.
Recall that b01 = E{1(Uit)(0(Uit) - b0)} and b02 = E{2(Uit)(0(Uit) - b0)}. Then, by (A.9)-(A.11) one could verify that S1  (I - B22)-1{(1 + 2b1 - b22)I  I + (1 - b22)(I - B22)-1(B12 + B21)}(1 - b2)-2c021N2 - {1 + 2(1 - b2)-1b1}(1 - b2)-2c021N2, S2  2(1 - b2)-1(I - B22)-1{b02I + (1 - b2)-1b1b02I + b02M12 + b01I}c0vec(IN ), and S3  (I -B22)-1(I +M12)vec(V ). Let Sj = vec(j) for j = 1, 2, 3 and j = (j,kl)  RN×N . Specifically, one can verify for the diagonal elements that 1,ii  [(1 - b22)-1{1 - b22 + 2b1 +2(1-b22)-1(1-b22)b12}-(1-b2)-1(1-b2 +2b1)](1-b2)-2c20, 2,ii  2(1-b22)-1(1- b2)-2{bv(1 - b2) + b1b02}c0 (bv = b01 + b02), 3,ii  (1 - b22)-1V2 . Similarly, we have 1,i1i2  {(1 - b2)-2(1 - b22)-2(1 - b22 + 2b1 + 2b1b2) - (1 - b2)-3(1 - b2 + 2b1)}c02, 2,i1i2  2(1 - b22)-2(1 - b2)-1c0b1b2b02(wi1i2 + wi2i1 ), 3,i1i2  (1 - b22)-2b1b2(wi1i2 + wi2i1 )V2 for i1 = i2, where wi1i2 = n-i11ai1i2 is the (i, j)th element of W . This leads to the desired
29

results.

APPENDIX B

In Appendix B, we give the proof of the asymptotic properties in the estimation part. Specifically, Theorem 4.1 and Theorem 4.2 are going to be proved in Section B.2 respectively.

Appendix B.1: A Useful Lemma

In this section, we give the proof of a useful lemma, which is employed as tools in later proof of asymptotic properties.
LEMMA 5.1. Assume c < 1 and (C1)­(C3), where c is defined in (C1). Let U = (U1, · · · , UN )  RN and V = (V1, · · · , VN )  RN , where Ui and Vi are identically distributed respectively for 1  i  N , and independent with l. Assume E(Ui4)1/4  u, E(Vi4)1/4  v, Cov(Ui, Vi) = 0, and Cov(Ui, Uj) = 0 for i = j. Define G = 1 4W +
2 4I  RN×N . Then the following results hold. (a) For any integer l1, l2, l3, l4 > 0 we have

E(|l1 l2 l3 l4 |a) |Gl1 Gl2 Gl3 Gl4 |a.

(B.1)

(b) There exists a finite integer K > 0, such that for any l > 0, we have

GlGl

lK c2lM,

(B.2)

where M = M M

with M = cm1 +

K j=1

Wj,

cm

>

1

is

a

constant,

and



is

defined

30

in (C2.1). Denote Mij as the (i, j)th element of M. We then have
N -21 M1  0, N -2 tr(M2)  0,

(B.3) (B.4)

as N  . (c) For any integer l1  l2, it holds that

Var(U l1l2V )  8u2v2c2(l1+l2)l12K {1 M1 + tr(M2)}.

(B.5)

(d) We have 0 p 0 as min{N, T }  .

Proof of (a). We first derive an inequality of E |l1l2l3l4|a as

l1-1

E |l1 l2 l3 l4 |a E

|Gt-l|a

l=0

l2-1
|Gt-l|a
l=0

l3-1
|Gt-l|a
l=0

l4-1
|Gt-l|a
l=0

.

We first prove

E(|Gt|a |Gt|aM |Gt|a |Gt|a) G G E(M )G G

(B.6)

for any elementwisely positive stochastic matrix M , where M = (mij)  RN×N is assumed to be independent with Gt. Let W11 = W W , W10 = W , W01 = W , W00 = I. Further denote Wq1q2,i·  RN as the ith row vector of Wq1q2, where q1, q2 = 0, 1. Then one could verify the (i, j)th element of E(|Gt|a |Gt|aM |Gt|a |Gt|a) involves a sum of terms like E{|1k1(Ui1t)2k2(Ui2t)1k3(Ui3t)2k4(Ui4t)|(Wq1q2M Wq3q4)ij}, where q1, q2 = 0, 1, k1, k2, k3, k4 are integers, 0  ki  4, k1 + k2 + k3 + k4 = 4, 0  i1, i2, i3, i4  N .

By Ho¨lder's inequality, we have for all i1, i2, i3, i4, E |1k1(Ui1t)2k2(Ui2t)1k3(Ui3t)2k4(Ui4t)| 

1

k1+k3 4

2

.4-(k1+k3)
4

By applying the inequality one could obtain (B.6).

Subse-

31

quently, (B.1) can be derived by recursively applying (B.6).

Proof of (b). Note we have 1 4+ 2 4 < 1. Then (B.2) can be obtained by (5.1) of Lemma 2 (a) in the supplementary material of Zhu et al. (2016). For the completeness

of the proof, we briefly repeat the step as below. Firstly, for any integer l > 0, we have

Gl = ( 1 4W + 2 4I)l =

l j=0

Clj

1

j 4

2

l-j 4

W

j

,

where

Clj

=

l!/{j!(l - j)!}.

Since

W is an element-wise non-negative matrix, |Gl|a

l j=0

Clj

1

j 4

2

l-j 4

W

j

.

Then for

l > K we have |Gl|a

( 1 4 + 2 4)lC1 +

K j=0

Clj

1

j 4

2

l-j 4

W

j

,

,

where

this

fact is due to W l

C1 . Further note that

1

j 4

2

n-j 4

<

cl

(0



j



l),

and

ClK  lK. As a result, for l > K we have,

|Gl|a lK ( 1 4 + 2 4)lM,

(B.7)

where recall that M = C1 +

K j=0

W

j.

It

is

easy

to

verify

that

(B.7)

also

holds

for

l = 1, · · · , K - 1. Then we have |Gl(G )l|a l2Kc2lM M for any positive integer n.

As a result, (B.2) can be proved.

Next (B.3) and (B.4) can be obtained by (5.11) and (5.12) of the supplementary material by Zhu et al. (2016) respectively.

Proof of (c). We first prove that (B.5) holds for l1 = l2 = l, then extend the results to l1 > l2. Let l1 = l2 = l, we have

Var(U l lV ) = Var{E(U l lV |l)} + E{Var(U l lV |l)}.

(B.8)

We then derive the upper bound for E{Var(U l lV )|l} and Var{E(U l lV )|l} in the following respectively.
Upper Bound for E{Var(U l lV )|l}. One could first verify that U l lV =

32

vec(l) (I  l)vec(V U ). Denote V = vec(V U )  RN . As a consequence, we have

Var(U l lV |l) = vec(l) (I  l) Cov(V)(I  l )vec(l).

Further more, by the Cauchy's inequality for the N × N block matrices V,ii and V,ij the following bound can be attained,

V,ii = Cov(ViU )  2u2v211 , V,ij = Cov(ViU, VjU )  2u2v2(I + ej1 + 1ei )

(B.9) (B.10)

for i = j, where ei  RN is a vector with all elements to be 0 but only the ith element

being 1. Denote l,·i as the ith column vector of l. Then we have vec(l) (I 

l) Cov(V)(I  l )vec(l) =

N i,j=1

l,·ilV,ij l

l,·j

=

N i=1

l,·ilV

,ii

l

l,·i

+

i=j l,·ilV,ijl l,·j  2u2v2{3 tr(|l l|a11 |l l|a) + tr(|l l|a|l l|a)} 

6u2v21 |l |a

|l|a|l|a |l|a1 + 2u2v2 tr(|l |a|l|a|l|a |l|a). By taking expectation on the right

side we have

E Var(U |l l|aV ||l|a)  6u2v2c4ll2K 1 M1 + 2u2v2c4llK tr(M2). (B.11)

Upper Bound for Var{E(U l lV |l)}. It can be calculated that E(U l lV |l)  uv tr(|l|a |l|a). Firstly we have Var{tr(|l|a |l|a)} = E[Var{tr(|l|a |l|a)|l-1}] + Var[E{tr(|l|a |l|a) |l-1}]. We first write tr(|l|a |l|a) = i Gt-l+1,·i|l-1|a |l-1|aGt-l+1,·i. Therefore we have Var{tr(|l|a |l|a)|l-1} = i Var(Gt-l+1,·i|l-1|a |l-1|aGt-l+1,·i|l-1) 
33

2 i(G·i |l-1|a |l-1|aG·i)2

 2c2 i G·i |l-1|a |l-1|a11 |l-1|a |l-1|aG·i = 2c21 |l-1|a |l-1|aGG |l-1|a |l-1|a1.

Moreover, by similar proofs of (B.1), we have E(1 |l-1|a |l-1|aGG |l-1|a |l-1|a1) 

lKc4l-21 M1 by (B.1) and (B.3). Lastly, one could verify that E{tr(|l|a |l|a)|l-1} 

i G·i |l-1|a |l-1|aG·i = tr(G |l-1|a |l-1|aG). By applying the deduction recur-

sively, one should have Var{E(U |l|a |l|aV |l)}  2u2v2c4l

l k=1

kK

1

M1. By com-

bining the results of (B.11), we have

l
Var(U |l|a |l|aV )  2u2v2c4l 3lK1 M1 + kK1 M1 + l2K tr(M2) . (B.12)
k=1

Consequently we have (B.5) holds. For l1 > l2, it can be derived that Var(U l1l2V ) =

Var{E(U l1l2V |l1)} + E{Var(U l1l2V |l1)}. For E{Var(U l1l2V |l1)}, one

can achieve a direct bound by (B.1) and (B.2) similar to the case of l1 = l2:

E{Var(U

l1l2V |l1)}  2u2 E(V

|l2 |a

|l1 |a|l1 |a

|l2 |aV

)



2u2

v2

c2(l1+l2 

)l1K

1

M1.

For Var{E(U l1l2V |l1)}, we would like to bound by a recursive formula so that one can utilize the conclusion we achieved for the l1 = l2 case. Var{E(U l1l2V |l1)}  2u2 Var(1 |l1|a |l2|aV ). So we have

Var(U |l1|a |l2|aV )  2u2v2c2(l1+l2)l1K 1 M1 + u2 Var(1 |l1|a |l2|aV ). (B.13)

Note 1 |l1|a |l2|aV = Bt-l1+1|l1-1|a|l2|aV . Then by letting U = Bt-l1+1 one could

obtain the result that Var(U

|l1 |a

|l2 |aV

)



2u2v2

c2(l1+l2 

){l1K

+

(l1

-

1)K }1

M1 +

u2c2 Var(1 |l1-1|a |l2|aV ). By applying the same technique recursively, we have

l1

Var(U

|l1 |a

|l2 |aV

)



2u2

v2

c2(l1+l2 

)

kK 1 M1+u2c2(l2-l1-1) Var(Bt-l2 |l2 |a |l2 |aV ).

k=l2+1

34

By combining the results from (B.12), we have Var(U

|l1 |a

|l2 |aV

)



2u2v2

c2(l1+l2 

)

{(

l1 k=1

kK

+ 3l2K)1 M1 + l22K tr(M2)}  8u2v2c2(l1+l2)l12K{1 M1 + tr(M2)}, which proves (B.5).

Proof of (d): Write 0 =



 1 S12 S13 S14 







1T NT

N

Xit

Xit

=

 



t=1 i=1







S22 S23 S24 

 

,



S33

S34

 





 S44

where

S12

=

1 N

N
Zi ,

S13

=

1 NT

T

N
wi Yt,

S14

=

1 NT

T

N
Yit,

i=1 t=1 i=1

t=1 i=1

N
S22 = N -1 ZiZi ,

S23

=

1 NT

T

NT
wi YtZi, S24 = (N T )-1

N
YitZi,

i=1 t=1 i=1

t=1 i=1

S33

=

1 NT

T

N
(wi Yt)2,

S34

=

1 NT

T

N
wi YtYit,

S44

=

1 NT

T

N
Yi2t .

t=1 i=1

t=1 i=1

t=1 i=1

One can directly conclude that S12 p 0p and S22  Z by the law of large numbers. Recall that 1 = limN N -1tr(Y ), 2 = limN N -1tr(W Y ), 3 = limN N -1tr(W Y W ),

and 4 = limN N -1tr{(I - G)-1}, 5 = limN N -1tr{W (I - G)-1}. z =

E(ZiZi ). S12 p (0, 0, · · · , 0)p×1, S13 p cb, and S14 = N -1 t 1 Yt-1 p cb .

We first list the component wise limit for each element in 0 in expectation, and we

will verify the variance of these components in the next steps. S22 p z and S23 =

1 NT

T t=1

Z

W Yt-1

p

1 N

E{Z

W (I - G)-1Z} = 5z, with Z = (Z1, Z2, · · · , Zn)

and  = [

l(u)du].

S24

=

1 NT

T t=1

1

Yt-1Zi p N -1 E(Z

(I - G)-1Z) = 4z.

35

S33

=

1 NT

t

i(wi Yt-1)

=

1 NT

T t=1

Yt-1

W

W Yt-1 p N -1 tr{W

W Y } + cb2.

Finally S34 p N -1 tr{W Y } + c2b , S44 p N -1 tr{Y } + cb2.

Some tedious steps are needed for verifying the variance of the aforementioned compo-

nent. For the interest of space, we will only show one of the hardest part S44 p 1 +cb2

which involves the fourth moment of Yt. The proof contains two steps. In the first

step, we prove that for any fixed t, N -1

N i=1

Xit

Xit

p

0

as

N



.

Next,

we

deal

with the dependence cross over time (i.e., 1  t  T ). Specifically, the near epoch

dependence of Yit and its functional forms are presented and consequently the desired law of large numbers results are established.

Step 1. Proof of N -1

N i=1

Yi2t

p

1

+

cb2.

In this step, we prove N -1

N i=1

Yi2t

p

1 + cb2

as

N





for

any

fixed

t

under

conditions (C1)­(C3). To this end, it suffices to show N -1 E(Yt Yt)  1 + cb2 and

N -2 Var( t Yt Yt)  0 as N  . By condition (C3) we have N -1 E(Yt Yt) = N -1{tr(Y )+E(Yt) E(Yt)}  1 +cb2. We then prove N -2 Var(Yt Yt)  0 as N  

in the following.

Recall that Yt has the decomposition in the (A.1). Without loss of generality, assume

 = 1N . Then we have Yt Yt =

 l1,l2

=0

(1

l1 l2 1+21

l1 l2 Vt-l2 +Vt-l1 l1 l2 Vt-l2 ).

By the Cauchy's inequality, it suffices to show N -2 Var

 l1,l2=0

Vt-l1

l1

l2

Vt-l2

 0,

N -2 Var

1
l1,l2=0

l1 l2 Vt-l2

 0, and N -2 Var

1
l1,l2=0

l1 l2 1

 0 as N 

. Since their proofs are almost the same, we prove N -2 Var

1
l1,l2=0

l1 l2 1

0

in the following for simplicity. To this end, first it can be shown Var(

1
l1,l2=0

l1 l2 1) =

36

 l1=l2

Var

1

l1 l2 1

+

 l1,l2=0

Cov

1

l1 l1 1, 1

l2l21 . Then it suffices to show



N -2

Var 1 l1l21  0,

l1=l2



N -2

Cov 1 l1l11, 1 l2l2 1

l1,l2=0

0

(B.14) (B.15)

N  . We then prove (B.14) and (B.15) separately as follows. Write 1 l1l21 =

Bt-l1+1l1-1l2-1Bt-l2+1, where Bt-l1+1 = B1(t-l1+1)1N +B2(t-l1+1)1N = (1(Ui(t-l1+1)))+

(2(Ui(t-l1+1))). It can be calculated

 l1=l2

Var

Bt-l1+1l1-1l2-1Bt-l2+1

=2

 l2=0

l1>l2 Var

Bt-l1+1l1-1l2-1Bt-l2+1 . By (B.5) we have Var Bt-l1+1l1-1l2-1Bt-l2+1

 8c2(l1+l2-2)l12K{1 M1 + tr(M2)}. Then we have (B.14) due to that

 l2=0

l1>l2

c2(l1+l2-2)l12K <  and N -2{1 M1 + tr(M2)}  0 by (B.3) and (B.4). For (B.15), it

can be shown by Cauchy's inequality that Cov(1 l1l11, 1 l2l21)  Var(1 l1l11)1/2 Var(1 l2l21)1/2  8c2(l1+l2)l1Kl2K{1 M1+tr(M2)} by (B.5). Then (B.15) holds since
l1,l2 c2(l1+l2)l1K l2K <  and N -2{1 M1 + tr(M2)}  0 as N  . This completes

the proof.

Step 2. L1 Near Epoch Dependence.
In this step, we further prove N -1 i Yi2t satisfies near epoch dependence cross 1  t  T . First we give the definition of L1 near epoch dependence as below.
DEFINITION 5.2. (L1 near epoch dependence) A triangular array Uit in R1 is said to be L1 near epoch dependent (NED) if there exists constants cit and a sequence {vJ , J  1} such that vJ  0 when J   satisfying

E (Uit) - E(Uit|Ft-J , · · · , Ft, · · · , Ft+J )  citvJ .

Given the definition, we firstly prove that Yits are L1 NED by Andrews (1988). Next, 37

according to Chapter 7 Lemma 1 of Gallant (2009), the smooth transformations of Yits

(e.g., N -1 i Yi2t ) are also NED. Since Yit has finite forth moment, then by Gallant

(2009) we have N -1

N i=1

Yi2t

is

a uniformly integrable L1

mixinggale.

Consequently,

according to Theorem 1 of Andrews (1988), we could have (N T )-1

T t=1

N i=1

Yi2t

con-

verge in probability as N   and T  . We then prove Yit is NED in the

following.

Denote Ftt-+JJ

=

{Ft-J , · · ·

, Ft, · · ·

, Ft+J }

and

t2 t1

=

following inequality as

t2 t=t1

Gt.

We then have the

E ei Yt - E(Yt|Ftt-+JJ ) a

 E ei



lVt-l +

J+1(tt--l(-J+J 1) - Gl-J-1)

l=J +1

l=J +1





(b1a + b2a)lcv +

2(ba1 + ba2)lc0,

l=J +1

l=J +1

where cv = E |Vit|. Let vJ = (b1a + ba2)J+1 and cit = (1 - b1a - b2a)-1(2c0 + cv). By condition (C1) we have b1a + b2a < 1, thus Yits are L1 NED according to Definition 5.2. This completes the proof of Step 2.

Appendix B.2: Proof of Theorem 4.1 and Theorem 4.2

Denote Vit

 = Yit - Xi(t-1)( ) and v^ = N T

^( ) - ( )

.

Then we have 

Yit -

Xi(t-1)^( ) =  Vit - (N T )-1/2Xi(t-1)v^ , where Vit = Yit - Xi(t-1)( ). Then the

minimization of (4.1) is equivalent to minimizing

NT
ZNT (v) =
i=1 t=1

 Vit - (N T )-1/2Xi(t-1)v -  (Vit ) .

One could verify that v^ = arg minv ZNT (v). The objective function ZNT (v) is a convex random function. Define  (u) =  - I(u < 0) and let it = (N T )-1/2v Xit, and one

38

could further write ZNT (v) as ZNT (v) =

i(t-1)

- (N T )-1/2v Xi(t-1) (Vit ) +

1(Vit  s) - 1(Vit < 0) ds

i,t 0

d=ef v 1 + 2. According to Kato (2009), in order to prove v^ takes the representation in (4.2), it suffices to prove (a) 2 p v 1v with 1 defined in (C3) being a positive definite matrix with uniformly bounded eigenvalues on B, and (b) 1 is tight for   B  (0, 1), and 1 converges in distribution to a Brownian Bridge. We then prove (a), (b) in the following two steps.

Step a. Proof of (a).

Define 2it =

i(t-1) 0

1(Vit  s) - 1(Vit < 0) ds. To prove 2 =

N i=1

T t=1

2it

p

v 1v, we decompose 2it as 2it = E(2it|Ft-1)+2it, where 2it = 2it-E(2it|Ft-1). We

then prove

N i=1

T t=1

E(2it|Ft-1)

p

2-1v

1v and

N i=1

T t=1

2it

p

0

respectively

as follows.

We first evaluate

N i=1

T t=1

E(2it

|Ft-1

).

It

can

be

expressed

that

i,t E(2it|Ft-1) =

N i=1

T t=1

E[

i(t-1) 0

{1(Vit

 s)-1(Vit

< 0)}ds|Ft-1] =

N i=1

T t=1

i(t-1) 0

{Fi(t-1)(s+

Fi-(t1-1)( )) - Fit-1(Fi-(t1-1)( ))}/s · sds. This yields that

E(2it|Ft-1) =
i,t i,t

i(t-1)
fit-1(Fi-t-11( ))sds + Op(1)
0

= (2N T )-1fi(t-1) Xi(t-1)( ) v Xi(t-1)Xi(t-1)v + Op(1) p 1/2v 1v
i,t

(B.16)

according to condition (C3).

Next, we prove i,t 2it p 0. It is not difficult to see that 2it is a martingale

difference sequence, which can be written as 2it =

i(t-1) 0

it (s)

-

it (0)ds,

where

39

it (s) = 1(Vit  s) - Fi(t-1)(s + Xi(t-1)( )) . It suffices to show E(| i,t 2it|)2 =

i1,i2 t1,t2 E(2i1t12i2t2)  0. Importantly, recall that Vit = Xi(t-1)((Uit) - ( )),

therefore Vit and Vjt would be conditionally independent on Ft-1. Thus it can be

shown that E{

i(t-1) 0

it

(s)ds

j(t-1) 0

jt

(s)ds}

=

E[E{

i(t-1) 0

it

(s)ds

j(t-1) 0

jt (s)ds

|Ft-1}] = 0 due to the conditional independence of it (s) and jt (s) given Ft-1. Simi-

larly, for t1

> t2 we have E{

i(t1 -1) 0

it1

(s)ds

i(t2 -1) 0

jt2 (s)ds}

=

E[E{

i(t1 -1) 0

it1 (s)ds

i(t2 -1) 0

jt2

(s)ds|Ft1-1}]

=

0.

Therefore, we have E{2i1t1 2i2t2 } = 0 for i1 = i2

or t1 = t2. Then i1,i2 t1,t2 E(2i1t1 2i2t2 ) = i t E(22it). Next, write E(22it) =

E(22it) - E{E(2it|Ft-1)}2. Further it can be derived that E(22it) = E |

i(t-1) 0

{1(Vit



s) - 1(Vit  0)}ds|2  |i(t-1)| E

|i(t-1) | 0

{1(Vit



s) - 1(Vit



0)}2ds

by

the

Cheby-

shev's inequality.

Further we have |i(t-1)| E[

|i(t-1) 0

|

{1(Vit

 s) - 1(Vit

 0)}ds] =

|i(t-1)| E[

|i(t-1) | 0

{Fi(t-1)

(s

+

Fi-(t1-1)(

))

-

Fi(t-1)(Fi-(t1-1)(

))}/s

·

sds].

By

similar

tech-

nique with (B.16), one could obtain i,t E(22it)  E{ i,t 2-1(N T )-3/2|fi(t-1)(Xi(t-1)v)|

|Xi(t-1)v|3/2}+O(1). Since we have fit(·) is bounded and (N T )-3/2 i,t E(v XitXit v)2 = O (N T )-1/2  0, then it can be obtained that i,t E(22it)  0. Lastly, following sim-

ilar argument of tightness as in Wagener et al. (2012), we can prove that i,t 2it p 0

uniformly over   B. This completes the proof of 2 p v 1v for any   (0, 1).

Step b. (Proof of Theorem 4.2)

In this step, we are going to show 1 converges in distribution to a Brownian Bridge 01/2Bq+3( ), where 0 is defined in (C3), and Bq+3( ) is a (q+3)-dimensional Brownian bridge. To prove this conclusion, we adopt two steps:

(B.1) For arbitrary k-dimensional vector (1, 2, · · · , k)  Rp and   Rq+3, 1(1), 1(2), · · · , 1(k)   Rk converge to a k-dimensional multivariate normal distribution.
(B.2)  1( ) for   B  (0, 1) is tight, where B is a compact set in (0, 1).

40

Step B.1. Denote t = (V1t ), · · · , (VNt )  RN for convenience. We then

have E(Xt-1t|Ft-1) = 0. Therefore, Xt-1t is a martingale difference sequence for

1  t  T . To prove (B.1), we define t = (N TN )-1/2 Xt-1t and SNt =

t s=1

t.

Then one can see that {t, Ft-1, - < t < TN , N  1} is a martingale array, where

the number of observed time points TN is assumed to depend on N with TN  

as N  . As a result, the double sequence {SNt, Ft, - < t  TN , N  1} is a

martingale array. As a consequence, the martingale difference central limit theorem can

be applied (Hall and Heyde, 2014). Specifically, it requires two conditions as follows.

First we have

TN TN
E{t21|t2| > |Ft-1}  -2 E(|t|4|Ft-1)
t=1 t=1
TN
 -2 2(1 -  )2(N TN )-2 ( Xt-1Xt-1)2 p 0,
t=1

(B.17)

where the last inequality is due to E 4(Vit )   2(1 -  )2. Since by the proof of (d)

of Lemma 5.1, we have (N TN )-2

TN t=1

E(

Xt-1Xt-1)2  0. Therefore (B.17) can be

implied. Secondly, we also have the condition

TN

E{t2|Ft-1}

=

 (1 - NT

)

TN



Xt-1Xt-1 p  (1 -  )

0,

t=1 t=1

(B.18)

by (d) of Lemma 5.1 in Appendix B.1. Therefore, by the central limit theorem for martingale difference sequence in Hall and Heyde (2014), we have that 1( ) converge in distribution to Gaussian distribution N(0,  (1- ) 0) for fixed  . The conclusion also holds for any finite dimensional vector (1, 2, · · · , k) , which proves (B.1).
Step B.2. Then we prove that  1( ) for   B  (0, 1) is tight. The definition of tightness is given as follows.

41

DEFINITION 5.3. A process WNT ( ) is said to be tight if and only if for any  > 0 there exists a compact set E such that supEP(WNT ( )  E) > 1 - .

Define 1(D) = -(N T )-1/2 i,t Xi(t-1){2(Vit2) - 1(Vit1)} for any interval D = (1, 2]. To show the tightness, we adopt Theorem 15.6 in Billingsley (1968) and prove a sufficient Chentsov-Billingsley type of inequality as follows.
LEMMA 5.2. For any two intervals D1 = (1, 2] and D2 = (2, 3], we have

E  1(D1) 2  1(D2) 2  C(3 - 1),

(B.19)

where C is a finite positive constant.

To prove Lemma 5.2, we have E[{ 1(D1)}2{ 1(D2)}2] = (N T )-2 E[{ i,t  Xi(t-1) it(1, 2)}2{ i,t  Xi(t-1)it(2, 3)}2], where it(,  ) =  (Vit ) -  (Vit ). Next, by Cauchy's inequality, we have E[{ i,t  Xi(t-1)it(1, 2)}2{ i,t  Xi(t-1)it(2, 3)}2]  [E{ i,t  Xi(t-1)it(1, 2)}4]1/2[E{ i,t  Xi(t-1)it(2, 3)}4]1/2. Since it can be derived E{it(,  )|Ft-1} = 0, then E[{ Xi1(t1-1)i1t1 (,  )}{ Xi2(t2-1)i2t2 (,  )}{ Xi3(t3-1) i3t3(,  )}{ Xi4(t4-1)i4t4(,  )}] is non-zero only if (a) i1 = i2, t1 = t2 and i3 = i4 = i1, t3 = t4 = t1 or (b) i1 = i2 = i3 = i4 and t1 = t2 = t3 = t4. It is straightforward to verify (N T )-2 E{ i,t  Xi(t-1)it(1, 2)}4 =

(N T )-2

E ( Xi(t-1))2i2t(1, 2) 2 + (N T )-2

E ( Xi(t-1))4i4t(1, 2) .

i,t i,t

By the proof of (d) in Lemma 5.1, we know that E( Xit)2 = O(1) and E( Xit)4 = O(1). Moreover, it can be verified E{i2t(1, 2)}  2 - 1 and E{i4t(1, 2)}  2 - 1.

42

By combining the results together, we have
E  1(D1) 2  1(D2) 2  C(2 - 1)(3 - 2)  C|3 - 1|,
for some positive constant C. This completes the proof of Lemma 5.2. We then conclude that the 1( ) converge weakly to a (q + 3)-dimensional Brownian bridge. Consequently, the Theorem 4.2 can be proved.
References
Koenker, R.; Hallock, K. F. Journal of Econometric Perspectives 2001, 15, 143­156. Fitzenberger, B.; Koenker, R.; Machado, J. A. Economic applications of quantile
regression; Springer Science & Business Media, 2013. Gaglianone, W. P.; Lima, L. R.; Linton, O.; Smith, D. R. Journal of Business &
Economic Statistics 2012, Ha¨rdle, W. K.; Wang, W.; Yu, L. Journal of Econometrics 2016, Sankarasubramanian, A.; Lall, U. Water Resources Research 2003, 39. Wang, W.; Bobojonov, I.; H¨ardle, W.; Odening, M. Stochastic environmental research
and risk assessment 2013, 27, 1565­1574. Koenker, R.; Bassett, G. W. Econometrica 1978, 46, 33­50. Portnoy, S. Journal of Multivariate Analysis 1991, 38, 100­113. Portnoy, S. The Annals of Statistics 1997, 414­434. Koenker, R. Quantile regression; Cambridge university press, 2005.
43

Hallin, M.; Jurevecckov´a, J.; Picek, J.; Zahaf, T. Journal of statistical planning and inference 1999, 75, 319­330.
Hasan, M. N.; Koenker, R. W. Econometrica: Journal of the Econometric Society 1997, 133­161.
Engle, R. F.; Manganelli, S. Journal of Business and Economic Statistics 2004, 22, 367­381.
Koenker, R.; Xiao, Z. Journal of the American Statistical Association 2006, 101, 980­ 990.
White, H.; Kim, T.-H.; Manganelli, S. 2010, Barun´ik, J.; Kley, T. Available at SSRN 2678977 2015, Sewell, D. K.; Chen, Y. Journal of the American Statistical Association 2015, 110,
1646­1657. Zhao, Y.; Levina, E.; Zhu, J. Proceedings of the National Academy of Sciences 2011,
108, 7321­7326. others,, et al. The Annals of Statistics 2013, 41, 2097­2122. others,, et al. Bayesian Analysis 2016, Bickel, P. J.; Chen, A. Proceedings of the National Academy of Sciences 2009, 106,
21068­21073. Zhao, Y.; Levina, E.; Zhu, J. The Annals of Statistics 2012, 2266­2292. Zhang, J.; Chen, Y. Journal of the American Statistical Association 2013, 108, 1295­
1307.
44

Zhou, J.; Tu, Y.; Chen, Y.; Wang, H. Journal of Business & Economic Statistics 2015, To appear.
Huang, L. W. Z. H., D.; Wang, H. Working Paper 2016, Zhu, X.; Pan, R.; Li, G.; Liu, Y.; Wang, H. Annals of statistics 2016, Carrington, P. J.; Scott, J.; Wasserman, S. Models and methods in social network
analysis; Cambridge university press, 2005; Vol. 28. Newman, M. Networks: an introduction; Oxford university press, 2010. Kolaczyk, E. D.; Csa´rdi, G. Statistical analysis of network data with R; Springer, 2014. Abello, J.; Pardalos, P. M.; Resende, M. G. Handbook of massive data sets; Springer,
2013; Vol. 4. Li, G.; Li, Y.; Tsai, C.-L. Journal of the American Statistical Association 2015, 110,
246­261. Lu¨tkepohl, H. New introduction to multiple time series analysis; Springer Science &
Business Media, 2005. Chen, X.; Chen, Y.; Xiao, P. Journal of Marketing Research 2013, 50, 95­110. Fan, J.; Fan, Y. Manuscript 2010, Holland, P. W.; Leinhardt, S. Journal of the american Statistical association 1981, 76,
33­50. Wang, Y. J.; Wong, G. Y. Journal of the American Statistical Association 1987, 82,
8­19.
45

Nowicki, K.; Snijders, T. A. B. Journal of the American Statistical Association 2001, 96, 1077­1087.
Baraba´si, A.-L.; Albert, R. science 1999, 286, 509­512. Clauset, A.; Shalizi, C. R.; Newman, M. E. SIAM review 2009, 51, 661­703. Fama, E. F.; French, K. R. Journal of Financial Economics 2015, 116, 1­22. Banerjee, S.; Carlin, B. P.; Gelfand, A. E. Hierarchical modeling and analysis for spatial
data; Crc Press, 2014. Bardet, J.-M.; Doukhan, P.; Lang, G.; Ragache, N. ESAIM: Probability and Statistics
2008, 12, 154­172. Andrews, D. W. Econometric theory 1988, 4, 458­467. Gallant, A. R. Nonlinear statistical models; John Wiley & Sons, 2009; Vol. 310. Kato, K. Journal of Multivariate Analysis 2009, 100, 1816­1829. Wagener, J.; Volgushev, S.; Dette, H. Mathematical Methods of Statistics 2012, 21,
127­141. Hall, P.; Heyde, C. C. Martingale limit theory and its application; Academic press,
2014. Billingsley, P. Convergence of Probability Measures; Wiley, New York, 1968.
46

Figure 1: The left panel: dyad independence network; The middle panel: stochastic block
model; the right panel: power-law distribution network.

0
-1.0 -0.5 0.0 0.5 1.0

0
-1.0 -0.5 0.0 0.5 1.0

q

q q q q q q q

q
0.2

0.4 0.6


0.8

q

q q q q q q q

q
0.2

0.4 0.6


0.8

q

q q q q q q q

q
0.2

0.4 0.6


0.8

1
0.02 0.04 0.06 0.08

1
0.02 0.04 0.06 0.08

1
0.02 0.04 0.06 0.08

q

q q

q q q q q

q
0.2

0.4 0.6


0.8

q

q

q q q

q q

q q
0.2

0.4 0.6


0.8

q

q q q q q

q q

q
0.2

0.4 0.6


0.8

2
0.10 0.15 0.20 0.25 0.30

2
0.10 0.15 0.20 0.25 0.30

2
0.10 0.15 0.20 0.25 0.30

q

q q q q q q q

q
0.2

0.4 0.6


0.8

q

q q q q q q q

q
0.2

0.4 0.6


0.8

q

q q q q q q q

q
0.2

0.4 0.6


0.8

0
-1.0 -0.5 0.0 0.5 1.0

Figure 2: The estimated 0 to 2 against  . The top panel: dyad independence network; The
middle panel: stochastic block model; the bottom panel: power-law distribution network.

47

Table 1: Simulation Results for dyad independence network with 1000 Replications. The RMSE (×10-2) and the Coverage Probability (%) are reported for 0 to 1. The RMSE is also reported for . Lastly, the network density is computed and given. Z
stands for normal distribution and T stands for t- distribution

N Dist.

0

1

2  ND

 = 0.1

100 Z 2.60(95.0) 10.10(95.8) 2.47(94.3) 3.09 22.7

T 3.43(96.4) 11.22(95.2) 2.37(95.6) 4.17

500 Z 1.08(96.2) 4.61(95.4) 1.04(96.0) 1.32 4.7

T 1.51(95.4) 4.90(95.9) 1.03(96.1) 1.82

1000 Z 0.77(95.8) 3.29(95.0) 0.80(94.0) 0.93 2.4

T 1.06(95.8) 3.66(95.0) 0.75(95.0) 1.29

 = 0.5

100 Z 1.90(95.5) 6.62(95.4) 1.65(96.7) 2.11 22.7

T 1.99(95.7) 5.67(94.5) 1.32(93.3) 2.15

500 Z 0.84(94.4) 2.99(95.5) 0.79(94.9) 0.87 4.7

T 0.90(94.9) 2.43(96.2) 0.55(92.3) 0.91

1000 Z 0.59(94.7) 2.17(95.0) 0.53(95.7) 0.63 2.4

T 0.62(94.2) 1.77(95.0) 0.37(93.5) 0.66

 = 0.9

100 Z 2.57(95.3) 9.96(95.1) 2.49(94.1) 2.92 22.7

T 3.61(95.0) 10.61(95.4) 2.41(94.5) 3.98

500 Z 1.08(96.3) 4.27(95.8) 1.10(94.0) 1.30 4.7

T 1.53(95.6) 4.75(94.8) 1.11(93.9) 1.75

1000 Z 0.78(95.5) 3.14(95.5) 0.76(95.0) 0.90 2.4

T 1.09(95.9) 3.41(96.0) 0.84(93.5) 1.26

48

Table 2: Simulation Results for stochastic block network with 1000 Replications. The RMSE (×10-2) and the Coverage Probability (%) are reported for 0 to 1. The RMSE
is also reported for . Lastly, the network density is computed and given.

N Dist.

0 1  = 0.1

2  ND

100 Z 2.61(95.8) 3.29(94.9) 2.45(94.3) 3.03 2.6

T 3.33(96.7) 3.37(96.0) 2.40(94.2) 4.29

500 Z 1.14(94.3) 1.40(94.5) 1.08(94.9) 1.32 0.5

T 1.57(94.0) 1.50(95.1) 1.04(95.6) 1.82

1000 Z 0.79(94.6) 0.89(95.0) 0.74(95.9) 0.94 0.2

T 1.09(95.4) 0.95(94.9) 0.78(94.5) 1.28

 = 0.5

100 Z 1.88(94.5) 2.15(94.2) 1.74(95.2) 2.07 2.6

T 2.03(94.0) 1.76(95.1) 1.28(93.4) 2.17

500 Z 0.84(94.5) 0.92(94.5) 0.77(94.9) 0.90 0.5

T 0.86(94.7) 0.75(94.5) 0.52(93.2) 0.90

1000 Z 0.59(94.4) 0.59(95.9) 0.53(95.6) 0.63 0.2

T 0.61(95.4) 0.47(95.6) 0.38(93.0) 0.64

 = 0.9

100 Z 2.56(95.0) 2.91(96.0) 2.46(94.5) 2.94 2.6

T 3.44(95.8) 3.28(94.3) 2.39(94.3) 4.07

500 Z 1.08(95.4) 1.33(94.6) 1.07(95.3) 1.29 0.5

T 1.52(95.9) 1.45(95.8) 1.12(94.0) 1.78

1000 Z 0.80(95.2) 0.89(94.4) 0.75(96.0) 0.91 0.2

T 1.03(96.4) 0.90(95.3) 0.82(93.4) 1.23

49

Table 3: Simulation Results for power-law distribution network with 1000 Replications. The RMSE (×10-2) and the Coverage Probability (%) are reported for 0 to 1. The
RMSE is also reported for . Lastly, the network density is computed and given.

N Dist.

0 1  = 0.1

2  ND

100 Z 2.44(95.9) 2.95(95.4) 2.32(96.2) 3.08 2.4

T 3.45(96.3) 3.28(93.9) 2.36(95.1) 4.19

500 Z 1.09(95.5) 1.24(96.3) 1.07(95.4) 1.35 0.5

T 1.53(94.7) 1.42(94.8) 1.04(96.2) 1.79

1000 Z 0.76(95.8) 0.91(95.6) 0.77(94.7) 0.94 0.2

T 1.06(95.0) 0.99(95.5) 0.75(95.3) 1.28

 = 0.5

100 Z 1.87(95.3) 1.96(95.7) 1.79(94.4) 2.07 2.4

T 1.94(96.4) 1.55(96.2) 1.29(93.1) 2.15

500 Z 0.82(95.7) 0.85(94.6) 0.77(95.8) 0.89 0.5

T 0.90(95.5) 0.71(94.0) 0.54(93.3) 0.92

1000 Z 0.58(95.1) 0.62(94.2) 0.54(96.0) 0.62 0.2

T 0.63(94.4) 0.51(92.2) 0.37(94.6) 0.64

 = 0.9

100 Z 2.55(95.8) 2.94(93.5) 2.43(94.3) 2.91 2.4

T 3.53(95.3) 3.01(94.7) 2.43(94.1) 4.11

500 Z 1.12(95.1) 1.20(96.3) 1.09(95.1) 1.29 0.5

T 1.51(95.5) 1.33(95.1) 1.10(94.3) 1.80

1000 Z 0.79(95.2) 0.87(95.7) 0.76(95.1) 0.90 0.2

T 1.09(94.7) 0.98(95.3) 0.83(92.1) 1.26

50

Table 4: The detailed NQAR analysis results for the Stock dataset ( =0.05, 0.5, 0.95). The yearly estimates (×10-2) are reported with the standard error (×10-2) given in
parentheses. The p-values are also reported.

 = 0.05

 = 0.5

 = 0.95

Est. p-value Est. p-value Est. p-value
^0 0.05 (0.00) < 0.01 1.00 (0.04) < 0.01 2.96 (0.13) < 0.01 ^1 0.00 (0.02) 0.99 -0.04 (0.77) 0.95 6.09 (2.16) < 0.01 ^2 4.16 (0.14) < 0.01 35.70 (0.47) < 0.01 67.84 (1.13) < 0.01

SIZE 0.00 (0.01) 0.98 -1.00 (0.09) < 0.01 -4.10 (0.28) < 0.01 BM 0.00 (0.01) 0.99 -0.29 (0.04) < 0.01 -0.71 (0.25) < 0.01 PR 0.00 (0.00) 1.00 -0.30 (0.12) 0.01 0.39 (0.38) 0.31 AR -0.02 (0.03) 0.53 -0.66 (0.11) < 0.01 -0.47 (0.36) 0.20 CASH -0.01 (0.01) 0.03 -0.14 (0.06) 0.01 -0.05 (0.27) 0.86 LEV 0.00 (0.01) 0.97 -0.79 (0.05) < 0.01 -2.42 (0.44) < 0.01

Figure 3: The left panel: the average stock volatility of Chinese A stock market in 2013; the
right panel: the common shareholder network of top 100 market value stocks in 2013.
51

Frequency 200 400 600

01234 Weighted Degree

Influential Power 1.56 1.58 1.60 1.62 1.64

q qqqqq
qqq
qq q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

q

01234 Weighted Degree

0

Figure 4: The left panel: the histogram of the weighted degrees; the right panel: the
influential power against weighted degrees.

impulse ICBC

0.0025 0.0020 0.0015 0.0010 0.0005 0.0000
0.0025 0.0020 0.0015 0.0010 0.0005 0.0000
0.0025 0.0020 0.0015 0.0010 0.0005 0.0000

SPDB

PAB

CMB

BOC

BOC

0.05 q

0.5

2 4 6 8 10

CMB

ICBC

0.95 2 4 6 8 10 PAB

SPDB

qqqqqqqqqq qqqqqqqqqq qqqqqqqqqq qqqqqqqqqq

qqqqqqqqqq

qqqqqqqqqq qqqqqqqqqq qqqqqqqqqq

0.0025 0.0020 0.0015 0.0010 0.0005 0.0000

qqqqqqqqqq qqqqqqqqqq

qqqqqqqqqq qqqqqqqqqq

qqqqqqqqqq qqqqqqqqqq qqqqqqqqqq

qqqqqqqqqq

0.0025 0.0020 0.0015 0.0010 0.0005 0.0000

qqqqqqqqqq qqqqqqqqqq qqqqqqqqqq qqqqqqqqqq

2 4 6 8 10

2 4 6 8 10 step

2 4 6 8 10

Figure 5: Impulse analysis for  = 0.05, 0.5, 0.95 . The cross-sectional impulse effect intensity
between BOC, CMB, ICBC, PAB, and SPDB are given. The impulse direction is from column to row.

52

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001
002 003
004 005 006
007 008 009 010
011 012
013 014
015 016 017
018 019 020

"Downside risk and stock returns: An empirical analysis of the long-run and short-run dynamics from the G-7 Countries" by Cathy Yi-Hsuan Chen, Thomas C. Chiang and Wolfgang Karl Härdle, January 2016. "Uncertainty and Employment Dynamics in the Euro Area and the US" by Aleksei Netsunajev and Katharina Glass, January 2016. "College Admissions with Entrance Exams: Centralized versus Decentralized" by Isa E. Hafalir, Rustamdjan Hakimov, Dorothea Kübler and Morimitsu Kurino, January 2016. "Leveraged ETF options implied volatility paradox: a statistical study" by Wolfgang Karl Härdle, Sergey Nasekin and Zhiwu Hong, February 2016. "The German Labor Market Miracle, 2003 -2015: An Assessment" by Michael C. Burda, February 2016. "What Derives the Bond Portfolio Value-at-Risk: Information Roles of Macroeconomic and Financial Stress Factors" by Anthony H. Tu and Cathy Yi-Hsuan Chen, February 2016. "Budget-neutral fiscal rules targeting inflation differentials" by Maren Brede, February 2016. "Measuring the benefit from reducing income inequality in terms of GDP" by Simon Voigts, February 2016. "Solving DSGE Portfolio Choice Models with Asymmetric Countries" by Grzegorz R. Dlugoszek, February 2016. "No Role for the Hartz Reforms? Demand and Supply Factors in the German Labor Market, 1993-2014" by Michael C. Burda and Stefanie Seele, February 2016. "Cognitive Load Increases Risk Aversion" by Holger Gerhardt, Guido P. Biele, Hauke R. Heekeren, and Harald Uhlig, March 2016. "Neighborhood Effects in Wind Farm Performance: An Econometric Approach" by Matthias Ritter, Simone Pieralli and Martin Odening, March 2016. "The importance of time-varying parameters in new Keynesian models with zero lower bound" by Julien Albertini and Hong Lan, March 2016. "Aggregate Employment, Job Polarization and Inequalities: A Transatlantic Perspective" by Julien Albertini and Jean Olivier Hairault, March 2016. "The Anchoring of Inflation Expectations in the Short and in the Long Run" by Dieter Nautz, Aleksei Netsunajev and Till Strohsal, March 2016. "Irrational Exuberance and Herding in Financial Markets" by Christopher Boortz, March 2016. "Calculating Joint Confidence Bands for Impulse Response Functions using Highest Density Regions" by Helmut Lütkepohl, Anna StaszewskaBystrova and Peter Winker, March 2016. "Factorisable Sparse Tail Event Curves with Expectiles" by Wolfgang K. Härdle, Chen Huang and Shih-Kang Chao, March 2016. "International dynamics of inflation expectations" by Aleksei Netsunajev and Lars Winkelmann, May 2016. "Academic Ranking Scales in Economics: Prediction and Imdputation" by Alona Zharova, Andrija Mihoci and Wolfgang Karl Härdle, May 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

021 022
023 024
025
026 027
028 029 030
031
032 033
034 035
036
037 038
039

"CRIX or evaluating blockchain based currencies" by Simon Trimborn and Wolfgang Karl Härdle, May 2016. "Towards a national indicator for urban green space provision and environmental inequalities in Germany: Method and findings" by Henry Wüstemann, Dennis Kalisch, June 2016. "A Mortality Model for Multi-populations: A Semi-Parametric Approach" by Lei Fang, Wolfgang K. Härdle and Juhyun Park, June 2016. "Simultaneous Inference for the Partially Linear Model with a Multivariate Unknown Function when the Covariates are Measured with Errors" by Kun Ho Kim, Shih-Kang Chao and Wolfgang K. Härdle, August 2016. "Forecasting Limit Order Book Liquidity Supply-Demand Curves with Functional AutoRegressive Dynamics" by Ying Chen, Wee Song Chua and Wolfgang K. Härdle, August 2016. "VAT multipliers and pass-through dynamics" by Simon Voigts, August 2016. "Can a Bonus Overcome Moral Hazard? An Experiment on Voluntary Payments, Competition, and Reputation in Markets for Expert Services" by Vera Angelova and Tobias Regner, August 2016. "Relative Performance of Liability Rules: Experimental Evidence" by Vera Angelova, Giuseppe Attanasi, Yolande Hiriart, August 2016. "What renders financial advisors less treacherous? On commissions and reciprocity" by Vera Angelova, August 2016. "Do voluntary payments to advisors improve the quality of financial advice? An experimental sender-receiver game" by Vera Angelova and Tobias Regner, August 2016. "A first econometric analysis of the CRIX family" by Shi Chen, Cathy YiHsuan Chen, Wolfgang Karl Härdle, TM Lee and Bobby Ong, August 2016. "Specification Testing in Nonparametric Instrumental Quantile Regression" by Christoph Breunig, August 2016. "Functional Principal Component Analysis for Derivatives of Multivariate Curves" by Maria Grith, Wolfgang K. Härdle, Alois Kneip and Heiko Wagner, August 2016. "Blooming Landscapes in the West? - German reunification and the price of land." by Raphael Schoettler and Nikolaus Wolf, September 2016. "Time-Adaptive Probabilistic Forecasts of Electricity Spot Prices with Application to Risk Management." by Brenda López Cabrera , Franziska Schulz, September 2016. "Protecting Unsophisticated Applicants in School Choice through Information Disclosure" by Christian Basteck and Marco Mantovani, September 2016. "Cognitive Ability and Games of School Choice" by Christian Basteck and Marco Mantovani, Oktober 2016. "The Cross-Section of Crypto-Currencies as Financial Assets: An Overview" by Hermann Elendner, Simon Trimborn, Bobby Ong and Teik Ming Lee, Oktober 2016. "Disinflation and the Phillips Curve: Israel 1986-2015" by Rafi Melnick and Till Strohsal, Oktober 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

SFB 649 Discussion Paper Series 2016
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

040 041 042 043 044 045 046 047 048 049 050

"Principal Component Analysis in an Asymmetric Norm" by Ngoc M. Tran, Petra Burdejová, Maria Osipenko and Wolfgang K. Härdle, October 2016. "Forward Guidance under Disagreement - Evidence from the Fed's Dot Projections" by Gunda-Alexandra Detmers, October 2016. "The Impact of a Negative Labor Demand Shock on Fertility - Evidence from the Fall of the Berlin Wall" by Hannah Liepmann, October 2016. "Implications of Shadow Bank Regulation for Monetary Policy at the Zero Lower Bound" by Falk Mazelis, October 2016. "Dynamic Contracting with Long-Term Consequences: Optimal CEO Compensation and Turnover" by Suvi Vasama, October 2016. "Information Acquisition and Liquidity Dry-Ups" by Philipp Koenig and David Pothier, October 2016. "Credit Rating Score Analysis" by Wolfgang Karl Härdle, Phoon Kok Fai and David Lee Kuo Chuen, November 2016. "Time Varying Quantile Lasso" by Lenka Zbonakova, Wolfgang Karl Härdle, Phoon Kok Fai and Weining Wang, November 2016. "Unraveling of Cooperation in Dynamic Collaboration" by Suvi Vasama, November 2016. "Q3-D3-LSA" by Lukas Borke and Wolfgang K. Härdle, November 2016. "Network Quantile Autoregression" by Xuening Zhu, Weining Wang, Hangsheng Wang and Wolfgang Karl Härdle, November 2016.

SFSBF6B4694, 9S,pSapnadnaduaeureSrtrSatßraeß1e, 1D,-D10-1107187B8eBrleinrlin htthpt:t/p/:/s/fbs6fb4694.w9.iwwiiw.hiu.h-bue-brleinrl.idne.de
ThTishrisesreasercahrcwhawsassupsuppoprtoerdtebdybtyhethDeeDuetsucthseche ForFsocrhsuchnugnsgesgmeeminesicnhsachftatfht rtohuroguhgthhethSeFSBF6B4694"9Ec"oEnconmoimc RicisRki"s.k".

