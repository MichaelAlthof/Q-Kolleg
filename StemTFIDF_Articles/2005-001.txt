BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2005-001
Nonparametric Risk Management with
Generalized Hyperbolic Distributions
Ying Chen* Wolfgang Härdle* Seok-Oh Jeong**
* CASE - Center for Applied Statistics and Economics, Humboldt-Universität zu Berlin, Germany
** Institute de Statistique, Université Catholique de Louvain, Belgium
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Nonparametric Risk Management with Generalized Hyperbolic Distributions
Chen, Ying Ha¨rdle, Wolfgang and Jeong, Seok-Oh CASE - Center for Applied Statistics and Economics Humboldt-Universita¨t zu Berlin Wirtschaftswissenschaftliche Fakulta¨t Spandauerstrasse 1, 10178 Berlin Germany  Institut de Statistique Universit´e Catholique de Louvain Voie du Roman Pays, 20 1348 Louvain-la-Neuve Belgium
1st August 2005
Abstract
In this paper we propose the GHADA risk management model that is based on the generalized hyperbolic (GH) distribution and on a nonparametric adaptive methodology. Compared to the normal distribution, the GH distribution possesses semi-heavy tails and represents the financial risk factors more appropriately. The nonparametric adaptive methodology has the desirable property of estimating homogeneous volatility in a short time interval. For DEM/USD exchange rate data and a German bank portfolio data the proposed GHADA model provides more accurate value at risk calculation than the traditional model based on the normal distribution. All calculations and simulations are done with XploRe.
Keywords: adaptive volatility estimation, generalized hyperbolic distribution, value at risk, risk management.
Acknowledgement: This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk" and the SFB 373 "Simulation and Quantification of
1

Economic Processes" at Humboldt-Universit¨at zu Berlin. Special thanks are due to Prof. Dr. Ernst Eberlein for his kind contribution to the proof of Lemma 1.
1 INTRODUCTION
One of the most challenging tasks in the analysis of financial markets is to measure and manage risks properly. After the breakdown of the fixed exchange rate system of the Bretton Woods Agreement in 1971, a sudden increase of volatility was observed in the financial markets. The following boom of financial derivatives accelerated the turbulence of the markets. The incoming scale of losses astonished the world and pushed the development of sound risk management systems. Financial risks have many sources and are typically mapped into a stochastic framework where various kinds of risk measures such as Value at Risk (VaR), expected shortfall, lower partial moments are calculated. Among them, VaR has become the standard measure of the market risk since J.P. Morgan launched RiskMetrics in 1994, making the analysis of VaR simple and standard, Jorion (2001).
The importance of VaR was even reinforced after it was used by the central banks to govern and supervise the capital adequacy of the banks in the Group of Ten (G10) countries in 1995. Mathematically VaR at p probability level is defined as:
V aRp,t = Ft-1(p),
where Ft-1 is the inverse function of the conditional cumulative distribution function of the underlying at time t, Franke, Ha¨rdle and Hafner (2004). From the definition, it is clear that the accuracy of VaR and the other risk measures depends heavily on the assumption of the underlying distribution. In the literature, for reasons of stochastic and numerical simplicity, it is often assumed that the involved risk factors are normally distributed. This is done e.g. in the RiskMetrics framework. However empirical studies have shown that financial risk factors have leptokurtic distributions which include a high peak and fat tails.
Figure 1 illustrates this fact on the basis of the daily standardized (devolatilized) returns of the foreign exchange (FX) rates DEM/USD from 1979/12/01 to 1994/04/01. The nonparametrically estimated kernel density and log kernel density obviously deviate from the normal density. In order to capture this empirical fact, the heavy-tailed distribution families such as the hyperbolic distribution have been attracting the attention of the researchers. Conditional Gaussian models can mimic the fat tails as well and were found to perform well at a moderate VaR (e.g. 95%) confidence level. Nevertheless, they are unsatisfactory for the extreme events such as profit and loss (P&L) at 99% confidence level, Jaschke and Jiang (2002). Recently, Eberlein, Kallsen and Kristen (2003) applied the generalized hyperbolic
2

(GH) distribution to the VaR calculation. Based on their empirical studies, the model with GH distribution gave more accurate VaR values than that with the normal distribution.

Estimated density (nonparametric)

Estimated log density (nonparametric)

Y -8 -6 -4 -2

Y 0.1 0.2 0.3 0.4

0

-4 -2

0

2

4

X

-4 -2

0

2

4

X

Figure 1: Graphical comparison of the density (left) and the log-density (right) of the daily DEM/USD standardized returns from 1979/12/01 to 1994/04/01 (3719 observations). The kernel density estimate is graphed as a line and the normal density as dots with h  0.54.
GHADAfx.xpl

In addition to the distributional tail assumption, the usual heteroscedastic model on the returns Rt:
Rt = tt,
where t denotes the volatility and t the stochastic term, suggests that the role of the volatility model is of great significance. The most often used volatility estimation methods are not uniformly applicable in risk management. ARCH (Engle, 1995), GARCH (Bollerslev, 1995) and stochastic volatility models (Harvey and Shephard, 1995) are used to estimate or forecast volatility in specified time periods. This cannot be applied for long time series since the form of the volatility model is time-unstable with a very high possibility. It is therefore plausible to use more flexible methods by providing a data-driven "local" model, which can avoid this potential miss-specification problem. In Eberlein et al. (2003), parametric volatility models of GARCH type were studied and compared with a nonparametric approach using the rectangular moving average. They argued that the GARCH (1,1) model performed superior. However this "under-performance" of the nonparametric model can possibly be improved by an adaptive methodology. Mercurio and
3

Spokoiny (2004) proposed such an improvement by adaptively estimating the volatility. A simple "local constant" model was constructed but an intrinsic assumption in their study was the normality of the risk factors.
In this paper we intend to improve the risk management models by combining:
a. a heavy-tailed distribution family to mimic the empirical distribution of the underlying risk factors, and
b. a nonparametric adaptive methodology to estimate and forecast the local volatilities.
Motivated by the above two research lines, we combine these two approaches: we estimate the volatility adaptively and model the heavy-tailed risk factors by the GH distribution. Here we name this new VaR technique as Generalized Hyperbolic Adaptive Volatility (GHADA) technique. The devolatilized return density plot in Figure 1 is in fact calculated with the GHADA technique.
The paper is organized as follows. In Section 2 we will discuss the properties of the GH distribution and its subclasses. The adaptive volatility estimation methodology will be described based on the GH distribution. The validation of the GHADA technique is shown via Monte Carlo simulation. In Section 3 several VaR calculations will be presented based on a DEM/USD series and a German bank portfolio data. According to the backtesting result, the GHADA technique provides more accurate precision than the model with the normal distribution. Finally we will conclude our study in Section 4. All the pictures may be recalculated and redrawn using the indicated link to an XploRe Quantlet Server.

2 PILLARS

Let Rt = log St - log St-1 denote the (log) return where St is the asset price at time point t for t = 1, 2, .., T. The return process is modelled in a heteroscedastic form:

Rt = tt,

(1)

where t is assumed to be independently and identically distributed (i.i.d.) with E(t) = 0 and Var (t) = 1. Volatility t is time varying and unobservable in the market. In the case that volatility is measurable with respect to a -field Ft-1 generated by the preceding returns R1, . . . , Rt-1, the variance t2 can be interpreted as the conditional variance of the return.
In risk management models we are interested in estimating the future return distribution accurately. It depends on the estimation of volatility t and the assumption on the

4

stochastic term t. In this paper, we use a nonparametric algorithm to estimate the volatility "locally", avoiding the potential mis-specification problem. Details will be discussed in Section 2.2. Another key factor, the distribution assumption of the stochastic term, influences the performance of the risk management procedures to a great extent. VaR is defined through a pre-decided quantile of the P&L distribution. Models based on the normality assumption achieve almost the same values at the 5% quantile (95% confidence level) as those with a leptokurtic (more heavy tailed) distribution. This explains to a certain extent the popularity of the normal distribution in the risk management models although the financial risk factors are empirically leptokurtic distributed. However concerning the extreme events, we need to consider lower quantiles such as 1% quantile. The difference relative to the normal becomes larger for lower quantiles of course. In this case, the normality assumption becomes invalid. Therefore in the next section, we concentrate on a heavy-tailed distribution, the generalized hyperbolic distribution, and discuss the application of this distribution family in risk management.

2.1 Generalized Hyperbolic Distribution

The GH distribution introduced by Barndorff-Nielsen (1977) is a heavy-tailed distribution that can well replicate the empirical distribution of the financial risk factors. The density of the GH distribution for x  IR is:

fGH (x; , , , , µ)

=

 (/) 2K()

K-1/2 2 +

 (x

2 + (x - µ)2
1/2-
- µ)2/

· e(x-µ)

(2)

under the conditions:

·   0, || <  if  > 0 ·  > 0, || <  if  = 0 ·  > 0, ||   if  < 0

where , , ,  and µ  IR are the GH parameters, 2 = 2 - 2. The density's location and scale are mainly controlled by µ and  respectively:

E[X] = µ + 2 K+1()  K()

Var[X ]

=

2

K+1()

+

 (

)2[ K+2()

-

{ K+1() }2]

,

K()  K()

K()

whereas  and  play roles in the skewness and kurtosis of the distribution. For more details of the parameters' domains we refer to Bibby and Sørensen (2001). K(·) is the modified

5

Bessel function of the third kind with index , Barndorff-Nielsen and Blæsild (1981):

1 K(x) = 2

 y-1exp{- x (y + y-1)} dy 02

Furthermore, the GH distribution has semi-heavy tails:

fGH (x; , , , , µ = 0)  x-1e-(-)x as x  ,

(3)

where a(x)  b(x) as x   means that both a(x)/b(x) and b(x)/a(x) are bounded as x  . Compared to the normal distribution, the GH distribution decays more slowly. However compared to the other two heavy-tailed distributions: Laplace distribution and Cauchy distribution, the decaying speed of the GH distribution is often faster. The Laplace distribution is also called double exponential distribution with the form:

fLaplace

=

1 e-|x-µ|/ 2

where µ is the location parameter and  is the scale parameter. The Cauchy distribution is defined as:

1 fCauchy = [1 + (x - M )2/2]

where M is the median and  is the scale parameter.

In Figure 2 we compared these four distributions and especially their tail-behavior. In

order to keep the comparability of these distributions, we specified the means to 0 and

standardized the variances to 1. Furthermore we used one important subclasses of the GH

distribution:

the

normal-inverse

Gaussian

(NIG)

distribution

with



=

-

1 2

introduced

more

precisely in the following text. On the left panel, the complete forms of these distributions

are revealed. The Cauchy (dots) distribution has the lowest peak and the fattest tails, in

other words, it has the flattest distribution. The NIG distribution decays second fast in

the tails although it has the highest peak, which is more clearly displayed on the right

panel. Generally the GH distribution has an exponential decaying speed as shown in (3).

By changing  the GH distribution family covers a wide range of tail behavior. A parameter

 > 1 introduces heavier tails than the double exponential. One may therefore think of  as

the tail control parameter with which one (loosely speaking) may model the range between

a normal and a Cauchy tail.

The moment generating function of the GH distribution is:

mf (z)

=

eµz

·

 z

·

K(z) , K()

| + z| < ,

(4)

6

10 15

Y 0.1 0.2 0.3 0.4 0.5

Distribution comparison
NIG Laplace Normal Cauchy

Tail comparison
Cauchy

Y*E-3

5

Laplace

0

-5 0 X

5

0

NIG Normal
-5 -4.5 -4 X

Figure 2: Graphical comparison of the NIG distribution (line), standard normal distribution (dashed), Laplace distribution (dotted) and Cauchy distribution (dots).
GHADAtail.xpl

where 2z = 2 - ( + z)2. The GH distribution has the property that mf is infinitely many times differentiable near 0, as a result every moment of a GH variable exists. In Section 2.2, this feature and the tail behavior (3) of the GH distribution will be used in the adaptive volatility estimation methodology.
In the current literature, subclasses of the GH distribution such as the hyperbolic (HYP) or the normal-inverse Gaussian (NIG) distribution are frequently used. This is motivated by the fact that the four parameters (µ, , , ) simultaneously control the four moment functions of the distribution, i.e. the trend, the riskiness, the asymmetry and the likeliness of the extreme events. Eberlein and Keller (1995), Barndorff-Nielsen (1997) have shown that these subclasses are rich enough to model financial time series and have the benefit of numerical tractability. Therefore in our study we concentrate ourselves on these two important subclasses of the GH distribution: HYP with  = 1 and NIG distribution with  = -1/2. The corresponding density functions are given as:

· Hyperbolic (HYP) distribution:  = 1,

fHY P (x; , , , µ)

=



 e{- 2+(x-µ)2+(x-µ)},

2K1()

where x, µ  IR, 0   and || < ,

(5)

7

· Normal-inverse Gaussian (NIG) distribution:  = -1/2,

 K1 fNIG(x; , , , µ) = 

 2 + (x - µ)2 2 + (x - µ)2

e{+(x-µ)}.

where x, µ  IR,  > 0 and ||  .

(6)

In order to estimate the unknown parameters (, , , µ) , the maximum likelihood (ML) and numerical optimization methods are used. For an i.i.d HYP resp. NIG distributed variable X, the log-likelihood functions are:

LHY P = T log  - T log 2 - T log  - T log  - T log K1()

(7)

T

+ {- 2 + (xt - µ)2 + (xt - µ)}

t=1

LNIG = T log  + T log  - T log  + T 

(8)

T
+ log K1
t=1



2 + (xt - µ)2

-

1 2

log{2

+

(xt

-

µ)2}

+

(xt

-

µ)

Figure 3 shows the estimated HYP and NIG densities with the corresponding ML estimators of the DEM/USD devolatilized returns. It can be seen that the estimated densities almost coincide with the empirical density and log-density of the financial risk factor. The empirical density f^h(x) (line) was estimated by the kernel estimation:

f^h(x)

=

1 nh

n i=1

x K(

- Xi ), h

(9)

where n is the number of observations and K is the kernel function which gives weights

to the observations according to the distances of them to the fixed point x. The further

an observation is from the fixed point, the smaller weight it will be given. We chose the

Quartic

kernel

function

with

a

closed

form:

K (u)

=

15 16

(1

-

u2)21(|u|

 1),

where

1(·)

is

the indicator function which has a value of 1 if the condition in the parenthesis exists, and

a value of 0 otherwise. In addition, we used the Silverman's rule of thumb to select the

bandwidth h:

h^rot  1.06^n-1/5,

where ^ is the empirical standard deviation of the variable X. Since the rule of thumb assumes that the unknown density belongs to the normal family and we chose the Quartic kernel, the bandwidth was adjusted to h^  2.62h^rot = 2.78^n-1/5 using the canonical bandwidths. For details of the kernel and the bandwidth selections, see Chapter 3 in H¨ardle, Mu¨ller, Sperlich and Werwatz (2004). Compared to the normal distribution in Figure 1, it is convincing that the GH distribution family can represent the empirical distribution of financial data better.

8

Estimated fx density (HYP)

Estimated fx log density (HYP)

Y -4 -2

Y 0.1 0.2 0.3 0.4

-6

0

-4 -2 0 2 4 X
Estimated fx density (NIG)

-4 -2 0 2 4 X
Estimated fx log density (NIG)

Y -4 -2

Y 0.1 0.2 0.3 0.4

-6

0

-4 -2 0 2 4 X

-4 -2 0 2 4 X

Figure 3: The kernel estimated density (left) and log density (right) of the standardize return of FX rates (line) (h  0.55). The HYP density (dashed line) on the top with the maximum likelihood estimators ^ = 1.744, ^ = -0.017, ^ = 0.782, µ^ = 0.012 and a
simulated NIG density (dashed line) on the bottom with the maximum likelihood estimators ^ = 1.340, ^ = -0.015, ^ = 1.337, µ^ = 0.010.
GHADAfx.xpl

2.2 Adaptive Volatility Estimation
The basic idea of adaptive volatility estimation comes from the observation that although the volatility is heteroscedastic in a long time period, its change in a short time interval, a socalled time homogeneous interval, is very small. Evidence for this argument has been given by Mercurio and Spokoiny (2004). According to time homogeneity, one specifies an interval I = [ -m,  ) for a fixed time point  with 0  m   -1, where the volatility t, t  I is almost constant. One may for example estimate in this case the local constant volatility  by

9

averaging the past squared returns Rt2 for t  I:

^2

=

1 |I |

tI

Rt2,

(10)

where |I| is the cardinality of I. Two questions arise in this procedure: how well does this estimate work and how to specify the homogeneous interval I?

The squared returns Rt2 are always nonnegative and have for the stochastic errors t (i.i.d. GH, HYP or NIG) a skewed distribution. In order to apply an interval selection procedure for I we use the power transformation of the return Rt:

|Rt| = C t + D t ,t

(11)

where  is the power transformation parameter, ,t = (|t| -C)/D are standardized i.i.d. innovations, C = E(|||Ft-1) is the conditional mean of ,t and D2 = E[(|| - C)2|Ft-1] is the conditional variance of ,t.

Additionally, lighter tails are obtained after this power transformation. Such a tail behavior is required later to derive theoretical properties of the estimate later. Let us denote the conditional mean of the transformed return |Rt| by t:

t = C t .

(12)

Since C is a constant given a fixed , the estimate of the volatility t is proportional to t. In a time homogeneous interval I the local parameter t is constant for t  I and is

estimated by:

^I

=

1 |I |

tI

|Rt|

.

(13)

Writing |Rt| out in (13), one has:

^I

=

1 |I| tI t

+

s |I |

tI

t

t

where s = D/C. One sees that the multiplicative error structure (1) is turned via (11) into an additive one and the random variable |Rt| distributes more evenly. Straightforwardly, one can calculate the conditional expectation and variance of the estimate ^I :

E[^I |F-1]

=

1 E |I| tI t,

vI2 = Var [^I |F-1]

=

s2 |I |2

E( tt)2
tI

=

s2 |I |2

E t2.
tI

10

In a time homogeneous interval I, the volatilities are expected to be time invariant, therefore ^I can be considered as an estimate of t for each time point t  I. Therefore vI can be estimated by:
v^I = s ^I |I|-1/2.
In other words, the volatility estimate t can be induced from an estimate t. However the specification of the local homogeneous interval is still open. Mercurio and Spokoiny (2004) have derived a homogeneity test for a supermartingale process. We show that a supermartingale of GH distributed variable can be obtained from the following lemma. It therefore leads to the same homogeneity test theory.

LEMMA 1 For every 0    1 there exists a constant a > 0 such that log E[eu ]  au2 , 2
where  = (|| - C)/D is the transformed GH distributed variable .

The proof of this lemma is given in the Appendix.

Consider a predictable process pt (such as the volatility t or the local parameter t) w.r.t. the information set Ft-1:

tt

t = exp

pss - (a/2) ps2

s=1 s=1

tis a supermartingale, since

E(t|Ft-1) - t-1 = E(t|Ft-1) - E(t-1|Ft-1)

tt

= E[exp

pss - (a/2) ps2

s=1 s=1

t-1 t-1

- exp

pss - (a/2) ps2 |Ft-1]

s=1 s=1

t-1 t-1

= E[exp

pss - (a/2) ps2 {exp(ptt - a/2p2t ) - 1}|Ft-1]

s=1 s=1

=

exp(p11) exp(a /2p1 )

·

·

·

exp(pt-1t-1) exp(a /2pt-1 )

·

E[

exp(ptt) exp(a /2pt )

-1|Ft-1]

1,Lemma1
0

1

1

i.e. E(t|Ft-1)  t-1. With this supermartingale property, the statistical properties of ^I are given in:

11

THEOREM 1 If R1, ..., R obey the heteroscedastic model and the residual  satisfies
Lemma 1. Furthermore, the volatility coefficient t satisfies the condition b  t2  bB with some positive constants b and B, then it holds for the estimate ^I of  :

P

{|^I

-



|

>

I (1

+

s |I|-1/2)

+

v^I }



 4 e(1

+

log

B)

exp{- 2a (1

+

2 s |I|-1/2)2

}.

where I is the squared bias defined as 2I = |I|-1 tI (t -  )2.

Theorem 1 indicates that the estimation error |^I -  | is small relative to v^I for   I with

a high probability, if I is a time homogeneous interval and therefore the squared bias I is

negligible. Straightforwardly, the following condition can be used to test the homogeneity

hypothesis in an interval I:

|^I -  |  v^I .

In the test, I is split into two subintervals: I\J and J. If I is a time homogeneous interval, the estimates based on the two subintervals must be very close. The homogeneity condition can be stated as:

|^I\J - ^J |  (v^I\J + v^J ) =  ( ^J2 |J |-1 + ^I2\J |I\J |-1).

(14)

provided  = s sufficiently large. If condition (14) is violated, the homogeneity hypothesis for the interval I is rejected.
The test procedure starts from an initial small interval I that satisfies the homogeneity and consists of 4 steps:

Step 1: Enlarge the interval I from [ - m0,  ) to [ - k × m0,  ), i.e. m = k × m0, and split the new interval into two subintervals J and I\J. The parameters m0 and k are integers specified according to data. In this paper, we chose m0 = 5 and k = 2.

Step 2:

Start homogeneity test for interval J

=

[

-

2 3

m,



).

If

the

homogeneity

hypothesis

isn't

rejected,

enlarge

J

one

point

further

to

[

-

2 3

m

-

1,

)

and

repeat

the

homogeneity test (14). The loop will continue until the left point of the subinterval J

reaches

the

point



-

1 3

m.

The

choice

of

1 3

comes

from

the

fact

that

the

right

1 3

part

has been tested in the last homogeneous interval and the left one-thirds will be tested

in the next homogeneous interval, Mercurio and Spokoiny (2004).

Step 3: If (14) is violated at point s, the loop stops and the time homogeneous interval I is specified from point  to point s + 1.

Step 4: If time homogeneity holds for this interval, go back to Step 1.

The largest interval I is finally chosen as the time homogeneous interval for point  , based on 12

which the local volatility  is estimated. However there are still two threshold parameters

to be specified:  in the power transformation and  in the homogeneity test condition.

According to Lemma 1, the parameter  is bounded in [0, 1]. In our study, we chose  = 0.5

as same as the model based on the normal distribution to satisfy the comparability. The

value of  is similar to a smoothing parameter of the nonparametric regression. We thus

propose a nonparametric way to pick up a global  . Given a starting point t0 and provided

that there are enough past observations to estimate ^(t0, ), the value  minimizes the

forecast error:

 -1
 = argmin

|Rt| - ^(t, )

2
.

(15)

t=t0

2.3 Monte Carlo simulation

The GHADA technique consists of two main parts: estimate the GH distribution parameters, from which one calculates the quantile of the P&L, and predict the volatility using the adaptive methodology. The calculation procedure can be described as:

1. Select the transformation parameter  and a starting point t0.
2. Given different  s, estimate the local volatilities using the adaptive methodology. Choose the  which minimizes the forecast error and the corresponding estimated volatility process ^t.
3. Calculate the devolatilized returns t = Rt/^t and estimate the GH parameters.
4. Calculate the V aRt+1, multiplying the volatility forecast ~t+1 and the quantile of the devolatilized return.

The previous calculations and the comparison in Section 2.1 have provided evidence that the GH distributions can represent the empirical distribution of the underlying well. In this section, we focus on the volatility estimation. In order to check the performance of the adaptive methodology, Monte Carlo simulation was applied. We intend to estimate the local volatility on the basis of simulated HYP and NIG variables and to analyze the sensitivity of the GHADA algorithm to jumps of the volatility. We considered two volatility processes:

  0.01 : 1  t  400   
1,t = 0.05 : 400 < t  750





 

0.01

:

750 < t  1000

  |0.02t - 5|   
2,t = |0.02t - 10|

: 1  t  300 : 300 < t  600





 

|0.12t - 100|

:

600 < t  1000

(16) (17)

13

We simulated 200 HYP and 200 NIG random variables. Each series consists of 1000 observations. We constructed the HYP return series by multiplying the HYP variables and the volatility 1,t. The NIG return series were constructed by multiplying the NIG trajectories and the volatility 2,t.

We applied the GHADA algorithm to estimate the local volatilities of these simulated returns. The first 200 observations were used to estimate local volatility at the starting point. The transformation parameter  was pre-set as 0.5. The value  that minimized the forecast error was selected and used in the homogeneity test. We repeated the multiple homogeneity test and specified the homogeneity intervals for the points from 201 to 1000. Two examples of the estimated local constant volatility series - one belongs to the HYP distributed returns and the other the NIG returns - are displayed in Figure 4. The estimated volatility processes for the simulated 200 HYP and 200 NIG distributed returns can be downloaded at http://ise.wiwi.hu-berlin.de/ychen/ghada/ : simulation1.AVI and simulation2.AVI respectively. Compared to the true volatility processes (dashed line), these estimates (straight line) satisfactorily represent the movements and the sudden changes of the volatility process. To study the sensitivity of our approach, we introduced a percentage rule that told us after how many steps the sudden volatility jump is detected at 40%, 50% or 60% level of the jump size. The 40% rule, for example, refers to the number of time steps necessary to reach 40% of the jump size. Table 1 gives an overview of the detection delays. One interesting feature of the GHADA approach is its out-performance of catching up a sudden increasing jump in the volatility process. Concerning the volatility process 1, the first jump (occurring at the 401st point) was at 40% level on average detected after 6 steps. This number increases to 8 for the 60% rule of the jump. In contrast, the detection speed is slow for downward jumps. For the second jump at t = 750, where the volatility decreases from 5% to 1%, the yields 40% rule about 12 steps, twice slower reacting than that to the increasing jump with the same quantity. This fact results from a loose test power. In the homogeneity test (14), the squared conditional variance vI depends on t, a larger value of t thus induces a loose test power. Consequently, a slower detection speed occurs in the jump down from a high value. Additionally, we considered two jumps in the second volatility process 2. The mean values of the steps detecting 40%, 50% and 60% of these two jumps at t = 300 and t = 600 are even smaller than those in 1.

The

mean

squared

error

(MSE)

of

the

estimation

1 800

1000 t=201

(^1,t

- 1,t)2

is

3.26(10-5)

for the simulation 115 (HYP). Figure 5 illustrates the mean process and the 99% confidence

interval of the estimates. At the jump point the estimate is turbulent and the confidence

interval increases. In the NIG case, the volatility process (2) is more volatile and not be

constant in a short interval any more. Nevertheless the GHADA model gave fine results

and the big changes of the volatility movement were catched in about 6 points as well. The

MSE of the simulation example is 6.88. From Figure 6 one can further see that the mean

process repeats the movement of the volatility process. The confidence interval around the

14

Y*E-2 1 23 4 5 6

Y 5 10 15 20 25

sim 115
0 5 10 X*E2
sim 44
0 5 10 X*E2
Figure 4: The estimated volatility processes on the basis of two simulated examples (dashed line): sim 115 (HYP) and sim 44 (NIG). The power transformation parameter  = 0.5 and the starting point t0 = 201.
GHADAsim1.xpl GHADAsim2.xpl biggest change is larger than the HYP example. The scale of the volatility process could be one reason for the large span.
15

0

mean standard deviation maximum minimum

Detection decay to the first jump at t = 400 - 1

40% rule 5.9

2.4 15

1

50% rule 6.9

2.6 19

2

60% rule 7.9

2.9 19

2

Detection decay to the second jump at t = 750 - 1

40% rule 11.8

4.4 39

3

50% rule 13.5

6.5 58

5

60% rule 15.9

10.9 98

6

Detection decay to the first jump at t = 300 - 2

40% rule 4.9

2.4 13

0

50% rule 6.2

3.0 18

2

60% rule 7.6

4.2 33

2

Detection decay to the first jump at t = 600 - 2

40% rule 4.7

1.9 12

0

50% rule 5.7

2.7 23

2

60% rule 6.8

3.4 24

2

Table 1: Descriptive statistics for the detecting speeds to the sudden jumps of the volatility processes.
3 RISK MANAGEMENT

3.1 Data Set

Two data sets: DEM/USD exchange rate and a German bank portfolio were used in our empirical analysis. They are available at MD*Base (www.quantlet.org/mdbase).

The exchange rate is daily registered from 1979/12/01 to 1994/04/01. There are 3720 observations. The first 500 observations are used as a basis to estimate the local volatility and the GH distribution parameters. The bank portfolio data reports the market value of the portfolio holden by a German bank. There are 5603 observations. Figure 1 and Figure 3 have shown the empirical (log) densities and the estimated HYP and NIG densities of the exchange rate data. We estimated the GH distribution parameters using the maximum likelihood method. Using the proposed GHADA approach, the local volatility estimates of the DEM/USD exchange rates are displayed in Figure 7 where t0 is 501 and  = 1.06. Figure 8 and Figure 9 show the respective estimates of the bank portfolio data.

The devolatilized return

^t = Rt/^t

(18)

16

Y*E-2 5

0

Y 10 20 30 40 50

Estimation mean and 99% confidence interval - HYP
2 4 6 8 10 X*E2
Figure 5: The mean process of the local volatility estimates based on 200 simulations and the 99% point confidence interval (dashed line) of the estimation.
GHADAsim1.xpl
Estimation mean and 99% confidence interval - NIG
2 4 6 8 10 X*E2
Figure 6: The mean process of the local volatility estimates based on 200 simulations and the 99% point confidence interval (dashed line) of the estimation.
GHADAsim2.xpl is expected to be i.i.d. but not necessarily normally distributed. We assume that the devolatilized return is HYP or NIG distributed. Table 2 summarizes the descriptive statistics of the devolatilized returns for the exchange rate data and the bank portfolio data. The first and the second moments are close to 0 and 1, but the kurtoses of these two data sets are high with respect to the normal distribution. The graphics of the devolatilized return processes are displayed in Figure 7 and Figure 9. Figure 10 shows the boxplots of the time homogeneous intervals' length for these two data sets. The average interval length for the exchange rate data is 51 while the length for the German bank portfolio data is 71. It can be seen that the spans of the interval length are wide for two data sets. It provides an evidence
17

0

mean std skewness kurtosis

exchange rate data -0.0052 0.9938 -0.0121 4.0329

bank portfolio data 0.0113 0.9264 -0.0815 5.1873

Table 2: Descriptive statistics for the devolatilized residuals of the exchange rate data and bank portfolio data.
that the volatility model changes from time to time. Compared to a fixed time-invariant model, the adaptive model is flexible and simple to catch the fluctuation of the volatility.

3.2 Value at Risk
Value at risk (VaR) is one of the most often used risk measure. It measures the possible loss level over a given horizon at a given confidence level 1 - p and answers the question: How much can I lose with p probability over the pre-set horizon. The research on VaR models has been ignited and prompted by the rule of Basel Committee on Banking Supervision in 1995: financial institutions may use their internal VaR models. The selection of the internal VaR model as well as the volatility estimation is essential to the VaR based risk management. Let qp denote the p-th quantile of the distribution of t, i.e. P (t < qp) = p, we have P (Rt < tqp | Ft-1) = p. Mathematically the VaR is defined as:
VaRp,t = Ft-1(p) = tqp.
where Ft-1 is the inverse function of the conditional cumulative distribution function of the underlying at time t, Franke et al. (2004).
In practice, we are interested in the forecast of VaR. Using the GHADA approach, we adaptively estimated the volatility ^t. Since the volatility process is a supermartingale, it is natural to use the estimate today as the volatility forecast ~t+1 for tomorrow, i.e. ~t+1 = ^t. Furthermore we estimated the HYP and NIG distribution parameters of the devolatilized returns and calculated the quantile qp. The VaR at the probability level p was forecasted as:
Va~Rp,t+1 = ~t+1q^p.
As same as the volatility model, the distribution parameters could be time-variant as well. Figure 11 shows the HYP-quantile forecasts based on the 500 past devolatilized returns of the exchange rate for each time point. It provides an evidence that the quantile varies as time passes, especially for extreme probability levels such as p = 0.005. In this context we

18

Y*E-2 -4 -2 0 2

10 15

Y*E-3

5

Daily DEM/USD returns from 1979/12/01 to 1994/04/01
0 12 3 X*E3
Adaptive local constant volatility estimators (exchange rate)
5 10 15 20 25 30 35 X*E2
Figure 7: The return process of DEM/USD exchange rates (upper) and its adaptive volatility estimates (lower) for t0 = 501 and  = 1.06.
GHADAfx.xpl couldn't stick to the assumption that the innovations are identically distributed. Instead, we updated the distribution parameters daily based on the previous 500 data points. The same phenomenon holds for NIG distribution, which is omitted here.
The observations whose losses exceed the VaR are called exceptions. The daily VaR forecasts of the DEM/USD rates are displayed in Figure 12 and Figure 13. The VaR forecasts are different between the GHADA model and the model with the normal distribution (normal model). As we have discussed, the VaRs based on the normal distribution are almost identical to the HYP distribution at the 5% probability level. Sometime the normal model performs even better than the GHADA model. However at the 5% level, there are more than 169 exceptions observed in 17 years, i.e. more than 12 exceptions annually. In order to control the risk exposure to the market, the extreme events (lower levels) should
19

Estimated bank portfolio density (HYP)

Estimated bank portfolio log density (HYP)

Y 0.2 0.4 0.6

Y -10 -5

0

-5 0 X
Estimated bank portfolio density (NIG)

-5 0 X
Estimated bank portfolio log density (NIG)

Y -8 -6 -4 -2

Y 0.1 0.2 0.3 0.4 0.5 0.6

0

-5 0 X

-5 0 X

Figure 8: The estimated density (left) and log density (right) of the standardize return of the German bank portfolio rates (red) with nonparametric kernel (h  0.61) and the
estimated HYP density (blue) on the top with the maximum likelihood estimators ^ = 1.819, ^ = -0.168, ^ = 0.705, µ^ = 0.145 and the estimated NIG density (blue) with the maximum likelihood estimators ^ = 1.415, ^ = -0.171, ^ = 1.254 and µ^ = 0.146.
GHADAkupfer.xpl

also be considered. It is obvious that as the probability level decreases to some extreme values such as 1% or 0.5%, the gaps of these two models get larger and larger. Except in the 5% case, the GHADA model is superior to the normal model at the other three levels of 2.5%, 1% and 0.5%.

20

Daily returns of a German bank

Y*E-2 -10 -5 0 5 10

01 23 45
X*E3
Adaptive local constant volatility estimators (bank portfolio)

Y*E-2 12345

12 345 X*E3

Figure 9: The return process of a German bank's portfolio (upper) and its adaptive volatility estimates (lower) for t0 = 501 and  = 1.23. The average length of time homogeneous interval is 71.
GHADAkupfer.xpl
3.3 Backtesting VaR

To evaluate the validation of the VaR calculation, consider the backtesting procedures

presented in Christoffersen (1998). Above all, a VaR calculation should not underestimate

the market risk. Let 1t denote the indicator of exceptions at time point t, t = 1, 2, . . . , T .

If the proportion of exceptions, N/T = T -1

T t=1

1t,

is

much

larger than

p,

i.e.

N/T

> p,

it means that the possible losses happen more often than the fixed level. In this case, the

VaR model should be rejected. One constructs a hypothesis to test:

H0 : E[N ] = T p vs. H1 : E[N ] = T p

(19)

21

1. Exchange rate

2. Bank portfolio

Figure 10: Boxplots of the DEM/USD exchange rates (left) and the German bank portfolio data (right).

Under H0, N is a Binomial random variable with parameters T and p, the likelihood ratio test statistic can be derived as:

LR1 = -2 log {(1 - p)T -N pN } + 2 log {(1 - N/T )T -N (N/T )N },

(20)

which is asymptotically 2(1) distributed, Jorion (2001).

In addition, a VaR model that yields exception clusters should also be rejected. Since a

cluster of VaR exceedances means that if there is an exception today, an exception may also

occur tomorrow with a higher probability than the prescribed level p. Another important

test is the test of independence. Let us denote ij = P (1t = j|1t-1 = i) as the transition

probability and nij =

T t=1

1(1t

=j

and

1t-1

=

i),

where

i, j

=0

or

1.

The

independence

hypothesis is given as:

H0 : 00 = 10 = , 01 = 11 = 1 - 

(21)

One can test this hypothesis using the likelihood ratio statistic:

LR2 = -2 log {^n0 (1 - ^)n1 } + 2 log {^0n000 ^0n101 ^1n010 ^1n111 },

(22)

where ^ij = nij/(nij + ni,1-j), nj = n0j + n1j, and ^ = n0/(n0 + n1). Under H0, LR2 is asymptotically 2(1) distributed as well, Jorion (2001).
Table 3 and Table 4 summarize the results of the backtesting for the DEM/USD data and the bank portfolio data. On average, the GHADA model gives more accurate forecasts at each probability level than the normal model. For example, the proportions of exceptions

22

-2 0 2

1000

1500

2000

2500

3000

3500

Figure 11: Quantiles estimated based on the past 500 devolatilized returns of the exchange rate. From the top the evolving HYP quantiles for p = 0.995, p = 0.99, p = 0.975, p = 0.95, p = 0.90, p = 0.10, p = 0.05, p = 0.025, p = 0.01, p = 0.005.
at 1% level relative to the normal model is about 1.5%, which is one and a half times of these same level indices of GHADA models. The test of VaR level is not rejected for HYP and NIG based model at all levels. In contrast, the normal model fails to provide acceptable results at the extreme levels. In addition, all these models fulfill the independence test.

23

Y*E-2 -4 -2 0 2

Y*E-2 -4 -2 0 2

(a) p = 0.005
5 10 15 20 25 30 35 X*E2
(b) p = 0.01
5 10 15 20 25 30 35 X*E2
Figure 12: Value at Risk forecast plots for DEM/USD data. The dots are the returns, the solid line is the VaR forecast based on HYP underlying distribution, the yellow line is the VaR forecast with normal distribution, and the crosses indicate the VaR exceptions of HYP model. (a) p = 0.005. (b) p = 0.01.
GHADAfxvar.xpl
24

Y*E-2 -4 -2 0 2

Y*E-2 -4 -2 0 2

(c) p = 0.025
5 10 15 20 25 30 35 X*E2
(d) p = 0.05
5 10 15 20 25 30 35 X*E2
Figure 13: Value at Risk forecast plots for DEM/USD data. The dots are the returns, the solid line is the VaR forecast based on HYP underlying distribution, the yellow line is the VaR forecast with normal distribution, and the crosses indicate the VaR exceptions of HYP model. (c) p = 0.025. (d) p = 0.05.
GHADAfxvar.xpl
25

Model Normal
HYP
NIG

p 0.005 0.01 0.025 0.05 0.005 0.01 0.025 0.05 0.005 0.01 0.025 0.05

N/T 0.01025 0.01460 0.02858 0.05250 0.00403 0.00963 0.02485 0.05312 0.00404 0.00994 0.02516 0.05405

LR1 13.667 6.027 1.619 0.417 0.640 0.045 0.003 0.648 0.640 0.001 0.004 1.086

p-value 0.000* 0.014 0.203 0.518 0.424 0.832 0.957 0.421 0.424 0.973 0.953 0.297

LR2 0.735 0.138 0.056 0.007 0.189 0.655 0.666 0.008 0.189 0.694 0.719 0.040

p-value 0.391 0.710 0.813 0.934 0.664 0.419 0.415 0.927 0.664 0.405 0.396 0.841

Table 3: Backtesting results for DEM/USD example. * indicates the rejection of the model which is used.
GHADAfxvar.xpl

Model Normal
HYP
NIG

p 0.005 0.01 0.025 0.005 0.01 0.025 0.005 0.01 0.025

N/T 0.010 0.016 0.028 0.003 0.008 0.025 0.003 0.009 0.027

LR1 19.809 13.278
2.347 5.111 2.131 0.053 5.111 0.747 0.438

p-value 0.000* 0.000* 0.126 0.024 0.144 0.819 0.024 0.387 0.508

LR2 1.070 0.422 0.781 0.160 0.705 1.065 0.160 0.841 1.429

p-value 0.301 0.516 0.377 0.689 0.401 0.302 0.689 0.359 0.232

Table 4: Backtesting results for the bank portfolio example. * indicates the rejection of the model which is used.

26

4 FINAL REMARKS
We have proposed a risk management (GHADA) model based on the adaptive volatility estimation and the generalized hyperbolic distribution. Our study is summarized as follows.
· The adaptive volatility estimation methodology by Mercurio and Spokoiny (2004) is also applicable with generalized hyperbolic distribution. The threshold parameter used to specify the time homogeneity interval can be estimated in a nonparametric way.
· The distribution of the devolatilized returns from the adaptive volatility estimation is found to be leptokurtic and, sometimes, asymmetric. We found that the distribution of the innovations can be perfectly modelled by the HYP and NIG distributions, subclasses of the generalized hyperbolic distribution.
· The proposed approach can be easily applied to calculate and forecast risk measures such as value at risk and expected shortfall. On the basis of the DEM/USD data and a German bank portfolio data it shows that the proposed approach performs better than a model with the normal distribution.
In financial markets, it is more interesting and challenging to measure the risk levels of multiple time series. Ha¨rdle, Herwartz and Spokoiny (2003) proposed an idea of the adaptive volatility estimation method for a multiple time series. We expect to apply the idea to a model with the multivariate generalized hyperbolic distribution. A detailed study on this approach is left for our future research.

5 APPENDIX

Proof of Lemma 1.

Proof: Firstly we show that the moment generating function E[eu ] exists for all u  R.

Suppose that L(x) = GH(, , , , µ) with the density function f for the transformed variable y d=ef |x|, we have

11

1 1 z

-z 

P (y  z) = P (-z   x  z  ) = f (x)dx -

f (x)dx, z > 0

- -

27

Then the density of y  (0, ) is:

g(z) = d P (y  z)

=

-1{f (z

1 

)z

1 

-1

+

f (-z

1 

)z

1 

-1}

dz

=



-1

z

1 

-1{f

(z

1 

)

+

f

(-z

1 

)},

z > 0.

Since fGH (x; , , , , µ = 0)  x-1e-(-)x as x  ±, it follows

g(z)



z

1 

-1

{z

-1 

e(-)z

1 

+

z

-1 

e-(+)z

1 

}



=

z

 

-1

{e(-)z

1 

1
+ e-(-)z  },

z - 



For  < 1, it holds that

 0

euz g(z )dz

<



u



R,

since

1
limz( - )z  + uz  - u  R
1
limz - ( + )z  + uz  - u  R

Since the integration depends only on the exponential part, it holds also that


zneuzg(z)dz =
0

 0

n un

(euz

)g(z)dz

=

n un

E[euy ]

<

,

then it can be shown that the moment generating function and log(E[euy]) are smooth. It holds for every t > 0,

E[euy] = E[eu|x| ] = E[eu|x| 1(|x|  t)] + E[eu|x| 1(|x| > t)]  eut + E[e|x|ut-1 I(|x| > t)],

(23)

Without loss of generality, we assume µ = 0. Further

fGH (x; , , , , µ = 0)  x-1e-(-)x as x  ,

and

 y

x-1e-xdx



y-1e-y

as

y

,

Press,

Teukolsky,

Vetterling

and

Flannery

(1992).

For an arbitrary but fixed u  R+ and t0 > 1 so that ut-1 <  - , it holds for all t  t0

f (t)  C1t-1e(-)t  x-1e-xdx  C2[( -  - ut-1)t]-1e-(--ut-1)t
(--ut-1)t
where C1, C2 > 1.

28

Consequently for t  t0,

E[eu|t|-1x1(|x| > t)] =

 eut-1xf (x)dx  C1  eut-1xx-1e-(-)xdx

tt

= C1  x-1e-(--ut-1)xdx

t



= C1( -  - ut-1)-

x-1e-xdx

(--ut-1)t

 C1C2t-1e-(--ut-1)t( -  - ut-1t)-1

(24)

If u is so large that t

d=ef

(

- 2

)

1 -1

uc



t0

with

1 1-



c, then (24) holds true since

ut-1

=

(

- 2

)uuc(-1)



- 2

<

 - .

Given

t

=

(

- 2

u)

1 1-

,

we

get

E[eut-1x1(|x|

>

t)]



2C1C2

(



-



u)

-1 1-

e-

- 2

(

- 2

u)

1 1-

.

- 2

From which we get

log(E[eut-1 1(x

>

t)])



C3

+

- 1-

1 

log(u)

-

 (

- 2



2-
) 1-

1
u 1-

Further log(E[eut-1 1(x

>

t)])u-

1 1-

is

also bounded for u



. Analogously we can

show

the

bounding

of

log(E[eut-1 1(x

<

-t)])u-

1 1-

.

Therefore

for



<

1

the

whole

term

E[eu|x|

1(|x|

>

t)]u-

1 1-

is bounded as u  .

Given

t

=

(

- 2

u)

1 1-

,

we

have

e = eut

(

- 2

)

 1-

u

1 1-

u-

1 1-

log(eut )

=

-  ( ) 1- = constant

2

Thus

u-

1 1-

log(E[eu|x| ])



u-

1 1-

[log(eut

)

+

log{E[eut-1|x|1(|x|

>

t)]}]

is

bounded

for

u  , i.e. for a sufficient large u0 there exist a constant Cu > 0 such that

E[eu|x| ]



1
Cuu 1-

,

u  u0.

2

29

References
Barndorff-Nielsen, O. (1977). Exponentially decreasing distributions for the logarithm of particle size, Proceedings of the Royal Society of London A 353: 401­419.
Barndorff-Nielsen, O. (1997). Normal inverse gaussian distributions and stochastic volatility modelling, Scandinavian Journal of Statistics 24: 1­13.
Barndorff-Nielsen, O. E. and Blæsild (1981). Hyperbolic distribution and ramifications: Contributions to theory and Applications, Vol. 4 of Statistical Distributions in Scientific Work, D. Reidel, pp. 19­44.
Bibby, B. M. and Sørensen, M. (2001). Hyperbolic Processes in Finance, Technical Report 88, University of Aarhus, Aarhus School of Business.
Bollerslev, T. (1995). Generalied autoregressive conditional heteroskedasticity, ARCH, selected readings, Oxford University Press, pp. 42­60.
Christoffersen, P. F. (1998). Evaluating interval forecast, International Economic Review 39: 841­862.
Eberlein, E. and Keller, U. (1995). Hyperbolic distributions in finance, Bernoulli 1: 281­299.
Eberlein, E., Kallsen, J. and Kristen, J. (2003). Risk management based on stochastic volatility, Journal of Risk 5: 19­44.
Engle, R. F. (1995). Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation, ARCH, Oxford University Press.
Franke, J., H¨ardle, W. and Hafner, C. (2004). Statistics of Financial Markets, SpringerVerlag Berlin Heidelberg New York.
Ha¨rdle, W., Herwartz, H. and Spokoiny, V. (2003). Time inhomogeneous multiple volatility modelling, Journal of Financial Econometrics 1: 55­95.
H¨ardle, W., Mu¨ller, M., Sperlich, S. and Werwatz, A. (2004). Nonparametric and Semiparametric Models, Springer-Verlag Berlin Heidelberg New York.
Harvey, A., R. E. and Shephard, N. (1995). Multivariate stochastic variance models, ARCH, selected readings, Oxford University Press, pp. 256­276.
Jaschke, S. and Jiang, Y. (2002). Approximating value at risk in conditional gaussian models, in W. Ha¨rdle, T. Kleinow and G. Stahl (eds), Applied Quantitative Finance, Springer-Verlag Berlin Heidelberg New York.
Jorion, P. (2001). Value at Risk, McGraw-Hill.
30

Mercurio, D. and Spokoiny, V. (2004). Statistical inference for time inhomogeneous volatility models, Annals of Statistics 32: 577­602.
Press, W., Teukolsky, S., Vetterling, W. and Flannery, B. (1992). Numerical Recipes in C, Cambridge University Press.
31

SFB 649 Discussion Paper Series
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de. 001 "Nonparametric Risk Management with Generalized
Hyperbolic Distributions" by Ying Chen, Wolfgang Härdle and Seok-Oh Jeong, January 2005.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

