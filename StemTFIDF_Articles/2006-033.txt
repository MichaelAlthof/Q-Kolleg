BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2006-033
Varying coefficient GARCH versus local constant volatility
modeling. Comparison of the predictive power
Jörg Polzehl* Vladimir Spokoiny*
* Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Varying coefficient GARCH versus local constant
volatility modeling. Comparison of the predictive power

Polzehl, J¨org
Weierstrass-Institute, Mohrenstr. 39,
10117 Berlin, Germany polzehl@wias-berlin.de

Spokoiny, Vladimir
Weierstrass-Institute, Mohrenstr. 39,
10117 Berlin, Germany spokoiny@wias-berlin.de

Abstract

GARCH models are widely used in financial econometrics. However, we show by mean of a simple simulation example that the GARCH approach may lead to a serious model misspecification if the assumption of stationarity is violated. In particular, the well known integrated GARCH effect can be explained by nonstationarity of the time series. We then introduce a more general class of GARCH models with time varying coefficients and present an adaptive procedure which can estimate the GARCH coefficients as a function of time. We also discuss a simpler semiparametric model in which the  parameter is fixed. Finally we compare the performance of the parametric, time varying nonparametric and semiparametric GARCH(1,1) models and the locally constant model from Polzehl and Spokoiny (2002) by means of simulated and real data sets using different forecasting criteria. Our results indicate that the simple locally constant model outperforms the other models in almost all cases. The GARCH(1,1) model also demonstrates a relatively good forecasting performance as far as the short term forecasting horizon is considered. However, its application to long term forecasting seems questionable because of possible misspecification of the model parameters.

Keywords: varying coefficient GARCH, adaptive weights JEL classification: C14, C22, C53.

This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649

Economic Risk.

1

2 garch versus local constant modeling
1 Introduction
Autoregressive conditionally heteroscedastic (ARCH) and generalized autoregressive conditionally heteroscedastic (GARCH) models gained a lot of attention and are widely used in financial engineering since they were introduced by Engle (1982) and Bollerslev (1986). The simple GARCH(1,1) model is particularly popular. It models the observed log-returns Rt of the asset price process by the following two equations:
Rt = tt , t2 =  + Rt2-1 + t2-1.
Here , ,  are coefficients and t2 is the time varying volatility that is usually the target of analysis. The innovations t are assumed zero mean and variance one conditioned on the  -field Ft-1 generated by the past observations. The GARCH(1,1) suggests a very natural and tractable model with only three parameters to be estimated. Moreover, this model allows to mimic many important stylized facts of financial time series like volatility clustering (alternating periods of small and large volatility) and persistent autocorrelation (slow decay of the autocovariance function of the absolute or squared returns). We cite from Engle (1995): "The GARCH(1,1) is the leading generic model for almost all asset classes of returns. . . . it is quite robust and does most of the work in almost all cases.".
A simple parametric structure allows to directly apply the well developed parametric statistical methodology for estimation of the parameters and calibration of the model for real life applications and for studying the asymptotic properties of the estimates. The GARCH models are successfully applied to short term ahead forecasting of the volatility and particularly to Value-at-Risk problems, see McNeil and Frey (2000), Eberlein and Prause (2002).
However, a thorough analysis of the results delivered by the GARCH modeling raises some questions and indicates some problems.
For estimating the GARCH coefficients one usually applies a quasi likelihood approach. This means that the innovations t are assumed i.i.d. standard normal and then the coefficients are obtained by maximizing the corresponding log-likelihood function. The resulting estimate is root-n consistent and asymptotically normal, see e.g., Berkes, Horvath and Kokoszka (2003) or Straumann and Mikosch (2003).

polzehl, j. and spokoiny, v.

Estimates of 
Median estimate .05/.95 quantiles .25/.75 quantiles true parameter

Estimates of 

Estimates of 

3

 0.0 0.2 0.4 0.6 0.8 1.0 1.2
 0.0 0.1 0.2 0.3
 0.0 0.2 0.4 0.6 0.8 1.0

500

1000

1500

2000

500

1000

1500

2000

500

1000

1500

2000

time time time

Figure 1: The true parameters (red) and the pointwise quantiles of the MLE's t, t, t obtained from the last 500 historical observations Rs for s < t .

However, for practical applications, the convergence is quite slow and one needs about 500 to 1000 data points to get a reasonable quality of estimation, especially for the coefficient  , see Mikosch and Starica (2002, 2004). Giraitis and Robinson (2001) and Mikosch and Starica (2004) discussed an alternative approach based on the Whittle estimator, for GARCH parameters, while Peng and Yao (2003) considered the LAD approach. However its performance is similar. Particularly, for 250 observations (corresponds to one year for daily data) the variability in the estimated  -parameter is quite high.
We also face a small identifiability problem. If  = 0 , then the parameters  and  are not identifiable. Some additional boundary conditions on the process t are necessary in this case. However, under the usual ergodicity condition, memory of boundary values is lost with the exponential speed. This yields some numerical problems for estimation of the parameters in the cases when  is near zero.
One more critical point is that GARCH modeling hardly extends to multiple time series, because of the overparametrization problem, see e.g. the BEKK model in Baba et al (1990) or Engle and Sheppard (2004).
However, it appears that the most crucial problem in the whole GARCH approach is that the GARCH models are not robust w.r.t. violation from the stationarity assumption. We illustrate this problem by a numerical experiment for an artificial change point model, see Figure 1. The observed data Rt for t = 1, . . . , 2000

4 garch versus local constant modeling
follow for t < tcp = 1000 one GARCH(1,1) model with parameters 1 = 0.25 , 1 = 0.2 and 1 = 0.1 and after t = tcp only the parameter  jumps to 2 = 1 . We apply a scrolling window estimation procedure, that is, for every point t we estimate the parameters of the GARCH(1,1) model from the last 500 historical data Rs for s  [t - 500, t - 1] . Therefore, for t  tcp we observe the performance of the GARCH estimator when the data generating process is indeed parametric GARCH(1,1). The resulting estimator is rather variable, however, it basically mimics the true model. For t  [tcp + 1, tcp + 500] , the GARCH parameters are estimated from the subsample Rt-499, . . . , Rt which contains a jump in the  parameter at tcp . We observe for such t that, even if most observations are from one model and only few of them come from the other model, the estimates are completely misspecified and in particular, the parameter  jumps to a value close to 1. Mikosch and Starica (2004) and Starica (2004) provide an explanation of this behavior: a GARCH(1,1) model, especially with a large value of the sum  +  , is effectively very close to an exponential smoothing filter with memory parameter  . In other words, if the stationarity assumption is violated, GARCH modeling is essentially reduced to exponential smoothing of the latest observed squared returns. Mikosch and Starica (2000, 2004) also argued that the other stylized facts of the financial time series like long range dependence, persistent autocorrelation and integration GARCH effect can be well explained by nonstationarity in the observed data.
In this paper we make an attempt to overcome this problem by considering the so called varying coefficients GARCH models. This means that the coefficients , ,  may vary with time and allows to model structural changes and external shocks in the considered time series. Varying coefficient models have been applied to model some financial time series in Fan, Jiang, Zhang and Zhou (2003) under the assumptions that the model parameters smoothly vary with time. We apply a more general approach that allows to include the case when the parameters spontaneously change. The estimation problem for such models is much more complicated than in the parametric case because we have to estimate three parameters which are possibly discontinuous functions of time. We also have to account that, even in a parametric case, a careful estimation of the GARCH-parameters from a small or moderate sample size is a hard task. To reduce the complexity of the model, apart

polzehl, j. and spokoiny, v.

5

from the fully nonparametric model in which all three parameters are functions of time we consider a semiparametric model in which the parameter  is kept fixed and the two other parameters may vary with time. Additionally we consider the local constant volatility model where the coefficients  and  are zero and only the coefficient  is a function of time. The latter model was considered in Polzehl and Spokoiny (2002) and Mercurio and Spokoiny (2004a, 2004b), see also Starica and Granger (2004). Finally we compare these three models with the classical parametric GARCH(1,1) model.

For a comparison we use a number of simulated examples and look at different criteria like the prediction error, excess probability in Value-at-Risk (VaR) forecast and mean predictive VaR values.

We also apply the considered methods to real data including the DAX time series and the USD/GBP exchange rate series. For a comparison we look at the empirical counterparts of the criteria used in the simulations.

The results indicate that for both simulated and real data examples, the simple local constant model outperforms the other models including the more complicated non- and semiparametric models and delivers, in all cases, very reasonable results. At the same time, we observe that the fully nonparametric model has problems in identifying all the parameters as functions of time. A less variable semiparametric modeling delivers more stable results which also help to judge about statistical significance of the integrated GARCH effect.

The paper is organized as follows. Section 2 discusses the parameter estimation problem for the GARCH(1,1) model and indicates the related problems. Section 3 presents a varying coefficient GARCH model. The estimation problem for this model is discussed in Section 4. A modified procedure for the semiparametric GARCH model is briefly discussed in Section 4.4. Section 4.5 explains how the results of estimation can be used for out-of-sample forecasting of the volatility. Sections 5 and 6 illustrate the numerical performance of the methods by means of some simulated examples and applications to real data.

6 garch versus local constant modeling

2 GARCH modeling and parameter estimation

Let the observed returns Rt obey the conditional heteroskedastic equation

Rt = tt

t  t0 ,

where t are "innovations" and t is the volatility process. It is usually assumed that t is measurable w.r.t. the  -field Ft-1 generated by the past observations Rs for s < t and that the conditional distribution of the innovations given Ft-1 fulfills E (t|Ft-1) = 0 and E (t2|Ft-1) = 1 .
The GARCH(1,1) model specifies the volatility process t2 by the equation
t2 =  + Rt2-1 + t2-1.

We denote Xs = s2 and Ys = Rs2 so that the process Xt obeys the linear autoregressive equation

Xs =  + Ys-1 + Xs-1 .

(2.1)

Usually all the coefficients are assumed nonnegative, that is,   0 ,   0 ,   0 . The condition  +  < 1 ensures ergodicity of the process Yt .
We denote by  = (, , ) the vector of parameters. Note that equation (2.1) does not uniquely determine the process {Xs} . Apart the vector  , one has to specify the boundary (initial) value  = Xt0 for some point t0 . However, the dependence on this parameter in the ergodic case is rather small, and we simply set Xt0 = Yt0 = Rt20 . We therefore use the notation Xs = Xs() to indicate the dependence of the volatility process on  .
The structural linear equation can now be written as

Xs() = s() =  + Ys-1 + Xs-1(),

(2.2)

with s() = 1, Ys-1, Xs-1() . Using this linear equation we can recursively compute the values Xs() , s > t0 , starting from the initial value Xt0 =  .
Similarly we obtain the derivatives Xs() = dXs()/d and 2Xs() = d2Xs()/d2 . Namely it holds

Xs() = s () + Xs-1(),

(2.3)

polzehl, j. and spokoiny, v.

7

with the initial condition Xt() = 0 for t = t0 . A similar recurrent formula applies for the matrix of second derivatives:

2Xs() = s() +  s() + 2Xs-1(),

(2.4)

where s() = (0, 0, Xs-1()) and 2Xs() = 0 for s  t0 . For estimating the parameter  , one usually applies the quasi maximum likeli-
hood approach assuming independent standard normal innovations {s}st0 . The log-likelihood for model (2.1) up to a constant term can be represented in the form

L()

=

1 2

(Rs, Xs())

st0

where (r, 2) = -(log 2 + r2/2)/2 . We define the (quasi) maximum likelihood estimate (MLE)  of the parameter  by maximizing L() :

 = argsup L() = argsup (Rs, Xs())).
  st0
The MLE  fulfills the estimating equation dL()/d = 0 leading to

(2.5)

Ys - Xs() |Xs()|-2 Xs() = 0.
st0

(2.6)

For solving this equation, one can apply an iterative Newton-Raphson procedure. Let some initial value (0) be fixed and let (k-1) be the estimated param-
eter vector after step k - 1 for k  1 . One can compute the latent volatility process Xs(k) = Xs((k-1)) by (2.2) and the derivatives Xs(k) = dXs((k-1))/d and 2Xs(k) = d2Xs((k-1))/d2 by (2.3) and (2.4) and define the update (k) as (k) = (k-1) + (B(k))-1S(k) with

S(k) =

Xs(k) -2 Us - Xs(k) Xs(k),

st0

B(k) =

Xs(k) -2Xs(k) Xs(k)

st0

+ Xs(k) -2 Us - Xs(k)
st0

2 Xs(k)

Xs(k)

Xs(k)

- 2Xs(k) . (2.7)

The update (k) can be interpreted as gradient decent in direction of the estimated gradient of the log-likelihood. It is recommended to check that the this

8 garch versus local constant modeling
update really improves the likelihood, that is, L((k)) < L((k-1)) . If this inequality does not hold, the step in the gradient direction should be taken smaller, (k) = (k-1) + (B(k))-1S(k) for some  < 1 , e.g.  = 1/2 and checked again.
The constrains   0 ,   0 ,   0 and  +  < 1 can be naturally incorporated in the Newton-Raphson procedure using a barrier function. We omit the details.

3 Varying coefficient GARCH

Having the problems mentioned in the introduction in mind, we aim to extend the GARCH approach by including a possibility for structural changes. This can be done using the notion of a varying coefficient model. Namely, we assume that the GARCH parameters may depend on time t . We denote them as t = (t, t, t) . Two special cases are usually considered in the literature. For change point models, the parameters change spontaneously at some time points and remain constant between them, see e.g. Chu (1995) and Mikosch and Starica (2002). Smooth transition models assume that the parameters vary slowly and smoothly in time, cf. Fan, Jiang, Zhang and Zhou (2003). We do not assume any special dependence of the GARCH-parameters on time, in particular, our modeling approach applies to both change point and smooth transition models. Moreover, our approach applies even if t is a predictable random process. The varying coefficient GARCH(1,1) reads as follows:

Rt  (·, Xt),

Xt = t + tRt2-1 + tXt-1 = t t

(3.1)

where t = (1, Rt2-1, Xt-1) and t is now the vector composed by the elements t, t and t . Each of them may vary with time t .
The target of the analysis is the parameter process  = (t)tt0 . This process uniquely defines the process X = X() due to (3.1), and hence, the distribution of the process (Rt)tt0 . Similarly to the parametric case, we define the (quasi) maximum likelihood estimate of the process  by maximizing the corresponding log-likelihood expression

L() =

(Rs, Xs()).

st0

(3.2)

polzehl, j. and spokoiny, v.

9

The maximization is done over the class of all "admissible" processes  . Two examples of such classes have been already mentioned: change point models assume that the process  is piecewise constant while smooth transition models are effectively based on the smoothness assumption of this process. Our approach is more general and it includes these two examples as special cases. The only assumption we make about the process  is local time homogeneity. This means that for every time point t the parameter vector s is nearly constant within some neighborhood of the point t . To state this assumption in a more formal way, we need to explain how a local neighborhood of a point t can be described. Similarly to Polzehl and Spokoiny (2000, 2002, 2003) we apply localization by weights. Let, for a fixed t , a nonnegative weight wt,s  [0, 1] be assigned to the observation Ys . The collection of weights Wt = (wt,s)st0 describes a local model corresponding to the point t .
We mention two examples of choosing the weights wt,s . Localization by a bandwidth is defined by weights of the form wt,s = Kloc(lt,s) with lt,s = |(t-s)/h|2 where h is a bandwidth and Kloc is a location kernel. This method is applied e.g. in Fan, Jiang, Zhang and Zhou (2003). Localization by a window simply means that the parametric structure is assumed to hold within some subset (window) Ut containing t . In this case the weights are defined as wt,s = 1(s  Ut) . This approach suits well to change point models where the parameter  is a piecewise constant function of t .
Following to the adaptive weights idea from Polzehl and Spokoiny (2000), we do not assume any special structure for the weights wt,s . The weights will be computed from the data in a data driven way.
We apply a local perturbation approach to maximize the log likelihood L() from (3.2). This means that we change the process  locally near every point t and obtain the local estimation equation by maximizing L() for such local perturbations. Before we discuss this method in detail, it is important to note that, even if the parameter process  is changed only locally around some point t , the corresponding process Xs() changes for all s > t . This requires to consider the global log-likelihood even if the parameters are only locally perturbed.
Suppose that a process  = (t ) is fixed. This process can be viewed as starting value or preliminary estimate of the true process  = (t) . Let now Wt be a collection of weights (wt,s)st0 describing a local model at a point t . We

10 garch versus local constant modeling

define for every value  a locally perturbed process  = (s) as

s = wt,s + (1 - wt,s)s,

s  t0.

The corresponding latent process denoted by Xt,s() = Xt,s(Wt, ; ) , s  t0 , fulfills the equation

Xt,s() = t,s() (wt,s + (1 - wt,s)s) = (wt,s + (1 - wt,s)s) + (wt,s + (1 - wt,s)s) Ys-1 + (wt,s + (1 - wt,s)s) Xt,s-1()

(3.3)

where t,s() = (1, Ys-1, Xt,s-1()) . The updated value t of the process  at t is defined by maximizing the
(quasi) likelihood expression corresponding to the process Xt,s() :

t = argsup L(Wt, , ) = argsup

Ys, Xt,s() .

  st0

(3.4)

As in the parametric case, the corresponding estimate t solves the equation

Xt,s() Ys - Xt,s() |Xt,s()|-2 = 0.
st0
A numerical solution of this equation can be obtained by the Newton-Raphson procedure as described in Section 2. The definition of the process Xt,s() in (3.3) leads to the following expression for the derivatives Xt,s() :

Xt,s() = wt,st,s() + (wt,s + (1 - wt,s)s) t,s() = wt,st,s() + (wt,s + (1 - wt,s)s) Xt,s-1(),

(3.5)

with the starting conditions Xt,s() = 0 for s  t0 . A similar recurrence formula applies for the matrix of second derivatives:

2Xt,s() = wt,ss() + wt,s t,s() + wt,s + (1 - wt,s)s 2Xt,s-1(). (3.6)

We can proceed exactly as in the parametric case described in Section 2. The AWS procedure presented in the next section combines this method of
estimating the process  with an approach for defining the weights wt,s .

polzehl, j. and spokoiny, v.

11

4 Adaptive weights smoothing

This section presents an estimation method for a varying coefficient GARCH given by (3.1). The underlying idea is to maximize the log likelihood L() from (3.2) in an iterative way. At every step we first describe in a data driven way a neighborhood of every point t in which the varying coefficient model (3.1) can be well approximated by a model with constant parameter values. We then apply the local perturbation approach to update the estimate of the process  as described in the previous section.
More precisely, we start defining at every point t a local model Wt(0) using the classical kernel weights with a very small bandwidth h(0) . We then successively repeat two basis steps: for all t  t0 , we estimate the parameter t for the local model Wt(k) = (wt(,ks))st0 , and then, again for all t  t0 , we generate new larger local models Wt(k+1) using the obtained estimates t(k) , k = 0, 1, 2 . . . .
4.1 Defining weights
The method for assigning weights wt(,ks) which define the local model Wt(k) is the central point of the AWS procedure. As suggested in Polzehl and Spokoiny (2002, 2003), for every pair (t, s) , the weight wt(,ks) is defined using two different values: a location penalty l(t,ks) and a statistical penalty st(,ks) .
The location penalty lt(,ks) = (|t - s|/h(k))2 is deterministic and depends only on the distance between t and s and on the bandwidth h(k) applied at step k . At the beginning of the iteration process, the bandwidth h(0) is taken very small leading to a strong localization. During iteration the bandwidth h(k) grows which relaxes the location penalty and allows to increase every local model. However, this increase is done in an adaptive (data-driven) way by use of the statistical penalty st(,ks) which measures the difference in the parameter values for the local models Wt(k-1) and Ws(k-1) . Following Polzehl and Spokoiny (2002, 2003), this penalty can be defined by the expressions
s(t,ks) = Tt(,ks)/ Tt(,ks) = L(Wt(k-1), (tk-1), (k-1)) - L(Wt(k-1), (sk-1), (k-1))
where (k-1) = ((sk-1)) is the estimate of the process  = (s) obtained at the step k - 1 . The value Tt(,ks) can be interpreted as the test statistic for testing the

12 garch versus local constant modeling

two sample hypothesis t = s : indeed, L(Wt(k), (tk-1), (k-1)) is the maximum of the log-likelihood L(Wt(k), , (k-1))) for the local model Wt(k-1) over all possible  and Tt(,ks) is defined as the discrepancy between this maximum and the particular value L(Wt(k), , (k-1)) with  = s(k-1) coming from another local model Ws(k-1) . The value  can be treated as a critical value for this test. If the statistical penalty si(jk) is large, then one can say that there is an empirical
evidence that the GARCH parameters  are different at points s and t .
To reduce the computational effort of the procedure, one may also use the
quadratic approximation of the log-likelihood:

Tt(,ks) = t(k-1) - s(k-1) Bt(k-1) (tk-1) - (sk-1) /2,

(4.1)

where Bt(k-1) is defined similarly to (2.7) using the weights wt(,ks-1) . Suppose that for the pair (t, s) , the penalties l(t,ks) and s(t,ks) have been computed.
Polzehl and Spokoiny (2002) suggested to define the new weight wt(,ks) such that the value wt(,ks) is small if any of the penalties is large and that the different penalties
act independently. This leads to a definition in form of a product:

wt(,ks) = Kloc lt(,ks) Kst st(,ks) ,

where Kloc and Kst are two kernel functions on the positive semiaxis.

The choice of the initial estimates t(0) is important. At the beginning we set

the parameter t(0) to zero which reduces the GARCH(1,1)-model to ARCH(1). In

such a case, the structural equation (3.1) reads Xt = t + tRt2-1 and the value

Xt is independent of the values s, s for s = t . Therefore, one can define the

starting

values



(0) t

= (t(0), t(0))

by

optimization

of

the

local

log-likelihood

L(Wt(0), ) =

(Rs,  + Rs2-1)wt(,0s)

st0

w.r.t.  = (, ) where wt(,0s) = Kloc(|s - t|2/h20) .

(4.2)

4.2 The procedure

We now present a formal description of the method. Important ingredients of the procedure are:
- the kernels Kloc and Kst ;

polzehl, j. and spokoiny, v.

13

- the parameter  ;
- the initial bandwidth h(0) , a factor a > 1 and the maximal bandwidth hmax .
The choice of the parameters is discussed in Section 4.3. The procedure reads
as follows: 1. Initialization: For every t  t0 , define the local model Wt(0) with weights wt(,0s) = Kl(l(t,0s)) where l(t,0s) = |t - s|/h(0) 2 for all s . Next, set t(0) = 0 and (t(0), t(0)) = argmax=(,) L(Wt(0), ) , see (4.2). Set k = 1 . 2. Iteration: for every t = t0, . . . , T

· Calculate the adaptive weights: For every point s  t0 compute the penalties

lt(,ks) = |t - s|/h(k) 2 ,

s(t,ks)

=

-1

L(Wt(k-1), t(k-1), (k-1)) - L(Wt(k-1), s(k-1), (k-1))

(4.3) .

where L(W, ; ) is given by (3.2) and (3.3). Define

wt(,ks) = Kloc lt(,ks) Kst s(t,ks)
and Wt(k) = wt(,ks) st0 . · Estimate the parameter t : Define the local MLE (tk) as
(tk) = argsup L(Wt(k), , (k-1)).


(4.4)

3. Stopping: Increase k by 1, set h(k) = ah(k-1) . If h(k)  hmax continue with
step 2. Otherwise terminate.
We denote the total number of iterations by k . The final estimates are obtained as t = (tk) . The value Xt(,kt) can be naturally viewed as the estimate of the parameter t2 for the varying coefficient model (3.1).

4.3 Choice of parameters

The parameters of the procedure are selected similarly to Polzehl and Spokoiny (2002). We briefly discuss each of the parameters.
Kernels Kst and Kloc : The kernels Kst and Kloc must fulfill Kst(0) = Kloc(0) = 1 , with Kst decreasing and Kloc non-increasing on the positive semiaxis. We recommend to take Kst(z) = e-zI{z6} . We also recommend to apply a

14 garch versus local constant modeling
compactly supported localization kernel Kloc to reduce the computational effort of the method. Similarly to Polzehl and Spokoiny (2002) we apply the triangle kernel Kloc(z) = (1 - z)+ .
Initial bandwidth h(0) , parameter a and maximal bandwidth hmax : The starting bandwidth h(0) should be small. In general we select h(0) such that every initial local neighborhood [t - h(0), t + h(0)] contains sufficiently many design points to obtain an estimate of the parameter t .
The parameter a controls the growth of the local neighborhoods. Our default choice is a = 1.25 . The maximal bandwidth hmax may be very large, e.g. hmax = T . However, this parameter can be used to bound the numerical complexity of the procedure. The exponential growth of the bandwidth h(k) ensures that the number of iterations k is at most logarithmic in the sample size.
Parameter  : The most important parameter of the procedure is  which scales the statistical penalty st,s . Small values of  lead to overpenalization which may result in unstable performance of the method in a homogeneous situation. Large values of  result in a loss of adaptivity, i.e. less sensitivity to structural changes. A reasonable way to define the parameter  for a specific application is based on the condition of free extension, which we also call "propagation condition". This means that in a homogeneous situation, i.e. when the process  is constant, the impact of the statistical penalty on the computed weights wt,s is negligible. This would result in a free extension of every local model. If the value hmax is sufficiently large, all the weights wt,s will be close to one at the end of iteration process and every local model will essentially coincide with the global one. Therefore, one can adjust the parameter  using Monte-Carlo simulations. Simply select the minimal value of  that still provides a prescribed probability to obtain the global model at the end of iteration process for the homogeneous (parametric) model t =  . The theoretical justification for such a choice is given in Polzehl and Spokoiny (2002).
Our default choice, obtained by this method, is  = q(32) , that is, the  quantile of the 2 distribution with 3 degree of freedom, where  = 0.99 .

polzehl, j. and spokoiny, v.

15

4.4 Semiparametric modeling

In many situations a reasonable estimate of the parameter  requires a large

sample size. This makes a local analysis relatively inefficient. A natural way

to solve this problem is a semiparametric approach assuming that the parameter

 is constant while the other parameters ,  may vary with time. The AWS

procedure can be easily adjusted to such models. Namely, at every iteration we

locally estimate the varying coefficients  = (, ) while the value  = (k-1)

is kept fixed. Afterwards we update the parameter  . The basic AWS procedure

reads exactly as described in Section 4.2. The only difference is that the parameter

 should be replaced by  and in the definition of the process (k-1) one should

apply (k-1) in place of t(k-1) . For updating the parameter  , at the end of the iteration k , define for every vector  the process Xs(k)() = Xs(k)(,  (k)) with



(k)

=

(

(k) s

=

(s(k),

s(k))

, s  1)

using the recurrence equation

Xs(k)() = s(k) + s(k)Ys-1 + Xs(-k)1().
The new estimate (k) maximizes the log-likelihood L() = st0 Ys, Xs(k)() w.r.t.  . Again, the Newton-Raphson algorithm with the quadratic approximation (2.7) can be used.

4.5 Application to forecasting
The forecasting problem for the model (3.1) can be formulated as follows. Given the observations R1, . . . , RT estimate the value of the latent process Xt for some future point t = T +j for j  1 , and predict the distribution of future observations Rt . A natural way of solving this problem (at least if the forecast horizon j is not too large) is to model the processes Rt and Xt for t > T from the latest estimated model corresponding to t = T .
Let  = (, , ) be (Tk) and Xs = XT(k,s) = Xs(T ) for s = t0, . . . , T . We then define XT +1 as
XT +1|T = T +1 =  + RT2 + XT ,
where T +1 = (1, RT2 , XT ) . Using the estimate XT +1|T of XT +1 we can generate RT +1 from a GAussian distribution with variance XT +1|T . These two steps, compute XT +j and generate RT +j , can be repeated for t = T + 2, T + 3 . In general

16 garch versus local constant modeling

Ex 1: parametric GARCH(1,1)
t t t

Ex 2: local const. volatility
t t t

Ex 3: SP - GARCH(1,1)
t t t

parameter value 0.0 0.2 0.4 0.6 0.8

parameter value 0.0 0.2 0.4 0.6 0.8

parameter value 0.0 0.2 0.4 0.6 0.8

parameter value 0.0 0.2 0.4 0.6 0.8

parameter value 0.0 0.2 0.4 0.6 0.8

parameter value 0.0 0.2 0.4 0.6 0.8

0 200 400 600 800 1000 Index
Ex 4: NP - GARCH(1,1)
t t t

0 200 400 600 800 1000 Index
Ex 5: NP - GARCH(1,1)
t t t

0 200 400 600 800 1000 Index
Ex 6: SP - GARCH(1,1)
t t t

0 200 400 600 800 1000 Index

0 200 400 600 800 1000 Index

0 200 400 600 800 1000 Index

Figure 2: Parameters of simulated examples as functions of time.

there is no closed form expression for the distribution of the forecasted value RT+j , but it can be numerically evaluated by Monte-Carlo simulations.

5 Simulated examples
The aim of this section is to illustrate the performance of the proposed models and compare them with the classical GARCH(1,1) model and the local constant AWS procedure for volatility estimation from Polzehl and Spokoiny (2002). The latter is a very particular and much simpler special case of the varying coefficient GARCH model with  =  = 0 and only  varying with time.
We especially focus on the "integrated GARCH" effect (value  close to one) and demonstrate that it can be artificially produced if the stationarity assumption is violated.
We use a set of six artificial examples to illustrate the predictive performance of parametric, non- and semiparametric GARCH(1,1) models. The sample size is set to n = 1000 . Example 1 is a parametric GARCH(1,1) model with  = 0.2 ,  = 0.1 and  = 0.8 . Example 2 describes a local constant volatility model (  =  = 0 ). Example 3 and 6 are generated as semiparametric GARCH(1,1) models with small and large values of  , respectively, while examples 4 and 5

polzehl, j. and spokoiny, v.

17

Table 1: Simulation results for artificial examples 1-6. Simulation size 50. Mean estimated values of  , mean predictive likelihood, probability of exceeding the VaR and mean VaR obtained for the scrolling GARCH(1,1) estimate (from the last 250 observations), sequential AWS for nonparametric and semiparametric GARCH(1,1), and the sequential local constant volatility AWS procedure.

Mean 
Mean  GARCH
Mean  NP-GARCH
Mean  SP-GARCH PL(10) GARCH PL(10) NP-GARCH PL(10) SP-GARCH PL(10) Local Const
100PEVaR(0.01, 10) GARCH 100PEVaR(0.01, 10) NP-GARCH 100PEVaR(0.01, 10) SP-GARCH 100PEVaR(0.01, 10) Local Const MVaR(0.01, 10) GARCH MVaR(0.01, 10) NP-GARCH MVaR(0.01, 10) SP-GARCH MVaR(0.01, 10) Local Const

Ex 1 0.8
0.609
0.558
0.365 -1.732 -1.745 -1.735 -1.724
1.45 1.54 1.31 1.38 7.21 7.20 7.33 7.28

Ex 2 0.0
0.781
0.551
0.258 1.450 1.474 1.481 1.511
1.86 1.81 1.59 1.48 2.07 2.10 2.12 2.11

Ex 3 0.2
0.802
0.491
0.220 1.313 1.397 1.449 1.493
2.70 2.30 1.97 1.82 2.12 2.18 2.22 2.19

Ex 4 0.181
0.821
0.520
0.241 1.395 1.470 1.517 1.558
2.62 2.27 2.00 1.82 2.02 2.08 2.11 2.09

Ex 5 0.65
0.802
0.566
0.325 0.073 0.145 0.213 0.252
2.84 2.62 2.18 2.11 3.75 3.82 3.88 3.85

Ex 6 0.8
0.804
0.622
0.398 -0.903 -0.816 -0.737 -0.710
3.00 2.70 2.29 2.25 6.04 6.17 6.32 6.24

are entirely nonparametric GARCH(1,1) again with small and large values of  . Parameters are local constant and may change every 125 observations. Figure 2 illustrates the parameters used. The AWS estimates are computed sequentially based on all the observations from the past. For the parametric GARCH(1,1) model, a scrolling estimate from the last 250 observations is used.
We use the following criteria to compare the behavior of the estimates:

· Mean estimated value of 

1 750

t

t>250

· A predictive likelihood risk PL(k) with horizon k = 10

PL(k)

=

-

(n

-

k

1 -

250)k

n-k

k

t=251 s=1

log

Xt+s|t

+

Xt+s Xt+s|t

where Xt+s|t denotes the predicted volatility at time t + s based on the estimated process using observations up to time t .

18 garch versus local constant modeling

· Let the Value at Risk (VaR) at level  and time horizon k be defined as

k
VaRt(, k) = -q Xt+s|t
s=1

(5.1)

with q denoting the  -quantile of the standard Gaussian distribution. We report an estimate of the mean probability PEVaR(, k) of exceeding VaR at level  and time horizon k

PEVaR(, k) =

1 n - k - 250

n-k

P

t=251

t+k
Rs < - VaRt(, k)
s=t+1

(5.2)

obtained from the simulations. This value should be possibly close to the nominal level  .

· Finally we provide a mean VaR at level  and time horizon k as

MVaR(, k) =

1 n - k - 250

n-k

VaRt(, k)

t=251

(5.3)

again obtained from our simulations, cf. Fan and Gu (2003). This value characterizes the cost required to secure the asset.

Results of the simulations are summarized in Table 1. The results lead to the following conclusions:

· The GARCH model applied to data following a change point GARCH model leads to a misspecification with a large value of the estimated parameter  .
· The fully nonparametric GARCH model did not succeed to get a reasonable estimate of the varying parameter  . Again, the estimated t is in mean much larger than the true value in Examples 2 to 4, while the semiparametric GARCH model seems to be much more successful in handling the change point models considered in our examples.
· The local constant model provides the best prediction quality for the 10 days forecasting horizon for all examples. The GARCH model leads to the worst results in almost all examples, while the semiparametric model is typically at the second place.

polzehl, j. and spokoiny, v.

19

· The excess probability for the predicted VaR-quantiles is again optimized by the local constant estimate while for examples 5 and 6 the semiparametric model shows slightly better results. However, all the models provide a reasonable fit of the 1%-quantile.
· The averaged value of the VaR-quantile is in most cases minimized by the GARCH-model. In combination with the excess probability results one can judge that the GARCH-model tends to underestimate the VaR. This probably explains why GARCH models are so popular in risk management.

6 Applications to financial time series

We now apply our methodology to two time series, the German DAX index (August 1991 to July 2003) and the USD/GBP exchange rate (January 1990 to December 2000). Similarly to the simulation study, we compare four methods: the parametric GARCH(1,1), the non- and semiparametric GARCH(1,1) models and the local constant volatility model from Polzehl and Spokoiny (2003). We show up to which extend the four methods can explain phenomena observed for financial time series like heavy tails and long range dependence.
We investigate the predictive performance of the methods by estimating the predictive empirical likelihood risk PEL(k) at different time horizons k ranging from 2 weeks to half a year:

PEL(k)

=

-

(n

-

k

1 -

500)k

n-k

k

t=501 s=1

log

Xt+s|t

+

Rt2+s Xt+s|t

(6.1)

where Xt+s|t denotes the predicted volatility at time t + s based on the estimated process using observations up to time t . We also provide estimates for the excess probability (5.2) of VaR and the mean VaR (5.3).
The top of Figure 3 shows the logarithmic returns of the DAX series, emphasizing strong variations in volatility. Additionally global and sequential estimates of the square root of the volatility obtained by the four methods under consideration are provided. Note that in principle all methods capture the same volatility structure over time. Similar results are observed for the USD/GBP exchange rate series.

20 garch versus local constant modeling
DAX : Logarithmic returns

rdax -0.05 0.05

Volatility 0.00 0.02 0.04

estimated sequential (n=250)

SQRT of volatility procetsseqsdaxParametric GARCH(1,1)

Volatility 0.01 0.03 0.05

estimated sequential (n=250)

SQRT of volatility process AWtseSqdfaoxr Nonparametric GARCH(1,1)

Volatility 0.00 0.02 0.04 0.06

estimated sequential (n=250)

SQRT of volatility process AWtseSqdfaoxr Semiparametric GARCH(1,1)

Volatility 0.005 0.020 0.035

estimated sequential (n=250)

SQRT of volatility processtseloqdcaaxl constant AWS (Volatility)

1992

1994

1996

1998

2000

2002

Figure 3: DAX: Logarithmic returns (top) and estimated volatility processes. Given are global estimates (dashed line) and sequential estimates (obtained from the last 500 observations, solid line) by parametric GARCH(1,1), AWS for nonparametric GARCH(1,1), AWS for semiparametric GARCH(1,1) and the local constant volatility model (from top to bottom).

Table 2: DAX and USD/GBP: Mean values for the nonlinear parameter.

GARCH NP-GARCH SP-GARCH

DAX 0.862

0.609

0.250

USD/GBP 0.777

0.411

0.227

Cointegration in DAX and USD/GBP: fact or artifact?
In Table 2 we provide the mean estimate of the parameter  obtained using the parametric GARCH(1,1) model and its non- and semiparametric generalizations. Exactly as in our simulation study, for both time series, the estimated value of parameter  for the scrolling parametric GARCH(1,1) is close to one, while the results for the semiparametric model (given in boldface) indicate that this IGARCH effect can be artifact of nonstationarity of the time series.

DAX: ACF of R2t

polzehl, j. and spokoiny, v.

21

Seq. GARCH(1,1): ACF of Rt2 X^t

Seq. NP-GARCH(1,1): ACF of R2t X^t

Seq. SP-GARCH(1,1): ACF of Rt2 X^t

Seq. local const. AWS: ACF of Rt2 X^t

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0.0 0.2 0.4ACF 0.6 0.8 1.0

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0 5 10 15Lag 20 25 30 35 USD - GDP : ACF of R2t

0 5 10 15Lag 20 25 30 35 Seq. GARCH(1,1): ACF of R2t X^t

0 5 10 15Lag 20 25 30 Seq. NP-GARCH(1,1): ACF of R2t X^t

35

0 5 10 15Lag 20 25 30 Seq. SP-GARCH(1,1): ACF of R2t X^t

35

0 5 10 15Lag 20 25 30 Seq. local const. AWS: ACF of Rt2 X^t

35

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0.0 0.2 0.4ACF 0.6 0.8 1.0

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0.0 0.2 0.4 ACF 0.6 0.8 1.0

0 5 10 15Lag 20 25 30 35

0 5 10 15Lag 20 25 30 35

0 5 10 15Lag 20 25 30 35

0 5 10 15Lag 20 25 30 35

0 5 10 15Lag 20 25 30 35

Figure 4: DAX and USD/GBP: ACF of squared log returns and squared standardized residuals (using sequential estimates) obtained for the four methods for DAX (top) and USD/GBP (bottom) volatility estimates, respectively.

Table 3: DAX and USD/GBP: Tail index of absolute logarithmic returns and standardized residuals (using sequential estimates). Critical values for Gaussian distributions with same sample size: 0.193 (.95), 0.202 (.99).

DAX USD/GBP

log returns 0.324 0.310

residuals GARCH
0.225 0.232

residuals NP-GARCH
0.195 0.166

residuals SP-GARCH
0.190 0.148

residuals Local Const
0.188 0.171

DAX and USD/GBP: Persistent ACF and Long Range Dependence Phenomenon
The autocorrelation function (ACF) of squared log returns Rt2 and of squared standardized residuals 2t = Rt2/t2 obtained for the four estimates are provided in Figure 4. The ACF of the log returns clearly indicates persistency, however, all four models under consideration, despite their quite different structure, allow to successfully explain the dependence structure. Hence, the long range dependence phenomenon in financial returns can be easily explained by nonstationarity of the financial market.
DAX and USD/GBP: Tail index behavior of the returns
To investigate the phenomenon of heavy tails we estimate the tail index of logarithmic returns Rt and standardized residuals t obtained by the four methods.

22 garch versus local constant modeling

Table 4: DAX and USD/GBP: Mean predictive empirical likelihood risk for different forecast horizons. The best result for each time horizon in boldface.

Method
GARCH NP-GARCH SP-GARCH Local Const

two weeks

DAX
7.54 7.54 7.53 7.56

USD/GBP
9.44 9.31 8.46 9.46

one month

DAX
7.42 7.47 7.49 7.52

USD/GBP
9.40 9.25 7.65 9.45

three months

DAX
7.02 7.28 7.35 7.39

USD/GBP
9.31 8.57 7.68 9.40

six months
DAX USD/GBP
6.73 9.22 7.15 8.47 7.26 7.70 7.3 9.35

Table 5: DAX and USD/GBP: Probability to exceed the Value at Risk at 10 trading days. The best result in boldface.

Level
0.01 0.05

GARCH
DAX USD/GBP
0.0118 0.0173 0.0556 0.0542

NP-GARCH
DAX USD/GBP
0.0133 0.0168 0.0551 0.0561

SP-GARCH
DAX USD/GBP
0.0129 0.0230 0.0594 0.0571

Local Const
DAX USD/GBP
0.0137 0.0149 0.0480 0.0538

We use the AWS tail index estimate proposed in Polzehl and Spokoiny (2003). Results are provided in Table 3. Note that the estimated parameter for the standard normal random sample of the same size should be below 0.193 with probability 0.95 and below 0.202 with probability 0.99.
The logarithmic returns clearly show heavy tails. The estimated tail index for the standardized residuals is smaller for all methods. Note that the use of the parametric GARCH(1,1) model only partly explains the heavy tail effect while the other methods succeeded to eliminate the heavy tails in the standardized returns.
DAX and USD/GBP: Out-of-sample performance
Table 4 provides estimates of the predictive empirical likelihood risk (6.1) for four different time horizons ranging from two weeks to half a year. We observe, with respect to this criterion, that the local constant forecast significantly improves on the other three methods.
DAX and USD/GBP: Value-at-Risk performance
In Table 5 we provide estimates of the probability to exceed the VaR (5.1), defined at a 1% and 5% level using quantiles of a standard Gaussian distribution. The time horizon is two weeks. One can see that all the methods succeeded in forecasting the VaR-quantiles with, in most cases, best results for the local constant model.

polzehl, j. and spokoiny, v.

23

Table 6: DAX and USD/GBP: Value at Risk at 10 trading days. The best result in boldface.

Level
0.01 0.05

GARCH
DAX USD/GBP
0.1021 0.0402 0.0722 0.0284

NP-GARCH
DAX USD/GBP
0.1057 0.0405 0.0748 0.0286

SP-GARCH
DAX USD/GBP
0.1070 0.0401 0.0757 0.0283

Local Const
DAX USD/GBP
0.1056 0.0400 0.0747 0.0283

Table 6 provides the mean (over time) VaR (5.3) assigned by the four methods. This value characterizes the cost required to secure the asset. Here all four methods demonstrate a similar performance with a small benefit of using the parametric GARCH(1,1) model for the DAX series and of the local constant modeling for the USD/GBP series.
DAX and USD/GBP: Conclusion
Overall we see an advantage in using the local constant volatility model. It seems preferable with respect to risk management and also provides a better explanation for heavy tails, long range dependence and many other stylized facts of the financial time series.
7 Conclusion and Outlooks
The paper shows that the parametric GARCH(1,1) modeling has serious problems if the assumption of stationarity is violated. In particular, the IGARCH effect in the GARCH(1,1) model seems to be an artifact of nonstationarity. An integrated GARCH performs essentially as an exponential smoothing filter. This yields a very good short term ahead forecasting performance. However, an application of the estimated model to long term prediction is questionable because of possible model misspecification. More arguments and a similar conclusion can be found in Starica (2004).
Two new procedures are suggested which allow to model the nonstationarity in the observed financial time series via varying coefficient GARCH modeling. The method of estimation of time varying GARCH-models suggested in this paper as an extension of the Adaptive Weights idea from Polzehl and Spokoiny (2003) is very general in nature and can be easily extended to GARCH (p, q) , or to EGARCH (p, q) and TGARCH (p, q) models. The both methods demon-

24 garch versus local constant modeling
strate a reasonable performance, compared to the parametric GARCH(1,1) model. Especially the semiparametric model can be useful for the analysis of the integrated GARCH effect. However, the simulated results and applications to real data demonstrated that a more simple local constant model delivers better results in term of short time forecasting and applications to risk management.
We do not investigate the asymptotic properties and the rate of estimation delivered by the two proposed procedures. Although some properties can be established similarly to Polzehl and Spokoiny (2002), particularly, the important propagation condition. The main reason is that the obtained numerical results are mostly discouraging and do not motivate a rigorous theoretical study.
The general approach proposed in this paper and based on the adaptive weights idea seems to be applicable to many other models like hidden Markov chains, and can be very powerful in that area. This can be viewed as a topic of further research.
References
[1] Baba, Y., Engle, R.F., Kraft, D.F., and Kroner, K.F. (1990). Multivariate Simultaneous Generalized ARCH". mimeo, Department of Economics, University of California,
[2] I. Berkes, L. Horvath and P. Kokoszka (2003). Estimation of the maximal moment exponent of a GARCH(1,1) sequence, Econometric Theory 19, 565­586,
[3] Bollerslev, T. (1986). Generalized Autoregressive Conditional Heteroscedasticity. Journal of Econometrics, 31, 307-327.
[4] Chu, C.-S.J. (1995). Detecting Parameter Shift in GARCH Models. Econometric Reviews, 14, 241-266.
[5] Eberlein, E., Prause, K. (2002). The generalized hyperbolic model: financial derivatives and risk measures. In Mathematical Finance-Bachelier Congress 2000, H. Geman, D. Madan, S. Pliska, T. Vorst (Eds.), Springer Verlag, 245­267.
[6] Engle, R.F. (1982). Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of UK Inflation. Econometrica, 50, 987-1008.
[7] Engle, R. F. (ed.) (1995). ARCH, selected readings, Oxford University Press, Oxford.
[8] Engle, R.F. and Sheppard, K. (2004). Theoretical and Empirical Properties of Dynamic Conditional Correlation Multivariate GARCH. Unpublished manuscript.
[9] Fan, J., Jiang, J., Zhang, C. and Zhou, Z. (2003). Time-dependent diffusion models for term structure dynamics and the stock price volatility. Statistica Sinica, 13, 965-992.

polzehl, j. and spokoiny, v.

25

[10] Fan, J., Gu, J. (2003). Semiparametric estimation of value at risk, Econometrics Journal, 6, 260­289.

[11] Giraitis, L. and Robinson, P.M. (2001). Whittle estimation of ARCH models. Econometric Theory, 17, 608-631.

[12] McNeil, A. J., Frey, R. (2000). Estimation of tail-related risk measures for heteroscedastic financial time series: an extreme value approach, J. Empirical Finance 7: 271­300.

[13] Mercurio, D. and Spokoiny, V. (2004a). Statistical inference for time-inhomogeneous volatility models. Ann. Statist., 32 (2004), no.2, 577-602.

[14] Mercurio, D. and Spokoiny, V. (2004b). Estimation of time dependent volatility via local change point analysis with applications to Value-at-Risk, WIAS-Preprint No. 904, 2004.

[15] Mikosch, T. and Starica , C. (2000). Is it really long memory we see in financial returns? Extremes and Integrated Risk Management, Ed. P. Embrechts, Risk Books, (2000).

[16] Mikosch, T. and Starica , C. (2002). Changes of structure in financial time series and the GARCH model. Unpublished manuscript. http://www.math.chalmers.se/ starica/publi1.html

[17] Mikosch, T. and Starica , C. (2004). Non-stationarities in financial time series, the long range dependence and the IGARCH effects. The Review of Economics and Statistics 86 378-390.

[18] Yao, Q., and Peng, L. (2003). Least Absolute Deviations Estimation for ARCH and GARCH Models. Biometrika 90, no. 4 (2003), pp. 967-975.

[19] Polzehl, J. and Spokoiny, V. (2000). Adaptive weights smoothing with applications to image segmentation. J. of Royal Stat. Soc., 62, Series B, 335­354.

[20] Polzehl, J. and Spokoiny, V. (2002). Local likelihood modeling by adaptive weights smoothing. Preprint 787. WIAS 2002. http://www.wias-berlin.de/publications/preprints/787.

[21] Polzehl, J. and Spokoiny, V. (2003). Varying coefficient regression modeling by adaptive weights smoothing. Preprint 818, WIAS, 2003. http://www.wias-berlin.de/publications/preprints/818

[22] Starica , C. (2004). Is Garch(1,1) as good a the accolades of the Nobel prize would imply? http://www.math.chalmers.se/ starica/papersoft.pdf

model as Preprint.

[23] Starica , C., and Granger, C. (2004) Non-stationarities in stock returns. Preprint. http://www.math.chalmers.se/ starica/0903.pdf. To appear in Review of Economics and Statistics.

[24] Straumann, D. and Mikosch, T. (2003) Quasi-MLE in heteroscedastic times series: a stochastic recurrence equations approach. Preprint. http://www.math.ku.dk/ mikosch/preprint.html

SFB 649 Discussion Paper Series 2006
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Calibration Risk for Exotic Options" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
002 "Calibration Design of Implied Volatility Surfaces" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
003 "On the Appropriateness of Inappropriate VaR Models" by Wolfgang Härdle, Zdenk Hlávka and Gerhard Stahl, January 2006.
004 "Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification" by Harald Uhlig, January/February 2006.
005 "British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis" by Enzo Weber, January 2006.
006 "A Combined Approach for Segment-Specific Analysis of Market Basket Data" by Yasemin Boztu and Thomas Reutterer, January 2006.
007 "Robust utility maximization in a stochastic factor model" by Daniel Hernández­Hernández and Alexander Schied, January 2006.
008 "Economic Growth of Agglomerations and Geographic Concentration of Industries - Evidence for Germany" by Kurt Geppert, Martin Gornig and Axel Werwatz, January 2006.
009 "Institutions, Bargaining Power and Labor Shares" by Benjamin Bental and Dominique Demougin, January 2006.
010 "Common Functional Principal Components" by Michal Benko, Wolfgang Härdle and Alois Kneip, Jauary 2006.
011 "VAR Modeling for Dynamic Semiparametric Factors of Volatility Strings" by Ralf Brüggemann, Wolfgang Härdle, Julius Mungo and Carsten Trenkler, February 2006.
012 "Bootstrapping Systems Cointegration Tests with a Prior Adjustment for Deterministic Terms" by Carsten Trenkler, February 2006.
013 "Penalties and Optimality in Financial Contracts: Taking Stock" by Michel A. Robe, Eva-Maria Steiger and Pierre-Armand Michel, February 2006.
014 "Core Labour Standards and FDI: Friends or Foes? The Case of Child Labour" by Sebastian Braun, February 2006.
015 "Graphical Data Representation in Bankruptcy Analysis" by Wolfgang Härdle, Rouslan Moro and Dorothea Schäfer, February 2006.
016 "Fiscal Policy Effects in the European Union" by Andreas Thams, February 2006.
017 "Estimation with the Nested Logit Model: Specifications and Software Particularities" by Nadja Silberhorn, Yasemin Boztu and Lutz Hildebrandt, March 2006.
018 "The Bologna Process: How student mobility affects multi-cultural skills and educational quality" by Lydia Mechtenberg and Roland Strausz, March 2006.
019 "Cheap Talk in the Classroom" by Lydia Mechtenberg, March 2006. 020 "Time Dependent Relative Risk Aversion" by Enzo Giacomini, Michael
Handel and Wolfgang Härdle, March 2006. 021 "Finite Sample Properties of Impulse Response Intervals in SVECMs with
Long-Run Identifying Restrictions" by Ralf Brüggemann, March 2006. 022 "Barrier Option Hedging under Constraints: A Viscosity Approach" by
Imen Bentahar and Bruno Bouchard, March 2006.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "How Far Are We From The Slippery Slope? The Laffer Curve Revisited" by Mathias Trabandt and Harald Uhlig, April 2006.
024 "e-Learning Statistics ­ A Selective Review" by Wolfgang Härdle, Sigbert Klinke and Uwe Ziegenhagen, April 2006.
025 "Macroeconomic Regime Switches and Speculative Attacks" by Bartosz Makowiak, April 2006.
026 "External Shocks, U.S. Monetary Policy and Macroeconomic Fluctuations in Emerging Markets" by Bartosz Makowiak, April 2006.
027 "Institutional Competition, Political Process and Holdup" by Bruno Deffains and Dominique Demougin, April 2006.
028 "Technological Choice under Organizational Diseconomies of Scale" by Dominique Demougin and Anja Schöttner, April 2006.
029 "Tail Conditional Expectation for vector-valued Risks" by Imen Bentahar, April 2006.
030 "Approximate Solutions to Dynamic Models ­ Linear Methods" by Harald Uhlig, April 2006.
031 "Exploratory Graphics of a Financial Dataset" by Antony Unwin, Martin Theus and Wolfgang Härdle, April 2006.
032 "When did the 2001 recession really start?" by Jörg Polzehl, Vladimir Spokoiny and Ctlin Stric, April 2006.
033 "Varying coefficient GARCH versus local constant volatility modeling. Comparison of the predictive power" by Jörg Polzehl and Vladimir Spokoiny, April 2006.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

