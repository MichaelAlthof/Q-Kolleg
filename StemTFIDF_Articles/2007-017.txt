BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2007-017
Empirical Pricing Kernels and Investor Preferences
Kai Detlefsen* Wolfgang Härdle* Rouslan Moro**
* Humboldt-Universität zu Berlin, Germany ** Humboldt-Universität zu Berlin and DIW Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Empirical Pricing Kernels and Investor Preferences
K. Detlefsen1, W. K. H¨ardle2, R. A. Moro3,
1CASE ­ Center for Applied Statistics and Economics, Humboldt-Universita¨t zu Berlin, Spandauer Straße 1, 10178 Berlin, Germany; e-mail: detlefsen@wiwi.huberlin.de; phone: +49(0)30 2093-5807
2CASE ­ Center for Applied Statistics and Economics, Humboldt-Universita¨t zu Berlin, Spandauer Straße 1, 10178 Berlin, Germany; e-mail: haerdle@wiwi.huberlin.de; phone: +49(0)30 2093-5630
3German Institute for Economic Research, K¨onigin-Luise-Straße 5, 14195 Berlin, Germany; e-mail: rmoro@diw.de; phone: +49(0)30 8978-9262 and CASE ­ Center for Applied Statistics and Economics, Humboldt-Universit¨at zu Berlin, Spandauer Straße 1, 10178 Berlin

Abstract
This paper analyzes empirical market utility functions and pricing kernels derived from the DAX and DAX option data for three market regimes. A consistent parametric framework of stochastic volatility is used. All empirical market utility functions show a region of risk proclivity that is reproduced by adopting the hypothesis of heterogeneous individual investors whose utility functions have a switching point between bullish and bearish attitudes. The inverse problem of finding the distribution of individual switching points is formulated in the space of stock returns by discretization as a quadratic optimization problem. The resulting distributions vary over time and correspond to different market regimes.
JEL classification: G12, G13, C50
Keywords: Utility function, pricing kernel, behavioral finance, risk aversion, risk proclivity, Heston model

1 Introduction
Numerous attempts have been undertaken to describe basic principles on which the behaviour of individuals are based. Expected utility theory was originally proposed by J. Bernoulli in 1738. In his work J. Bernoulli used such terms as risk aversion and risk premium and proposed a concave (logarithmic) utility function, see Bernoulli (1956). The utilitarianism theory that emerged in the 18th century considered utility maximization as a principle for the organisation of society. Later the expected utility idea was applied to game theory and formalized by von Neumann and Morgenstern (1944). A utility function relates some observable variable, in most cases consumption, and an unobservable utility level that this consumption delivers. It was suggested that individuals' preferences are based on this unobservable utility: such bundles of goods are preferred that are associated with higher utility levels. It was claimed that three types of utility functions ­ concave, convex and linear ­ correspond to three types of individuals ­ risk averse, risk neutral and risk seeking. A typical economic agent was considered to be risk averse and this was quantified by coefficients of relative or absolute risk aversion. Another important step in the development of utility theory was the prospect theory of Kahneman and Tversky (1979). By behavioural experiments they found that people act risk averse above a certain reference point and risk seeking below it. This implies a concave form of the utility function above the reference point and a convex form below it.
Besides these individual utility functions, market utility functions have recently been analyzed in empirical studies by Jackwerth (2000), Rosenberg and Engle (2002) and others. Across different markets, the authors observed a common pattern in market utility functions: There is a reference point near the initial wealth and in a region around this reference point the market utility functions are convex. But for big losses or gains they show a concave form ­ risk aversion. Such utility functions disagree with the classical utility functions of von Neumann and Morgenstern (1944) and also with the findings of Kahneman and Tversky (1979). They are however in concordance with the utility function form proposed by Friedman and Savage (1948).
In this paper, we analyze how these market utility functions can be explained by aggregating individual investors' attitudes. To this end, we first determine empirical pricing kernels from DAX data. Our estimation procedure is based on historical and risk neutral densities and these distributions are derived with stochastic volatility models that are widely used in industry. From these pricing kernels we construct the corresponding market utility functions. Then we describe our method of aggregating individual utility functions to a market utility function. This leads to an inverse problem for
1

the density function that describes how many investors have the utility function of each type. We solve this problem by discrete approximation. In this way, we derive utility functions and their distribution among investors that allow to recover the market utility function. Hence, we explain how (and what) individual utility functions can be used to form the behaviour of the whole market.
The paper is organized as follows: In section 2, we describe the theoretical connection between utility functions and pricing kernels. In section 3, we present a consistent stochastic volatility framework for the estimation of both the historical and the risk neutral density. Moreover, we discuss the empirical pricing kernel implied by the DAX in 2000, 2002 and 2004. In section 4, we explain the utility aggregation method that relates the market utility function and the utility functions of individual investors. This aggregation mechanism leads to an inverse problem that is analyzed and solved in this section. In section 5, we conclude and discuss related approaches.

2 Pricing kernels and utility functions

In this section, we derive the fundamental relationship between utility functions and pricing kernels. It describes how a representative utility function can be derived from historical and risk-neutral distributions of assets. In the following sections, we estimate the empirical pricing kernel and observe in this way the market utility function.

First, we derive the price of a security in an equilibrium model: we consider an investor with a utility function U who has as initial endowment one share of stock. He can invest into the stock and a bond up to a final time when he can consume. His problem is to choose a strategy that maximizes the expected utility of his initial and terminal wealth. In continuous time, this leads to a well known optimization problem introduced by Merton (1973) for stock prices modelled by diffusions. In discrete time, it is a basic optimization problem, see Cochrane (2001).
From this result, we can derive the asset pricing equation

P0 = EP [(ST )MT ]

for a security on the stock (St) with payoff function  at maturity T . Here, P0 denotes the price of the security at time 0 and EP is the expectation with
respect to the real/historical measure P . The stochastic discount factor MT
is given by

MT = U (ST )/U (S0)

(1)

2

where  is a fixed discount factor. This stochastic discount factor is actually the projection of the general stochastic discount factor on the traded asset (St). The stochastic discount factor can depend on more variables in general. But as discussed in Cochrane (2001) this projection has the same interpretation for pricing as the general stochastic discount factor.

Besides this equilibrium based approach, Black and Scholes (1973) derived the price of a security relative to the underlying by constructing a perfect hedge. The resulting continuous delta hedging strategy is equivalent to pricing under a risk neutral measure Q under which the discounted price process of the underlying becomes a martingale. Hence, the price of a security is given by an expected value with respect to a risk neutral measure Q:

P0 = EQ [exp(-rT )(ST )]

If p denotes the historical density of ST (i.e. P (ST  s) =

s -

p(x)

dx)

and

q the risk neutral density of ST (i.e. Q(ST  s) =

s -

q(x)

dx)

then

we

get

P0 = exp(-rT ) (x)q(x)dx

= exp(-rT )

q(x) (x) p(x)dx

p(x)

= EP

exp(-rT

)(ST

)

q(ST p(ST

) )

(2)

Combining equations (1) and (2) we see

 U (s) = exp(-rT ) q(s) .

U (S0)

p(s)

Defining the pricing kernel by K = q/p we conclude that the form of the market utility function can be derived from the empirical pricing kernel by integration:

s exp(-rT ) q(x)

U (s) = U (S0) + U (S0)
S0



dx p(x)

s exp(-rT )

= U (S0) + U (S0)
S0



K (x)dx

because S0 is known.

3

As an example, we consider the model of Black and Scholes (1973) where the stock follows a geometric Brownian motion

dSt/St = µdt + dWt

(3)

Here the historical density p of St is log-normal, i.e.

p(x) = 1  1 exp - 1 log x - µ~ 2 , x > 0

x 2~2

2 ~

 where µ~ = (µ - 2/2)t + log S0 and ~ =  t. Under the risk neutral measure Q the drift µ is replaced by the riskless interest rate r, see e.g. Harrison and Pliska (1981). Thus, also the risk neutral density q is log-normal. In this way, we can derive the pricing kernel

K(x) =

x

-

µ-r 2

exp{(µ

-

r)(µ

+

r

-

2)T /(22)}.

S0

This pricing kernel has the form of a derivative of a power utility

x - K(x) = 
S0

(µ-r)(µ+r-2 )T
where the constants are given by  = e 22

and



=

µ-r 2

.

This gives

a utility function corresponding to the underlying (3)

U (ST )

=

(1 -

µ- 2

r )-1

S (1-

µ-r 2

)

T

where we ignored additive and multiplicative constants. In this power utility function the risk aversion is not given by the market price of risk (µ - r)/. Instead investors take the volatility more into account. The expected return µ - r that is adjusted by the riskfree return is related to the variance. This results in a higher relative risk aversion than the market price of risk.

A utility function corresponding to the Black-Scholes model is shown in the upper panel of figure 1 as a function of returns. In order to make different market situations comparable we consider utility functions as functions of (half year) returns R = S0.5/S0. We chose the time horizon of half a year ahead for our analysis. Shorter time horizons are interesting economically and moreover the historical density converges to the Dirac measure so that results become trivial (in the end). Longer time horizons are economically

4

utitlity

utility

-0.4 -0.6 -0.8
-1 -1.2 -1.4
0.5 1 1.5 2
1.5 returns
1
0.5
0 0.5 1 1.5 2
returns
Figure 1: up: Utility function in the Black Scholes model for T = 0.5 years ahead and drift µ = 0.1, volatility  = 0.2 and interest rate r = 0.03. down: Market utility function on 06/30/2000 for T = 0.5 years ahead.
5

more interesting but it is hardly possible to estimate the historical density for a long time ahead. It neither seems realistic to assume that investors have clear ideas where the DAX will be in e.g. 10 years. For these reasons we use half a year as future horizon. Utility functions U~ of returns are defined by:
U~ (R) := U (RS0), R > 0
where S0 denotes the value of the DAX on the day of estimation. Because of U = cK for a constant c we have U~ (R) = cK(RS0)S0 and we see that also utility functions of returns are given as integrals of the pricing kernel. The change to returns allows us to compare different market regimes independently of the initial wealth. In the following we denote the utility functions of returns by the original notation U . Hence, we suppress in the notation the dependence of the utility function U on the day of estimation t.
The utility function corresponding to the model of Black and Scholes (1973) is a power utility, monotonically increasing and concave. But such classical utility functions are not observed on the market. Parametric and nonparametric models that replicate the option prices all lead to utility functions with a hump around the initial wealth level. This is described in detail later but is shown already in figure 1. The upper panel presents the utility function corresponding to Black-Scholes model with a volatility of 20% and an expected return of 10%. The function is concave and implies a constant relative risk aversion. The utility function estimated on the bullish market in summer 2000 is presented in the lower panel. Here, the hump around the money is clearly visible. The function is no more concave but has a region where investors are risk seeking. This risk proclivity around the money is reflected in a negative relative risk aversion.
3 Estimation
In this section, we start by reviewing some recent approaches for estimating the pricing kernel. Then we describe our method that is based on estimates of the risk neutral and the historical density. The risk neutral density is derived from option prices that are given by an implied volatility surface and the historical density is estimated from the independent data set of historical returns. Finally, we present the empirical pricing kernels and the inferred utility and relative risk aversion functions.
6

3.1 Estimation approaches for the pricing kernel
There exist several ways and methods to estimate the pricing kernel. Some of these methods assume parametric models while others use nonparametric techniques. Moreover, some methods estimate first the risk neutral and subjective density to infer the pricing kernel. Other approaches estimate directly the pricing kernel.
Ait-Sahalia and Lo (1998) derive a nonparametric estimator of the risk neutral density based on option prices. In Ait-Sahalia and Lo (2000), they consider the empirical pricing kernel and the corresponding risk aversion using this estimator. Moreover, they derive asymptotic properties of the estimator that allow e.g. the construction of confidence bands. The estimation procedure consists of two steps: First, the option price function is determined by nonparametric kernel regression and then the risk neutral density is computed by the formula of Breeden and Litzenberger (1978). Advantages of this approach are the known asymptotic properties of the estimator and the few assumptions necessary.
Jackwerth (2000) analyses risk aversion by computing the risk neutral density from option prices and the subjective density from historical data of the underlying. For the risk neutral distribution, he applies a variation of the estimation procedure described in Jackwerth and Rubinstein (1996): A smooth volatility function derived from observed option prices gives the risk neutral density by differentiating it twice. The subjective density is approximated by a kernel density computed from historical data. In this method bandwidths have to be chosen as in the method of Ait-Sahalia and Lo (1998).
Rosenberg and Engle (2002) use a different approach and estimate the subjective density and directly (the projection of) the pricing kernel. This gives the same information as the estimation of the two densities because the risk neutral density is the product of the pricing kernel and the subjective density. For the pricing kernel, they consider two parametric specifications as power functions and as exponentials of polynomials. The evolution of the underlying is modelled by GARCH processes. As the parametric pricing kernels lead to different results according to the parametric form used this parametric approach appears a bit problematic.
Chernov (2003) also estimates the pricing kernel without computing the risk neutral and subjective density explicitly. Instead of assuming directly a parametric form of the kernel he starts with a (multi dimensional) modified model of Heston (1993) and derives an analytic expression for the pricing kernel by the Girsanov theorem, see Chernov (2000) for details. The ker-
7

nel is estimated by a simulated method of moments technique from equity, fixed income and commodities data and by reprojection. An advantage of this approach is that the pricing kernel is estimated without assuming an equity index to approximate the whole market portfolio. But the estimation procedure is rather complex and model dependent.
In a recent paper, Barone-Adesi et al. (2004) price options in a GARCH framework allowing the volatility to differ between historical and risk neutral distribution. This approach leads to acceptable calibration errors between the observed option prices and the model prices. They estimate the historical density as a GARCH process and consider the pricing kernel only on one day. This kernel is decreasing which coincides with standard economic theory. But the general approach of changing explicitly the volatility between the historical and risk neutral distribution is not supported by the standard economic theory.
We estimate the pricing kernel in this paper by estimating the risk neutral and the subjective density and then deriving the pricing kernel. This approach does not impose a strict structure on the kernel. Moreover, we use accepted parametric models because nonparametric techniques for the estimation of second derivatives depend a lot on the bandwidth selection although they yield the same pricing kernel behaviour over a wide range of bandwidths. For the risk neutral density we use a stochastic volatility model that is popular both in academia and in industry. The historical density is more difficult to estimate because the drift is not fixed. Hence, the estimation depends more on the model and the length of the historical time series. In order to get robust results we consider different (discrete) models and different lengths. In particular, we use a GARCH model that is the discrete version of the continuous model for the risk neutral density. In the following, we describe these models, their estimation and the empirical results.
3.2 Estimation of the risk neutral density
Stochastic volatility models are popular in industry because they replicate the observed smile in the implied volatility surfaces (IVS) rather well and moreover imply rather realistic dynamics of the surfaces. Nonparametric approaches like the local volatility model of Dupire (1994) allow a perfect fit to observed price surfaces but their dynamics are in general contrary to the market. As Bergomi (2005) points out the dynamics are more important for modern products than a perfect fit. Hence, stochastic volatility models are popular.
We consider the model of Heston (1993) for the risk neutral density be-
8

cause it can be interpreted as the limit of GARCH models. The Heston model has been refined further in order to improve the fit, e.g. by jumps in the stock price or by a time varying mean variance level. We use the original Heston model in order to maintain a direct connection to GARCH processes. Although it is possible to estimate the historical density also with the Heston model e.g. by Kalman filter methods we prefer more direct approaches in order to reduce the dependence of the results on the model and the estimation technique.

The stochastic volatility model of Heston (1993) is given by the two stochastic differential equations:

dSt = rdt + St

VtdWt1

where the variance process is modelled by a square-root process:

dVt = ( - Vt)dt +  VtdWt2
and W 1 and W 2 are Wiener processes with correlation  and r is the risk free interest rate. The first equation models the stock returns by normal innovations with stochastic variance. The second equation models the stochastic variance process as a square-root diffusion.

The parameters of the model all have economic interpretations:  is called the long variance because the process always returns to this level. If the variance Vt is e.g. below the long variance then  - Vt is positive and the drift drives the variance in the direction of the long variance.  controls the speed at which the variance is driven to the long variance. In calibrations, this parameter changes a lot and makes also the other parameters instable. To avoid this problem, the reversion speed is kept fixed in general. We follow this approach and choose  = 2 as Bergomi (2005) does. The volatility of variance  controls mainly the kurtosis of the distribution of the variance. Moreover, there are the initial variance V0 of the variance process and the correlation  between the Brownian motions. This correlation models the leverage effect: When the stock goes down then the variance goes up and vice versa. The parameters also control different aspects of the implied volatility surface. The short (long) variance determines the level of implied volatility for short (long) maturities. The correlation creates the skew effect and the volatility of variance controls the smile.
The variance process remains positive if the volatility of variance  is small enough with respect to the product of the mean reversion speed  and

9

the long variance level  (i.e. 2 > 2). As this constraint leads often to significantly worse fits to implied volatility surfaces it is in general not taken into account and we follow this approach.

The popularity of this model can probably be attributed to the semiclosed form of the prices of plain vanilla options. Carr and Madan (1999) showed that the price C(K, T ) of a European call option with strike K and maturity T is given by

exp{- ln(K)} C(K, T ) =


+
exp{-iv ln(K)}T (v)dv
0

for a (suitable) damping factor  > 0. The function T is given by

T (v)

=

exp(-rT )T {v - ( + 1)i} 2 +  - v2 + i(2 + 1)v

where T is the characteristic function of log(ST ). This characteristic function is given by

T (z)

=

exp{



(z)

-(z2 +

coth

(z)T 2

iz)V0 +-

iz

}

×

exp{

T

(-iz) 2

(cosh

(z)T 2

+

+ izT r + iz log(S0)}

-iz (z)

sinh

(z)T 2

2
) 2

(4)

where (z) d=ef 2(z2 + iz) + ( - iz)2, see e.g. Cizek et al. (2005).

For the calibration we minimize the absolute error of implied volatilities based on the root mean square error:

ASEt d=ef

n
n-1{IVimod(t) - IVimar(t)}2
i=1

where mod refers to a model quantity, mar to a quantity observed on the market and IV (t) to an implied volatility on day t. The index i runs over all n observations of the surface on day t.
It is essential for the error functional ASEt which observed prices are used for the calibration. As we investigate the pricing kernel for half a year to maturity we use only the prices of options that expire in less than 1.5 years. In order to exclude liquidity problems occurring at expiry we consider for the

10

calibration only options with more than 1 month time to maturity. In the moneyness direction we restrict ourselves to strikes 50% above or below the spot for liquidity reasons.
The risk neutral density is derived by estimation of the model parameters by a least squares approach. This amounts to the minimization of the error functional ASEt. Cont and Tankov (2004) provided evidence that such error functionals may have local minima. In order to circumvent this problem we apply a stochastic optimization routine that does not get trapped in a local minimum. To this end, we use the method of differential evolution developed by Storn and Price (1997).

Having estimated the model parameters we know the distribution of

XT = log ST in form of the characteristic function T , see (4). Then the corresponding density f of XT can be recovered by Fourier inversion:

1 f (x) =
2


eitxT (t)dt,
-

see e.g. Billingsley (1995). This integral can be computed numerically. Finally, the risk neutral density q of ST = exp(XT ) is given as a trans-
formed density:

q(x) = 1 f {log(x)}. x

This density q is risk neutral because it is derived from option prices and options are priced under the risk neutral measure. This measure is applied because banks replicate the payoff of options so that no arbitrage conditions determine the option price, see e.g. Rubinstein (1994). An estimated risk neutral density is presented in figure 2. It is estimated from the implied volatility shown in figure 3 for the day 24/03/2000. The distribution is right skewed and its mean is fixed by the martingale property. This implies that the density is low for high profits and high for high losses. Moreover, the distribution is not symmetrical around the neutral point where there are neither profits nor losses. For this and all the following estimations we approximate the risk free interest rates by the EURIBOR. On each trading day we use the yields corresponding to the maturities of the implied volatility surface. As the DAX is a performance index it is adjusted to dividend payments. Thus, we do not have to consider dividend payments explicitly.

3.3 Estimation of the historical density
While the risk neutral density is derived from option prices observed on the day of estimation we derive the subjective density from the historical time

11

3 2.5
2 1.5
1 0.5
0 0.5

1 1.5 return

2

Figure 2: Risk neutral density on 24/03/2000 half a year ahead.

implied volatility

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15 0.5
0.75

1 1.25

return

1 0.5

1.5

time to maturity

2

Figure 3: Implied volatility surface on 24/03/00.

12

model GARCH in mean discrete Heston observed returns

time period 2.0y 2.0y 1.0y

Table 1: Models and the time periods used for their estimation.

series of the index. Hence, the two data sets are independent in the sense that the option prices reflect the future movements and the historical time series the past.
The estimation of the historical density seems more difficult than the estimation of the risk neutral density because the drift is not fixed and it depends in general on the length of the time series. Because of these difficulties we use different models and time horizons for the historical density: First, we estimate a GARCH in mean model for the returns. Returns are generally assumed to be stationary and we confirmed this at least in the time intervals we consider. The mean component in the GARCH model is important to reflect different market regimes. We estimate the GARCH model from the time series of the returns of the last two year because GARCH models require quite long time series for the estimation in order to make the standard error reasonably small. We do not choose longer time period for the estimation because we want to consider special market regimes. Besides this popular model choice we apply a GARCH model that converges in the limit to the Heston model that we used for the risk neutral density. As this model is also hard to estimate we use again the returns of the last 2 years for this model. Moreover, we consider directly the observed returns of the last year. The models and their time period for the estimation are presented in table 1. All these models give by simulation and smoothing the historical density for half a year ahead.
The GARCH estimations are based on the daily log-returns
Ri = log(Sti) - log(Sti-1)
where (St) denotes the price process of the underlying and ti, i = 1, 2, . . . denote the settlement times of the trading days. Returns of financial assets have been analyzed in numerous studies, see e.g. Cont (2001). A model that has often been successfully applied to financial returns and their stylized facts

13

is the GARCH(1,1) model. This model with a mean is given by
Ri = µ + iZi i2 =  + Ri2-1 + i2-1
where (Zi) are independent identically distributed innovations with a standard normal distribution, see e.g. Franke et al. (2004). On day tj the model parameters µ, ,  and  are estimated by quasi maximum likelihood from the observations of the last two years, i.e. Rj-504, . . . , Rj assuming 252 trading days per year.
After the model parameters have been estimated on day tj from historical data the process of logarithmic returns (Ri) is simulated half a year ahead, i.e. until time tj + 0.5. In such a simulation µ, ,  and  are given and the time series (i) and (Ri) are unknown. The values of the DAX corresponding to the simulated returns are then given by inverting the definition of the log returns:
Sti = Sti-1 exp(Ri)
where we start with the observed DAX value on day tj. Repeating the simulation N times we obtain N samples of the distribution of Stj+0.5. We use N = 2000 simulations because tests have shown that the results become robust around this number of simulations.
From these samples we estimate the probability density function of Stj+0.5 (given (Stj-126, . . . , Stj )) by kernel density estimation. We apply the Gaussian kernel and choose the bandwidth by Silverman's rule of thumb, see e.g. Silverman (1986). This rule provides a trade-off between oversmoothing ­ resulting in a high bias ­ and undersmoothing ­ leading to big variations of the density. We have moreover checked the robustness of the estimate relative to this bandwidth choice. The estimation results of a historical density are presented in figure 4 for the day 24/03/2000. This density that represents a bullish market is has most of its weight in the profit region and its tail for the losses is relatively light.

As we use the Heston model for the estimation of the risk neutral density we consider in addition to the described GARCH model a GARCH model that is a discrete version of the Heston model. Heston and Nandi (2000) show that the discrete version of the square-root process is given by

Vi =  + Vi-1 + (Zi-1 -  Vi-1)

and the returns are modelled by 1
Ri = µ - 2 Vi +

ViZi

14

2 1.8 1.6 1.4 1.2
1 0.8 0.6 0.4 0.2
0 0.5

1 1.5 return

2

Figure 4: Historical density on 24/03/2000 half a year ahead.
where (Zi) are independent identically distributed innovations with a standard normal distribution. Having estimated this model by maximum likelihood on day tj we simulate it half a year ahead and then smooth the samples of Stj+0.5 in the same way as in the other GARCH model.
In addition to these parametric models, we consider directly the observed returns over half a year
R~i = Sti /Sti-126 .
In this way, we interpret these half year returns as samples from the distribution of the returns for half a year ahead. Smoothing these historical samples of returns gives an estimate of the density of returns and in this way also an estimate of the historical density of Stj+0.5.
3.4 Empirical pricing kernels
In contrast to many other studies that concentrate on the S&P500 index we analyze the German economy by focusing on the DAX, the German stock index. This broad index serves as an approximation to the German economy. We use two data sets: A daily time series of the DAX for the estimation of the subjective density and prices of European options on the DAX for the estimation of the risk neutral density.
15

dax

9000 8000 7000 6000 5000 4000 3000 2000
1998

1999

2000

2001

2002

year

2003

2004

Figure 5: DAX, 1998 - 2004.
1.0y 2.0y 03/2000 1.63 1.57 07/2002 0.66 0.54 06/2004 1.11 0.98
Table 2: Market regimes in 2000, 2002 and 2004 described by the return S0/S0- for periods  = 1.0y, 2.0y.
In figure 5, we present the DAX in the years 1998 to 2004. This figure shows that the index reached its peak in 2000 when all the internet firms were making huge profits. But in the same year this bubble burst and the index fell afterwards for a long time. The historical density is estimated from the returns of this time series. We analyze the market utility functions in March 2000, July 2002 and June 2004 in order to consider different market regimes. We interpret 2000 as a bullish, 2002 as a bearish and 2004 as a unsettled market. These interpretations are based on table 2 that describes the changes of the DAX over the preceding 1 or 2 years. (In June 2004 the market went up by 11% in the last 10 months.)
A utility function derived from the market data is a market utility function. It is estimated as an aggregate for all investors as if the representative investor existed. A representative investor is however just a convenient con-

16

struction because the existence of the market itself implies that the asset

is bought and sold, i.e. at least two counterparties are required for each

transaction.

In section 2 we identified the market utility function (up to linear trans-

formations) as

R

U (R) = K(x)dx

R0

where K is the pricing kernel for returns. It is defined by

K(x) = q(x)/p(x)

in terms of the historical and risk neutral densities p and q of returns. Any utility function (both cardinal and ordinal) can be defined up to a linear transformation, therefore we have identified the utility functions sufficiently. In section 3.3 we proposed different models for estimating the historical density. In figure 6 we show the pricing kernels resulting from the different estimation approaches for the historical density. The figure shows that all three kernels are quite similar: They have the same form, the same characteristic features like e.g. the hump and differ in absolute terms only a little. This demonstrates the economic equivalence of the three estimation methods on this day and this equivalence holds also for the other days. In the following we work with historical densities that are estimated by the observed returns.
Besides the pricing kernel and the utility function we consider also the risk attitudes in the markets. Such risk attitudes are often described in terms of relative risk aversion that is defined by

RRA(R) = -R U

(R) .

U (R)

Because of U = cK = cq/p for a constant c the relative risk aversion is also given by

RRA(R)

=

-R q

(R)p(R)

-

q(R)p

(R) q(R) /

=

R

p (R) - q (R)

.

p2(R)

p(R)

p(R) q(R)

Hence, we can estimate the relative risk aversion from the estimated historical and risk neutral densities.

In figure 7 we present the empirical pricing kernels in March 2000, July 2002 and June 2004. The dates represent a bullish, a bearish and an unsettled markets, see table 2. All pricing kernels have a proclaimed hump located

17

epk

7 returns GARCH
6 GARCH in mean
5
4
3
2
1
0 0.5 1 1.5 2
return
Figure 6: Empirical pricing kernel on 24/03/2000 (bullish market).
at small profits. Hence, the market utility functions do not correspond to standard specification of utility functions. We present the pricing kernels only in regions around the initial DAX (corresponding to a return of 1) value because the kernels explode outside these regions. This explosive behaviour reflects the typical pricing kernel form for losses. The explosion of the kernel for large profits is due to numerical problems in the estimation of the very low densities in this region. But we can see that in the unsettled market the kernel is concentrated on a small region while the bullish and bearish markets have wider pricing kernels. The hump of the unsettled market is also narrower than in the other two regimes. The bullish and bearish regimes have kernels of similar width but the bearish kernel is shifted to the loss region and the bullish kernel is located mainly in the profit area. Moreover, the figures show that the kernel is steeper in the unsettled markets than in the other markets. But this steepness cannot be interpreted clearly because pricing kernels are only defined up to a multiplicative constant.
The pricing kernels are the link between the relative risk aversion and the utility functions that are presented in figure 8. These utility functions are only defined up to linear transformations, see section 2. All the utility functions are increasing but only the utility function of the bullish market is concave. This concavity can be seen from the monotonicity of the kernel, see figure 7. Actually, this non convexity can be attributed to the quite special
18

7 bearish market bullish market
6 sidewards market
5
4
3
2
1
0 0.5 1 1.5 2
return
Figure 7: Empirical pricing kernel on 24/03/2000 (bullish), 30/07/2002 (bearish) and 30/06/2004 (unsettled or sidewards market).
form of the historical density which has two modes on this date, see figure 4. Hence, we presume that also this utility function has in general a region of convexity. The other two utility functions are convex in a region of small profits where the bullish utility is almost convex. The derivatives of the utility functions cannot be compared directly because utility functions are identified only up to multiplicative constants. But we can compare the ratio of the derivatives in the loss and profit regions for the three dates because the constants cancel in these ratios. We see that the derivatives in the loss region are highest in the bullish and lowest in the bearish market and vice versa in the profit region. Economically these observations can be interpreted in such a way that in the bullish market a loss (of 1 unit) reduces the utility stronger than in the bearish market. On the other hand, a gain (of 1 unit) increases the utility less than in the bearish market. The unsettled market shows a behaviour between these extreme markets. Hence, investors fear in a good market situation losses more than in a bad situation and they appreciate profits in a good situation less than in a bad situation.
Finally, we consider the relative risk aversions in the three market regimes. These risk aversions are presented in figure 9, they do not depend on any constants but are completely identified. We see that the risk aversion is smallest in all markets for a small profit that roughly corresponds to the
19

90 80 70 60 50 40 30 20 10
0 0.5

bearish market bullish market sidewards market
1 1.5 return

2

Figure 8: Market utility functions on 24/03/2000 (bullish), 30/07/2002 (bearish) and 30/06/2004 (unsettled or sidewards market).
initial value plus a riskless interest on it. In the unsettled regime the market is risk seeking in a small region around this minimal risk aversion. But then the risk aversion increases quite fast. Hence, the representative agent in this market is willing to take small risks but is sensitive to large losses or profits. In the bullish and bearish regimes the representative agent is less sensitive to large losses or profits than in the unsettled market. In the bearish situation the representative agent is willing to take more risks than in the bullish regime. In the bearish regime the investors are risk seeking in a wider region than in the unsettled regime. In this sense they are more risk seeking in the bearish market. In the bullish market ­ on the other hand ­ the investors are never risk seeking so that they are less risk seeking than in the unsettled market.
The estimated utility functions most closely follow the specification proposed by Friedman & Savage (1948). The utility function proposed by Kahneman & Tversky (1979) consists of one concave and one convex segment and is less suitable for describing the observed behaviour, see figure 10. Both utility functions were proposed to account for two opposite types of behaviour with respect to risk attitudes: buying insurance and gambling. Any utility function that is strictly concave fails to describe both risk attitudes. Most notable examples are the quadratic utility function with the linear pricing
20

60 50 40 30 20 10
0 -10
0.5

bearish market bullish market sidewards market

1 1.5 return

2

Figure 9: Relative risk aversions on 24/03/2000 (bullish), 30/07/2002 (bearish) and 30/06/2004 (unsettled or sidewards market).
kernel as in the CAPM model and the CRRA utility function. These functions are presented in figure 10. Comparing this theoretical figure with the empirical results in figure 7 we see clearly the shortcoming of the standard specifications of utility functions to capture the characteristic hump of the pricing kernels.
4 Individual investors and their utility functions
In this section, we introduce a type of utility function that has two regions of different risk aversion. Then we describe how individual investors can be aggregated to a representative agent that has the market utility function. Finally, we solve the resulting estimation problem by discretization and estimate the distribution of individual investors.
4.1 Individual Utility Function
We learn from figures 10 and 7 that the market utility differs significantly from the standard specification of utility functions. Moreover, we can observe
21

1
0.5
0 0.5 1 1.5 2 2 return 1.5 1 0.5 0.5 1 1.5 2 3 return 2 1 0 0.5 1 1.5 2
return
Figure 10: Common utility functions (solid) and their pricing kernels (dotted) (upper: quadratic, middle: power, lower panel: Kahneman and Tversky utility function).
22

from the estimated utility functions 8 that the loss part and the profit part of the utility functions can be quite well approximated with hyperbolic absolute risk aversion (HARA) functions, k = 1, 2:

U (k)(R) = ak(R - ck)k + bk,

where the shift parameter is ck. These power utility functions become infinitely negative for R = ck and can be extended by U (k)(R) = - for R  ck, i.e. investors will avoid by all means the situation when R  ck. The CRRA utility function has ck = 0.
We try to reconstruct the market utility of the representative investor by
individual utility functions and hence assume that there are many investors
on the market. Investor i will be attributed with a utility function that
consists of two HARA functions:

Ui(R) =

max {U (R, 1, c1); U (R, 2, c2,i)} , -, if R  c1

if

R > c1

where U (R, , c) = a(R - c) + b,  = (a, b, ) , c2,i > c1. If a1 = a2 = 1, b1 = b2 = 0 and c1 = c2 = 0, we get the standard CRRA utility function.
The parameters 1 and 2 and c1 are the same for all investors who differ only with the shift parameter c2. 1 and c1 are estimated from the lower part of the utility market function, where all investors probably agree that
the market is "bad". 2 is estimated from the upper part of the utility function where all investors agree that the state of the world is "good". The
distribution of c2 uniquely defines the distribution of switching points and is computed in section 4.3. In this way a bear part Ubear(R) = U (R, 1, c1) and a bull part Ubull(R) = U (R, 1, c2) can be estimated by least squares.
The individual utility function can then be denoted conveniently as:

Ui(R) =

max {Ubear(R); Ubull(R, ci)} , -, if R  c1.

if

R > c1;

(5)

Switching between Ubear and Ubull happens at the switching point z, whereas Ubear(z) = Ubull(z, ci). The switching point is uniquely determined by ci  c2,i. The notations bear and bull have been chosen because Ubear is activated when returns are low and Ubull when returns are high.
Each investor is characterised by a switching point z. The smoothness
of the market utility function is the result of the aggregation of different
attitudes. Ubear characterizes more cautious attitudes when returns are low and Ubull describes the attitudes when the market is booming. Both Ubear

23

35
30
25
20
15
10
5 0.95 1 1.05 1.1 1.15 1.2 1.25
return
Figure 11: Market utility function (solid) with bearish (dashed) and bullish (dotted) part of an individual utility function 5 estimated in the unsettled market of 30/06/2004.
and Ubull are concave. However, due to switching the total utility function can be locally convex.
These utility functions are illustrated in figure 11 that shows the results for the unsettled market. We observe/estimate the market utility function that does not correspond to standard utility approaches because of the convex region. We propose to reconstruct this phenomenon by individual utility functions that consist of a bearish part and a bullish part. While the bearish part is fixed for all investors the bullish part starts at the switching point that characterizes an individual investor. By aggregating investors with different switching points we reconstruct the market utility function. We describe the aggregation in section 4.2 and estimate the distribution of switching points in section 4.3. In this way we explain the special form of the observed market utility functions.
4.2 Market Aggregation Mechanism
We consider the problem of aggregating individual utility functions to a representative market utility function. A simple approach to this problem is to identify the market utility function with an average of the individual utility functions. To this end one needs to specify the observable states of the world
24

in the future by returns R and then find a weighted average of the utility

functions for each state. If the importance of the investors is the same, then

the weights are equal:

1N

U (R) = N

Ui(R),

i=1

where N is the number of investors. The problem that arises in this case is

that utility functions of different investors can not be summed up since they

are incomparable.

Therefore, we propose an alternative aggregation technique. First we

specify the subjective states of the world given by utility levels u and then

aggregate the outlooks concerning the returns in the future R for each per-

ceived state. For a subjective state described with the utility level U , such

that

u = U1(R1) = U2(R2) = . . . = UN (RN )

the aggregate estimate of the resulting returns is

1 RA(u) = N

N

Ui-1(u)

i=1

(6)

if all investors have the same market power. The market utility function UM resulting from this aggregation is given by the inverse RA-1.
In contrast to the naive approach described at the beginning of this sec-
tion, this aggregation mechanism is consistent under transformations: if all
individual utility functions are changed by the same transformation then the
resulting market utility is also given by the transformation of the original
aggregated utility. We consider the individual utility functions Ui and the resulting aggregate UM . In addition, we consider the transformed individual utility functions Ui(x) = {Ui(x)} and the corresponding aggregate UM where  is a transformation. Then the aggregation is consistent in the sense that UM = (UM ). This property can be seen from

(UM )-1(u)

=

1 N

N
(Ui)-1(u)

i=1

1 =
N

N

Ui-1{-1(u)}

i=1

= UM-1{-1(u)}

The naive aggregation is not consistent in the above sense as the following example shows: We consider the two individual utility functions U1(x) = x

25

 and U2(x) = x/2 under the logarithmic transformation  = log. Then the naively aggregated utility is given by UM (x) = 3 x/4. Hence, the transformed aggregated utility is {UM (x)} = log(3/4) + log(x)/2. But the aggregate of the transformed individual utility functions is

UM (x)

=

1 2

 log( x) + log( x/2)

11 = log + log(x)/2.
22

This implies that UM = (UM ) in general.

This described aggregation approach can be generalized in two ways: If the individual investors have different market power then we use the corresponding weights wi in the aggregation (6) instead of the uniform weights. As the number of market participants is in general big and unknown it is better to use a continuous density f instead of the discrete distributions given by the weights wi. These generalizations lead to the following aggregation

RA(u) = U -1(·, z)(u)f (z)dz

where U (·, z) is the utility function of investor z. We assume in the following that the investors have utility function of the form described in section 4.1. In the next section we estimate the distribution of the investors who are parametrized by z.

4.3 The Estimation of the Distribution of Switching Points
Using the described aggregation procedure, we consider now the problem of replicating the market utility by aggregating individual utility functions. To this end, we choose the parametric utility functions U (·, z) described in 4.1 and try to recover with them the market utility UM . We do not consider directly the utility functions but minimize instead the distance between the inverse functions:

min
f

U -1(·, z)f (z)dz - UM-1 L2(P~)

(7)

where P~ is image measure of the historical measure P on the returns under the transformation UM . As the historical measure has the density p the

26

transformation theorem for densities implies that P~ has the density
p~(u) = p{UM-1(u)}/UM {UM-1(u)}.
With this density the functional to be minimized in problem (7) can be stated as
2
U -1(u, z)f (z)dz - UM-1(u) p~(u) du
2
= U -1(u, z)f (z)dz - UM-1(u) p{UM-1(u)}/UM {UM-1(u)} du
2
= U -1(u, z)f (z)dz - UM-1(u) p{UM-1(u)}(UM-1) (u) du
because the derivative of the inverse is given by (g-1) (y) = 1/g {g-1(y)}. Moreover, we can apply integration by substitution to simplify this expression further
2
U -1(u, z)f (z)dz - UM-1(u) p{UM-1(u)}(UM-1) (u) du
2
= U -1{UM (x), z}f (z)dz - x p(x) dx.

For replicating the market utility by minimizing (7) we observe first that we have samples of the historical distribution with density p. Hence, we can replace the outer integral by the empirical expectation and the minimization problem can be restated as

1n min
fn
i=1

2
g{UM (xi), z}f (z)dz - xi

where x1 . . . , xn are the samples from the historical distribution and g = U -1.

Replacing the density f by a histogram f (z) =

J j=1

j

IBj

(z

)

with

bins

Bj, hj = |Bj|, the problem is transformed into

1n

min

j

n
i=1

J2
g~(i, j)j - xi
j=1

where g~(i, j) = Bj g{UM (xi), z}dz.

27

Hence, the distribution of switching points can be estimated by solving the quadratic optimization problem

1n

min

j

n
i=1

J2
g~(i, j)j - xi ,
j=1
s.t. j  0,
J
jhj = 1.
j=1

Such quadratic optimization problems are well known and their solutions can be obtained using standard techniques, see e.g. Mehrotra (1992) or Wright (1998).

We present in figures 12­14 the estimated distribution of switching points in the bullish (24/03/2000), bearish (30/07/2002) and unsettled (30/06/2004) markets. The distribution density f was computed for 100 bins but we checked the broad range of binwidths. The width of the distribution varies greatly depending on the regularisation scheme, for example as represented by the number of bins. The location of the distribution maximum, however, remains constant and independent from the computational method.

The maximum and the median of the distribution, i.e. the returns at which half of investors have bearish and bullish attitudes, depend on the year. For example, in the bullish market (Figure 12) the peak of the switching point distribution is located in the area of high returns around R = 1.07 for half a year. On the contrary, in the bearish market (Figure 13) the peak of switching points is around R = 0.93. This means that when the market is booming, such as in year 1999­2000 prior to the dot-com crash, investors get used to high returns and switch to the bullish attitude only for comparatively high R's. An overall high level of returns serves in this respect as a reference level and investors form their judgements about the market relative to it. Since different investors have different initial wealth, personal habits, attitudes and other factors that our model does not take into account, we have a distribution of switching points. In the bearish market the average level of returns is low and investors switch to bullish attitudes already at much lower R's.

28

Utility Functions

Distribution of the Switching Points

Density*E-2 0 12 3 45 6

60 70 80 90

Utility 10 20 30 40 50

0

0.5 1 1.5 2 Returns

0.5 1 1.5 2 Returns

Figure 12: Left panel: the market utility function (red) and the fitted utility function (blue). Right panel: the distribution of the reference points. 24 March 2000, a bullish market.

Utility Functions

Distribution of the Switching Points

Density*E-2 0 12 3 45 6

Utility 10 20 30 40 50 60 70 80 90

0

0.5 1 1.5 2 Returns

0.5 1 1.5 2 Returns

Figure 13: Left panel: the market utility function (red) and the fitted utility function (blue). Right panel: the distribution of the reference points. 30 July 2002, a bearish market.

29

Utility Functions

Distribution of the Switching Points

Density*E-2 0 12 3 45 6

Utility 10 20 30 40 50 60 70 80 90

0

-40 10

60 110

0.9+Returns*E-2

0.5 1 1.5 2 Returns

Figure 14: Left panel: the market utility function (red) and the fitted utility function (blue). Right panel: the distribution of the reference points. 30 June 2004, an unsettled market.

5 Conclusion
We have analyzed in this paper empirical pricing kernels in three market regimes using data on the German stock index and options on this index. In the bullish, bearish and unsettled market regime we estimate the pricing kernel and derive the corresponding utility functions and relative risk aversions.
In the unsettled market of June 2004, the market investor is risk seeking in a small region around the riskless return but risk aversion increases fast for high absolute returns. In the bullish market of March 2000, the investor is on the other hand never risk seeking while he becomes more risk seeking in the bearish market of July 2002. Before the stock market crash in 1987 European options did not show the smile and the Black-Scholes model captured the data quite well. Hence, utility functions could be estimated at that times by power utility functions with a constant positive risk aversion. Our analysis shows that this simple structure does not hold anymore and discusses different structures corresponding to different market regimes.
The empirical pricing kernels of all market regimes demonstrate that the corresponding utility functions do not correspond to standard specifications of utility functions including Kahneman and Tversky (1979). The observed utility functions are closest to the general utility functions of Friedman and Savage (1948). We propose a parametric specification of these functions,

30

estimate it and explain the observed market utility function by aggregating individual utility functions. In this way, we can estimate a distribution of individual investors.
The proposed aggregation mechanism is based on homogeneous investors in the sense that they differ only with switching points. Future research can reveal how nonlinear aggregation procedures could be applied to heterogeneous investors.
6 Acknowledgements
The research work of R. A. Moro was supported by the German Academic Exchange Service (DAAD). K. Detlefsen was supported by Bankhaus Sal. Oppenheim. This research was supported by Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
References
Ait-Sahalia, Y. and A. Lo, 1998: Nonparametric estimation of state-price densitites implicit in financial asset prices. Journal of Finance, 53(2).
Ait-Sahalia, Y. and A. Lo, 2000: Nonparametric risk-management and implied risk aversion. Journal of Econometrics, 94(9).
Barone-Adesi, G., R. Engle, and L. Mancini, 2004: Garch options in incomplete markets. working paper, University of Lugano.
Bergomi, L., 2005: Smile dynamics 2. Risk, 18(10).
Bernoulli, D., 1956: Exposition of a new theory on the measurement of risk. Econometrica, 22, 23­36.
Billingsley, P., 1995: Probability and Measure. Wiley-Interscience.
Black, F. and M. Scholes, 1973: The pricing of options and corporate liabilities. Journal of Political Economy, 81, 637­659.
Breeden, D. and R. Litzenberger, 1978: Prices of state-contingent claims implicit in option prices. Journal of business, 51, 621­651.
Carr, P. and D. Madan, 1999: Option valuation using the fast fourier transform. Journal of Computational Finance, 2, 61­73.
31

Chernov, M., 2000: Essays in financial econometrics. Phd thesis, Pennsylvania State University.
Chernov, M., 2003: Empirical reverse engineering of the pricing kernel. Journal of Econometrics, 116, 329­364.
Cizek, P., W. Ha¨rdle, and R. Weron, 2005: Statistical Tools in Finance and Insurance. Springer, Berlin.
Cochrane, J., 2001: Asset Pricing. Princeton University Press.
Cont, R., 2001: Empirical properties of asset returns: stylized facts and statistical issues. 223-349.
Cont, R. and P. Tankov, 2004: Nonparametric calibration of jump-diffusion option pricing models. Journal of Computational Finance, 7(3), 1­49.
Dupire, B., 1994: Pricing with a smile. Risk, 7, 327­343.
Franke, J., W. H¨ardle, and C. Hafner, 2004: Statistics of Financial Markets. Springer Verlag, Berlin.
Friedman, M. and L. P. Savage, 1948: The utility analysis of choices involving risk. Journal of Political Economy, 56, 279­304.
Harrison, M. and S. Pliska, 1981: Martingales and stochastic integrals in the theory of continuous trading. Stochastic Processes and their Applications, 11, 215­260.
Heston, S., 1993: A closed-form solution for options with stochastic volatility with applications to bond and currency options. Review of Financial Studies, 6(2), 327­343.
Heston, S. and S. Nandi, 2000: A clsed form garch option pricing model. Review of Financial Studies, 13, 585­625.
Jackwerth, J., 2000: Recovering risk aversion from option prices and realized returns. Review of Financial Studies, 13(2), 433­451.
Jackwerth, J. and M. Rubinstein, 1996: Recovering probability distributions from option prices. Journal of Finance, 51(5), 1611­1631.
Kahneman, D. and A. Tversky, 1979: Prospect theory: An analysis of decision under risk. Econometrica, 47, 263­291.
32

Mehrotra, S., 1992: On the implementation of a primal-dual interior point method. SIAM Journal on Optimization, 2(4), 575­601.
Merton, R. C., 1973: An intertemporal capital asset pricing model. Econometrica, 41(5), 867­887.
Rosenberg, J. and R. Engle, 2002: Empirical pricing kernels. Journal of Financial Economics, 64(7), 341­372.
Rubinstein, M., 1994: Implied binomial trees. Journal of Finance, 69, 771­ 818.
Silverman, B., 1986: Density Estimation. Chapman and Hall, London. Storn, R. and K. Price, 1997: Differential evolution - a simple and efficient
heuristic for global optimization over continuous space. Journal of Global Optimization, 11, 341­359. von Neumann, J. and O. Morgenstern, 1944: The Theory of Games and Economic Behavior. Princeton University Press. Wright, S., 1998: Primal-dual interior-point methods. Mathematics of Computation, 67(222), 867­870.
33

SFB 649 Discussion Paper Series 2007
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Trade Liberalisation, Process and Product Innovation, and Relative Skill Demand" by Sebastian Braun, January 2007.
002 "Robust Risk Management. Accounting for Nonstationarity and Heavy Tails" by Ying Chen and Vladimir Spokoiny, January 2007.
003 "Explaining Asset Prices with External Habits and Wage Rigidities in a DSGE Model." by Harald Uhlig, January 2007.
004 "Volatility and Causality in Asia Pacific Financial Markets" by Enzo Weber, January 2007.
005 "Quantile Sieve Estimates For Time Series" by Jürgen Franke, JeanPierre Stockis and Joseph Tadjuidje, February 2007.
006 "Real Origins of the Great Depression: Monopolistic Competition, Union Power, and the American Business Cycle in the 1920s" by Monique Ebell and Albrecht Ritschl, February 2007.
007 "Rules, Discretion or Reputation? Monetary Policies and the Efficiency of Financial Markets in Germany, 14th to 16th Centuries" by Oliver Volckart, February 2007.
008 "Sectoral Transformation, Turbulence, and Labour Market Dynamics in Germany" by Ronald Bachmann and Michael C. Burda, February 2007.
009 "Union Wage Compression in a Right-to-Manage Model" by Thorsten Vogel, February 2007.
010 "On -additive robust representation of convex risk measures for unbounded financial positions in the presence of uncertainty about the market model" by Volker Krätschmer, March 2007.
011 "Media Coverage and Macroeconomic Information Processing" by Alexandra Niessen, March 2007.
012 "Are Correlations Constant Over Time? Application of the CC-TRIGt-test to Return Series from Different Asset Classes." by Matthias Fischer, March 2007.
013 "Uncertain Paternity, Mating Market Failure, and the Institution of Marriage" by Dirk Bethmann and Michael Kvasnicka, March 2007.
014 "What Happened to the Transatlantic Capital Market Relations?" by Enzo Weber, March 2007.
015 "Who Leads Financial Markets?" by Enzo Weber, April 2007. 016 "Fiscal Policy Rules in Practice" by Andreas Thams, April 2007. 017 "Empirical Pricing Kernels and Investor Preferences" by Kai Detlefsen,
Wolfgang Härdle and Rouslan Moro, April 2007.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

