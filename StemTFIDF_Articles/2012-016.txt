BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2012-016
Nonparametric adaptive estimation of linear functionals for low
frequency observed LÈvy processes
Johanna Kappus*
* Humboldt-Universit‰t zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Nonparametric adaptive estimation of linear functionals for low frequency observed L¥evy
processes
Johanna Kappus Institut fu®r Mathematik Humboldt-Universita®t zu Berlin kappus@mathematik.hu-berlin.de
February 15, 2012
Abstract For a L¥evy process X having finite variation on compact sets and finite first moments, µ( dx) = x( dx) is a finite signed measure which completely describes the jump dynamics. We construct kernel estimators for linear functionals of µ and provide rates of convergence under regularity assumptions. Moreover, we consider adaptive estimation via model selection and propose a new strategy for the data driven choice of the smoothing parameter.
Keywords: Statistics of stochastic processes ∑ Low frequency observed L¥evy processes ∑ Nonparametric statistics ∑ Adaptive estimation ∑ Model selection with unknown variance
MSC(2010): 60G51 ∑ 60J75 ∑ 62G05 ∑ 62M05 ∑ 62G20
JEL Classification: C14
1 Introduction
L¥evy processes are the building blocks for a large number of continuous time stochastic models with jumps which play an important role, for example, in the modeling of financial data. Let us mention exponential L¥evy models (see e.g. [4, 5] and [1, 24]), time changed L¥evy processes ([22]) or stochastic volatility models ([21]). Estimating the parameters of a L¥evy process is thus
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk" and by the Deutsche Akademische Austauschdienst
1

not only of theoretical relevance, but also an important issue for practition-

ers. The problem of estimating, nonparametrically, the jump density of a

L¥evy process, has received considerable attention over the past few years

Depending on the nature of the observations, there exist two fundamen-

tally different approaches to this problem:

When disposing of continuous time observations of the process, the jumps

are directly feasible, which suggests to use the observed number of jumps as

an estimator of the expected number and apply some smoothing procedure.

This approach has been investigated in [14]. When placing oneself in a high

frequency model, that is, when assuming that the distance  between the

observation times tends to zero at a high enough rate, one might discretise

this procedure. A large increment within a small time interval will be due to

a large jump, so one is eventually able to "see" the jumps. For the details,

we refer to [13, 12] and to [6] and [8] and the discussion therein.

In the present setup, when working in a low frequency model, that is,

when assuming that  is fixed, the jumps are no longer directly feasible

so the above approach is no longer possible. Instead, one has to take into

account the structural properties of L¥evy processes and infinitely divisible

laws. In this setting, one faces a more complicated statistical inverse prob-

lem. For earlier work on this subject, see [19, 15, 16, 7].

This paper is organized as follows: In Section 2 we introduce the statis-

tical model and assumptions. We define kernel estimators for linear func-

tionals of µ( dx) := x( dx) and provide upper bounds on the corresponding

risk. This approach covers typical examples such as point estimation or

estimation of integrals over compact sets.

Section 3 is devoted to the problem of the adaptive choice of the smooth-

ing parameter. The interesting point about these considerations is that we

consider a model selection problem with unknown variance and this issue

is not only of interest in the L¥evy model (see [7]), but also a topic of on-

going research in the related field of density deconvolution with unknown

distribution of the noise. For most recent work on this subject, we refer

to [9].

We propose here a new approach towards this problem. With  denoting

the

characteristic

function,

an

estimator

1 en

of

1 

has

been

introduced

in

[20].

The key of our analysis lies in the fact that we consider a slight modification

of this estimator. This will enable us to make the pointwise control on

1 

-

1 en

which has been proved in [20] uniform on the real line. This will

be the key result for dealing with the stochastic penalty term in the model

selection procedure.

2

2 Nonparametric estimation of linear functionals in the L¥evy model

2.1 Statistical model and assumptions
A L¥evy process X = {Xt : t  R+} taking values in R is observed at discrete, equidistant time points , ∑ ∑ ∑ , 2n. We assume throughout the rest of this paper, that the distance  between the observation times is fixed.
We shall work under the following structural assumptions on the process X under consideration:
2.1 Assumptions.
(A1) X is of pure jump type.
(A2) X has moderate activity of small jumps in the sense that the following holds true for the L¥evy measure :

|x|( dx) < .
{|x|1}

(2.1)

(A3) X has no drift component.
(A4) For one and hence for any t > 0, Xt has a finite second moment. This is equivalent to stating that

|x|2( dx) < .

(2.2)

Imposing the assumptions (A1) and (A2) is equivalent to stating that

the process has finite variation on compact sets.

It is well known that under (A1)-(A4), the L¥evy-Khintchine representa-

tion takes the following special form: The characteristic function of X is

given by

(u) := E eiuX = e(u),

(2.3)

with characteristic exponent

(u) =

eiux - 1 ( dx) =

eiux - 1 x( dx).

x

(2.4)

(a proof can be found, for example, in [23]). The process is thus fully

described by the signed measure µ( dx) := x( dx), which is finite thanks to

(2.1) and (2.2). We are interested in the problem of estimating some linear functional of
µ. That is, given some function or distribution f , the parameter of interest is

 := f, µ := f (x)µ( dx).

(2.5)

3

To simplify the problem and avoid a general discussion about distributions, we assume that one of the following conditions is met:
(F1) f is a function in L1(R)  L(R).
(F2) For some y  R \{0}, f is the Dirac distribution f = y.
In the latter case, we formulate the following additional assumption on µ which makes the problem well defined:
(A5) For some open interval D = (d1, d2) with y  D, the restriction µ D possesses a continuous Lebesgue density gD.
That is, the parameter of interest is the density g of µ, evaluated at y.

2.2 Estimation procedure and risk bounds

In a low frequency model, the jumps of a L¥evy process are not directly feasible, so we have to take into account the structural properties of infinitely divisible laws to infer the underlying jump dynamics.
Using formula (2.4), we see that the Fourier transform of µ can be recovered by derivating the characteristic exponent:

  (u) =

(eiux - 1)( dx) = i eiuxx( dx) = iFµ(u).

u

(2.6)

We do thus have

Fµ(u) =

1 

(u)

.

i(u)

(2.7)

Under mild regularity assumptions, we can express the parameter of interest

in the Fourier domain using the Plancherel formula:

1  = f (x)µ( dx) =

Ff (-u)Fµ(u) du.

2

(2.8)

Together with formula (2.7), this yields

1 =

Ff (-u)

1 

(u)

du.

2 i(u)

(2.9)

These formulae suggest to estimate  by Fourier methods, replacing the

characteristic function as well as its derivative by their empirical counter-

parts.

Since the increments Z,j := Xj - X(j-1), j = 1, ∑ ∑ ∑ , 2n of X form

i.i.d. copies of X, we can define the empirical versions of  and  as

follows:

1 ,n(u) := n

n

eiuZ,j

j=1

(2.10)

4

and

1 ,n(u) := n

2n

iZ,j eiuZ,j .

(2.11)

j=n+1

Moreover, the empirical characteristic function appearing in the denominator is replaced by its truncated version, setting

1

:=

1({|,n|



(n)-1/2}) .

,n(u)

,n(u)

(2.12)

This approach is originally due to Neumann (see [20]). In case that f has integrable Fourier transform, we are in a position to
define a direct plug-in-estimator:

2.2 Definition. Assume that (A1)-(A4) are satisfied and that (F1) is met. Moreover, assume that Ff  L1(R). Then we set

1 ,n := 2

Ff

(-u)

1 

,n(u)

du.

i,n(u)

(2.13)

The integral appearing in (2.13) is well defined since |,n| as well as

1 e,n

are by definition bounded above and Ff is integrable by assumption.

On the other hand, when being interested in point estimation, Ff is

certainly not integrable and the integral appearing in (2.13) generally fails

to converge. For this reason, we have to introduce an additional smoothing

procedure. This leads to defining kernel estimators:

2.3 Definition. Assume that (A1)-(A4) are satisfied and that (F 1) or both,

(F 2) and (A5) are met. Let a continuous kernel K be given such that for

arbitrary h > 0, F Kh Ff (-∑) is integrable. Then we define for a bandwidth

h > 0:

1 ,h,n := 2

Ff (-u)

1 

,n(u)

F

K

(hu)

du.

i,n(u)

(2.14)

This definition is meaningful since boundedness of

b,n e,n

and integrabil-

ity of F Kh Ff (-∑) guarantee that the integral in (2.14) is well defined and

finite.

We can proof the following bound on the risk of ,h,n:

2.4 Theorem. Let the assumptions which are summarized in Definition 2.3 be satisfied. Assume, moreover, that for arbitrary h > 0,

5

F Kh

Ff 

 L1(R)  L2(R).

Then

we

can

estimate

2
E  - ,h,n

(2.15)

2
 2 f (x)µ( dx) - f (x) (Kh µ) ( dx)

T -1 + 22 C1

|F K (hu) |2

Ff (-u) (u)

2

du 

C2

|F K (hu) | Ff (-u) du 2 , (u)

with 2T = 2n denoting the time horizon and with constants

C1 = C | (x)| dx + 2 | (x)|2 dx  

(2.16)

and

C2 = C



+2 

2 

< ,

where C is some universal positive constant.

(2.17)

The assumption that F K Ff is integrable and square integrable depends



on

the

unknown

characteristic

function

1 

.

However,

we

can

always

ensure

that this assumption is met by choosing a kernel function which has compact

support in the Fourier domain.

Next, we obtain the following upper bound on the risk of the estimator

,n, which is defined without any additional smoothing procedure:

2.5 Theorem. Assume that Ff  L1(R). Let ,n be defined as in Definition 2.2. Then we can estimate for arbitrary m  0:

2
E  - ,n

 2

1  22 C1

|Ff

(-u)|2

du



C2

 

|Ff (-u)| du 

{|u|>m}

{|u|>m}

 2

T -1 + 22 C1

|Ff (-u)|2 |(u)|2

du



C2

 

|Ff (-u)| du
|(u)| 

{|u|m}

{|u|m}

,

with constants C1 and C2 defined as in Theorem 2.4.

It is interesting to note that the estimator ,n, which is defined without any additional smoothing procedure can be understood as the constructive analogue of the minimum distance estimator which has been proposed in [19]. The clear advantage is that our estimator can be calculated directly from the data and does not require an abstract minimization procedure over spaces of measures, which is certainly comfortable in applications.

6

2.3 Rates of convergence
In this section, we investigate the rates of convergence which can be derived form the upper risk bounds given in Theorem 2.4 and Theorem 2.5 under the assumption that the signed measure µ, which describes the jump dynamics of the underlying L¥evy process belongs to some prescribed smoothness class.
Let us introduce the following abstract nonparametric classes:
2.6 Definition.
(i) We denote by F(, , Cf , Cf , cf , cf ) the class of functions f such that for any u  R:
Cf (1 + |u|)- exp (-cf |u|)  |Ff (u)|  Cf (1 + |u|)- exp -cf |u| .
If  = 0 and  > 0, the functions in F(, , Cf , Cf , cf , cf ) are called ordinary smooth. For  > 0, they are called supersmooth.
(ii) Given a > 0, let a := sup {k  N : k < a}. For an open subset D  R, we denote by HD(, L, R) the class of functions f such that sup |f (x)|  R, f |D is a times continuously differentiable and we
xD
have sup |f ( a )(x) - f ( a )(y)|  L|x - y|a- a .
x,yD
x=y
The functions belonging to HD(a, L, R) are called locally H®older regular with index a.
(iii) For a, M  0, the Sobolev class S(a, M ) consists of all square integrable functions, for which

(1 + |u|2)a|Ff (-u)|2 du  M

(2.18)

holds. For negative indices, we are still in a position to define corresponding Sobolev classes. The objects collected in S(a, M ) for a < 0 need no longer be square integrable functions, but are those tempered distributions for which (2.18) holds true.

We start by providing rate results under global regularity assumptions on the test function f and on µ, measured in a Sobolev sense. Let us first recall the following definition:

2.7 Definition. A kernel K is called a k-th order kernel, if for all integers 1  m < k,

xmK(x) dx = 0

(2.19)

7

and moreover,

|x|k| K(x)| dx < .

(2.20)

Equation (2.19) is equivalent to stating that the derivatives (F K)(m) (0) vanish for m = 1, ∑ ∑ ∑ , k .

Let us first have a look at the approximation error which results from smoothing with some kernel function K:

2.8 Lemma. Assume that for some real valued s and some positive constant Mf , f  S(s, Mf ). Assume, moreover, that for some a > -s, µ  S(a, Mµ). Let K be chosen such that either K is the sinc kernel or K has order a + s and F K is H®older-regular with index a + s. Then we can estimate

2
f (x)µ( dx) - f (x)(Kh µ)(x) dx  Cbh2a+2s =: bh.

(2.21)

with some Cb depending on Mf , Mµ and on K. Next, we have the following bound on the error in the model:

2.9 Lemma. Assume that F K is supported on [-, ]. Assume, moreover, that f  S(s, Mf ) and that for positive constants C and c,

u  R : |(u)|  (1 + C|u|)- exp (-c|u|) .

(2.22)

Let

h2

:=

C1 22

|F K(hu)|2

Ff (-u) (u)

2

du 

C2 22

Then we have h2  v,h with

|F K(hu)| Ff (-u) du 2 (u)

v,h

:=

Cv 22

C1 sup (1 + |u|)2-2s exp (c|u|)

{|u|

 h

}

 C2

(1 + |u|)2-2s exp (2c|u|) du

{|u|

 h

}

and some constant Cv depending on C and Mf .

Now, let us introduce the following abstract nonparametric classes of signed measures:

2.10 Definition. Let M := M(CØ1, CØ2, C, c, , , a, Mµ) be the collection of finite signed measures µ, such that the following holds:

8

(i) There is a L¥evy process X, for which (A1)-(A4) are satisfied, such that µ( dx) = x( dx).

(ii) For the characteristic function

(u) := exp

eiux - 1 µ( dx)
x

(2.23)

of X1, the following holds: u  R : |(u)|  (1 + C|u|)-e-c|u| .

(2.24)

(iii) For C1 and C2 defined as in (2.16) and (2.17), we have C1  CØ1 and C2  CØ2.

(iv) µ is contained in the Sobolev class S(a, Mµ).
Let Pµ = PX1 be the infinitely divisible law with characteristic function  defined by 2.23 and Eµ the expectation with respect to Pµ.

We can now provide rates of convergence, uniformly over those nonparametric classes:

2.11 Theorem. Assume that f  L1(R)  L(R) and f  S(s, Mf ). Consider the nonparametric class M := M(CØ1, CØ2, C, c, , , a, Mµ) with

a > -s. For h > 0, let ,h,n be defined by (2.14). Assume that the

conditions on the kernel function which are summarized in Lemma 2.8 and

Lemma 2.9 are met. Let bh and v,h be defined as in Lemma 2.8 and Lemma

2.9. Then, selecting h = h,n as the minimizer of bh + T -1v,h, we find

that

2

sup Eµ  - ,h,n = O (r,n)

(2.25)

µM

with (r,n) denoting the sequences which are summarized in the following table:

CØ1 < 

CØ1 = 

=0 >0

s   s < 

T -1

T-

2a+2s 2a+2

(

log T 

)-

2a+2s 

s





+

1 2

s

<



+

1 2

T -1

T-

2a+2s 2a+2+1

(

log T 

)-

2a+2s 

Let us compare this result to the rates of convergence which can be obtained for the estimator ,n, which is defined without an additional smoothing procedure.

9

2.12

Theorem.

Let

f



S(s, Mf )

for

some

s

>

1 2

.

Consider the non-

parametric class M := M(CØ1, CØ2, C, c, , , a, Mµ). Let ,n be defined by

(2.13). Then we find that

2
sup Eµ  - ,n = O (r,n) ,
µM

(2.26)

with (r,n) collected in the following table:

CØ1 < 

CØ1 = 

=0 >0

s >  s =  s < 

T -1

T -1

T-

2s 2

log T

-

2s 



s

>



+

1 2

s

=



+

1 2

s

<



+

1 2

T -1

(log T )T -1

T-

(2s-1) 2

log T

-

2a+2s 



Examples

(i) For Compund Poisson processes, the absolute value of the characteris-

tic function  is bounded below. Consequently, (2.24) is satisfied with

 = 0 and  = 0. If the test function is contained in the Sobolev class

S(s, Mf )

with

s

>

1 2

,

Theorem

2.12

immediately

tells

us

that

,n

attains the parametric rate.

(ii) For Gamma processes with parameters  and , the characteristic function is given by

(u) =

1-

i u

-
.



From

this

we

conclude

that

for

test

functions

f



S(s, Mf )

with

s

>

1 2

,

the

estimator

,n

attains

the

parametric

rate,

provided

that



<

s-

1 2



.

(iii) A tempered -stable law is constructed by multiplying the L¥evy measure of a -stable law with a decreasing exponential. The activity of small jumps is the same as for -stable laws, so the process has finite variation on compacts if  < 1. The characteristic function decays exponentially, with  = , so the rates of convergence are logarithmic. For the exact parameters, we refer to Section 4.5 in [10].

We recover in Theorem 2.12 the rates of convergence which have been derived for the minimum distance estimator. This confirms the analogy

10

between the constructive estimator defined by (2.13) and the estimator proposed in [19].
Theorem 2.11 suggests that better rates of convergence can be obtained, under regularity assumptions on µ, when applying some kernel smoothing procedure. However, we must be careful about the fact that µ( dx) = x( dx) cannot possess a globally smooth Lebesgue density, unless we are in the Compound Poisson case. In the case of infinite jump activity, we will always have a point of discontinuity at zero. Consequently, when considering test functions with integrable Fourier transform which do not vanish at the origin, the gain in the rate which results from kernel smoothing is small and one might prefer ,n in applications.
The situation changes if f is bounded away from the origin. In this case, one has to localize the procedure, working with some kernel function K which decays fast enough. The appropriate concept to take into account is no longer global Sobolev regularity but local regularity round the point or interval of interest, measured in a H®older sense.
We can give the following bound on the approximation error under local regularity assumptions on µ and f :
2.13 Lemma. Let f be compactly supported with supp(f ) := [a, b]  R \{0} and assume that for some s  N,
u  R : |Ff (u)|  Cf (1 + |u|)-s.

Assume that for some bounded open set D = (d1, d2)  [a, b], µ possesses
D
a Lebesgue density gD  HD(a, R, L). Let K have order a + s and assume that for some positive constant CK, we have

z  R : | K(z)|  CK (1 + |z|)-a-s-1.

(2.27)

Then we can give the following bound on the approximation error:

2
f (x)µ( dx) - f (x)(Kh µ)(x) dx  Cbh2a+2s

(2.28)

with a positive constant Cb depending on K, a, b, D, R, and L.

The following result is in analogy with Lemma 2.9. However, we need to pay attention to the fact, that the definition of the smoothness parameter s is now slightly different.

2.14 Lemma. In the situation of the preceding lemma, assume that for positive constants C and c, we have

u  R : |(u)|  (1 + C|u|)-e-c|u| .

(2.29)

11

Assume, moreover, that F K is supported on [-, ]. Then, with h2 defined as in Lemma 2.9, we have h2  v,h with

v,h

:=

Cv 22

C1

(1 + |u|)2-2s exp (2c|u|) du

|u|

 h



2



C2

 

(1

+

|u|)-s

exp

(c|u|)

du 

|u|

 h

,

where Cv is a positive constant depending on Cf and C.

We consider now the following class of locally Ho®lder regular measures:

2.15 Definition. Let M := M(CØ1, CØ2, C, c, , , a, D, L, R) be the collection of finite signed measures µ, such that the following holds: The items (i)-(iii) from Definition 2.10 are true and

(iv) µ possesses a Lebesgue density gD  HD(a, L, R).
D
The rate results which can be derived from Lemma 2.13 and Lemma 2.14 are summarized in the following theorem:

2.16 Theorem. Let the assumptions of Lemma 2.13 and

Lemma 2.14 be satisfied.

Consider the nonparametric class

M = M(CØ1, CØ2, C, c, , , a, D, L, R) defined in 2.15. Let h be se-

lected as the minimizer of bh + T -1v,h. Then we find that

2
sup Eµ  - h,n = O (r,n)
µM

(2.30)

with the rates r,n collected in the following table:

=0 >0

CØ1 < 

s

>



+

1 2

s

=



+

1 2

s

<



+

1 2

T -1

(log T ) T -1

T-

2s+2a 2+2a+1

log T 

-

2a+2s 

CØ1 = 

s >  + 1

T -1

s =  + 1 s <  + 1

(log T )T -1

T-

2a+2s 2a+2+2

log T 

-

2a+2s 

12

Examples

(i) For point estimation, we have |Ff (u)| = |Fy(u)| = |eiuy|  1, so (2.29)

is met with s = 0 and Cf = 1. Under the above assumptions on

the local smoothness and on the kernel function, we end up with the

polynomial

rate

T-

2a 2+2a+1

in

case

that



decays

polynomially

and

with the logarithmic rate

log T 

-

2a 2s

for exponentially decaying .

Again, one might think about Gamma processes and tempered stable

processes. This should be compared to the rates of convergence which

are found (an known to be minimax optimal) in density deconvolution

problems. It should not come as a surprise, that we recover in the

continuous limit (that is, for  close to zero) the rates which are

known from density estimation with pointwise loss.

(ii) When longing to estimate µ([a, b]) = 1([a, b])(x)µ( dx) for some compact set [a, b] bounded away from the origin, we have s = 1. The rate is parametric in the Compound Poisson case or for Gamma processes observed at a high enough frequency. Else, the rate is polynomial for polynomial decay and logarithmic for exponential decay of the characteristic function.

3 Adaptive estimation

3.1 The problem at hand

Let a collection M = {m1, ∑ ∑ ∑ , mn}  N of indices be given and let

H := {h1, ∑ ∑ ∑

,

hmn }

:=

{

1 m1

,

∑

∑

∑

,

1 mn

}

be

a

collection

of

bandwidths

asso-

ciated with M.

For notational simplicity, we shall suppress in this section the dependence

on  and assume that the distance between the observations of the L¥evy

process X is equal to one. Moreover, we slightly change the notation and

write, when referring to the kernel estimator defined by (2.14), m,n instead
of 1,hm,n. The goal of this section is to provide a strategy for the data driven

choice of the smoothing index within the collection M and to derive, for the

corresponding estimator mb ,n, the oracle inequality

E | - mb ,n|2









C

inf
mM

| 

-

mn |2

+

sup |k
km

-

m|2

+

pen(m) 

+

O

n-1

,



kM

with

1 m := 2

f (x) K 1 µ (x) dx,
m

13

with some constant C which does not depend on the unknown smoothness parameters and a penalty term pen(m) to be specified, which equals, up to some logarithmic factor, the quantity

1 n

m2

:=

n-1 22

C1

Ff (-u) 2

u2

K du

(u)

m

 C2

Ff (-u)

u

2

K du

(u)

m

which bounds the error within the model.

3.2 Some heuristics

We start by giving some intuition on how the model selection procedure
should work without going into the technical details. These considerations
will be made precise in the next section.
If the characteristic function  appearing in the denominator were fea-
sible (which is, of course, not the case in the present setting), the way to go would be to estimate the quantities |k - m|2 involved in the oracle bound by their corrected version, that is, to consider |k - m|2 - H2(m, k), with some deterministic correction term H2(m, k) which is chosen large enough
to ensure that with high probability,

|k - m|2 - H2(m, k)  |k - m|2 m, k  M.
On the other hand, H2(m, k) should ideally not be much larger than the variance term.
Typically, this would lead to choosing

H2(m, k)

:=

1 n

m2 ,k

m2 ,k + x2m,k

with some positive constant  to be appropriately chosen and

m2 ,k

:=

1 22

C1

Ff (-u) 2 F K u - F K

u

2
du

(u) k m

 C2

Ff (-u) F K u - F K u

2
du

(u)

k

m

and

xm,k

:=

1 n

1 2

Ff (-u) F K u - F K u du

(u)

k

m

and with logarithmic weights m,k chosen large enough to ensure that e-m,k < .
k>m kM

14

Indeed, this is the fundamental idea about model selection via penaliza-

tion: Some deterministic term is applied in order to control the fluctuation

of certain stochastic quantities, uniformly over some countable index set.

For further reading, we refer to [2, 18, 3] among others.

Obviously, the situation is different in the present set up since the definition of the correction term H2(m, k) involves the characteristic function

in the denominator which is itself unknown.

It is well intuitive to replace the unknown characteristic func-

tion appearing in the denominator by its truncated empirical version

1 en(u)

=

1({|bn

(u)|n-1/2 bn(u)

})

,

thus

considering

m2 ,k

:=

1 22

C1

Ff (-u)

2
|F K

u

-FK

u

|2 du

n(u) k m

 C2

Ff (-u) |F K

u

-FK

u

2
| du

n(u)

k

m

instead of m2 ,k and a stochastic version xm,k of xm,k and to introduce

a

stochastic

correction

term

2
H (m, k)

=

2m,k

m2 ,k + x2m,k

rather than

H2(m, k).

Now,

it

is

obvious

that

1 en(u)

may

be

sufficiently

close

to

1 (u)

for

large

val-

ues of |(u)|, but is a drastic underestimate if |(u)| is small. Consequently,

2
the stochastic bias correction term H (m, k) will systematically underesti-

mate the true H2(m, k), for which reason it seems doubtful if penalizing with

2
H (m, k) can possibly make sense.

In the setting of nonparametric estimation for L¥evy processes with L2-loss, Comte and Genon-Catalot [7] have dealt with the problem of the

unknown variance by proposing an a priori assumption on the size of the

collection M of smoothing parameters. However, this approach turns out to

be critical since this assumption depends itself on the unknown decay of .

Only recently, Comte and Lacour [9] have proposed an approach to-

wards model selection with unknown variance, which does not depend on

any prior knowledge of the smoothness parameters . However, this approach is designed for L2-loss and spectral cutoff estimation and the generalization

to the estimation of linear functionals and general kernels is not straightfor-

ward. Moreover, it would lead to a loss of polynomial order in the present

model. For this reason, we propose, in what follows, a different strategy.

Roughly speaking, the strategy in the above mentioned papers can be

described as follows: At the first stage, one penalizes with some theoretical

correction term which involves the unknown characteristic function. This

makes the model selection procedure work as if  were feasible. At the

second stage, one has to control the fluctuation of the stochastic penalty

round the theoretical penalty.

15

Compared to this, we undergo here some change of perspective by having

a direct look at the stochastic penalty term:

For

one

thing,

we

may

hope

that,

for

large

values

of

|(u)|,

1 en(u)

is

not

only

pointwise,

but

uniformly

close

to

1 (u)

,

for

which

reason

working

with

2
H (m, k)

rather

than

H2(m,

k)

will

work

out

right.

On the other hand, there remains the undeniable fact that for |(u)|

small,

1 en(u)

is by no means close to

1 (u)

,

but

a

systematic

underesti-

mate.

For

this

reason

penalizing

with

2
H (m, k)

rather

than

H2(m, k)

seems

hopeless.

Still, one may ask oneself what is the use in penalizing at all. Certainly, the point about correcting with H2(m, k) is that one wishes that with hight

probability

|k - m|2 - H2(m, k)  |k - m|2 m, k  M.

(3.1)

Now, if  is unknown and has to be estimated, we must beware of the fact

that

the

empirical

version

1 en

is

involved

not

only

in

the

definition

of

the

stochastic correction term H(m, k), but also appears in the definition of

k - m =

Ff (-u) n(u) F K u - F K u

in(u)

k

m

du.

When considering small values of |(u)| there is certainly no danger of over-

estimating in |k - m|2. For this reason, subtracting some penalty term

is simply not necessary at this stage, for which reason underestimating the

quantity in H2(m, k) as well causes no damage.

What remains to be done is to give some rigorous argument which allows

to

control

the

fluctuation

of

1 en(u)

round

1 (u)

uniformly

on

the

whole

real

line.

3.3 Adaptive estimation procedure and oracle bound

We have argued that we will need some result allowing to control the fluc-

tuation of the empirical characteristic function in the denominator round

its target uniformly on the whole real line. This will be done by applying

concentration inequalities of Talagrand type.

For this purpose, we will need an alternative definition of an estimator

of

1 

and

of

the

kernel

estimator

m,n:

3.1 Definition.

(i) Let the weight function w be defined by

w(u)

=

(log(e

+

|u|))-

1 2

-

(3.2)

16

for some  > 0. For some constant  to be specified, let

,
n (u) :=

n(u),

(log

n)

1 2

w(u)-1

n-

1 2

,

if

|n(u)|



(log

n)

1 2

w(u)-1n-

1 2

else.

The

corresponding

estimator

of

1 (u)

is

1 een(u)

:=

1
,

.

een (u)

(3.3)

(ii)

In what follows, let m,n

:=

1,

1 m

,n

be

defined

as

in

Definition

2.3,

apart

from

the

fact

that

1 e1,n

is

replaced

in

(2.14)

by

1,
een

defined

as

in

(i).

What will be important about this redefinition is the fact that we have

introduced an extra logarithmic factor which will enable us to give uniform

2

control on

1 

-

1 een(u)

. More precisely, we can proof the following key result

which makes the well known Lemma by Neumann (see Lemma 2.1 in [20])

uniform on the real line:

3.2

Lemma.

Let c1

be the constant appearing in Lemma 5.4.

Let

1 een

be

defined by Definition 3.1 (i) with  be chosen such that for some  > 0, we

have   2( 2c1 + ). Then we have for some constant CN,K depending on

the choice of ,  and :

 2

E

sup 

uR

1 een(u)

-

1 (u)

(log n)w(u)-2n-1 |(u)|4



1 |(u)|2

 



CN,K

First of all, we observe that thanks to Lemma 3.2, for the squared risk of the newly defined estimator m,n, we have

E

2
 - m,n



|

-

m|2

+

1 n

m2 ,w

with

m2 ,w

:=

log n 22

C1

Ff (-u) 2 F K u 2 w(u)-2 du

(u)

m

 C2

Ff (-u) F K u

2
w(u)-1 du ,

(u)

m

that is, the upper risk bound is preserved up to a logarithmic factor. The proof is the same as the proof of the upper risk bound given in Theorem 2.4.
Let us introduce some definitions which will be needed in the sequel.

17

For m, k  M, let

m2 ,k,w

:=

log n 22

CØ1

Ff (-u) 2 F K u - F K u 2 w(u)-2 du (u) k m

 CØ2

Ff (-u) F K u - F K u

2
w(u)-1 du .

(u)

k

m

Let

xm,k,w

:=

log n n

1 2

Ff (-u) F K u - F K u w(u)-1 du

(u)

k

m

and

m,k,w :=

log n + log x2m,k,w(k - m)2 log m2 ,k,w + xm2 ,k,w (k - m)2 + log log n + log x2m,k,w(k - m)2

Finally, let

H2(m, k)

:=



1 n

m2 ,k,w

m2 ,k,w + x2m,k,w

and pen(m) := H2(0, m).

The

stochastic

counterparts

m2 ,k,w,

xm,k,w ,

m,k,w ,

2
H (m, k)

and

pen(m)

are

defined

by

replacing,

in

each

of

the

above

definitions,

1 

by

1.
een

Now, let the random smoothing parameter be defined to be


  m := arginf sup mM k>m kM



k

-

m

2

-

2
H (m,

k)

 
+ pen(m) .

 

(3.4)

We are now ready to formulate the main result of this section:

3.3 Theorem. Let observations X1, ∑ ∑ ∑ , X2n of a L¥evy process be given.

Let M = {1, ∑ ∑ ∑ , mn}. Assume that for some positive constant , E [exp (X1)] < . Assume, moreover, that C1  CØ1 and C2  CØ2.

For m (3.4) and

 M, let m,n be defined assume that we have and

byD1e2fi8n3it2ioannd3.1.L2e(tm2c1be+de)fi.neTdhebny

we can estimate

2
E  - m









C

inf
mM

| 

-

mn |2

+

sup |k
k>m

-

m|2

+

pen(m) 

+

O

n-1


kM



18

for some positive constant C which does not depend on the decay of  nor on the smoothness of µ.
Theorem 3.3 will tell us that the estimation procedure attains, up to a logarithmic loss, the optimal rates of convergence. It is worth mentioning that we can relax the exponential moment condition on X1, but at the cost of losing an polynomial factor.
Our reasoning is not particular to the L¥evy model nor to the estimation of linear functionals, but generalizes to the setting of nonparametric deconvolution with unknown error distribution and to L2-loss. A detailed discussion on the subject will be given in [17]

4 Acknowledgement
I want to thank Fabienne Comte and Valentine Genon-Catalot for giving me the opportunity for a research stay at Universit¥e Paris D¥escartes and for inspiring discussions on model selection. Moreover, I thank Markus Reiﬂ for useful hints and comments and Matthias Trabs for careful reading.

5 Proofs

5.1 Proofs of the main results of Section 2

The following lemma is the key result for the proofs Theorem 2.4 and Theorem 2.5.

5.1 Lemma.

Let

,n

and

1 e,n

and

be

defined

by

(2.11)

and

(2.12).

Then

we can estimate

E

1 

,n(u)

-

1 

(u)

,n(u)

(u)

1 

,n

(-v)

-

1 



(-v)

,n(-v)

(-v)

C

T -1  1 | (u - v)| + | (u - v)| + | (u)|| (-v)|

|(u)||(-v)|

with some universal constant C.

Proof. We start by noting that for some constant CN,k, we have

1 1k

T

-

k 2

1

E

- n (u) (u)

 CN,k |(u)|2k  |(u)|k ,

(5.1)

which is a direct consequence of Neumann's Lemma, drawing back the dependence on .

19

We can write

E =E

n(u) - (u) n (u) (u)

n(-v) - (-v) n(-v) (-v)

(n(u) - (u)) n (u)

+

(u)

1 -1 n (u) (u)

(n(-v) - (-v)) n (-v)

+

(-v)

1 -1 n(-v) (-v)

(5.2) (5.3) (5.4)

Using

the

fact that n

and

1 en

are independent by construction

and

that

n(u) - (u) and (-v) - (-v) are centered, we find that

E

(n(u) - (u)) n (u)

+

(u)

1 -1 n(u) (u)

(5.5)

(n(-v) - (-v)) n (-v)

+

(-v)

11 -
n(-v) (-v)

(5.6)

1

=E

n(u) - (u)

n(-v) - (-v)

E

(5.7) n (u)n (-v)

11

+ (u)(-v) E

- n (u) (u)

11 - (5.8)
n(-v) (-v)

1 = Cov n (u), n (-v) E n (u)n (-v)

(5.9)

11

+ (u)(-v) E

- n (u) (u)

11 - (5.10)
n(-v)  - (-v)

1 =: Cov n (u), n (-v) E n (u)n (-v)

(5.11)

+ (u)(-v) E [Rn(u)Rn(-v)] .

(5.12)

The Cauchy-Schwarz-inequality and then an application of (5.1) imply

E [|Rn(u)Rn(-v)|]
1
 E |Rn (u)|2 2 E |Rn (-v)|2

1 2



CN,2

T -1 |(u)|2|(-v)|2



1 |(u)||(-v)|

.

(5.13) (5.14) (5.15)

Next, using the triangular inequality, again (5.1) and then the same reason-

20

ing as in (5.13)-(5.15), we find that

1

E n (u)n (-v)



1 |(u)||(-v)|

+

1 |(u)|

E

[|Rn (-v)]

1 + |(-v)| E [|Rn(u)|] + E [|Rn(u)| |Rn(-v)|]



1 (1 + 2CN,1 + CN,2) |(u)||(-v)| .

Moreover,

by

definition

of

1 en

,

we

have

1 E n(u)n(-v)  T.

Next, we calculate

(5.16) (5.17) (5.18) (5.19)
(5.20)

Cov(n(u), n(v)) = n-1 E (iZ)2ei(u-v)Z - E iZeiuZ E iZe-ivZ = n-1 (u - v) - (u)(-v)

(5.21) (5.22) (5.23)

Moreover, we clearly have

(u) (-v) =  (u)(u)  (-v)(-v)  | (u)|| (-v)|.

and
|(u - v)| =  (u - v)(u - v) + 2( (u - v))2(u - v)  | (u - v)| + 2( (u - v))2

Putting (5.16)-(5.19), (5.20) and (5.21)-(5.23) together, the expression appearing in (5.11) can be estimated as follows:

1 Cov n (u), n (v) E n (u)n (-v)



(1 + 2CN,1 + CN,2)

T



1 |(u)||(-v)|

(5.24) (5.25)

n-1  (u - v) + | (u - v)|2 +  (u)  (-v) (5.26)



(1 + 2CN,1 + CN,2)2

1  T -1 |(u)||(-v)|

(5.27)

 (u - v) + | (u - v)|2 +   (u)  (-v) .(5.28)

21

Next, using (5.13)-(5.15) and then the fact that (u) =  (u)(u), the expression in (5.12) and be estimated:

|(u)||(-v)| |E [Rn(u)Rn(-v)]|

(5.29)



CN,2

T -1 |(u)|2|(-v)|2



1 |(u)||(-v)|

(u) (-(v5).30)

=

CN,2

T -1 |(u)||(-v)|  1

2  (u)  (-v) .

(5.31)

Putting (5.24)-(5.28) and (5.29)-(5.31) together, we have shown that

1 2 E

n(u) - (u) n (u) (u)

n(-v) - (-v) n(-v) (-v)

C

T -1  1

|(u)||(-v)|

 (u - v) + | (u - v)|2| (u)|| (-v)| ,

which is the statement of the lemma.

We can now use Lemma 5.1 to prove Theorem 2.4.

Proof of Theorem 2.4. The risk of h,n can be decomposed as follows: With

h

:=

1 2

f (x)(Kh g)(x) dx, we have

2
E  - h,n



2 | - h|2 + 2 E

2
h - h,n

2
= 2 f (x)µ( dx) - f (x)(Kh µ)(x) dx

 + 2E

f (x)(Kh

µ)(x)

dx

-

1 2

Ff (-u)F

Kh(u)

1 

n

(u)

in (u)

du

2 

.

By assumption on K, we have FKhFf  L1(R), so we can pass to the Fourier domain and find that

E

f (x)(Kh

µ)(x)

dx

-

1 2

Ff (-u)F

Kh

(u)

n (u) in (u)

du

2

 1
= E  2

Ff (-u)F

Kh(u)Fµ(u)

du

-

1 2

Ff (-u)F

Kh(u)

1 

n

(u)

in (u)

du

2 

1 = E 2

Ff

(-u)F

Kh(u)

1 

n(u) - (u) n (u) (u)

2
du .

22

An application of Fubini's theorem yields

E

Ff

(-u)F

Kh(u)

1 

n(u) - (u) n (u) (u)

2
du

= Ff (-u)Ff (v)F Kh(u)F Kh(-v)

◊

1 2

E

n(u) - (u) n (u) (u)

and next, Lemma 5.1 gives

n(-v) - (-v) n(-v) (-v)

du dv

Ff (-u)Ff (v)F Kh(u)F Kh(-v)

◊

1 2

E

n(u) - (u) n (u) (u)

n(-v) - (-v) n(-v) (-v)

 CT -1

|Ff (-u)||Ff (v)| |(u)||(-v)|  (u)

 (-v)

du dv

du dv

+

|Ff (-u)| |Ff (v)| |(u)||(-v)|

|Kh(u)|

|Kh(-v)|

 (u - v) +  (u - v)

du dv .

In case that   L1(R) and   L2(R), we apply the Cauchy-Schwarz inequality and Fubini's theorem to find that

|Ff (-u)| |Ff (v)| |(u)||(-v)|

|Kh(u)|

|Kh(-v)|

 (u - v) +  (u - v) 2

du dv



|Ff (-u)|2 |(u)|2

|F

Kh(u)|2

(



(u - v)

+ | (u - v)|2) du dv

=

|Ff (-u)|2 |(u)|2

|F

Kh(u)|2

(| (u - v)| + | (u - v)|2) dv du

 sup (|(u - v)| + | (u - v)|2) dv
uR

|Ff (-u)|2 |(u)|2

|F

Kh(u)|2

du



(| (x)| dx + | (x)|2 dx

|Ff (-u)|2 |(u)|2

|F

Kh(u)|2

du.

Another application of the Chauchy-Schwarz-inequality gives

|Ff (-u)||Ff (v)| |(u)||(-v)|

|Kh(u)|

|Kh(-v)|

|

(u)||

(-v)|

du

dv

=

|Ff (-u)| |(u)|

|F

Kh(u)||

(u)|

du

2



| (u)|2 du

|Ff (-u)|2 |(u)|2

|F

Kh(u)|2

du.

23

We have thus shown that


1 E  2

f (x)(Kh

µ)(x)

dx

-

1 2

Ff (-u)F

Kh(u)

1 

n

(u)

in (u)

du

2 



C (2)2

T

-1

| (x)| dx + 2 | (x)|2 dx

|Ff (-u)|2 |(u)|2

|F

Kh(u)|2

du.

On the other hand, we can always estimate

|Ff (-u)| |Ff (v)| |(u)||(-v)|

|Kh(u)|

|Kh(-v)|

(



(u - v)

+ | (u - v)|2) du dv

 sup  (u - v) + | (u - v)|2
u,vR

|Ff (-u)| |Ff (v)| |(u)||(-v)|

|Kh(u)|

|Kh(-v)|

du

dv

 sup  (x) + | (x)|2
xR

|Ff (-u)| |(u)|

|F

Kh(u)|

du

2

and

|Ff (-u)||Ff (v)| |(u)||(-v)|

|F

Kh(u)||F

Kh(-v)||

(u)||

(-v)|

du

dv

=

|Ff (-u)| |(u)|

|F

Kh

(u)||

(u)|

du

2

 sup  (x) 2
xR

|Ff (-u)| |(u)|

|F

Kh(u)|

du

2
.

This yields


1 E  2

f (x)(Kh

µ)(x)

dx

-

1 2

Ff (-u)F

Kh(u)

1 

n

(u)

in (u)

du

2 



C (2)2

T

-1

sup | (x)| + 2 sup | (x)|2

xR

xR

|Ff (-u)| |(u)|

|F

Kh(u)|

du

2
.

Putting the above results together, we have shown that

2
E  - h,n

2
 f (x)µ( dx) - f (x)(Kh µ)(x) dx

+

C (2)2

T

-1

 (x) dx + 2 | (x)|2 dx

|Ff (-u)|2 |(u)|2

|F

Kh(u)|2

du

 sup | (x)| + 2 sup | (x)|2

xR

xR

|Ff (-u)| |(u)|

|F

Kh

(u)|

du

2

,

which is the statement of the theorem.

24

Next, we prove Theorem 2.5.

Proof of Theorem 2.5. First, recall that we now assume that |Ff |  L1(R), so we certainly have |Ff Fµ|  Fµ |Ff |  L1(R). We can thus express 
in the Fourier domain, writing

1  = f (x)µ( dx) =

Ff (-u)Fµ(u) du.

2

and express the squared risk of n as follows:

2
E  - n

 = E

f (x)µ( dx) - 1 2

Ff (-u)

1 

n

(u)

du

2 

n (u)

 1
= E  2

Ff (-u)Fµ(u) du - 1 2

Ff (-u)

1 

n

(u)

du

2 

n (u)

11 = 2 E 2

Ff (-u)

n(u) - (u)

2
du .

n (u) (u)

Next, we can write for arbitrary m  0:

1 E 2

Ff (-u)

n(u) - (u)

2
du

n (u) (u)





1

2

E

 

2

Ff (-u) n(u) - (u) n (u) (u)

{|u|>m}



1

+

2

E

 

2

Ff (-u) n(u) - (u) n (u) (u)

{|u|m}

2 du 

2 du  .


Applying Fubini's theorem, Lemma 5.1 and again the Cauchy-Schwarz inequality yields



1

2

E

 

Ff (-u)

{|u|>m}

n(u) - (u) n (u) (u)

2 du 


= Ff (-u)Ff (v)
{|u|>m} {|v|>m}

25

◊

1 2

E

n(u) - (u) n (u) (u)

n(-v) - (-v) n(-v) (-v)

du dv

 |Ff (-u)||Ff (v)|

{|u|>m} {|v|>m}

◊  (u - v) + | (u - v)|2 +  (u)  (-v) du dv

 | (x)| dx + 2 | (x)|2 dx

|Ff (-u)|2 du

{|u|>m}


2

 sup | (x)| + 2 sup | (x)|2 

|Ff (-u)| du



xR

xR

{|u|>m}

Arguing along the same lines, we find that



1

2

E

 

Ff (-u)

{|u|m}

n(u) - (u) n (u) (u)

2 du 


= Ff (-u)Ff (v)

{|u|m} {|v|m}

◊

1 2

E

n(u) - (u) n (u) (u)

n(-v) - (-v) n(-v) (-v)

du dv

 CT -1

| (x)| dx + 2

| (x)|2 dx

Ff (-u) 2 du
(u)

{|u|m}

 2

sup | (x)| + 2 sup | (x)|2 

Ff (-u) du

xR xR  (u) 

{|u|m}

Putting the above results together, we have shown that for arbitrary m  0,

2
E  - n



1 22

C1

|Ff (-u)|2 du  C2

{|u|>m}

2
|Ff (-u)| du

 2

T -1 + 22 C1

|Ff (-u)|2 |(u)|2

du



C2

 

|Ff (-u)| du
|(u)| 

{|u|m}

{|u|m}

,

which is the statement of the theorem.

26

5.2 Rate results
There is no need to give a detailed discussion on the rate results since the bounds on the bias terms are standard analysis and the esitimate of the variance term and the minimzation problems leading to Theorm 2.11 and 2.12 are trivial. The proofs are given in full length in [17].

5.3 Proof of Theorem 3.3

5.3.1 Preliminaries

We start by restating for the reader's convenience, some well known results.

5.2 Lemma (Bernstein's Inequality). Let X1, ∑ ∑ ∑ , Xn be complex valued

i.i.d. random variables with Var(X1)  v2 and suppose that X1   B for

n

some

B

< .

Let

Sn

:=

1 n

(Xi - E[Xi]). Then the following holds true

i=1

for arbitrary  > 0:

P

|Sn|  

 4 exp -n

2


4v2 + 4 3 2 B

A proof of this fundamental result can be found, for example in [11]. The following integral version of the classical Berstein inequality can be
derived readily from Lemma 5.2:

5.3 Lemma. In the situation of the preceding lemma, suppose that E[|Sn|]  H. Then we have

E

|Sn|2 - H2 +



v2 32

exp

n

-n

H2 8v2

 B2 + 128 2 n2 exp

-n

H


16 3

2B

.

Finally, we need the Talagrand inequality, which strengthens the classical Bernstein inequality to countable sets of random variables:

5.4 Lemma (Talagrand's inequality). Let I be some countable index set. For each i  I, let X1(i), ∑ ∑ ∑ , Xn(i) be centered i.i.d. complex valued random variables with X1(i)   B for some B < . Let v2 := sup Var X1(i). Then
iI
for arbitrary  > 0, there are positive constants c1 and c2() depending only
on  such that for any  > 0:

P

sup |Sn(i)|  (1 + ) E sup |Sn(i)| + 
iI iI

 4 exp

-n

c1v2

2 + c2()B

27

5.3.2 Auxiliary results

In what follows, we formulate and prove a number of auxiliary results which will be needed in order to prove the main result of Section 3.3.

5.5 Lemma. Let  > 0 be given. Let  be the constant appearing in the def-
inition of the weight function w and let c1 be the constant in Talagrand's inequality. Then, for arbitrary  > 0, there is a positive constant CK = CK,, depending on the choice of ,  and  such that we have for n  1:

P

u  R : |n(u) - (u)|   (log n)1/2w(u)-1n-1/2



C nK

-

(

-)2 c1

Proof. We proof the claim for the countable set of rational numbers. By continuity of the characteristic function and of w, it carries over to the whole range of real numbers.
By Theorem 4.1 in [19], we have for some constant CRN :

E sup |n(u) - (u)|w(u)  CRN n-1/2.
uR
Since moreover, we trivially have sup Var[1(u)]  1,
uR
sup 1(u)w(u)   1, we can apply Talagrand's inequality. Setting
uR
n :=  (log n)1/2n-1/2 - (1 + )CRN n-1/2,

and

for some  > 0, we can estimate

P q  Q : |n(q) - (q)|   (log n)1/2w(q)-1n-1/2

= P sup |n(q) - (q)|w(q)   (log n)1/2n-1/2
qQ

 P sup |n(q) - (q)|w(q)  (1 + ) E sup |n(q) - (q)|w(q) + n

qQ

qQ

 4 exp -n

2n

.

c1 + c2()n

By definition of n, we have for CK large enough and arbitrary n  1:

4 exp -n

n2

c1 + c2()n

 CK exp

- ( - )2 (log n) c1

=

C nK

-

(

-)2 c1

This is the desired result for the rational numbers and hence, by continuity, for the real line.

We

can

now

use

Lemma

5.5

to

analyse

the

deviation

of

1 een

from

1 

.

28

5.6 Proposition. that for some  >

Let 1
een
0 and

=

1
,

een

some p

be >

defined as in Definition (3.1 ). Assume

0,

we

have





 2( pc1

+

),

where

c1

denotes the constant in Talagrands inequality. Then we find that

 2

 11

P  u  R : 

- n(u) (u)

>

(4)2

(log

n)w(u)-1n-1 |(u)|4



52 1 2 |(u)|2

 
 

= O n-p .

Proof. (Sketch) Let us introduce the favourable set

C := C, :=

u



R

:

|n(u)

-

(u)|



 (log
2

n)1/2w(u)-1n-1/2

.

We start by recalling that, thanks to Lemma 5.5, we have,

P

(C c )



C nK

-

(/2- c1

)2

=O

n-p

,

so it is enough to consider the set C.

Let us introduce the following partition of the real line: R = R1  R2  R3, with

R1 =

u



R

:

|(u)|

<

 (log n)1/2w(u)-1n-1/2 2

,

We have

R2 =

u



R

:

|(u)|

>

3 (log
2

n)1/2w(u)-1n-1/2

and

R3 =

u



R

:

 (log n)1/2w(u)-1n-1/2 2



|(u)|



3 (log n)1/2w(u)-1n-1/2 2

.

We prove the claim for each of these sets separately. Since the details are
elementary and rely on an repeated application of Lemma 5.5, we only give the details for R1 ond omitt the rest of the proof.
By definition of C, we find that for arbitrary u  R1 , we have

|n(u)|  |(u)| + |(u) - n(u)| < (log n)1/2w(u)-1n-1/2 (5.32)

and hence by definition of 1 :
een

1 - 1 2 = |(u) - n(u)|2

n(u) (u)

|(u)|2|n(u)|2

|(u) - (log n)1/2w(u)-1n-1/2|2 =
|n(u)|2|(u)|2



3 2



2 (log n)w(u)-2n-1 .

|(u)|2|n(u)|2

(5.33) (5.34) (5.35)

29

Next, notice that, using again the definition of C and R1 ,

1 = -2(log n)-1w(u)2n

|n(u)|2



1 4

 2

-2

(log

n)-1w(u)2n



1 4

1 |(u)|2 .

(5.36) (5.37)

Putting (5.33)- (5.37) together, we have shown that on C, we have for any u  R1 :

2
1 -1  n(u) (u)

3 
4

2

(log

n)w(u)-2n-1 |(u)|4



9 4

1 |(u)|2 .

Similar arguments can be applied to prove the claim for u  R2 and u  R3 .

The following result can be derived immediately from the preceding statement.

5.7 Corollary. In the situation of the preceding statement, we have



2

 11

P  u  R : 

- n(u) (u)

>



5 

2 (log n)w(u)-2n-1  =O

n-p

.

2 |n(u)|2|(u)|2 

(5.38)

This corollary is an immediate consequence of the proof of the preceding statement, see lines (5.33)- (5.34).
Lemma 3.2 can now be stated as a consequence of Proposition 5.6:

Proof of Lemma 3.2. Let the set C be defined as in the proof of Proposition 5.6. We can decompose

 2

E sup 
uR

1 een(u)

-

1 (u)

(log n)w(u)-2n-1 |(u)|4



1 |(u)|2

 



 E sup

uR

1 een(u)

-

1 (u)

(log n)w(u)-2n-1 |(u)|4



2
1 |(u)|2

 1(C )


 2

+

E sup 
uR

1 een(u)

-

1 (u)

(log n)w(u)-2n-1 |(u)|4



1 |(u)|2

1(C c ) 

(5.39) (5.40) (5.41)

30

The definition of C, together with Proposition 5.6, readily implies that



E

sup 

uR

1 een(u)

-

1 (u)

2

(log n)w(u)-2n-1 |(u)|4



1 |(u)|2

 1(C )




25 2. 4

(5.42)

On the other hand, since 1  -1(log n)-1/2w(u)n1/2 by definition, we
|een(u)|
can always estimate

2

1 -1

(log n)-1w(u)2n|(u)|4  |(u)|2  2n2,

n(u) (u)

(5.43)

which yields

 2

E sup 
uR

1 een(u)

-

1 (u)

(log n)w(u)-2n-1 |(u)|4



1 |(u)|2

1(C c ) 



2n2 P (Cc) ,

(5.44)

and this expression is bounded by some constant since P(Cc) = O(n-2) by assumption on  and by Lemma 5.5.
We conclude this section by formulating two more auxiliary resuls: 
5.8 Lemma. For some  > 0, let  = 2 2pc1 +  . Let

x2m,k,f,w

:=

1 22

CØ1

|Ff (-u)|2 F K u - F K u 2 w(u)-2 du km

 CØ2

|Ff (-u)| F K u - F K u

2
w(u)-1 du

km

and m,k,f,w := log x2m,k,f,w(k - m)2 . Then we have for some constant CK depending on :

P

u  R : |n(u) - (u)| 

 2

(log

n)1/2

+

m,k,f,w

w(u)-1n-1/2

 CK n-pn-pxm-2,k,f,w(k - m)-2.

Proof. The proof runs exactly along the same lines as the proof of Lemma 5.5, setting, this time

n :=

 (log
2

n)1/2

+

m,k,f,w

n-1/2 - CNRn-1/2.

31

5.9 Lemma. In the situation of the preceding statement, we have

 2

 P  u  R :


1- 1 (u) n(u)

>

5 2

(log

n)1/2

+

m,k,f,w

2

w(u)-2n-1

 



|n(u)|2|(u)|2



 CK n-pxm-2,k,f,w(k - m)-2.

This statement is derived from Lemma 5.8, using the same arguments which are given to derive Corollary 5.7.

5.3.3 Preparing the proof of the main result

To be able to prove the main result of this section, we will need the following auxiliary result:

5.10 Proposition. For arbitrary m  N, we can estimate



E

 

sup

k>m

kM

k - m

-

(k

-

m)

2

-

12 H (m,
2

k)

=O 
+

n-1

.

Proof. (Sketch) The proof of this statement is long, but the steps are elementary. We content ourselves with giving the main ideas. For the details, we refer to [17].
For m  M, let

1 m := 2

Ff (-u)  (u) F K u du.

n(u)

m

We use the estimate

2
k - m - (k - m)

22
 2 k - m - k - m + 2 k - m - (k - m) .

First, we show that



E

 

sup

k>m

kN

E
km kM



(k

-

m)

-

(k

-

m)

2

-

12 H (m,
8

k)



+

(k

-

m)

-

(k

-

m)

2

-

12 H (m,
8

k)

+

is negligible. This is done by applying the integral version of Bernstein's inequality to the conditional expectations

E

(k

-

m)

-

(k

-

m)

2

-

12 H (m,
8

k)

n
+

32

and concluding that the sum is (almost surely) negligible.
Since k - m is clearly unbounded, one has to truncate the random
variables Zj = Xj - Xj-1 at the threshold log n + log xm2 ,k(m - k)2 . Then one can directly apply Lemma 5.3 to see that the sum is negligible.
The remainder terms are seen to be negligible, using the Markov inequality. To do this, we need the exponential moment condition on X1.
Next, we consider


E  sup 
k>m kN



(k

-

m)

-

(k

-

m)

2

-

12 H (m,
8

k)

. 
+

To see that this term is negligible, we first introduce the favourable sets

C(m, k)



2

:=

 
u  R :
 

1 -1 n(u) (u)





5 2



(log n) + m,k,f,w

2

w(u)-1n-1

 

n(u) 2 |(u)|2

 

and show that on C(m, k):

(k

-

m)

-

(k

-

m)

2



12 H (m, k).
8

This inequality can be derived immediately from the definition of C(m, k) by a repeated application of the Cauchy-Schwarz inequality.
Finally, it remains to show that

E
k>m kM

(k

-

m)

-

(k

-

m)

2

-

12 H (m,
8

k)

1 (C(m, k)c)
+

is negligible. This fact can be derived from Lemma 5.9.

5.3.4 Proof of the main result

We are now ready to prove the oracle inequality given in Theorem 3.3 and thus the key result of our adapitve estimation procedure:

Proof of Theorem 3.3. In what follows, let m be the oracle cutoff, that is,





m

=

arginf

 sup |k

-

m|2

+

 pen(m)

.

mM k>m



kM



We start by considering the loss on the set {m  m}.

33

We can decompose

22

2

 - m  2  - m + 2 m - m .

bb

By definition of m, we can estimate

m - m 2 1 ({m  m})
b

 sup
km
kM

k - m 2 - H2(m, k)

+

pen(m)

+

2
H (m,

m)1

({m



m)

.

The definition of H(m, m) implies that we have

2
H (m,

m)1

({m



m})



pen(m)

(5.45)

and an application of Lemma 3.2 readily implies that E [pen(m)]  pen(m)O(1).

(5.46)

Finally, we can estimate

sup
km
kM

k - m 2 - H2(m, k)

(5.47)

 2 sup
km
kM

(k

-

m )

-

(k

-

m )

2

-

1 H2(m, k) 2

+ 2 sup |k - m|2.
km

kM

Taking expectation, we find that the first expression appearing in the last line of (5.47) is negligible thanks to Proposition 5.10. Using this, (5.45) and (5.46), we have shown that for some consant C,

E | - m|21 ({m  m})
b









C inf
mM

| 

-

mn |2

+

sup |k
km

-

m|2

+

pen(m) 

+

O

n-1

.



kM

It remains to consider the loss on {m > m}. We use the decomposition

| - mb |2  2| - mb |2 + 2|mb - mb |2. First, we can immediately estimate



|

-

mb |21

({m

>

m})



3

| 

-

mn |2

+

sup |k
k>m

-

m

|2

 

.

kM

34

Next, we can decompose

|m - m|21 ({m > m})
bb

(5.48)

 |k - k|2 - pen(k) + pen(k)1 ({m = k}) .

k>m

+ k>m

kM

kM

Again, using Proposition 5.10, we find that the expected value of the first expression appearing in the second line of (5.48) is readily negligible.
Next, we use the fact that by definition of m, we have on {m = k}:

pen(k) 

sup

|l

-

m

|2

-

2
H

(m,

l)

+ pen(m)

l>m

lM



2 sup
l>m

|(l

-

m )

-

(l

-

m )|2

-

1 H2(m, 2

l)

lM

+ 2 sup |l - m|2 + pen(m)
l>m lM

to see that

pen(k)1 ({m = k})

k>m kM



2 sup
l>m

|(l

-

m )

-

(l

-

m )|2

-

1 H2(m, 2

l)

lM

+ 2 sup |l - m|2 + pen(m).
l>m

lM

(5.49) (5.50) (5.51)

Again, we see that the second line in (5.49) is negligible and we use, once more, Lemma 3.2 to see that

E [pen(m)]  O(1) pen(m).

We have thus shown that for some constant C,

E | - m|21 ({m > m})
b







C

inf
mM

 |


-

mn |2

+

sup |k
k>m

-

m|2

+

 pen(m)


+

O(n-1).



kM

and hence the main result of Section 3.3).

35

References
[1] Denis Belomestny and Markus Reiﬂ. Spectral calibration of exponential L¥evy models. Finance Stoch., 10(4):449≠474, 2006.
[2] Lucien Birg¥e. An alternative point of view on Lepski's method. In State of the art in probability and statistics: Festschrift for Willem R. van Zwet, pages 113≠133. IMS Lecture Notes-Monograph Series, 1999.
[3] Christina Butucea and Fabienne Comte. Adaptive estimation of linear functionals in the convolution model and applications. Bernoulli, 15(1):69≠98, 2009.
[4] Peter Carr, H¥elyette Geman, Dilip Madan, and Marc Yor. The fine structure of asset returns: An empirical investigation. Journal of Business, 75(2):305≠332, 2002.
[5] Peter Carr, Dilip Madan, and Eric Chang. The variance Gamma process and option pricing. European Finance Review, 2:79≠105, 1998.
[6] Fabienne Comte and Valentine Genon-Catalot. Nonparametric estimation for pure jump L¥evy processes based on high frequency data. Stochastic Processes and their Applications, 119:4088≠4123, 2009.
[7] Fabienne Comte and Valentine Genon-Catalot. Nonparametric adaptive estimation for pure jump L¥evy processes. Annales de l'Institut Henri Poincar¥e, 3(46):595≠617, 2010.
[8] Fabienne Comte and Valentine Genon-Catalot. Estimation for L¥evy processes from high frequency data within a long time interval. Annals of Statistics, 39(2):803≠837, 2011.
[9] Fabienne Comte and Claire Lacour. Data-driven density estimation in the presence of additive noise with unknown distribution. Journal of the Royal Statistical Society: Series B, 73:601≠627, 2011.
[10] Rama Cont and Peter Tankov. Financial modelling with jump processes. Chapman & Hall/CRC Financial Mathematics Series. Boca Raton, 2004.
[11] Richard M. Dudley. Uniform central limit theorems. Cambridge Studies in Advanced Mathematics 63. Cambridge: Cambridge University Press. xiv, 436 p , 2008.
[12] Jos¥e E. Figueroa-L¥opez. Small-time moment asymptotics for L¥evy processes. Stat. Probab. Lett., 78(18):3355≠3365, 2008.
36

[13] Jos¥e E. Figueroa-Lo¥pez. Nonparametric estimation for L¥evy models based on discrete sampling. IMS Lecture Notes of the 3rd E.L. Lehmann Symposium, 57:117≠146, 2009.
[14] Jos¥e E. Figueroa-L¥opez and Christian Houdr¥e. Risk bounds for the non-parametric estimation of L¥evy processes. Gin¥e, Evarist (ed.) et al., High dimensional probability. Institute of Mathematical Statistics Lecture Notes - Monograph Series 51, 96-116, 2006.
[15] Shota Gugushvili. Nonparametric estimation of the characteristic triplet of a discretely observed L¥evy process. Journal of Nonparametric Statistics, 21(3):321≠343, 2009.
[16] Shota Gugushvili. Nonparametric estimation for discretely sampled L¥evy processes. Annales de l'Institut Henri Poincar¥e, 48(1):282≠307, 2012.
[17] J. Johanna Kappus. Nonparametric adaptive estimation for discretely observed L¥evy processes. PhD thesis, Humboldt Universit®at zu Berlin, 2012. (to appear).
[18] B¥eatrice Laurent, Carenne Luden~a and Cl¥ementine Prieur. Adaptive estimation of linear functionals by model selection. Electronic Journal of Statistics, 2:993≠1020, 2008.
[19] Michael Neumann and Markus Reiﬂ. Nonparametric estimation for L¥evy processes from low frequency observations. Bernoulli, 15(1):223≠ 248, 2009.
[20] Michael H. Neumann. On the effect of estimating the error density in nonparametric deconvolution. Journal of Nonparametric Statistics, 7(4):307≠330, 1997.
[21] Peter Carr and H¥elyette Geman and Dilip Madan and Marc Yor. Stochastic volatility for L¥evy processes. Mathematical Finance, 13:345≠ 382, 2003.
[22] Peter Carr and Liuren Wu. Time changed L¥evy processes and option pricing. Journal of Financial Economics, 71:113≠141, 2004.
[23] Ken-Iti Sato. L¥evy processes and infinitely divisible distributions. Cambridge University Press, 1999.
[24] Jakob S®ohl. Confidence sets in nonparametric calibration of exponential L¥evy models. SFB 649 Discussion Paper 2012-012, Sonderforschungsbereich 649, Humboldt Universita®t zu Berlin, Germany, 2012. available at http://sfb649.wiwi.hu-berlin.de/papers/pdf/SFB649DP2012012.pdf.
37

SFB 649 Discussion Paper Series 2012
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "HMM in dynamic HAC models" by Wolfgang Karl H‰rdle, Ostap Okhrin and Weining Wang, January 2012.
002 "Dynamic Activity Analysis Model Based Win-Win Development Forecasting Under the Environmental Regulation in China" by Shiyi Chen and Wolfgang Karl H‰rdle, January 2012.
003 "A Donsker Theorem for LÈvy Measures" by Richard Nickl and Markus Reiﬂ, January 2012.
004 "Computational Statistics (Journal)" by Wolfgang Karl H‰rdle, Yuichi Mori and J¸rgen Symanzik, January 2012.
005 "Implementing quotas in university admissions: An experimental analysis" by Sebastian Braun, Nadja Dwenger, Dorothea K¸bler and Alexander Westkamp, January 2012.
006 "Quantile Regression in Risk Calibration" by Shih-Kang Chao, Wolfgang Karl H‰rdle and Weining Wang, January 2012.
007 "Total Work and Gender: Facts and Possible Explanations" by Michael Burda, Daniel S. Hamermesh and Philippe Weil, February 2012.
008 "Does Basel II Pillar 3 Risk Exposure Data help to Identify Risky Banks?" by Ralf Sabiwalsky, February 2012.
009 "Comparability Effects of Mandatory IFRS Adoption" by Stefano Cascino and Joachim Gassen, February 2012.
010 "Fair Value Reclassifications of Financial Assets during the Financial Crisis" by Jannis Bischof, Ulf Br¸ggemann and Holger Daske, February 2012.
011 "Intended and unintended consequences of mandatory IFRS adoption: A review of extant evidence and suggestions for future research" by Ulf Br¸ggemann, Jˆrg-Markus Hitz and Thorsten Sellhorn, February 2012.
012 "Confidence sets in nonparametric calibration of exponential LÈvy models" by Jakob Sˆhl, February 2012.
013 "The Polarization of Employment in German Local Labor Markets" by Charlotte Senftleben and Hanna Wielandt, February 2012.
014 "On the Dark Side of the Market: Identifying and Analyzing Hidden Order Placements" by Nikolaus Hautsch and Ruihong Huang, February 2012.
015 "Existence and Uniqueness of Perturbation Solutions to DSGE Models" by Hong Lan and Alexander Meyer-Gohde, February 2012.
016 "Nonparametric adaptive estimation of linear functionals for low frequency observed LÈvy processes" by Johanna Kappus, February 2012.
SFB 649, Spandauer Straﬂe 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

