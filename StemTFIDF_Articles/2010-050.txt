BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2010-050
Estimation of the signal subspace without estimation of
the inverse covariance matrix
Vladimir Panov*
* Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Estimation of the signal subspace without estimation of the inverse covariance matrix $
V. Panova
aWeierstrass Institute for Applied Analysis and Stochastics, Mohrenstrasse 39, 12681 Berlin, Germany
Abstract Let a high-dimensional random vector X can be represented as a sum of two components - a signal S, which belongs to some low-dimensional subspace S, and a noise component N . This paper presents a new approach for estimating the subspace S based on the ideas of the Non-Gaussian Component Analysis. Our approach avoids the technical difficulties that usually exist in similar methods - it doesn't require neither the estimation of the inverse covariance matrix of X nor the estimation of the covariance matrix of N . Keywords: dimension reduction, non-Gaussian components, NGCA
JEL Classification: C13 C14 AMS 2000 Subject Classification: 62G05 62H99 60G35

$This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
Email address: panov@wias-berlin.de (V. Panov)

Preprint submitted to Statistics and Probability Letters

October 11, 2010

1. Introduction and set-up.
Assume that a high-dimensional random variable X  Rd can be represented as a sum of two independent components - a low-dimensional signal (which one can imagine as "an useful part" or "an information") and a noise component (which has a Normal distribution). More precisely,

X = S + N ,

(1)

where S belongs to some low-dimensional subspace S, N is a normal vector with zero mean and unknown covariance matrix , and S is independent of N . Denote the dimension of S by m; up to this paper, m is fixed such that the representation (1) is unique (the existence of such m is proved by Theis and Kawanabe, 2007).
The aim of this paper is to estimate vectors from the subspace S, which we call the signal subspace. A very related task, estimation of so called the non-Gaussian subspace I (the definition will be given below) is widely studied in the literature. The original method known as Non-Gaussian Component Analysis (NGCA) was proposed by Blanchard et al, 2006a, and later improved by Kawanabe et al., 2007, Dalalyan et al., 2007, Sugiyama et al., 2008, Diederichs et al., 2010.
In almost all papers mentioned above, the problem of estimation of the vectors from S is not considered in details; the natural estimators require the estimation of the unknown matrix . The exception is an article Sugiyama et al., 2008, where one estimator is proposed. But practical usage of this method meets another problem - the estimation of the inverse covariance matrix.

2

Each of these tasks, estimation of the unknown matrix  and inverse covariance matrix, is an obstacle in real-world applications of the method. In this article, we propose a new approach, which avoids the mentioned problems.
The main theoretical fact is given in theorem 1. Together with lemma 2, this result yields a method of estimation. For proving the main result, one needs a special representation of the density of X , which is given in lemma 7, discussed in section 3, and proved in section 4.

2. Estimation of the signal subspace

We begin with the main result.

Theorem 1. Let T : Rd  -1/2S be the linear transformation defined as

Tx := Pr-1/2S {-1/2x}, where by  we denote the covariance matrix of X . Then

(2)

S =  (Ker T) .

(3)

In Blanchard et al., 2006a, a transformation T is considered instead of T:
T x := Pr-1/2S {-1/2x}. In that paper, the subspace (Ker T ) is called the non-Gaussian subspace and is in fact the main object of interest. We would like to stress here, that T = T, and equalities like (3) are wrong for T . The linear transformation T acts on x in the following way: firstly, S and x are transformed by matrix -1/2; secondly, the transformed x is projected on the transformed S. Figure 1 illustrates this action.

3

Figure 1: The action of the linear transformation T: 1. x is transformed by S; 2. transformed x is projected on transformed S.

One of the main results of the NGCA approach gives the practical method for estimating vectors from (Ker T ). Similar result can be formulated for the subspace (Ker T) also.

Lemma 2. Assume that a structural assumption (1) is fulfilled. Then for

any function   C1(Rd, R) there exists a vector   (Ker T) such that

[] E[(X )] -  = -1E X (X ) .

(4)

Corollary 3. Let a structural assumption (1) be fulfilled and let a function []
  C1(Rd, R) be such that E X (X ) = 0. Then []
E (X )  (Ker T) .

Theorem 1 and lemma 2 yield a method for finding vectors from the subspace S.
The first step. On the first step, one estimates vectors from the subspace (Ker T) using lemma 2. Theoretically, the best way for the estima-
[] tion is to find a function  such that E X (X ) = 0, and then to use the corollary. In practice, it is difficult to find such functions; usually it is more

4

[] realistic to consider some  such that E X (X ) is close to ze[ro (but ]not exactly zero). In this case, according to lemma 2, the vector E (X ) is close to some vector from the subspace (Ker T). For discussing practical issues about finding functions , we refer to Diederichs, PhD dissertation, 2007.
The second step. Denote the vectors obtained on the first step by ^i. Now one can use theorem 1 and estimate vectors from the signal subspace by ^ ^i, where ^ is an estimator of the matrix .
Note that the inverse covariance matrix is presented in the formulae (4) but our approach doesn't require the estimation of it. In fact, lemma 2 is used only for theoretical justification of the first step; practical method described above doesn't need neither the estimation of -1 nor the estimation of . On the second step, on uses only the representation (3), which also allows to avoid the estimation of the inverse covariance matrix.

3. Density representation
The proofs of the facts formulated in the previous section lie on some special representation of the density function of X . Certain representations can be also found in previous papers about NGCA. Such facts are stated in the following form: if structural assumption (1) is fulfilled than the density function of a random vector X  Rd can be represented as

p(x) = g(T x)A(x),

(5)

where T : Rd  E is a linear transformation (E - some subspace with dim E = m), g : E  R - a function, and A - a d × d symmetric positive matrix.

5

Usually the formulae (5) is proven for A = , see e.g. Kawanabe et al.,

2007; rarely for A = , see Sugiyama, 2008. Another way is to start with

the representation (5) without giving the motivation in the spirit of (1), see

e.g. Blanchard et al., 2006b.

The main result of this section can be briefly explained as follows: one

can find a function g such that (5) is fulfilled with T = T and A = . The

precise formulation is given below in lemma 4.

The existence of the representation in the form (5) can be easily shown

as follows. Note that the model (1) can be equivalently formulated via linear

mixing model:

X = ASX S + AN X N ,

(6)

where X S  Rm, X N  Rd-m are two random variables; X N is a normal vector with unknown covariance matrix; X S is independent of X N ; AS  Matr(d × m), AN  Matr(d × (d - m)) are two deterministic matrices such that columns of these matrices are independent. In this formulation, the

signal subspace is spanned by the columns of matrix AS. From (6), one can easily see that the vector X is in fact a linear transfor-
mation of the vector X  := (X S; X N ) (vector X  is a concatenation of vectors X S and X N ). This yields that p(x)  g(X S)(X N ), where by g we denote the density of the m-dimensional non-Gaussian component, and by  - the density function of the normally distributed random variable X N . Thus, the representation (5) is proven with T = .
The next theorem gives the exact representation for the density of X that

is needed for our purposes.

6

Lemma 4. Let the structural assumption (1) be fulfilled . Then the density function of the random vector X can be represented in the following way:

p(x) = g(Tx)(x),

(7)

where

· T : Rd  S, S := -1/2S,

Tx = PrS{-1/2x}, by  we denote the covariance matrix of X .

(8)

· g : S  R,

g(t)

=

|-1/2|

q (t) m (t) ,

(9)

where q(·) is the density function of the random variable TX , and m(·)

is the density function of the m - dimensional standard normal vector.

The proof of this fact begins the next section.

4. Proofs of the main facts Proof of the lemma 4 Step 1. Denote by X  = -1/2X the standardized vector, -1/2X = -1/2S + -1/2N .
Introduce the notation S = -1/2S, N  = -1/2N .

(10) (11)

7

The first component in (10) belongs to the subspace S := -1/2S. Denote by N the subspace that is orthogonal to S. One can proof that N = 1/2S

(see Sugiyama et al., 2008). Vector N can be decomposed into the sum of two vectors, N  = NS +NN,
where NS  S, NN  N. So, we consider the following decomposition of X :
X  = S + N S + N N .  S  N
It is worth mentioning that the density function doesn't depend on a basis.

This means that for a calculation of the density function the basis can be

changed arbitrarily. Let us choose it such that the first m vectors v1, ..., vm

compose a basis of S and the next d - m vectors vm+1, ..., vd compose a basis

of N. In the following, we assume that this change is already made.

Step 2. By definition, X  is a standardized vector. This step shows that

the vectors Z  = S + NS and NN are also standardized.

[]

Id = Cov X  = E X X T

[ ][

][ ][

][

][

]

= E Z Z T +E N NN NT +E SN NT +E N SN NT +E N NST +E N NN ST

(12)

Note some facts:

(i) By the change of the basis, the last d - m components of the vectors S, Z , NS and the first m components of the vector NN are equal to zero.

(ii) The vectors S = -1/2S and NN = Pr N{-1/2N } are independent as functions of the independent vectors S and N .

8

[] (iii) ENN = E PrN{-1/2N } = 0, because of EN = 0 and (i).

Now it's easy to see that the third and the fifth summands in (12) are

equal to zero. In fact,

[] E SN NT = ES ENNT  = 0.

So, one can rewrite (12) in the following way

[ ][

][

][

]

Id = E Z Z T + E N NN NT + E N S N NT + E N NN ST .

Decompose the vectors Z , NS and NN into the basis v1, .., vd:

m Z  = zivi;
i=1

m N S = nivi;
i=1

d

N N =

nivi,

i=m+1

where all coefficients zi and ni are random values.

Equality (13) can be rewritten as follows:

(13) (14)

m d

Id =

E [zizi] vivi +

E [nini ] vivi

i,i=1

i,i=m+1

m d

d m

+

E [nini] vivi +

E [nini ] vivi

i=1 i=m+1

i=m+1 i=1

Then the second term in the right hand side is equal to Id-m, i.e.

[ ] d

E N N N NT =

E [nini ] vivi = Id-m.

i,i=m+1

Thus, the (d - m) - dimensional vector NN has the standard normal distri-

bution. Denote the density function by d-m(x). Step 3. Denote by F (x) and p(x) the distribution function and the

density function of the vector X . { }{
F (x) = P X  x = P Z  + NN

} x

(15)

9

Note some facts:
(i) Vectors Z  = S + NS and NN are independent. In fact, vectors S = -1/2S and N  = -1/2N are independent. Then vectors S, NN and NS are jointly independent (this follows from the choice of the basis). Finally, Z  and NN are independent as functions of independent variables.

(ii) The basis choice (14) enables us to split the inequality

Z  + N N

d x = xivi
i=1

into two:

m Z  xivi =: xS, N N

d xivi =: xN.

i=1 i=m+1

The function F  can be rewritten in the following way:

{ F (x) = P Z  + N N

}{ x = P Z 

xS , N N {
= P Z 

}
xN }{
xS P N N

} xN .

Taking derivatives of the both parts of the last formula gives the representation of the density function of X .

p(x)

=

q(xS  )d-m(xN )

=

q (xS) m (xS)

d

(x

)

=

q(PrS  {x}) m(PrS  {x })

d

(x),

where by q(·) denote the density function of the random vector Z  = S + N S = PrS{X }.

10

Step 4. The last step derives representation of the density function of the vector X = 1/2X from the density function of X . According to the

well-known formula for a density transformation,

()

p(x)

=

|-1/2|

p(-1/2x)

=

|-1/2|

q m

PrS  {-1/2x} (PrS  {-1/2x})

d(-1/2x).

The remark d(-1/2x) = (x) concludes the proof.

Proof of the lemma 2. Here we prove a more general result:

Lemma 5. Assume that the density function of a random vector X  Rd can be represented in the form (5), where T : Rd  E is any linear transformation (E - any subspace), g : E  R - any function, and A - any d × d symmetric

positive matrix.

Assume that a structural assumption (1) is fulfilled. Then for any func-

tion   C1(Rd, R) there exists a vector   (Ker T ) such that []
E[(X )] -  = -1E X (X ) .

(16)

Proof. Integration by parts yields 
E (X ) =  [(x)] p(x)dx = - (x) [p(x)] dx.

(17)

The gradient of the density function can be represented as a sum of two components:

p(x) =  [log p(x)] p(x) =  [log g(T x)] p(x) +  [log A(x)] p(x).

The summands in the right hand side can be transformed in the following

way:

 [log g(T x)] p(x)

=

g(T x) p(x)
g(T x)

=  [g(T x)] A(x) = T {Tx} [g(T x)] A(x)

11

 [log A(x)] p(x) = --1xp(x).

Denote  = T  . Then
 E (X ) -  = -T  (x){Tx} [g(T x)] A(x)p(x)dx = T 

 Im(T ) = (Ker T ) ,

where



=

 -

(x){Tx} [g(T x)] A(x)p(x)dx.

This completes the proof.

Proof of the theorem 1

The proof is straightforward:

{ Ker T = x :

-1/2x  -1/2S}

{ = x :

s



S

|

x

(-1/2)

-1/2s

=

} 0

=

{ x

:

s



S

|

x

-1s

=

} 0

{ = x :

x  -1S} .

Here we use the symmetry of the matrix -1/2.

Acknowledgements
Author would like to gratefully thank his science advisor professor Vladimir Spokoiny for attention to this work, as well as professor Gilles Blanchard for fruitful discussions and some useful advices.

References
Blanchard, G., Kawanabe, M., Sugiyama, M., Spokoiny, V., Mu¨ller, K.R., 2006a. In search of non-Gaussian components of a high-dimensional distribution. J. Mach. Learn. Res. 6, 247­282.
12

Blanchard, G., Kawanabe, M., Sugiyama, M., Spokoiny, V., Mu¨ller, K.-R., 2006b. Non-Gaussian component analysis: a semi-parametric framework for linear dimension reduction, in: Advances in Neural Inf. Proc. Systems (NIPS 05), MIT Press. pp. 131­138.
Dalalyan, A.S, Juditsky, A., Spokoiny, V., 2007. A new algorithm for estimating the effective dimension - reduction subspace. J. Mach. Learn. Res 9, 1647­1678.
Diederichs, E., 2007. Semi-parametric reduction of dimensionality. Ph.D. thesis. Free University of Berlin.
Diederichs, E., Juditsky, A., Spokoiny, V., Schu¨tte, C., 2010. Sparse nonGaussian component analysis. IEEE Trans. Inf. Theory. 15, 5249­5262.
Kawanabe, M., Sugiyama, M., Blanchard, G., Mu¨ller, K.-R., 2007. A new algorithm of non-Gaussian component analysis with radial kernel functions. Ann. Inst. Stat. Math. 59, 57­75.
Sugiyama, M., Kawanabe, M., Blanchard, G., Mu¨ller, K.-R., 2008. Approximating the best linear unbiased estimator of non-Gaussian signals with Gaussian noise. IEICE Trans. Inform. Syst. E91-D, 1577­1580.
Theis, F.J. and Kawanabe, M., 2007. Uniqueness of non-Gaussian subspace analysis, in: Proc. ICA 2007. Springer, London. volume 4666 of LNCS, pp. 917­925.
13

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Volatility Investing with Variance Swaps" by Wolfgang Karl Härdle and Elena Silyakova, January 2010.
002 "Partial Linear Quantile Regression and Bootstrap Confidence Bands" by Wolfgang Karl Härdle, Ya'acov Ritov and Song Song, January 2010.
003 "Uniform confidence bands for pricing kernels" by Wolfgang Karl Härdle, Yarema Okhrin and Weining Wang, January 2010.
004 "Bayesian Inference in a Stochastic Volatility Nelson-Siegel Model" by Nikolaus Hautsch and Fuyu Yang, January 2010.
005 "The Impact of Macroeconomic News on Quote Adjustments, Noise, and Informational Volatility" by Nikolaus Hautsch, Dieter Hess and David Veredas, January 2010.
006 "Bayesian Estimation and Model Selection in the Generalised Stochastic Unit Root Model" by Fuyu Yang and Roberto Leon-Gonzalez, January 2010.
007 "Two-sided Certification: The market for Rating Agencies" by Erik R. Fasten and Dirk Hofmann, January 2010.
008 "Characterising Equilibrium Selection in Global Games with Strategic Complementarities" by Christian Basteck, Tijmen R. Daniels and Frank Heinemann, January 2010.
009 "Predicting extreme VaR: Nonparametric quantile regression with refinements from extreme value theory" by Julia Schaumburg, February 2010.
010 "On Securitization, Market Completion and Equilibrium Risk Transfer" by Ulrich Horst, Traian A. Pirvu and Gonçalo Dos Reis, February 2010.
011 "Illiquidity and Derivative Valuation" by Ulrich Horst and Felix Naujokat, February 2010.
012 "Dynamic Systems of Social Interactions" by Ulrich Horst, February 2010.
013 "The dynamics of hourly electricity prices" by Wolfgang Karl Härdle and Stefan Trück, February 2010.
014 "Crisis? What Crisis? Currency vs. Banking in the Financial Crisis of 1931" by Albrecht Ritschl and Samad Sarferaz, February 2010.
015 "Estimation of the characteristics of a Lévy process observed at arbitrary frequency" by Johanna Kappusl and Markus Reiß, February 2010.
016 "Honey, I'll Be Working Late Tonight. The Effect of Individual Work Routines on Leisure Time Synchronization of Couples" by Juliane Scheffel, February 2010.
017 "The Impact of ICT Investments on the Relative Demand for HighMedium-, and Low-Skilled Workers: Industry versus Country Analysis" by Dorothee Schneider, February 2010.
018 "Time varying Hierarchical Archimedean Copulae" by Wolfgang Karl Härdle, Ostap Okhrin and Yarema Okhrin, February 2010.
019 "Monetary Transmission Right from the Start: The (Dis)Connection Between the Money Market and the ECB's Main Refinancing Rates" by Puriya Abbassi and Dieter Nautz, March 2010.
020 "Aggregate Hazard Function in Price-Setting: A Bayesian Analysis Using Macro Data" by Fang Yao, March 2010.
021 "Nonparametric Estimation of Risk-Neutral Densities" by Maria Grith, Wolfgang Karl Härdle and Melanie Schienle, March 2010.

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
022 "Fitting high-dimensional Copulae to Data" by Ostap Okhrin, April 2010. 023 "The (In)stability of Money Demand in the Euro Area: Lessons from a
Cross-Country Analysis" by Dieter Nautz and Ulrike Rondorf, April 2010. 024 "The optimal industry structure in a vertically related market" by
Raffaele Fiocco, April 2010. 025 "Herding of Institutional Traders" by Stephanie Kremer, April 2010. 026 "Non-Gaussian Component Analysis: New Ideas, New Proofs, New
Applications" by Vladimir Panov, May 2010. 027 "Liquidity and Capital Requirements and the Probability of Bank Failure"
by Philipp Johann König, May 2010. 028 "Social Relationships and Trust" by Christine Binzel and Dietmar Fehr,
May 2010. 029 "Adaptive Interest Rate Modelling" by Mengmeng Guo and Wolfgang Karl
Härdle, May 2010. 030 "Can the New Keynesian Phillips Curve Explain Inflation Gap
Persistence?" by Fang Yao, June 2010. 031 "Modeling Asset Prices" by James E. Gentle and Wolfgang Karl Härdle,
June 2010. 032 "Learning Machines Supporting Bankruptcy Prediction" by Wolfgang Karl
Härdle, Rouslan Moro and Linda Hoffmann, June 2010. 033 "Sensitivity of risk measures with respect to the normal approximation
of total claim distributions" by Volker Krätschmer and Henryk Zähle, June 2010. 034 "Sociodemographic, Economic, and Psychological Drivers of the Demand for Life Insurance: Evidence from the German Retirement Income Act" by Carolin Hecht and Katja Hanewald, July 2010. 035 "Efficiency and Equilibria in Games of Optimal Derivative Design" by Ulrich Horst and Santiago Moreno-Bromberg, July 2010. 036 "Why Do Financial Market Experts Misperceive Future Monetary Policy Decisions?" by Sandra Schmidt and Dieter Nautz, July 2010. 037 "Dynamical systems forced by shot noise as a new paradigm in the interest rate modeling" by Alexander L. Baranovski, July 2010. 038 "Pre-Averaging Based Estimation of Quadratic Variation in the Presence of Noise and Jumps: Theory, Implementation, and Empirical Evidence" by Nikolaus Hautsch and Mark Podolskij, July 2010. 039 "High Dimensional Nonstationary Time Series Modelling with Generalized Dynamic Semiparametric Factor Model" by Song Song, Wolfgang K. Härdle, and Ya'acov Ritov, July 2010. 040 "Stochastic Mortality, Subjective Survival Expectations, and Individual Saving Behavior" by Thomas Post and Katja Hanewald, July 2010. 041 "Prognose mit nichtparametrischen Verfahren" by Wolfgang Karl Härdle, Rainer Schulz, and Weining Wang, August 2010. 042 "Payroll Taxes, Social Insurance and Business Cycles" by Michael C. Burda and Mark Weder, August 2010. 043 "Meteorological forecasts and the pricing of weather derivatives" by Matthias Ritter, Oliver Mußhoff, and Martin Odening, September 2010. 044 "The High Sensitivity of Employment to Agency Costs: The Relevance of Wage Rigidity" by Atanas Hristov, September 2010. 045 "Parametric estimation of risk neutral density functions" by Maria Grith and Volker Krätschmer, September 2010.

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
046 "Mandatory IFRS adoption and accounting comparability" by Stefano Cascino and Joachim Gassen, October 2010.
047 "FX Smile in the Heston Model" by Agnieszka Janek, Tino Kluge, Rafal Weron, and Uwe Wystup, October 2010.
048 "Building Loss Models" by Krzysztof Burnecki, Joanna Janczura, and Rafal Weron, October 2010.
049 "Models for Heavy-tailed Asset Returns" by Szymon Borak, Adam Misiorek, and Rafal Weron, October 2010.
050 "Estimation of the signal subspace without estimation of the inverse covariance matrix" by Vladimir Panov, October 2010.

