BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2010-026
Non-Gaussian Component Analysis: New Ideas, New Proofs, New Applications
Vladimir Panov*
* Weierstrass Institute for Applied Analysis and Stochastics, Berlin
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Non-Gaussian Component Analysis: New Ideas, New Proofs, New Applications $
V. Panova
aWeierstrass Institute for Applied Analysis and Stochastics, Mohrenstrasse 39, 12681 Berlin, Germany
Abstract In this article, we present new ideas concerning Non-Gaussian Component Analysis (NGCA). We use the structural assumption that a high-dimensional random vector X can be represented as a sum of two components - a lowdimensional signal S and a noise component N . We show that this assumption enables us for a special representation for the density function of X. Similar facts are proven in original papers about NGCA ([1], [5], [13]), but our representation differs from the previous versions. The new form helps us to provide a strong theoretical support for the algorithm; moreover, it gives some ideas about new approaches in multidimensional statistical analysis. In this paper, we establish important results for the NGCA procedure using the new representation, and show benefits of our method.
Keywords: dimension reduction, non-Gaussian components, EDR subspace, classification problem, Value at Risk
JEL Classification: C13 C14
AMS 2000 Subject Classification: 62G05 62G07 62H30 62H99 62P20

$This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
Email address: panov@wias-berlin.de (V. Panov)

Preprint submitted to Elsevier

May 6, 2010

Introduction
Each method for solving the dimension reduction problem has its own "point of departure". In the approach based on Independent Component Analysis, high-dimensional data are assumed to be a linear or nonlinear mixture of unknown latent non-Gaussian variables [Hyv®arianen et al., 2001]. Another popular method - Principal Component Analysis - implies that the projection representing the data optimally in the least-squared sense is the best projection [Jackson, 1991].
In this paper, we discuss the new ideas concerning Non-Gaussian Component Analysis (NGCA). The method assumes that the "useful part" (which one can imagine as "a signal" or "an information") of the high - dimensional random variable belongs to some low-dimensional space, and the "rest part" has normal distribution. This assumption follows the observation that in real-world applications the "useful part" is non-Gaussian while the "rest part" has a nearly Gaussian distribution [Blanchard et al, 2006].
The aim of Non-Gaussian Component Analysis is to estimate the subspace of the "useful part", also known as an effective dimension reduction subspace (EDR subspace). The original method can be briefly explained as follows: 1) finding vectors that belong to the EDR subspace; 2) constructing a basis from vectors that were obtained on the first step. [Blanchard et al., 2006; Kawanabe et al., 2007; Dalalyan et al., 2007; Diederichs et al., 2010]
We focus on the first task - finding vectors from the EDR subspace. The proof of the original method is based on the special representation of the density function. A serious limitation is that the representation involves the covariance matrix of the noise component, which cannot be consistently estimated from the data. This fact is an obstacle in real-world applications of the method.
In this article, we describe the new representation of the density function. Our representation doesn't involve the noise covariance matrix; it only includes the covariance matrix of the observations, which can be assessed.
This paper is organized as follows. Subsection 2.1 explains the new representation. Then we formulate the main results of NGCA using this form and discuss practical issues. Afterwards we discuss advantages of the new representation. All proofs can be found in the appendices, as well as some examples of practical applications.
2

1. Set-up and General Scheme
This section gives a formal description of the method. Assume that a high-dimensional random variable X  Rd can be represented as a sum of two independent components - a low-dimensional signal and a noise component. More precisely,

X = S + N,

(1)

where
∑ S belongs to some low-dimensional subspace S, dim S = m < d;
∑ N is a normal vector with zero mean and a covariance matrix ;
∑ S is independent of N . The original aim of NGCA is to recover the subspace (Ker T ). Theoretical framework is presented in [1], [5], [13]. It can be split into the following steps:
(T1) A proof of the formula for the density function of X:

p(x) = g(T x)(x)
where i) T is a linear transformation from Rd to some subspace S that has dimensionality m; ii) g is some function from S to R;
iii) (x) is the density function for the normal distribution with zero mean and covariance matrix equals .
So, the density function of X can be represented as a product of two functions. The first one is a superposition of the linear transformation T from Rd to some low-dimensional subspace S and some function from S to R. The second function is the density function of the normal distribution. (T2) A proof of the key result: if function  : Rd  R is such that

E X(X) = 0,

(2)

3

then  := E (X)  (Ker T ) .
This result gives a method for estimating vectors from (Ker T ). It means that if one can construct a function that satisfies (2) then one can estimate a vector from the subspace (Ker T ). (T3) Finding a method for constructing a basis of (Ker T ) from vectors that were obtained on the previous step.
One can develop a method for estimating the EDR subspace, based on (T1)-(T3): (P1) Estimate vectors from (Ker T); the corresponding fact is proved in
(T2). (P2) Construct a basis of S using vectors from previous step; see item (T3).
This paper has a slightly different focus: the aim is to recover the subspace S. Relation between the subspace S and the subspace (Ker T ) uses an unknown matrix  (covariance matrix of noise). This means that it is impossible to estimate vectors from S using only estimates of vectors from (Ker T ); in other words, original approach produces estimates vectors from S only in the case of white noise.
This article is organized as follows: Subsection 2.1: a new "convenient" form of the density function of X (alternative version of the item (T1)), which doesn't use the matrix ; the proof is given in Appendix A.1. Subsection 2.2: the key result for the new representation of the density function (a generalization of the original key result is given); the proof can be found in Appendix A.2. Subsection 2.3: studying a relation between S and (Ker T ); this relation also doesn't include matrix . Section 3: discussion about the advantages of the new form. Appendix A: proofs of the main results. Appendix B: some ideas about application of NGCA in multidimensional statistical analysis.
2. Main results
2.1. Density representation The new representation for the density function is given in Theorem 1.
4

Theorem 1. Let the structural assumption (1) be fulfilled. Then the density function of the random vector X can be represented in the following way:

p(x) = g(Tx)(x),

where

∑ T : Rd  S , S := -1/2S, dim S = m,

Tx

=

PrS

{-

1 2

x},

(3) (4)

by  we denote the matrix E XXT .

∑ g : S  R,

g(t) = |-1/2| q PrS {t} , m PrS {t}

(5)

where q(∑) is the density function of the random variable Pr S {-1/2X}.

The proof is given in Appendix A.1.

Remark 1. One can find similar results in [1], [5], and [14]. Usually such facts are stated in the following form: if assumption (1) is fulfilled, then  T : Rd  S, dim S = m, g : S  R such that

p(x) = g(T x)(x)

(6)

with some linear transformation T and some function g. Main difference between (3) and (6) is that (6) uses the covariance matrix  while (3) relies on the covariance matrix of the Gaussian component.
Advantages of the new form will be discussed later, see subsection 3 for details.

Remark 2. The linear transformation T acts on x in the following way: 1. firstly, S and x are transformed by matrix -1/2; 2. secondly, transformed x is projected on transformed S.
Figure 1 illustrates this action.

5

Figure 1: The action of the linear transformation T: 1. x is transformed by S; 2. transformed x is projected on transformed S.

2.2. Key result of NGCA The key result of NGCA can be found e.g. in [14]. We state this result
in a slightly different form.
Theorem 2. Assume that the density function of a random vector X  Rd can be represented in the following way:

p(x) = g(T x)A(x),

(7)

where T : Rd  E is any linear transformation (E - any subspace), g : E  R - any function, and A - any d ◊ d symmetric positive matrix.
Let a function  : Rd  R be such that

E X(X) = 0.

Then

E (X)  (Ker T ) .

The proof is given in Appendix A.2. This theorem applies to the representation from the previous subsection (see (3)) and to the former versions of theorem 1 (see (6)). In the next section, we explain why this result is useful for finding vectors from S.
2.3. What is (Ker T) ?
The previous section gives us the method for finding vectors from the subspace (Ker T). Now we explain, why it is useful for our purposes (that is, finding vectors that belong to S).
The following lemma plays the key role in practical use of NGCA.

6

Lemma 3. Let T be the linear transformation defined by (4). Then (Ker T) = -1S.

Proof. By definition, for any x

Tx

=

Pr-1/2

S

{-

1 2

x}.

It is easy to see that

Ker T = x : -1/2x  -1/2S = x : s  S | x -1/2 -1/2s = 0 = x : s  S | x -1s = 0
= x : x  -1S .

Here we use the symmetry of the matrix -1/2.

From a practical point of view, the last lemma means that one can obtain
estimates for vectors from the subspace S using the key result. In fact, one can estimate vectors from the space -1S using the key result. Denote these vectors by ^i. Then one can estimate vectors from the space S by ^ ^i, where ^ is an estimator of the matrix .

3. Discussion
This section discusses the novelty of the proposed approach relative to the original papers about NGCA ([1], [5]).
Theorem 1 stands that in our set-up the density function of X can be represented in the following way:
p(x) = g(Tx)(x),
where the linear transformation T is given by (4) and the function g is given by (5). In the previous papers about NGCA, another result is proved (it has been already mentioned, see remark 1):
p(x) = g(T x)(x).
The new representation has only one principal difference - the covariance matrix of the normal component is equal to  while in the original version it is equal to .
The new approach has some advantages.

7

1. One doesn't need any knowledge of . In fact, the representation from Theorem 1 involves only the unknown subspace S; on the other hand, the former form involves the subspace S and the matrix .
2. It is clear what the linear transformation T and the subspace (Ker T)
are, see figure 1, remark 2, and section 2.3 for details. In the contrary of the new version, action of the linear transformation
T from the former version is much more involved. The explanation of
this action is given below. Consider three cases.

(i)  = Id and S is a span of first m basis vectors. Then the linear transformation T is simply a projector onto the first m components; denote this projector by m.
(ii) The noise covariance matrix  is still the identity matrix Id; S is a span of some m orthogonal vectors. Denote a transaction matrix by U . Then T = m U.

(iii)  is not necessary equal to the identical matrix; S is any subspace with dimensionality m. In this case

T = m U -1/2.

(8)

In the first and in the second cases and even in the third (general) case with a diagonal matrix , one can show that
(Ker T ) = S.
If  is not diagonal, then the formula for the subspace (Ker T ) includes the matrix , which cannot be estimated from the data. This fact is an obstacle in real-world applications of the method. 3. The new presentation for the density function allows us to apply NGCA to the classification problem. According to this method given in Appendix B.1, one has to test the assumption

1 = 2,
where 1 and 2 are the covariance matrices for the first and the second groups correspondingly. This assumption is widely used and the testing problem can be solved by different methods ([6], [9]). Possible applications of NGCA in the previous versions lead to the assumption of equality of the noise covariance matrices. Such hypothesis are more difficult to test.

8

Appendix A. Proofs of the main results
Appendix A.1. Proof of Theorem 1 Step 1. Denote by X = -1/2X the standardized vector,
-1/2X = -1/2S + -1/2N .

(A.1)

Introduce the notation

S = -1/2S, N = -1/2N .

(A.2)

The first component in (A.1) belongs to the subspace S := -1/2S. Denote by N the subspace that is orthogonal to S ,

N = 1/2S.

Studying a relation between the subspaces S and N can be found in Appendix A.3.
Vector N can be decomposed into the sum of two vectors,

N = NS + NN ,
where NS  S , NN  N . So, up to now the following decomposition of X is proved:

X = S + NS + NN .
S N
It is worth mentioning that the density function doesn't depend on a basis. This means that for a calculation of the density function the basis can be changed arbitrarily. Let us choose the basis such that the first m vectors v1, ..., vm compose a basis of S and the next d-m vectors vm+1, ..., vd compose a basis of N . In the following we assume that this change is already made.
Step 2. By definition, X is a standardized vector. This step shows that the vectors Z = S + NS and NN are also standardized.

Id = Cov X = E X X T
= E Z Z T +E NN NNT +E S NNT +E NS NNT +E NN S T +E NN NST (A.3)

Note some facts:

9

(i) By the change of the basis, the last d - m components of the vectors S , Z , NS and the first m components of the vector NN are equal to zero.
(ii) The vectors S = -1/2S and NN = Pr N {-1/2N } are independent as functions of the independent vectors S and N .

(iii) ENN = E PrN {-1/2N } = 0, because of EN = 0 and (i).

Now it's easy to see that the third and the fifth summands in (A.3) are equal to zero. In fact,

E S NNT = ES ENNT = 0.

So, one can rewrite (A.3) in the following way

Id = E Z Z T + E NN NNT + E NS NNT + E NN NST .

(A.4)

Decompose the vectors Z , NS and NN into the basis v1, .., vd:

m
Z = zivi;
i=1

m
NS = nivi;
i=1

d

NN =

nivi,

i=m+1

where all coefficients zi and ni are random values. Equality (A.4) can be rewritten as follows:

(A.5)

md

Id =

E [zizi ] vivi +

E [nini ] vivi

i,i =1

i,i =m+1

md

dm

+

E [nini ] vivi +

E [nini ] vivi

i=1 i =m+1

i=m+1 i =1

Then the second term in the right hand side is equal to Id-m, i.e.

E NN NNT

d
= E [nini ] vivi = Id-m.
i,i =m+1

Thus, the (d - m) - dimensional vector NN has the standard normal distribution. Denote the density function by d-m(x).

10

Step 3. Denote by F (x ) and p (x ) the distribution function and the density function of the vector X .

F (x ) = P X x = P Z + NN x

(A.6)

Note some facts:

(i) Vectors Z = S + NS and NN are independent. In fact, vectors S = -1/2S and N = -1/2N are independent. Then vectors S ,
NN and NS are jointly independent (this follows from the choice of
the basis). Finally, Z and NN are independent as functions of independent variables.

(ii) The basis choice (A.5) enables us to split the inequality

Z + NN

d
x = xivi
i=1

into two:

md

Z xivi, NN

xivi.

i=1 i=m+1

In the sequel, the following notation is used

m
xS := xivi,
i=1

d

xN :=

xivi

i=m+1

The function F can be rewritten in the following way:

F (x ) = P Z + NN

x =P Z

xS , NN xN = P Z xS P NN

xN .

Taking derivatives of the both parts of the last formula gives the representation of the density function of X .

p(x

)

=

q(xS

)d-m(xN

)

=

q (xS ) m (xS )

d(x

)

=

q(PrS {x }) m(PrS {x })

d

(x

),

11

where by q(∑) denote the density function of the random vector Z = S +
NS = PrS {X}. Step 4. The last step derives representation of the density function of
the vector X = 1/2X from the density function of X . According to the well-known formula for a density transformation,

p(x)

=

|-1/2|

p

(-1/2x)

=

|-1/2|

q m

PrS {-1/2x} (PrS {-1/2x})

d(-1/2x).

The remark d(-1/2x) = (x) concludes the proof.

Appendix A.2. Proof of Theorem 2 Integration by parts yields

E (X) =  [(x)] p(x)dx = - (x) [p(x)] dx. (A.7)

The gradient of the density function can be represented as a sum of two components:

p(x) =  [log p(x)] p(x) =  [log g(T x)] p(x) +  [log A(x)] p(x).
The summands in the right hand side can be transformed in the following way:

 [log g(T x)] p(x)

=

g(T x) p(x)

g(T x)

=  [g(T x)] A(x) = T {T x} [g(T x)] A(x)

 [log A(x)] p(x) = --1xp(x). Substitution of these expressions into (A.7) yields:

E (X) = -T

(x){T x} [g(T x)] A(x)p(x)dx + -1 (x)xp(x)dx
= T  + -1E X(X) = T   Im(T ) = (Ker T ) ,

where  = - (x){T x} [g(T x)] A(x)p(x)dx. This completes the proof.

12

Appendix A.3. Some facts about subspaces
Denote the subspace S by N. Motivation of introducing this subspace is given in [14]: it turns out, that oblique projection onto S along N is in some sense the optimal mapping onto S.

Lemma 4.

N := S.

Proof.

S = E xx S = E ss S + E sn + ns S + E nn S = E nn S,

because mathematical expectations in the first and in the second summands are equal to zero.

This result means that the space N can be described without any knowledge of .

Lemma 5. Let S1 be any subspace, A - any symmetric matrix. Then the subspace A-1/2S1 is orthogonal to the subspace A1/2S1.
Proof. It suffices to mention that two arbitrary elements of these subspaces are orthogonal.

Corollary 6. The subspace -1/2S is perpendicular to the subspace -1/2N.

Proof.

-1/2N = -1/2 S = 1/2S.

According to lemma 5, this subspace is perpendicular to the subspace -1/2S.

The last fact plays the key role in the proof of Theorem 1.
Appendix B. Examples of possible applications Appendix B.1. Classification problem
In this section we discuss how one can use NGCA algorithm for solving the classification problem.

13

Figure B.2: Non-Gaussian component analysis for the classification problem. We assume that the useful signal for both populations belongs to the same low-dimensional subspace S; rest parts have normal distribution, but with different covariance matrices.

Lemma 7. Consider two populations presented by vectors X1 and X2 from Rd. Assume that

X1 = S1 + N1, X2 = S2 + N2,

where
∑ the useful signals S1 and S2 belong to the same low-dimensional subspace S, dim(S) = m;

∑ the noise components N1, N2 are normal vectors with covariance matrices 1 and 2;
∑ S1 is independent of N1, S2 is independent of N2;
∑ covariance matrix of X1 is equal to covariance matrix of X2; denote this matrix by .

Then the density functions of X1 and X2 can be represented in the following form:

where

p1(x) = g1(Tx)(x) p2(x) = g2(Tx)(x)

14

∑ T : Rd  S , S := -1/2S, dim S = m,

Tx

=

PrS

{-

1 2

x}

∑ g1, g2 : S  R,

g1(t )

=

|-1/2| q1 PrS {t} m PrS {t}

g2(t )

=

|-1/2| q2 PrS {t} m PrS {t}

where qi(∑) are the density functions of the random variables Pr S {-1/2Xi}, i = 1, 2.

Remark 3. The fourth assumption is widely used in classical methods for solving the classification task, for example in Fisher discriminant analysis.

Proof. This lemma is a straightforward corollary of Theorem 1. One has to separately apply Theorem 1 for each population.

So, the density functions of the random vectors X1 and X2 can be represented in the following form:

p1(x)

=

|-1/2|

q1 m

Pr-1/2S {-1/2x} (Pr-1/2S {-1/2x})



(x)

p2(x)

=

|-1/2|

q2 m

Pr-1/2S {-1/2x} (Pr-1/2S {-1/2x})



(x)

(B.1) (B.2)

It is worth mentioning that these representations differ only in one place: the function q1 for the first group and the function q2 for the second.
Lemma 7 yields that one can estimate the space S using the following algorithm:

1. estimating vectors from S using the first population (by the key result) 2. estimating vectors from S using the second population (by the key
result) 3. constructing a basis of S; here one can use all vectors that were obtained
on the first and on the second steps

15

Consider a new object x and classify it into one category. Standard way for solving this problem - comparison of the density functions p1(∑) and p2(∑) at the point x. So, one has to compare

g1 (Tx) (x) vs g2 (Tx) (x).

According to (B.1) and (B.2), it is equivalent as to compare

q1

Pr-

1 2

S

{-

1 2

x}

vs

q2

Pr-

1 2

S

{-

1 2

x

}

.

(B.3)

If the EDR subspace S is already estimated, then the data can be projected

on

the

low-dimensional

subspace

-

1 2

S

.

Afterwards task (B.3) is a well-

known problem of comparison of the two densities of some low-dimensional

variables.

Appendix B.2. Portfolio Value at Risk This subsection discusses possible applications of NGCA in the estimating
of Value at Risk. At time t an investor has some endowment Wt and an additional reserve
amount Rt. An endowment can be calculated in the following way:
Wt = b T pt,
where b is a fixed allocation (a portfolio) and pt - market prices at time t (or logarithms of market prices). The reserve amount is supposed to compensate potential changes in the market price. Investor selects this reserve amount from the following condition:

P {Wt+h + Rt < 0} = ,

(B.4)

where  is some some fixed constant; h is some fixed amount of time points (usually h = 10 days).
Value at Risk is defined as the required capital at time t; actually it is the sum of the endowment and the reserve amount at time t:

VaRt = Wt + Rt.

The economical meaning of Value at Risk can be briefly explained as the maximal loss for h days. This meaning becomes clear if one rewrites (B.4) as follows:
P {Wt+h < Wt - VaRt} = .

16

On the other hand, one can rewrite the last formula as

P {Wt+h - Wt < - VaRt} = ,
and statistical meaning of Value at Risk becomes also clear: (- VaRt) is in fact the  - quantile of the distribution of Wt+h - Wt.
Let us denote the deference pt+h -pt by Xt, the correspondent  - quantile of the random variable b T Xt by q b T Xt .
The aim is to estimate q, see [7], [8]. In this article, we follow the proposal from [2] based on Independent Component Analysis (ICA). ICA is used to represent the portfolio loss as a result of several independent non - Gaussian factors. Independence allows to estimate and study each factor independently from the others. To be more specific, let us decompose this method into two steps:
∑ find statistically independent components Yt such that
Xt = AYt;

∑ simulate independent components N times (we denote this simulations by y^t(k), k = 1..N ) and estimate VaRt

VaRt

=

-1 N

N

q^

k=1

b T Ay^t(k)

.

The main idea of this approach is to reduce a sampling from a highdimensional to a low-dimensional variate. In the sequel, two methods for finding such variates using NGCA are discussed.
The first method based on NGCA Assume that the random variable Xt can be represented as a sum of two components - a low-dimensional useful component and a Gaussian noise:

Xt = St + Nt.

(B.5)

Then

Rt = b T Xt = b T St + b T Nt .

N(0,b T b)

Note that the second variable has a normal distribution; the first component is the scalar product of the fixed vector and the low-dimensional random vector. This means that one can

17

∑ generate N samples {s(ik)}di=1, k = 1..N from the distribution of St;

∑ generate N samples {ni(k)}di=1, k = 1..N from N 0, b T b ;

∑ estimate Value at Risk

VaRt

=

-1 N

N

q^

{b T s(ik) + n(ik)}

.

k=1

The second method based on NGCA Assume that a random variable X satisfies (B.5). Then the density function of X can be represented in the following way:

p(x)

=

g(Tx)(x)

=

|-1/2|

q m

Pr-1/2S {-1/2x} (Pr-1/2S {-1/2x})



(x).

If the subspace S is already estimated, than all elements in this representation are already known. This means that one can sample from this distribution and estimate Value at Risk via

1N VaRt = N q^
k=1

b T xa(k)

.

Acknowledgements
The Financial support from the Deutsche Forschungsgemeinschaft via SFB 649 "Economical Risks" is gratefully acknowledged.
Author would like to gratefully thank
∑ his science advisor professor Vladimir Spokoiny for attention to this work;
∑ professor Gilles Blanchard for fruitful discussions and a lot of useful advices;
∑ Elmar Diederichs for some implementations.

18

References
[1] Blanchard, G., Kawanabe, M., Sugiyama, M., Spokoiny, V., Mu®ller, K.R., 2006. In search of non-Gaussian components of a high-dimensional distribution. J. Mach. Learn. Res. 6, 247-282.
[2] Chen, Y., Haerdle, W., Spokoiny, V., 2007. Portfolio value at risk based on indenpendent component analysis. J. Comp. Apppl. Math. 205(1), 594-607.
[3] Dalalyan, A.S, Juditsky, A., Spokoiny, V., 2007. A new algorithm for estimating the effective dimension - reduction subspace. J. Mach. Learn. Res. 9, 1647 - 1678.
[4] Diederichs, E. Semi-parametric reduction of dimensionality, 2009. PhD thesis. Free University of Berlin.
[5] Diederichs, E., Juditsky, A., Spokoiny, V., Schu®tte, C., 2010. Sparse non-Gaussian component analysis. IEEE Trans. Inf. Theory. 15, 52495262.
[6] Duda, R.O. , Hart, P.E., Stork D.G., 2001. Patern classification. John Wiley.
[7] Gourieroux, C., Jasiak, J., 2002. Value at Risk. Manuscript, University of Toronto.
[8] Fan, J., Gu, J., 2003. Semiparametric estimation of Value-at-Risk. Econ. J. 6, 261-290.
[9] Hastie, T.J., Tibshirani, R., Friedman, J., 2001. The elements of statistical learning. Springer Series in Statistics. Springer.
[10] Hyv®arinen, A.,, Oja, E., 1999. Independent Component Analysis: algorithms and applications. Neural Networks, 13: 411 - 420.
[11] Hyv®arinen, A., Karhunen, J., Oja, E., 2001. Independent Component Analysis. Wiley, New York.
[12] Jackson, J.E., 1991. A user's guide to principal componenfs. Wiley, New York.
19

[13] Kawanabe, M., Sugiyama, M., Blanchard, G., Mu®ller, K.-R., 2007. A new algorithm of non-Gaussian component analysis with radial kernel functions. Ann. Inst. Stat. Math. 59, 57-75.
[14] Sugiyama, M., Kawanabe, M., Blanchard, G., Mu®ller, K.-R., 2008. Approximating the best linear unbiased estimator of non-Gaussian signals with Gaussian noise. IEICE Trans. Inform. Syst., E91-D (5), 1577-1580.
20

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Volatility Investing with Variance Swaps" by Wolfgang Karl H‰rdle and Elena Silyakova, January 2010.
002 "Partial Linear Quantile Regression and Bootstrap Confidence Bands" by Wolfgang Karl H‰rdle, Ya'acov Ritov and Song Song, January 2010.
003 "Uniform confidence bands for pricing kernels" by Wolfgang Karl H‰rdle, Yarema Okhrin and Weining Wang, January 2010.
004 "Bayesian Inference in a Stochastic Volatility Nelson-Siegel Model" by Nikolaus Hautsch and Fuyu Yang, January 2010.
005 "The Impact of Macroeconomic News on Quote Adjustments, Noise, and Informational Volatility" by Nikolaus Hautsch, Dieter Hess and David Veredas, January 2010.
006 "Bayesian Estimation and Model Selection in the Generalised Stochastic Unit Root Model" by Fuyu Yang and Roberto Leon-Gonzalez, January 2010.
007 "Two-sided Certification: The market for Rating Agencies" by Erik R. Fasten and Dirk Hofmann, January 2010.
008 "Characterising Equilibrium Selection in Global Games with Strategic Complementarities" by Christian Basteck, Tijmen R. Daniels and Frank Heinemann, January 2010.
009 "Predicting extreme VaR: Nonparametric quantile regression with refinements from extreme value theory" by Julia Schaumburg, February 2010.
010 "On Securitization, Market Completion and Equilibrium Risk Transfer" by Ulrich Horst, Traian A. Pirvu and GonÁalo Dos Reis, February 2010.
011 "Illiquidity and Derivative Valuation" by Ulrich Horst and Felix Naujokat, February 2010.
012 "Dynamic Systems of Social Interactions" by Ulrich Horst, February 2010.
013 "The dynamics of hourly electricity prices" by Wolfgang Karl H‰rdle and Stefan Tr¸ck, February 2010.
014 "Crisis? What Crisis? Currency vs. Banking in the Financial Crisis of 1931" by Albrecht Ritschl and Samad Sarferaz, February 2010.
015 "Estimation of the characteristics of a LÈvy process observed at arbitrary frequency" by Johanna Kappusl and Markus Reiﬂ, February 2010.
016 "Honey, I'll Be Working Late Tonight. The Effect of Individual Work Routines on Leisure Time Synchronization of Couples" by Juliane Scheffel, February 2010.
017 "The Impact of ICT Investments on the Relative Demand for HighMedium-, and Low-Skilled Workers: Industry versus Country Analysis" by Dorothee Schneider, February 2010.
018 "Time varying Hierarchical Archimedean Copulae" by Wolfgang Karl H‰rdle, Ostap Okhrin and Yarema Okhrin, February 2010.
019 "Monetary Transmission Right from the Start: The (Dis)Connection Between the Money Market and the ECB's Main Refinancing Rates" by Puriya Abbassi and Dieter Nautz, March 2010.
020 "Aggregate Hazard Function in Price-Setting: A Bayesian Analysis Using Macro Data" by Fang Yao, March 2010.
021 "Nonparametric Estimation of Risk-Neutral Densities" by Maria Grith, Wolfgang Karl H‰rdle and Melanie Schienle, March 2010.

SFB 649 Discussion Paper Series 2010
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
022 "Fitting high-dimensional Copulae to Data" by Ostap Okhrin, April 2010. 023 "The (In)stability of Money Demand in the Euro Area: Lessons from a
Cross-Country Analysis" by Dieter Nautz and Ulrike Rondorf, April 2010. 024 "The optimal industry structure in a vertically related market" by
Raffaele Fiocco, April 2010. 025 "Herding of Institutional Traders" by Stephanie Kremer, April 2010. 026 "Non-Gaussian Component Analysis: New Ideas, New Proofs, New
Applications" by Vladimir Panov, May 2010.

