BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2011-081
Parametric estimation. Finite sample theory
Vladimir Spokoiny*
* Weierstrass Institute (WIAS) Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universit‰t zu Berlin Spandauer Straﬂe 1, D-10178 Berlin

Parametric estimation. Finite sample theory
Vladimir Spokoiny  Weierstrass-Institute, Mohrenstr. 39, 10117 Berlin, Germany spokoiny@wias-berlin.de
Abstract
The paper aims at reconsidering the famous Le Cam LAN theory. The main features of the approach which make it different from the classical one are: (1) the study is non-asymptotic, that is, the sample size is fixed and does not tend to infinity; (2) the parametric assumption is possibly misspecified and the underlying data distribution can lie beyond the given parametric family. The main results include a large deviation bounds for the (quasi) maximum likelihood and the local quadratic majorization of the log-likelihood process. The latter yields a number of important corollaries for statistical inference: concentration, confidence and risk bounds, expansion of the maximum likelihood estimate, etc. All these corollaries are stated in a non-classical way admitting a model misspecification and finite samples. However, the classical asymptotic results including the efficiency bounds can be easily derived as corollaries of the obtained non-asymptotic statements. The general results are illustrated for the i.i.d. set-up as well as for generalized linear and median estimation. The results apply for any dimension of the parameter space and provide a quantitative lower bound on the sample size yielding the root-n accuracy.
AMS 2000 Subject Classification: Primary 62F10. Secondary 62J12,62F25,62H12
Keywords: maximum likelihood, local quadratic approximation, concentration, coverage, deficiency
JEL-Classification: C13, C14
Financial support by the German Research Foundation (DFG) through the Collaborative Research Center 649 "Economic Risk" is gratefully acknowledged
1

2 Parametric estimation. Finite sample theory
1 Introduction
One of the most popular approaches in statistics is based on the parametric assumption (PA) that the distribution IP of the observed data Y belongs to a given parametric family (IP,     IRp) , where p states for the number of parameters. This assumption allows to reduce the problem of statistical inference about IP to recovering the parameter  . The theory of parameter estimation and inference is nicely developed in a quite general set-up. There is a vast literature on this issue. We only mention the book by Ibragimov and Khas'minskij (1981), which provides a comprehensive study of asymptotic properties of maximum likelihood and Bayesian estimators. The theory is essentially based on two major assumptions: (1) the underlying data distribution follows the PA; (2) the sample size or the amount of available information is large relative to the number of parameters.
In many practical applications, both assumptions can be very restrictive and limiting the scope of applicability for the whole approach. Indeed, the PA is usually only an approximation of real data distribution and in the most of statistical problems it is too restrictive to assume that the PA is exactly fulfilled. Many modern statistical problems deal with very complex high dimensional data where a huge number of parameters are involved. In such situations, the applicability of large sample asymptotics is questionable. These two issues partially explain why the parametric and nonparametric theory are almost isolated from each other. Relaxing these restrictive assumptions can be viewed as an important challenge of the modern statistical theory. The present paper attempts at developing a unified approach which does not require the restrictive parametric assumptions but still enjoys the main benefits of the parametric theory. The main feature of the presentation is the non-asymptotic framework. The notions like asymptotic normality, convergence or tightness are meaningless in the non-asymptotic setup, the arguments based on compactness of the parameter space are not really helpful. Instead some exact exponential bounds and concentration results are systematically used. The main steps of the approach are similar to the classical local asymptotic normality (LAN) theory; see e.g. Chapters 1≠3 in the monograph Ibragimov and Khas'minskij (1981): first we establish a kind of large deviation bound allowing to localize the problem into a neighborhood of the target parameter. Then we use a local quadratic expansion of the log-likelihood to solve the corresponding estimation problem.
Let Y stand for the available data. Everywhere below we assume that the observed data Y follow the distribution IP on a metric space Y . We do not specify any particular structure of Y . In particular, no assumption like independence or weak dependence of individual observations is imposed. The basic parametric assumption is that IP can be

spokoiny, v.

3

approximated by a parametric distribution IP from a given parametric family (IP,     IRp) . Our approach allows that the PA can be misspecified, that is, in general,

IP  IP .

Let L(Y , ) be the log-likelihood for the considered parametric model: L(Y , ) =

log

dIP dµ0

(Y

)

,

where

µ0

is any dominating measure for the family

(IP) .

The classical like-

lihood principle suggests to estimate  by maximizing the corresponding log-likelihood

function L(Y , ) :

 d=ef argmax L(Y , ).


(1.1)

Our ultimate goal is to study the properties of the quasi MLE  . It turns out that such properties can be naturally described in terms of the maximum of the process L() rather then the point of maximum  . To avoid technical burdens it is assumed that the maximum is attained leading to the identity max L() = L() . However, the point of maximum needs not to be unique. If there are many such points we take  as any of them. Basically, the notation  is used for the identity L() = sup L() .
If IP  IP , then the (quasi) MLE estimate  from (1.1) is still meaningful and it appears to be an estimate of the value  defined by maximizing the expected value of L(Y , ) :

 d=ef argmax IEL(Y , )


(1.2)

which is the true value in the parametric situation and can be viewed as the parameter of the best parametric fit in the general case.
We focus on the properties of the process L(Y , ) as a function of the parameter  . Therefore, we suppress the argument Y there and write L() instead of L(Y , ) . One has to keep in mind that L() is random and depends on the observed data Y . We also define the excess or maximum log-likelihood L(, ) = L() - L() . The results below show that the main properties of the quasi MLE  like concentration or coverage probability can be described in terms of the quasi maximum likelihood L() - L() = max L() - L() , which is the difference between the maximum of the process L() and its value at the "true" point  .
The established results can be split into two big groups. A large deviation bound states some concentration properties of the estimate  . For specific local sets 0(r) with elliptic shape, the deviation probability IP   0(r) is exponentially small in r . This concentration bound allows for restricting the parameter space to a properly selected vicinity 0(r) . Our main results describe local properties of the process L() within 0(r) . They can be viewed as a non-asymptotic version of the Le Cam LAN

4 Parametric estimation. Finite sample theory
theory. The paper is organized as follows. Section 2 presents the list of conditions which are
systematically used in the text. The conditions only concern the properties of the quasi likelihood process L() .
Section 3 appears to be central in the whole approach and it focuses on local properties of the process L() within 0(r) . The idea is to sandwich the underlying (quasi) loglikelihood process L() for   0(r) between two quadratic (in parameter) expressions. Then the maximum of L() over 0(r) will be sandwiched as well by the maxima of the lower and upper processes. The quadratic structure of these processes help to compute these maxima explicitly yielding the bounds for the value of the original problem. This approximation result is used to derive a number of corollaries including the concentration and coverage probability, expansion of the estimate  , polynomial risk bounds, etc. In the contrary to the classical theory, all the results are non-asymptotic and do not involve any small values of the form o(1) , all the terms are specified explicitly. Also the results are stated under possible model misspecification.
Section 4 accomplishes the local results with the concentration property which bounds the probability that  deviates from the local set 0(r) . In the modern statistical literature there is a number of studies considering maximum likelihood or more generally minimum contrast estimators in a general i.i.d. situation, when the parameter set  is a subset of some functional space. We mention the papers Van de Geer (1993), Birg¥e and Massart (1993), Birg¥e and Massart (1998), Birg¥e (2006) and references therein. The established results are based on deep probabilistic facts from the empirical process theory; see e.g. Talagrand (1996, 2001, 2005), van der Vaart and Wellner (1996), Boucheron et al. (2003). The general result presented in Section 7 follows the generic chaining idea due to Talagrand (2005); cf. Bednorz (2006). However, we do not assume any specific structure of the model. In particular, we do not assume independent observations and thus, cannot apply the most developed concentration bounds from the empirical process theory.
Section 5 illustrates the applicability of the general results to the classical case of an i.i.d. sample. The previously established general results apply under rather mild conditions. Basically we assume some smoothness of the log-likelihood process and some minimal number of observations pro parameter: the sample size should be at least of order of the dimensionality p of the parameter space. We also consider the examples of generalized linear modeling and of median regression.
It is important to mention that the non-asymptotic character of our study yields an almost complete change of the mathematical tools: the notions of convergence and tightness become meaningless, the arguments based on compactness of the parameter space do not apply, etc. Instead we utilize the tools of the empirical process theory based

spokoiny, v.

5

on the ideas of concentration of measures and nonasymptotic entropy bounds. Section 6 presents an exponential bound for a general quadratic form which is very essential for getting the sharp risk bounds for the quasi MLE. This bound is an important step in the concentration results for the quasi MLE. Section 7 explains how generic chaining and majorizing measure device by Talagrand (2005) refined in Bednorz (2006) can be used for obtaining a general exponential bound for the log-likelihood process.

2 Conditions
Below we collect the list of conditions which are systematically used in the text. It seems to be an advantage of the whole approach that all the results are stated in a unified way under the same conditions which are quite general and not very much restrictive. We do not try to formulate the conditions and the results in the most general form. In some cases we sacrifice generality in favor of readability and ease of presentation. In some cases we indicate possible extensions of the results under more general conditions. It is important to stress that all the conditions only concern the properties of the quasi likelihood process L() .
The imposed conditions on the process can be classified into the following groups by their meaning:
∑ smoothness conditions on L() allowing the second order Taylor expansion;
∑ exponential moment conditions;
∑ identifiability and regularity conditions; We also distinguish between local and global conditions. The global conditions concern the global behavior of the process L() while the local conditions focus on its behavior in the vicinity of the central point  .
2.1 Global conditions
The first global condition (E) assumes some exponential moments for the quasi loglikelihood L() for each    . The second condition (ED) assumes some smoothness of L() and requires exponential moments of its gradient. The important identifiability condition is stated later; see Section 4.2.
The formulation involves a subset M of IR+ describing all possible exponents in the moment conditions.
(E) For each    , there exists a positive value µ  M such that
IE exp µL(, ) < .

6 Parametric estimation. Finite sample theory

Note that this condition is automatically fulfilled if IP = IP and all the IP 's are absolutely continuous w.r.t. IP with µ  1 . Indeed, L(, ) = log dIP/dIP
and log IE(dIP/dIP) = 0 . For µ < 1 , it holds by the Jensen inequality that - log IE(dIP/dIP)µ  0 .
Condition (E) enables us to define the function

M(µ, , ) d=ef - log IE exp µL(, ) < .

(2.1)

This definition can be extended to all µ  M by letting M(µ, , ) = - when the exponential moment of µL(, ) does not exists. The main observation behind
condition (E) is that

IE exp µL(, ) + M(µ, , ) = 1

provided that M(µ, , ) is finite. Note that M(0, , ) = 0 . The concentration results established in Section 4 require some identification properties. A pointwise identifiability means that one can separate the target measure IP and the measure IP corresponding to another point  in the parameter space. This condition can be expressed in terms of the function M(µ, , ) from (2.1). Namely, this value has to be significantly positive for some µ  M :
M(, ) d=ef sup M(µ, , ) > 0.
µM
This condition, however, only ensures a pointwise separation between  and  . A strict identification requires a quantitative lower bound on the value M(, ) : it has to grow at least logarithmically with the norm  -  . A precise formulation involves some more notation and it is given in Corollary 4.3 below; see condition (4.9).
To bound local fluctuations of the process L() , we introduce an exponential moment condition on the stochastic component () :
() d=ef L() - IEL().

Suppose that the random function () is differentiable in  and its gradient () = ()/  IRp fulfills the following condition:

(ED) There exist some constant 0 , a positive symmetric matrix V 2 , and constant g > 0 such that for all ||  g

 ()

sup sup log IE exp 

IRp 

V

 022/2.

spokoiny, v.

7

This condition effectively means that the gradient () normalized by the matrix V has bounded exponential moments. It can be relaxed by allowing the matrix V 2 and/or the value g to be dependent of  in a uniformly continuous way.

2.2 Local conditions
Local conditions describe the properties of L() in a vicinity of the central point  from (1.2). First we refine condition (ED) . It is fulfilled for all  including  =  . However, the matrix V can be improved if the only point  is concerned.

(ED0) There exist a positive symmetric matrix V02 , and constants g > 0 , 0  1 such that Var ()  V02 and for all ||  g

 ()

sup log IE exp 

 IRp

V0

 022/2.

In typical situation, the matrix V02 can be defined as the covariance matrix of the gradient vector () : V02 = Var () = Var L() . If L() is the loglikelihood for a correctly specified model, then  is the true parameter value and V02 coincides with the corresponding Fisher information matrix.
The matrix V0 shown in this condition determines the local geometry in the vicinity of  . In particular, define the local elliptic neighborhoods of  as

0(r) d=ef {   : V0( - )  r}.

(2.2)

The further conditions are restricted to such defined neighborhoods 0(r) . In fact, they quantify local smoothness properties of the log-likelihood function L() .

(ED1) For some R and each r  R , there exist a constant (r)  1/2 such that it holds for all   0(r)

 {() - ()}

sup log IE exp 

 Sp

(r) V0

 022/2,

||  g.

The main majorization result also requires second order smoothness of the expected log-likelihood IEL() . By definition, L(, )  0 and IEL() = 0 because  is the extreme point of IEL() . Therefore, -IEL(, ) can be approximated by a quadratic function of - in the neighborhood of  . The local identifiability condition
qualifies this quadratic approximation from above and from below on the set 0(r) from (2.2).

8 Parametric estimation. Finite sample theory

(L0) There are a positive matrix D0 and for each r  R and a constant (r)  1/2 , such that it holds on the set 0(r) = { : V0( - )  r}

-2IEL(, ) D0( - ) 2

-

1

 (r).

Note that if L(, ) is the log-likelihood ratio and IP = IP then -IEL(, ) = IE log dIP/dIP = K(IP, IP) , the Kullback-Leibler divergence between IP and IP . Then condition (L0) with D0 = V0 follows from the usual regularity conditions on the family (IP) ; cf. Ibragimov and Khas'minskij (1981).
If the log-likelihood process L() is sufficiently smooth in  , e.g. three times stochastically differentiable, then the quantities (r) and (r) are proportional to the radius
(r) of the set 0(r) defined as

(r) d=ef max  -  .
0(r)

In the important special case of an i.i.d. model one can take (r) = r/n1/2 and (r) = r/n1/2 for some constants ,  ; see Section 5.1.

3 Local non-asymptotic quadraticity
The Local Asymptotic Normality (LAN) condition since introduced by L.Le Cam in Le Cam (1960) became one of the central notions in the statistical theory. It postulates a kind of local approximation of the log-likelihood of the original model by the loglikelihood of a Gaussian shift experiment. The LAN property being once checked yields a number of important corollaries for statistical inference. In words, if you can solve a statistical problem for the Gaussian shift model, the result can be translated under the LAN condition to the original setup. We refer to Ibragimov and Khas'minskij (1981) for a nice presentation of the LAN theory including asymptotic efficiency of MLE and Bayes estimators. The LAN properties was extended to mixed LAN or Local Asymptotic Quadraticity (LAQ); see e.g. Le Cam and Yang (2000). All these notions are very much asymptotic and very much local. The LAN theory also requires that L() is the correctly specified log-likelihood. The strict localization does not allow for considering a growing or infinite parameter dimension and limits applications of the LAN theory to nonparametric estimation.
Our approach tries to avoid asymptotic constructions and attempts to include a possible model misspecification and a large dimension of the parameter space. The presentation below shows that such an extension of the LAN theory can be made essentially by no price: all the major asymptotic results like Fisher and Cram¥er-Rao information

spokoiny, v.

9

bounds, as well as the Wilks phenomenon can be derived as corollaries of the obtained non-asymptotic statements simply by letting the sample size to infinity. At the same time, it applies to a high dimensional parameter space.
The LAN property states that the considered process L() can be approximated by a quadratic in  expression in a vicinity of the central point  . This property is usually checked using the second order Taylor expansion. The main problem arising here is that the error of the approximation grows too fast with the local size of the neighborhood. Section 3.1 presents the non-asymptotic version of the LAN property in which the local quadratic approximation of L() is replaced by a local quadratic majorization of this process from above and from below by two different quadratic in  processes. More precisely, we apply the sandwiching idea: the difference L(, ) = L() - L() is put between two quadratic processes L (, ) and L (, ) :

L (, ) -   L(, )  L (, ) +  ,   0(r)

(3.1)

where is a numerical parameter, = - , and  and  are stochastic errors which only depends on the selected vicinity 0(r) . The upper process L (, ) and the lower process L (, ) can deviate substantially from each other, however, the errors  ,  remain small even if the value r describing the size of the local neighborhood 0(r) is large.
The sandwiching result (3.1) naturally leads to two important notions: value of the
problem and deficiency. It turns out that the most of statements like confidence and concentration probability rely upon the maximum of L(, ) over  which we call the
value of the problem. Due to (3.1) this value can be bounded from above and from below using the similar quantities max L (, ) and max L (, ) which can be called the values of the lower and upper problems. Note that max{L (, ) - L (, )} =  . However, this is not crucial. What really matters is the difference between the upper
and the lower values. The deficiency  can be defined as the width of the interval
bounding the value of the problem due to (3.1), that is, as the sum of the approximation
errors and of this difference:

 d=ef  +  + max L (, ) - max L (, ) .

The range of applicability of this approach can be described by the following mnemonic rule: "The value of the upper problem is larger in order than the deficiency." The further sections explain in details the meaning and content of this rule. Section 3.1 presents the key bound (3.1) and derives it from the general results on empirical processes. Section 3.2 presents some straightforward corollaries of the bound (3.1) including the coverage and concentration probabilities, expansion of the MLE and the risk bounds. It also indicates

10 Parametric estimation. Finite sample theory

how the classical results on asymptotic efficiency of the MLE follow from the obtained non-asymptotic bounds.

3.1 Local quadratic majorization
This section presents the key result about local quadratic approximation of the quasi log-likelihood process given by Theorem 3.1 below.
Let the radius r of the local neighborhood 0(r) be fixed in a way that the deviation probability IP   0(r) is sufficiently small. Precise results about the choice of r which ensures this property are postponed until Section 4.2. In this neighborhood 0(r) we aim to build a quadratic majorization of the process L() . The first step is the usual decomposition of this process into deterministic and stochastic components:

L() = IEL() + (),

where () = L() - IEL() . Condition (L0) allows for approximating the smooth deterministic function IEL()-IEL() around the point of maximum  by the quadratic form - D0( - ) 2/2 . The smoothness properties of the stochastic component () given by conditions (ED0) and (ED1) leads to linear approximation () - ()  ( - ) () . Putting these two approximations together yields the following ap-
proximation of the process L() on 0(r) :
L(, )  L(, ) d=ef ( - ) () - D0( - ) 2/2.

This expansion is used in the most of asymptotic statistical calculus. However, it does not suit our purposes because the error of approximation grows quadratically with the radius r and starts to dominate at some critical value of r . We slightly modify the construction by introducing two different approximating processes. They only differ in the deterministic quadratic terms which is either shrunk or stretched relative to the term
D0( - ) 2/2 in L(, ) . Introduce for a vector = (, ) the following notation:

L (, ) d=ef ( - ) L() - D ( - ) 2/2 =  D ( - ) - D ( - ) 2/2,

(3.2)

where

D2 = D02(1 - ) - V02,

 d=ef D-1L().

Here we implicitly assume that with the proposed choice of the constants  and , the matrix D2 is non-negative: D2  0 . The representation (3.2) indicates that the process

spokoiny, v.

11

L (, ) has the geometric structure of log-likelihood of a linear Gaussian model. We do not require that the vector  is Gaussian and hence, it is not the Gaussian log-likelihood. However, the geometric structure of this process appears to be more important than its distributional properties.
One can see that if , are positive, the quadratic drift component of the process L (, ) is shrunk relative to L(, ) and stretched if , are negative. Now, given r , define  = (r) , = (r)/(30) with the value (r) from condition (L0) and (r) from condition (ED1) . Finally set = - , so that

D2 = D02(1 + ) + V02.

Theorem 3.1. Assume (ED1) and (L0) . Let for some r , the values  30 (r) and   (r) be such that D02(1 - ) - V02  0 . Set = (, ) , = - = (-, - ) . Then

L (, ) -  (r)  L(, )  L (, ) +  (r),   0(r),

(3.3)

with L (, ), L (, ) defined by (3.2). Moreover, the random variable  (r) fulfills

IP -1 (r)  z0(x, p)  exp -x

(3.4)

with z0(x, p) given for g0 = g0  3 by

z0(x, p)

d=ef

 

 1 + x + c1p

2

1 + (1 + 2g-0 1)2 g-0 1(x +

c1p)

+ g0/2

2

 if 1 + x + c1p  g0, otherwise.

where c1 = 2 for p  2 and c1 = 2.4 for p = 1 . Similarly for  (r) .

Proof. Consider for fixed r and = (, ) a quantity

 (r) d=ef sup
0(r)

L(, ) - IEL(, ) - ( - )

L() - 2

V0( - )

2

.

In view of IEL() = 0 , this definition can be rewritten in the form

 (r) d=ef sup
0(r)

(, ) - ( - )

() - 2

V0( - )

2

.

Similarly define

 (r) d=ef sup
0(r)

L(, ) - IEL(, ) - ( - )

L() - 2

V0( - )

2

=

sup
0(r)

(, ) - ( - )

() - 2

V0( - )

2

.

12 Parametric estimation. Finite sample theory

Now the claim of the theorem can be easily reduced to an exponential bound for the quantities  (r),  (r) . We apply Theorem 7.9 to the process

U(, ) = 1 (, ) - ( - ) () , (r)

  0(r),

and H0 = V0 . Condition (ED) follows from (ED1) with the same 0 and g in view of U(, ) = () - () /(r) . So, the conditions of Theorem 7.9 are fulfilled
yielding (3.4) in view of  30 (r) .

Remark 3.1. If x is not too big, then the value z0(x, p) is close to x + c1p ; cf. (7.7). The bound (3.4) tells us that the errors  (r) and  (r) are of order (r) p .

3.2 Local inference. Deficiency
This section presents a list of corollaries from the basic approximation bounds of Theorem 3.1. The idea is to replace the original problem by a similar one for the approximating upper and lower models. It is important to stress once again that all the corollaries only rely on the majorization (3.1) and the geometric structure of the processes L and L .
The random quantity sup L (, ) can be called the value of the upper problem, while sup L (, ) is the value of the lower problem. The quadratic (in  ) structure of the functions L (, ) and L (, ) enables us to explicitly solve the problem of maximizing the corresponding function w.r.t.  . The value of the original problem which is the maximum of the original process L(, ) over  is sandwiched between the similar expressions for the two approximating processes in view of the approximation bound (3.3). This suggests to measure the quality of approximation by the difference between values of the upper and lower approximating problems. The approximating quality is sufficiently good if this difference is smaller in order than the value itself.

3.2.1 Upper and lower values

First consider the maximization problem for the upper approximating processes L (, ) . This is a quadratic optimization with the closed form solution  2/2 . Moreover, the Euclidean norm of the random vector  behaves nearly as a chi-squared random variable. The results below make these statements more concise. Define for

IB

d=ef

D-1V02D-1 max D-1V02D-1

,

p d=ef tr IB ,

v2 d=ef 2 tr(IB2).

(3.5)

spokoiny, v.

13

Moreover, with the constant g from (ED0) , define also µc = g2/(p + g2) , and
y2c d=ef g2/µc2 - p /µc, gc d=ef µcyc = g2 - µcp , 2xc d=ef gcyc + log det Ip - µcIB2 .

To gain some feeling of these quantities consider a special case with g2 = p . Then µc = 1/2 , and the inequality x + log(1 - x)  x/3 for 0  x  1/2 implies

yc = 4g2 - 2p = 2p , gc2 = (yc/2)2 = p /2, 2xc = p /2 ∑ 2p + log det Ip - IB /2  p /3.

Theorem 3.2. Assume (ED0) with 0 = 1 and g2  2p . It holds

sup L (, ) =  2/2.
IRp
Moreover, IE  2  p , and for each x > 0

(3.6)

IP  2  z(x, IB )  2e-x + 8.4e-xc,

(3.7)

where z(x, IB ) is defined by



p + 2xv ,





z(x, IB

)

d=ef

 p

+ 6x



  

yc

+

2(x

-

xc)/gc

2,

x  v /18, v /18 < x  xc, x > xc.

Proof. The unconstrained maximum of the quadratic form L (, ) w.r.t.  is attained at  yielding the expression (3.6). The second moment of  can be bounded from condition (ED0) . Indeed,

IE  2 = IE tr   = tr D-1 IEL(){L()} D-1 = tr D-2 Var L()

and (ED0) implies  Var L()    V02 and thus, IE  2  p . The deviation bound (3.7) is proved in Corollary 6.12.
Next result describes the lower value sup L (, ) and the difference between upper and lower values. The proof is straightforward.

14 Parametric estimation. Finite sample theory

Theorem 3.3. On the random set {   r} , it holds

Moreover,   

sup L (, ) =  2/2.
0(r)
and

 2 -  2 =  Ip - D D-2D     2,

with

 d=ef Ip - D D-2D  = max Ip - D D-2D .

(3.8)

If the value  is small then the difference  2 -  2 is automatically smaller than the upper value  2 .

3.2.2 Deficiency

In view of the results of Theorems 3.2 and 3.3, the sandwiching approach (3.3) bounds the value L(, ) = sup L(, ) in the interval  2 - (r),  2 + (r) . The width of this interval describes the accuracy of the approach. By analogy to the general Le Cam theory of statistical experiments, this value will be called the deficiency. Define the value  (r) by

 (r) d=ef  (r) +  (r) +  2 -  2 /2.

(3.9)

This quantity is random but it can be easily evaluated under the considered conditions.
Indeed, the approximation errors  (r),  (r) can be bounded by z0(x, p) with the probability at least 1 - 2e-x ; see (3.4). Also  2  z(x, IB ) with a probability of order 1 - 2e-x ; see (3.7). This yields for the deficiency  (r) with a probability about 1 - 4e-x

 (r)  2 z0(x, p) +  z(x, IB ).

3.2.3 The regular case The bound (3.9) can be further specified in the so called regular case under the condition

V0  aD0 .

(3.10)

If the parametric assumption is correct, that is, IP = IP for a regular parametric family, then the both matrices coincide with the total Fisher information matrix, and the regularity condition is fulfilled automatically with a = 1 . Otherwise, the regularity

spokoiny, v.

15

means that the local variability of the process L() measured by the matrix V0 is not significantly larger than the local information measured by the matrix D0 .

Theorem 3.4. Suppose (3.10). Then

D2  (1 -  - a2)D02,

=

Ip - D D-2D





2( + a2) 1 -  - a2

.

Moreover, IB2 = D-1V02D-1 satisfies with a d=ef a(1 -  - a2) :

IB2  a-2Ip,

p  a-2p,

v2  2a-4p,

  a-2.

(3.11)

Proof. The results follow directly from the definition of D and D and (3.5) by making use of (3.10).

In particular, the matrices D and D are close to each other if  and a2 are small. So, all we need in the regular case, is a large deviation bound for the probability IP   0(r) and that the quantities (r) and (r) are small.

3.2.4 Local coverage probability

Now we state some immediate corollaries of the exponential bound from Theorem 3.1. First we study the probability of covering  by the random set E(z) = { : 2L(, )  z} .

Theorem 3.5. Suppose (ED0) , (ED1) , and (L0) on 0(r) , and let  30 (r) ,   (r) , and D02(1 - ) - V02  0 . Then for any z > 0 , it holds

IP E(z) ,   0(r) = IP 2L(, ) > z,   0(r)  IP  2  z -  (r) .

(3.12)

Proof. The bound (3.12) follows from the upper bound of Theorem 3.1 and the statement (3.6) of Lemma 3.2.

The exponential bound (3.7) helps to answer a very important question about a proper choice of the critical value z providing the prescribed covering probability. Namely, this probability starts to decrease gradually when z grows over z(x, IB ) .

3.2.5 Local concentration
Now we describe local concentration properties of  assuming that  is restricted to 0(r) . More precisely, we bound the probability that  does not belong to the set

16 Parametric estimation. Finite sample theory

A (z) of the form

A (z) =  : D ( - )  z .

It is obvious that

  A (z) = D ( - ) > z .

Theorem 3.6. Assume (ED0) , (ED1) , and (L0) on 0(r) , and let  30 (r) ,   (r) , and D02(1 - ) - V02  0 . Then for any z > 0 , it holds with  from (3.8)

IP D ( - ) > z,   0(r)  IP  > z - 2 (r)

 IP

1

-

 



>z-

2 (r) + 2 (r) .

(3.13) (3.14)

Proof. It obviously holds on the set {  0(r)}

{  A (z)} = sup L(, ) = sup L(, )

A (z)



 sup L (, ) +  (r)  sup L (, ) -  (r) .

A (z)



As L (, ) is a quadratic function of D ( - ) ; cf. (3.2), its maximum on the complement Ac(z) of the set A (z) is attained at the point  satisfying D ( - ) =  with  = z/  . This implies for all   A (z)

L (, )    2 - 2  2/2 = z  - z2/2.

By Lemma 3.2 sup L (, ) =  2/2 . Therefore, {  A (z)}  z  - z2/2   2/2 -  (r) -  (r) = z2/2 - z  +  2/2   (r)

and (3.13) follows. Further, the bound  2 -  2    2 implies 
2 (r)  2 (r) + 2 (r) +  

which yields (3.14).
An interesting and important question is for which z the probability of the event { D ( - ) > z} becomes small. We use that max  (r),  (r)  z0(x, p) and on a set of probability at least 1 - 2e-x . This and the bound (3.14) imply
 2 (r)  2 z0(x, p) +   .

spokoiny, v.

17

Now it follows from the local concentration result of Theorem 3.6:

IP D ( - ) > z,   0(r) 
 IP 1 -   > z - 2

z0(x, p) + 2e-x.

The probability IP  > z starts to vanish when z2 significantly exceeds p = IE  2 . Under the regularity conditions, the value p is of order p . Moreover, z0(x, p) is also of order p for moderate x . If and  are small then the latter deviation probability can be bounded by IP  > z with z/z  1 which can be evaluated by (3.7).

3.2.6 Local expansions

Now we show how the bound (3.3) can be used for obtaining a local expansion of the quasi MLE  . The basic idea is to plug  in place of  in the definition of  (r) .

Theorem 3.7. Assume (ED0) , (ED1) , and (L0) on 0(r) and let  30 (r) ,   (r) , and D02(1 - ) - V02  0 . Then the following approximation holds on the random set C (r) = {  0(r),   r} :

 2/2 -  (r)  L(, )   2/2 +  (r).

(3.15)

Moreover, it holds on the same random set C (r) D  -  -  2  2 (r).

(3.16)

Proof. The bound (3.3) together with Lemma 3.2 yield on C (r)

Similarly

L(, ) = sup L(, )
0(r)
 sup L (, ) -  (r) = 
0(r)

2/2 -  (r).

(3.17)

L(, )  sup L (, ) +  (r)   2/2 +  (r)
0(r)

(3.18)

yielding (3.15). For getting (3.16), we again apply the inequality L(, )  L (, ) +  (r) from Theorem 3.1 for  equal to  . With  = D-1L() and u d=ef D ( -
) , this gives

L(, ) -  u + u 2/2   (r).

18 Parametric estimation. Finite sample theory

Therefore, by (3.17)  2/2 -  (r) -  u + u 2/2   (r)
or, equivalently  2/2 -  u + u 2/2   (r) +  (r) +  2 -  2 /2
and the definition of  (r) implies u -  2  2 (r) .

3.2.7 A local risk bound

Below we also bound the moments of the excess L(, ) when  is restricted to the local vicinity 0(r) of  .

Theorem 3.8. Assume (ED0) , (ED1) , and (L0) on 0(r) and let   (r) , and D02(1 - ) - V02  0 . Then for u > 0

 30 (r) ,

IELu(, ) 1I(  0(r))  IE  2/2 +  (r) u.

(3.19)

Moreover, for C (r) = {  0(r),   r} , it holds IE D ( - ) u 1I(C (r))  IE  + 2 (r) u.

(3.20)

Proof. The bound (3.19) follows from (3.18). Next, the expansion (3.16) yields on C (r) D ( - )   + 2 (r)

and (3.20) follows.

3.2.8 Range of applicability
The whole proposed approach relies implicitly on the two groups of assumptions: global and local. These assumptions are linked to each other by the value r . From one side, the global assumptions listed in Theorem 4.1 and its corollaries should ensure a sensitive bound for the deviation probability IP   0(r) . This particularly requires that r is sufficiently large. In the contrary, the local conditions are based on the assumption that the local set 0(r) is sufficiently small to guarantee that errors of approximation  (r) and  (r) and the deficiency  (r) from (3.9) are small as well in a probabilistic sense.
More precisely, the obtained results are sharp and meaningful if the deficiency  (r) is smaller in order than the value of the upper problem  2 . The general results of Section 6 state that  2 concentrates around its expected value p d=ef IE  2 . Therefore, the latter condition about the deficiency can be decomposed into two others:

spokoiny, v.

19

∑ the values  (r),  (r) are smaller in order than p ;
∑ the difference  2 -  2 is smaller in order than p ;
Due to the result of Theorem 3.1, the random quantity  (r) can be bounded with a probability larger than 1 - e-x by z0(x, p) , where z0(x, p)  p + x if x is not too large. So, the first conditions requires that p is smaller in order than p . If p is of order p (see the regular case in the next section) then the condition "  (r) is small" only requires that , or equivalently, (r) is small. The same holds for  (r) .
Summarizing the above discussion yields that the local results apply if, for a fixed r :
∑ (r)p/p is small;
∑  is small.
In the regular case studied in Section 3.2.3, these two conditions simplify to " (r), (r) are small".

3.2.9 Non-asymptotic efficiency
This section discusses the efficiency issues. The famous Cram¥er-Rao result describes the lower bound for the estimation risk of an unbiased estimate. For linear models this result implies that the true MLE is efficient while a quasi MLE for a misspecified noise covariance is not. The Le Cam LAN theory transfers this result on the general statistical model under the LAN condition; see e.g. Chapter 3 in Ibragimov and Khas'minskij (1981). The results obtained in the previous sections provide a non-asymptotic version of the LAN approach. As already mentioned in Section 2.2, if L() is the true loglikelihood function of a regular parametric family and IP = IP , then both matrices D02 and V02 are equal to the Fisher information matrix of this family. This implies the regularity condition with a = 1 ; see (3.10). Suppose in addition that the values (r), (r) are small, so that
a = 1 -  - = 1 - (r) - 30(r)
is close to one. By (3.11) this implies that D  D0 , the matrix IB is close to the identity matrix Ip and p  p , v2  2p ,   1 . This in turn implies that the twice upper value  2 behaves as a 2p random variable and the deficiency  (r) is small in probability. In particular, all the corollaries about confidence and coverage probability for  reproduce the similar statements for the correct linear Gaussian model. Moreover, the decomposition (3.16) can be rewritten as
D0  -   D0-1L()

20 Parametric estimation. Finite sample theory

and this is the famous expansion of the MLE in the LAN situation yielding the asymptotic normality and all other asymptotic properties of  including its asymptotic efficiency. We present a precise statement in Section 5.1 when studying the i.i.d. case.

4 Deviation bounds and concentration of the qMLE

A very important step in the analysis of the qMLE  is localization. This property means that  concentrates in a small vicinity of the central point  . This section states such
a concentration bound under the global conditions of Section 2.
Given a local vicinity 0 of  , the concentration result describes the deviation probability IP   0 . The key step in this large deviation bound is made in terms of a multiscale upper function for the process L(, ) d=ef L() - L() . Namely, we build a deterministic function C(µ, , ) such that the probability

IP sup sup µL(, ) + C(µ, , )  x
0 µM

(4.1)

is exponentially small in x . Here µ is a positive scale parameter and M is the discrete set of considered scale values. Concentration sets for  can be naturally defined via level sets of the function C(, ) = maxµM C(µ, , ) . The bound (4.1) is established by some rather crude methods of the theory of empirical processes; see Section 7. Once

established, the concentration properties of  can be refined in the local vicinity 0 using local majorization technique; see Section 3.2.

The other important result describes an upper function b(, ) for the non-scaled process L(, ) providing that the probability

IP sup L(, ) + b(, x) > 0
0
is exponentially small in x . Such bounds are usually called for in the analysis of the posterior measure in the Bayes approach.

4.1 A multiscale upper function for the log-likelihood
This section presents a construction of the multiscale upper function C(µ, , ) . Then it will be used for controlling the deviation probability of the estimate  . Below we suppose that the scaling factor µ runs over some discrete set M and consider the process µL(, ) for    and µ  M . Assume that for each  , there is some µ  M such that the exponential moment of µL(, ) is finite. This enables us to define for each  the function M(µ, , ) ensuring the identity
IE exp µL(, ) + M(µ, , ) = 1.

spokoiny, v.

21

This means that the process µL(, ) + M(µ, , ) is pointwise stochastically bounded in a rather strict sense. For each µ > 0 and z  0 by the Markov inequality

IP L(, )  z  exp -µz IE exp µL(, ) = exp -µz - M(µ, , ) .

(4.2)

In particular, with z = 0 ,

IP L(, )  0  exp - max M(µ, , ) .
µM
So, a reasonable choice of µ can be made via maximization of M(µ, , ) w.r.t. µ . The famous Chernoff result describes the asymptotic separation rate between these two measures IP and IP in terms of the value M(, ) with

M(, ) = sup M(µ, , ).
µM

(4.3)

The larger M(, ) is, the stronger is the pointwise identification. If  is a subset of  not containing  , then the event    is only possible
if sup L(, )  0 , because L(, )  0 . It is intuitively clear that the uniform identification over  requires that the rate function M(, ) is bounded away from zero on  . The result of Theorem 4.1 below quantifies this condition.

We present a result for the smooth case which assumes the conditions (E) , (ED) , and (ED0) to be fulfilled with the corresponding matrices V and V0 . The set  is
taken as the complement of the local set 0(r) with a sufficiently large r . The result
also involves a value r1 entering into the pilling device; see Section 7.3 for details. The choice of r1 is done by the equality det(r1-1V ) = det(V0) relating two matrices V0 and V to each others.
For stating the results, some further notations have to be introduced. Define (, ) = () - () and

N(µ, , ) d=ef log IE exp µ(, ) .

Then it holds for the function M(µ, , ) :

M(µ, , ) = - log IE exp µL(, ) = -µIEL(, ) - N(µ, , ).

Further, consider for each    a local ball Bµ() d=ef    : V ( - )  r1/µ . Next, for the Lebesgue measure  on IRp , define the smoothing operator Sµ by

Sµf () d=ef

1 µ()

f ()(d).
Bµ()

22 Parametric estimation. Finite sample theory

Let 0 and g be the constants from (ED) . Define c1 by c1 = 2 for p  2 and c1 = 2.4 
for p = 1 . Let a constant s be selected under the condition 30r1/s  g  2c1p . The value s = 1 is a proper candidate in typical situations.
For shortening the notation assume that the discrete set M is fixed under the condition M  µM µp+2  e/2 .

Theorem 4.1. Suppose that (E) , (ED) , and (ED0) hold with some g, 0 and with

matrices V

and V0 . 

Let r1

be such that det(r1-1V )  det(V0) , and s be such that

30r1/s  g  2c1p . For r  p/2 and x > 0 , it holds

IP sup sup µL(, ) + C(µ, , ) > z1(x)  e-x+1,
0(r) µM

(4.4)

with z1(x) d=ef 2sc1p + (1 + s)x and

C(µ, , ) d=ef -µIEL(, ) - SµN(µ, , ) - (1 + s)t() t() d=ef (p + 2) log V0( - ) .

(4.5)

Proof. First fix µ  M and apply the general results of Theorem 7.10 to the process U() = (, ) = L(, ) - IEL(, ) , M () = -IEL(, ) , and H0 = V0 on   = 0c(r) . The condition (E) is fulfilled, condition (ED) implies (ED) with H() = V , and Theorem 7.10 yields (4.4) in view of r-1 1V ( - )  V0( - ) .
Remark 4.1. The construction of the multiscale upper function C(µ, , ) in (4.5) deserves some discussion. More precisely, it is interesting to compare this construction with the pointwise upper function M(µ, , ) = -µIEL(, ) - N(µ, , ) . The smoothing operator in the term SµN(µ, , ) from (4.5) is only used for technical reasons and it can be handled by simple rescaling arguments. The other term (1 + s)t() in (4.5) is really important and it can be viewed as the price for uniform concentration. Simple white noise examples show that this penalty term is nearly sharp if s is taken small and p large. However, our aim is only to obtain a rough exponential bound because really sharp results are stated by mean of local quadratic approximation; see Section 3.2.5.

4.2 Concentration sets and deviation probability
This section describes so called concentration sets for the estimate  . Any such set is deterministic ensuring that  deviates from this set with a small probability. Such concentration sets are usually used in theoretical studies and their construction typically depends on the unknown quantities the moment generation function M(µ, , ) . Let

spokoiny, v.

23

C(µ, , ) be the upper multiscale function from (4.5). Define

µ(, ) d=ef argmax C(µ, , ),
µM

(4.6)

C(, ) d=ef max C(µ, , ) = C(µ(, ), , ).
µM

(4.7)

For simplicity we assume that the maximum in (4.7) is attained and µ(, ) is well

defined. In the considered case of a discrete set M this is always fulfilled. The value C(, ) shown in (4.7) replaces M(, ) from (4.3) if one is concerned with uniform non-asymptotic bounds. For any fixed subset A   define the value g(A) by

g(A) d=ef inf C(, ).
A

(4.8)

A particular choice of the set A is given by the level set of C(, ) : for every x > 0 , define A(x, ) with

A(x, ) = { : C(, )  z1(x)} =  : sup C(µ, , )  z1(x) ,
µM
where z1(x) = 2sc1p + (1 + s)x . Obviously g A(x, )  z1(x) . Below we show that the bound (4.4) yields some concentration properties of the
estimate  in terms of the function g(∑) from (4.8).

Corollary 4.2. Under (4.4), it holds for any set A with g(A)  z1(x)

IP   A  e-x+1.

In particular,
log IP   A(x, )  -x + 1.
Proof. Denote µ = µ(, ) with µ(, ) from (4.6). Then in view of L(, )  0 and C µ, ,   g(A) for   A
  A  C µ, ,   g(A)  µL(, ) + C µ, ,  > g(A) ,

and the result follows from (4.4).
Corollary 4.2 presents an upper bound for the probability that  deviates from a set A defined via the function C(µ, , ) . The local approach of Section 3.1 requires also to bound the deviation probability for a local set 0(r) d=ef { : V0( - ) > r} for

24 Parametric estimation. Finite sample theory

r sufficiently large. It suffices to evaluate the quantity g(A) for A = 0(r) . This in turns requires to bound from below the value C(, ) for   A .
The next result presents sufficient conditions ensuring a sensible large deviation probability bound in terms of the rate function M(, ) = maxµ - log IE exp µL(, ) . Define

µ() d=ef N(µ, , ) - SµN(µ, , ) .

Corollary 4.3. Suppose the conditions of Theorem 4.1. Let, given x > 0 , there be a value r = r(x) , such that it holds

M(, )  (1 + s)t() + µ() + z1(x),   0(r), with t() = (p + 2) log V0( - ) and z1(x) d=ef 2sc1p + (1 + s)x . Then
IP V0( - ) > r(x)  e-x+1.

(4.9)

Proof. The result follows from Corollary 4.2 by making use of the inequality

-µIEL(, ) - SµN(µ, , )  M(µ, , ) - µ()

for any µ  M .
Remark 4.2. Due to this result, a logarithmic growth of M(, ) as function of the distance  -  ensures a sensitive large deviation bound for the process L() .

4.3 Probability bounds for the quasi log-likelihood
This section presents a uniform upper bound for process L(, ) without scaling. Our starting point is again a pointwise bound on L(, ) for a fixed  =  . Namely, given x , we first try to find b(, x) providing

IP L(, ) + b(, x)  0  e-x.

(4.10)

Define

b(, x) d=ef max µ-1 -x + M(µ, , ) .
µM
Then applying (4.2) with the corresponding value µ yields (4.10). Now we aim to establish an extension of this pointwise bound to a uniform bound on a subset  of  , that is, to build a function b(, x) such that

IP sup L(, ) + b(, x) > 0  e-x+1.


(4.11)

spokoiny, v.

25

Such bounds are naturally called for in the analysis of the posterior measure in the Bayes
approach. The function b(, x) can be described via the multiscale upper function C(µ, , ) . Namely, for each  and x > 0 define

b(, x) d=ef max -µ-1z1(x) + µ-1C(µ, , ) .
µM

(4.12)

Corollary 4.4. Assume the bound (4.4) for a function C(µ, , ) . Then (4.11) holds

with b(, x) from (4.12).

Proof. The bound (4.4) implies

IP sup min µL(, ) + C(µ, , ) > z1(x)  e-x+1.
 µM
This yields that there exists a random set (x) with IP (x)  1 - e-x+1 such that for any µ  M and any    , it holds µL(, ) + C(µ, , )  z1(x) on (x) . Let µ(, x) fulfill

µ(, x) d=ef argmax -µ-1z1(x) + µ-1C(µ, , ) .
µM

Then particular choice µ = µ(, x) yields on (x)

L(, ) + 1 µ(, x)

C(µ(, x), , ) - z1(x)

0

or equivalently L(, ) + b(, x)  0 .

5 Examples
The model with independent identically distributed (i.i.d.) observations is one of the most popular setups in statistical literature and in statistical applications. The essential and the most developed part of the statistical theory is designed for the i.i.d. modeling. Especially, the classical asymptotic parametric theory is almost complete including asymptotic root-n normality and efficiency of the MLE and Bayes estimators under rather mild assumptions; see e.g. Chapter 2 and 3 in Ibragimov and Khas'minskij (1981). So, the i.i.d. model can naturally serve as a benchmark for any extension of the statistical theory: being applied to the i.i.d. setup, the new approach should lead to essentially the same conclusions as in the classical theory. Similar reasons apply to the regression model and its extensions. Below we try demonstrate that the proposed non-asymptotic viewpoint is able to reproduce the existing brilliant and well established results of the classical parametric theory. With some surprise, the majority of classical efficiency results can be easily derived from the obtained general non-asymptotic bounds.

26 Parametric estimation. Finite sample theory

The next question is whether there is any added value or benefits of the new approach being restricted to the i.i.d. situation relative to the classical one. Two important issues have been already mentioned: the new approach applies to the situation with finite samples and survives under model misspecification. One more important question is whether the obtained results remain applicable and informative if the dimension of the parameter space is high ≠ this is one of the main challenge in the modern statistics. We show that the dimensionality p naturally appears in the risk bounds and the results apply as long as the sample size exceeds in order this value p . All these questions are addressed in Section 5.1 for the i.i.d. setup, Section 5.2 focuses on generalized linear modeling, while Section 5.3 discusses linear median regression.

5.1 Quasi MLE in an i.i.d. model
The basic i.i.d. parametric model means that the observations Y = (Y1, . . . , Yn) are independent identically distributed from a distribution P from a given parametric family (P,   ) on the observation space Y1 . Each    clearly yields the product data distribution IP = Pn on the product space Y = Yn1 . This section illustrates how the obtained general results can be applied to this type of modeling under possible model misspecification. Different types of misspecification can be considered. Each of the assumptions, namely, data independence, identical distribution, parametric form of the marginal distribution can be violated. To be specific, we assume the observations Yi independent and identically distributed. However, we admit that the distribution of each Yi does not necessarily belong to the parametric family (P) . The case of non-identically distributed observations can be done similarly at cost of more complicated notation.
In what follows the parametric family (P) is supposed to be dominated by a measure µ0 , and each density p(y, ) = dP/dµ0(y) is two times continuously differentiable in  for all y . Denote (y, ) = log p(y, ) . The parametric assumption Yi  P  (P) leads to the log-likelihood

L() = (Yi, ),

where the summation is taken over i = 1, . . . , n . The quasi MLE  maximizes this sum over    :

 d=ef argmax L() = argmax





(Yi, ).

The target of estimation  maximizes the expectation of L() :

 d=ef argmax IEL() = argmax IE (Yi, ).





spokoiny, v.

27

Let i() d=ef (Yi, ) - IE (Yi, ) . Then () = implies

i() . The equation IEL() = 0

() = i() =  i().

(5.1)

I.i.d. structure of the Yi 's allows for rewriting the conditions (E) , (ED) , (ED0) , (ED1) , and (L0) in terms of the marginal distribution. In the following conditions the index i runs from 1 to n .
(e) For each    , there exists a positive value µ  M such that
m(µ, , ) d=ef - log IE exp µ (Yi, ) - (Yi, )

is finite.

(ed) There exist some constants 0 , and g1 > 0 , and a positive symmetric p ◊ p matrix v , such that for all ||  g1

sup sup log IE exp

 

i()

Sp 

v

 022/2.

(ed0) There exists a positive symmetric matrix v0 , such that for all ||  g1

sup log IE exp

 

i()

 Sp

v0

 022/2.

A natural candidate on v02 is given by the variance of the gradient  (Y1, ) , that is, v02 = Var  (Y1, ) = Var 1() .
Next consider the local sets

0(r) = { : v0( - )  r/n1/2}.

The local smoothness conditions (ED1) and (L0) require to specify the functions (r) and (r) . If the log-likelihood function (y, ) is sufficiently smooth in  , these functions can be selected proportional to r .

(ed1) For each r  R , there exists a constant  such that for all i = 1, . . . , n and ||  g1

 sup sup log IE exp 
Sp 0(r)

i() - i()  r v0

 022/2.

Further we restate the local identifiability condition (L0) in terms of the expected value () d=ef IE (Yi, ) of each (Yi, ) . We suppose that () is two times differentiable w.r.t.  and define the matrix IF0 = -2 () .

28 Parametric estimation. Finite sample theory

( 0) For each r  R , there is a constant  , such that it holds on 0(r)

() - () - ( - )  ()

( - ) IF0 ( - )/2

-1

 r.

In the regular parametric case with IP  (P) , the matrices v02 and IF0 coincide with the Fisher information matrix IF () of the family (P) at the point  .

Lemma 5.1. Let Y1, . . . , Yn be i.i.d. Then (e) , (ed) , (ed0) , (ed)1 , and ( 0) imply (E) , (ED) , (ED0) , (ED)1 , and (L0) with M(µ, , ) = nm(µ, , ) , V 2 = nv2 , V02 = nv02 , D02 = nIF0 , (r) = r , (r) = r , the same constant 0 , and

g

d=ef

 g1 n.

Proof. The identities M(µ, , ) = n m(µ, , ) , V 2 = nv2 , V02 = nv02 , D02 = nIF0 follow from the i.i.d. structure of the observations Yi . We briefly comment on condition (ED) . The use once again the i.i.d. structure yields by (5.1) in view of V 2 = nv2

 () log IE exp 
V

= nIE exp

 n1/2

1() v

 022/2

as long as   n1/2g1  g . Similarly one can check (ED0) and (ED1) .

Below we specify the general results of Sections 3 and 4 to the i.i.d. setup.

5.1.1 A large deviation bound First we describe the large deviation probability for the event {  0(r)} for a fixed r . Corollary 4.3 provides a sufficient condition for such a bound: the rate function M(, ) should grow at least logarithmic with the distance V ( - ) . Define
m(, ) = max m(µ, , ).
µ
(ld1) There exist constants b > 0 , such that it holds on the set 0c(r) m(, )  b log 1 + v0( - ) 2 .
This condition implies by M(, ) = nm(, )
M(, )  nb log 1 + v0( - ) 2 ,   0(r). For the next result we assume that the value r1  1 is fixed bydet(r-1 1v)  det(v0) . Further, the constant s has to be fixed such that 30r1/s  g  2c1p .

spokoiny, v.

29

Theorem 5.2. Suppose (e) , (ed) , (ld1) . For each x and r = r(x) such that (2/3)br2  (1 + s)(p/2 + 1) log(1 + r2) + z1(x),

(5.2)

it holds for n  r2

IP

 n

v0( - )

r

 e-x+1.

Proof. Given r and x , (5.2) implies for all u  r

nb log(1 + u2/n)  (1 + s)(p/2 + 1) log(1 + u2) + z1(x).

because n log(1+r2/n)  2r2/3 for r2  n . Now it follows from (ld1) for any   0(r) that

M(, )  nb log(1 + v0( - ) 2)  (1 + s)(p/2 + 1) log 1 + V0( - ) 2 + z1(x).

Now the result follows from Corollary 4.3. (to be done) treatment of µ .
Remark 5.1. The presented result helps to quantify two important values r and n providing a sensitive deviation probability bound: the radius r of the local neighborhood should be large enough to ensure (5.2), while the sample size n should be larger than r2 . It is straightforward to see that (5.2) starts to hold for r2  Const. p log p for some fixed constant Const. Therefore, for any r exceeding this value, a deviation probability IP   0(r) is negligible when n  r2  Const. p log p .

5.1.2 Local inference

Now we restate the general local bounds of Section 3 for the i.i.d. case. First we describe the approximating linear models. The matrices v02 and IF0 from conditions (ed0) , (ed1) , and ( 0) determine their drift and variance components. Define

IF d=ef IF0(1 - ) - v02.

Then D2 = nIF and

 d=ef D-1() = nIF -1/2  (Yi, ).

(5.3)

The upper approximating process reads as

L (, ) = ( - ) D  - D ( - ) 2/2.

30 Parametric estimation. Finite sample theory

This expression appears as log-likelihood for the linear model  = D + for a standard normal error  . The (quasi) MLE  for this model is of the form  = D-1 .
Theorem 5.3. Suppose (ed0) . Given r , assume (ed1) , and ( 0) on 0(r) , and let = 30 r/n1/2 ,  = (r) = r/n1/2 , and IF d=ef IF0(1 - ) - v02  0 . Then the
results of Theorem 3.1 through 3.8 apply to the case of i.i.d. modeling. In particular, for any z > 0 , it holds
IP nIF ( - ) > z,   0(r) 
 IP 1 -   > z - 2 (r) + 2 (r) ,

where  (r),  (r) follow the bound (3.4) and  is defined by  d=ef Ip - IF 1/2IF -1IF 1/2  = max Ip - IF 1/2IF -1IF 1/2 .

Moreover, on the random set C (r) =   0(r),   r , it holds nIF  -  -  2  2 (r).

The presented results are stated via the probability bound for the squared norm of the vector  from (5.3). One can apply the general results of Section 6.4. For ease of notation we consider the vector  instead of  :

 d=ef (nIF0)-1/2() = nIF0 -1/2  (Yi, ).

(5.4)

The necessary condition (6.18) coincides with (ED0) . Now Corollary 6.13 implies that the probability IP  2 > z(x, IB) is of order 2e-x for all moderate x and IB = IF0-1/2v02IF0-1/2 . In particular, this probability starts to degenerate when z significantly
exceeds the value p = tr(IB) .

5.1.3 The regular parametric case and asymptotic efficiency
The conditions and the results become even more transparent in the regular situation when IF0  a2v02 . An important special case corresponds to the correct parametric specification with a = 1 and IF0 = v02 . Under regularity one can use IF = (1 -  -
a2)IF0 . Here we briefly discuss the corollaries of Theorem 5.3 for the classical asymptotic setup when n tends to infinity.
Under the imposed conditions, the quantities  and can be taken of order r/n1/2 , where r2 p log(p) . If n grows then  and decreases to zero, and the matrix IF is close to IF0 . The value  (r) is close to zero in probability. Moreover, the random vector  from (5.4) fulfills Var()  IF0-1/2v02IF0-1/2 d=ef IB2 and by the central limit theorem 

spokoiny, v.

31

is

asymptotically

normal

N(0, IB2) .

This

yields

by

Theorem

5.3

that

 nIF0

 - 

is

asymptotically normal N(0, IB2) as well. The correct model specification implies IB  Ip

and hence  is asymptotically efficient; see Ibragimov and Khas'minskij (1981). Also 2L(, )   2 which is nearly 2 r.v. with p degrees of freedom. This result is

known as asymptotic Wilks theorem.

5.2 Generalized linear modeling
Now we consider a generalized linear modeling (GLM) which is often used for describing some categorical data. Let P = (Pw, w   ) be an exponential family with a canonical parametrization; see e.g. McCullagh and Nelder (1989). The corresponding log-density can be represented as (y, w) = yw - d(w) for a convex function d(w) . The popular examples are given by the binomial (binary response, logistic) model with d(w) = log ew + 1 , the Poisson model with d(w) = ew , the exponential model with d(w) = - log(w) . Note that linear Gaussian regression is a special case with d(w) = w2/2 .
A GLM specification means that every observation Yi has a distribution from the family P with the parameter wi which linearly depends on the regressor i  IRp :

Yi  Pi  . The corresponding log-density of a GLM reads as

(5.5)

L() =

Yii  - d(i ) .

First we specify the data distribution allowing that the parametric model (5.5) is misspecified. Misspecification of the first kind means that the vector f d=ef IEY cannot be
represented in the form   whatever  is. In this situation, the target of estimation  is defined by

 d=ef argmax IEL().

The other sort of misspecification concerns the data distribution. The model (5.5) assumes that the Yi 's are independent and the marginal distribution belongs to the given parametric family P . In what follows, we only assume independent data having certain exponential moments. The quasi MLE  is defined by maximization of L() :

 = argmax L() = argmax


Yii  - d(i ) .

Convexity of d(∑) implies that L() is a concave function of  , so that the optimization problem has a unique solution and can be effectively solved. However, a closed form

32 Parametric estimation. Finite sample theory

solution is only available for the constant regression or for the linear Gaussian regression. The corresponding target  is the maximizer of the expected log-likelihood:

 = argmax IEL() = argmax


fii  - d(i )

with fi = IEYi . The function IEL() is concave as well and the vector  is also well defined.

Define the individual errors (residuals) i = Yi - IEYi . Below we assume that these errors fulfill some exponential moment conditions.

(e1) There exist some constants 0 and g1 > 0 , and for every i a constant ni such that IE i/ni 2  1 and for all ||  g1

log IE exp i/ni  022/2, ||  g1.

(5.6)

A natural candidate for ni is i where i2 = IE2i is the variance of i ; see Lemma 7.12. Under (5.6), introduce a p ◊ p matrix V0 defined by

V02 d=ef

n2i ii .

(5.7)

Condition (e1) effectively means that each error term i = Yi - IEYi has some bounded exponential moments: for  = g1 , it holds f () d=ef log IE exp i/ni <  . This implies the quadratic upper bound for the function f () for ||  g1 ; see Lemma 7.12. In words, condition (e1) requires light (exponentially decreasing) tail for the marginal distribution of each i .
Define also

N -1/2 d=ef max

sup

ni|i

| .

i IRp V0

(5.8)

Lemma 5.4. Assume (e1) and let V0 be defined by (5.7) and N by (5.8). Then conditions (ED0) and (ED) follow with V = V0 , g = g1N 1/2 , and with the constant 0 from (e1) . Moreover, the stochastic component () is linear in  and the condition (ED1) is fulfilled with (r)  0 .

Proof. The gradient of the stochastic component () of L() does not depend on  :

() = ii

with i = Yi - IEYi . Now, for any unit vector   IRp and   g , independence of the i 's implies that



log IE exp



V0

ii =

log IE exp

nii V0



i/ni

.

(5.9)

spokoiny, v.

33

By definition ni|i |/ V0  N -1/2 and therefore, ni|i |/ V0  g1 . Hence, (5.6) implies



log IE exp



V0

ii



2

022 V0

2

n2i |i

|2 =

022 , 2

(5.10)

and (ED0) follows.

It remains only to bound the quality of quadratic approximation for the mean of the process L(, ) in a vicinity of  . An interesting feature of the GLM is that the effect of model misspecification disappears in the expectation of L(, ) .

Lemma 5.5. It holds

-IEL(, ) =

d(i ) - d(i ) - d (i )i ( - )

= K IP , IP ,

(5.11)

where K IP, IP is the Kullback-Leibler divergence between measures IP and IP . Moreover,

where   [, ] and

-IEL(, ) = 1 2

D ( - )

2,

(5.12)

D2() = d (i )ii .

Proof. The definition implies

IEL(, ) =

fii ( - ) - d(i ) + d(i ) .

As  is the extreme point of IEL() , it holds IEL() = fi -d (i ) i = 0 and (5.11) follows. The Taylor expansion of the second order around  yields the expansion
(5.12).

Define now the matrix D0 by D02 d=ef D2() =

d (i )ii .

Let also V0 be defined by (5.7). Note that the matrices D0 and V0 coincide if the model Yi  Pi  is correctly specified and n2i = d (i ) . The matrix V0 describes a local elliptic neighborhood of the central point  in the form 0(r) = { : V0( - ) 
r} . If the matrix function D2() is continuous in this vicinity 0(r) then the value (r) measuring the approximation quality of -IEL(, ) by the quadratic function
D0( - ) 2/2 is small and the identifiability condition (L0) is fulfilled on 0(r) .

34 Parametric estimation. Finite sample theory

Lemma 5.6. Suppose that

Ip - D0-1D2()D0-1   (r),

  0(r).

(5.13)

Then (L0) holds with this (r) . Moreover, as the quantities (r),  (r),  (r) vanish, one can take = 0 leading to the following representation for D and  :

D2 = (1 - )D02, D2 = (1 + )D02,

 = (1 + )1/2  = (1 - )1/2

with

 d=ef D0-1 = D0-1 i(Yi - IEYi).

Now we are prepared to state the local results for the GLM estimation.

Theorem 5.7. Let (e1) hold. Then for = (, 0) with   (r) and any   0(r)

L (, )  L(, )  L (, ).

(5.14)

Moreover, for any z > 0 and z > 0 , it holds

IP D0  -  > z, V0  -   r  IP  2 > z2[1 - (r)] IP L(, ) > z, V0  -   r  IP  2/2 > z[1 - (r)] .

Linearity of the stochastic component () in the considered GLM implies important fact that the quantities  (r),  (r) in the majorization bound (5.14) vanish for any r . However, the deterministic component is not quadratic in  unless the function d(w) is quadratic. Therefore, the presented bounds are local and have to be accomplished with the large deviation bounds.
An interesting question, similarly to the i.i.d. case, is the minimal radius r of the local vicinity 0(r) ensuring the desirable concentration property. We apply the sufficient condition (4.9) of Corollary 4.3 ensuring the concentration property. By Lemma 5.5, the function M(µ, , ) can be decomposed as

M(µ, , ) = µK IP, IP - N µ, ,  .
Let µ = µ() be selected such that µ|i ( - )|ni  g1 for all i . Then the arguments from (5.9) and (5.10) yield

N µ, ,   log IE exp µnii ( - )i/ni

 02µ2 2

nii

( - )

2

=

02µ2 2

V0( - )

2.

spokoiny, v.

35

Therefore,

M(µ, , )  µK IP, IP

- 02µ2 2

V0( - ) 2

and a reasonable choice of µ = µ() is given by µ() = 0-2K IP, IP / V0( - ) 2 leading to

M(, ) 

0-2K2 IP , IP 2 V0( - ) 2

.

So, the Kullback-Leibler divergence K IP, IP should of order at least

V0( - ) p log V0( - ) .

One can check that this condition is fulfilled if the convex function d(∑) satisfies d (t)  Const. /t for some Const > 0 and if the effective sample size N from (5.8) is sufficiently large.

5.3 Linear median estimation

This section illustrates how the proposed approach applies to robust estimation in linear models. The target of analysis is the linear dependence of the observed data Y = (Y1, . . . , Yn) on the set of features i  IRp :

Yi = i  + i

(5.15)

where i denotes the i th individual error. The study of the qMLE in the GLM (5.5) heavily relies on the assumption (e1) .
If this assumption is not verified, then the proposed approach would not apply. An explicit structure of the qMLE, especially in the linear regression case, allows for direct study of the properties of  under weaker moment assumptions than (e1) . However, we aim at establishing some exponential bounds and therefore, the condition of bounded exponential moments for each observation is really necessary within the least squares or generalized linear approach. In the case of heavily tailed data with only polynomial moments, one can obtain some convergence results for the LSE  , however an exponential bound is not available. In such cases, it is natural to use a robustified version of the contrast, e.g. the least absolute deviation (LAD) method. We consider the linear model (5.15) and suppose for a moment that the errors i are i.i.d. and follow the double exponential (Laplace) distribution with the density (1/2)e-|y| . Then the model (5.15) yields the log-likelihood

L() = - 1 2

|Yi - i |

36 Parametric estimation. Finite sample theory

and  d=ef argmax L() is called the least absolute deviation (LAD) estimate. In the context of linear regression, it is also called the linear median estimate. The target of estimation  is defined as usually by the equation  = argmax IEL() .
It is useful to define the residuals i = Yi - i  and their distributions

Pi(A) = IP i  A = IP Yi - i   A

for any Borel set A on the real line. If Yi = i +i is the true model then Pi coincides with the distribution of each i . Below we suppose that each Pi = L(Yi - i ) has a

positive density pi(y) .

Note that the difference

L() - L()

is bounded by

1 2

|i ( - )| and condition

(E) is fulfilled automatically. Next we check conditions (ED0) and (ED1) . Denote

i() = 1I(Yi - i   0) - qi() for qi() = IP (Yi - i   0) . This is a centered

Bernoulli random variable, and it is easy to check that

() = i()i.

(5.16)

This expression differs from the similar ones from the linear and generalized linear regression because the error terms i now depends on  . First we check the global condition (ED) . Fix any g1 < 1 . Then it holds for a Bernoulli r.v. Z with IP (Z = 1) = q ,  = Z - q , and ||  g1

log IE exp() = log q exp{(1 - q)} + (1 - q) exp(-q)  02q(1 - q)2/2,

(5.17)

where 0  1 depends on g1 only. Let now a vector   IRp and  > 0 be such that |i |  g1 for all i = 1, . . . , n . Then

log IE exp{ ()}  022 2

qi() 1 - qi() |i |2

 022 V () 2/2,

(5.18)

where

V 2() = qi() 1 - qi() ii .

(5.19)

Denote also

V2 = 1 4

ii .

spokoiny, v.

37

Clearly V ()  V for all  and condition (ED) is fulfilled globally with the matrix V and g = g1N 1/2 for N defined by

N -1/2 d=ef max sup i  ; i IRp 2 V 

(5.20)

cf. (5.9).

5.3.1 A local central bound
Now we restrict ourselves to the elliptic vicinity 0(r) = { : V0( - )  r} of the central point  for V0 = V () and some r > 0 . Define the matrix V0 = V () . Then condition (ED0) with the matrix V0 and g = N 1/2g1 is fulfilled on 0(r) due to (5.18). Next, for checking (ED1) suppose the following regularity condition:

V  1V0

(5.21)

for some 1  1 . This condition implies the inequality |i |  1N -1/2 V0 for any vector   IRp . By (5.16)

() - () = i i() - i() .

If i   i  , then

i() - i() = 1I(i   Yi < i ) - IP i   Yi < i  .

Similarly for i  < i 

i() - i() = - 1I(i   Yi < i ) + IP i   Yi < i  .

Define qi(, ) d=ef qi() - qi() . Now (5.17) yields similarly to (5.18)

log IE exp  () - ()  022 2

qi(, )|i |2



2022

max
in

qi(,



)

V

2  (r)022

V0

2/2,

with

(r) d=ef 41 max sup qi(, ).
in 0(r)
If each density function pi is uniformly bounded by a constant C then |qi() - qi()|  C i ( - )  C1N -1/2 V0( - )  C1N -1/2r.

Next we check the identifiability condition. We use the following technical lemma.

38 Parametric estimation. Finite sample theory

Lemma 5.8. It holds for any 

2 2

IEL()

=

D2()

d=ef

pi i ( - ) ii ,

(5.22)

where pi(∑) is the density of i = Yi - i  . Moreover, there is   [, ] such that

-IEL(, ) = 1 2

|i ( - )|2pi(i ( - ))

= ( - ) D2()( - )/2.

(5.23)

Proof. Obviously

IEL() =


IP (Yi  i ) - 1/2 i.

The identity (5.22) is obtained by one more differentiation. By definition,  is the extreme point of IEL() . The equality IEL() = 0 yields

IP (Yi  i ) - 1/2 i = 0. Now (5.23) follows by the Taylor expansion of the second order at  .

Define

D02 d=ef |i ( - )|2pi(0)

Due to this lemma, condition (L0) is fulfilled in 0(r) with this choice D0 for (r) from (5.13); see Lemma 5.6. Now all the local conditions are fulfilled yielding the general majorizing bound of Theorem 3.1 and all its corollaries.
Theorem 5.9. Let V0 = V () ; see (5.19). Assume (5.21) for V 2 = (1/4) ii . Fix any   (r) and  30(r) . Then Theorem 3.1 and its corollaries holds for the linear median estimation.

This example is one more confirmation of the applicability of the the general approach: as soon as the local conditions have been checked the main local statements follow for free. It only remains to accomplish them by a large deviation bound, that is, to describe the local vicinity 0(r) providing the prescribed concentration bound.

5.3.2 A large deviation bound A sufficient condition for the concentration property is that the rate function M(, ) grows at least logarithmic in  -  . For y > 0 , define for Ay = [y, ]
i(y) = -(2y)-1 log[Pi(Ay)] = -(2y)-1 log IP (Yi - i  > y) .

spokoiny, v.

39

The case with i(y)  0 > 0 corresponds to light tails while i(y)  0 as |y|   means heavy tails of the distribution Pi . Below we focus on the most interesting case when i(y) is positive and monotonously decreases to zero in y > 0 . For simplicity of presentation we also assume that i(y) is sufficiently regular and its first derivative i(y) is uniformly continuous on IR . Define
i(, ) d=ef |Yi - i | - |Yi - i | /2 fi(µ) d=ef log IE exp µi(, ) .
As |i(, )|  |i ( - )|/2 and fi(µ) is analytic in µ , it holds for any µ  0 with µ maxi |i ( - )|/2  1
fi(µ)  -µIEi(, ) - µ2 Var i(, ).

This implies by independence of the i 's

M(µ, , ) d=ef - log IE exp µL(, ) = fi(µ)  -µIEL(, ) - µ2 Var L(, ).

The choice µ = µ() = -IEL(, ) 2 Var L(, ) yields

M(, )



IEL(, ) 2 4 Var L(, ) .

Note that the inequality |i(, )|  i ( - ) /2 implies

Var L(, ) 

1 2

max
i

|i

( - )|

IE|i(, )|.

This easily yields that the sufficient condition (4.9) of Corollary 4.3 is fulfilled in this situation if N from (5.20) is sufficiently large.

6 Deviation probability for quadratic forms

The approximation results of the previous sections rely on the probability of the form IP  > y for a given random vector   IRp . The only condition imposed on this vector is that

log IE exp    02  2/2,   IRp,   g.

To simplify the presentation we rewrite this condition as

log IE exp     2/2,   IRp,   g.

(6.1)

40 Parametric estimation. Finite sample theory

The general case can be reduced to 0 = 1 by rescaling  and g :
log IE exp  /0   2/2,   IRp,   0g
that is, 0-1 fulfills (6.1) with a slightly increased g . In typical situations like in Section 5, the value g is large (of order root- n ) while the value 0 is close to one.

6.1 Gaussian case
Our benchmark will be a deviation bound for  2 for a standard Gaussian vector  . The ultimate goal is to show that under (6.1) the norm of the vector  exhibits behavior expected for a Gaussian vector, at least in the region of moderate deviations. For the reason of comparison, we begin by stating the result for a Gaussian vector  .
Theorem 6.1. Let  be a standard normal vector in IRp . Then for any u > 0 , it holds
IP  2 > p + u  exp -(p/2)(u/p)

with (t) d=ef t - log(1 + t).

Let -1(∑) stand for the inverse of (∑) . For any x , IP  2 > p + -1(2x/p)  exp(-x).

This particularly yields with  = 6.6

IP



2

>

p

+

 xp



(x)

 exp(-x).

Proof. The proof utilizes the following well known fact: for µ < 1

log IE exp µ  2/2 = -0.5p log(1 - µ).

It can be obtained by straightforward calculus. Now consider any u > 0 . By the exponential Chebyshev inequality

IP  2 > p + u  exp -µ(p + u)/2 IE exp µ  2/2 = exp -µ(p + u)/2 - (p/2) log(1 - µ) .

(6.2)

It is easy to see that the value µ = u/(u + p) maximizes µ(p + u) + p log(1 - µ) w.r.t. µ yielding

µ(p + u) - p log(1 - µ) = u - p log(1 + u/p).

spokoiny, v.

41

Further we use that x - log(1 + x)  a0x2 for x  1 and x - log(1 + x)  a0x for x > 1

with

a0 = 1 - log(2)  0.3 .

This

implies with

x = u/p

for

 u = xp

or

u = x

and

 = 2/a0 < 6.6 that

IP



2



p

+

 xp



(x)

 exp(-x)

as required.

The message of this result is that the squared norm of the Gaussian vector  con
centrates around the value p and the deviation over the level p + xp are exponentially small in x .
A similar bound can be obtained for a norm of the vector IB where IB is some given matrix. For notational simplicity we assume that IB is symmetric. Otherwise one should replace it with (IB A)1/2 .
Theorem 6.2. Let  be standard normal in IRp . Then for every x > 0 and any symmetric matrix IB , it holds with p = tr(IB2) , v2 = 2 tr(IB4) , and a = IB2 
IP IB 2 > p + (2vx1/2)  (6ax)  exp(-x).
Proof. The matrix IB2 can be represented as U diag(a1, . . . , ap)U for an orthogonal matrix U . The vector  = U  is also standard normal and IB 2 =  U IB2U  . This means that one can reduce the situation to the case of a diagonal matrix IB2 = diag(a1, . . . , ap) . We can also assume without loss of generality that a1  a2  . . .  ap . The expressions for the quantities p and v2 simplifies to
p = tr(IB2) = a1 + . . . + ap, v2 = 2 tr(IB4) = 2(a12 + . . . + ap2).
Moreover, rescaling the matrix IB2 by a1 reduces the situation to the case with a1 = 1 .
Lemma 6.3. It holds
IE IB 2 = tr(IB2), Var IB 2 = 2 tr(IB4).

Moreover, for µ < 1
p
IE exp µ IB 2/2 = det(1 - µIB2)-1/2 = (1 - µai)-1/2.
i=1

(6.3)

42 Parametric estimation. Finite sample theory

Proof. If IB2 is diagonal, then IB 2 = i aii2 and the summands aii2 are independent. It remains to note that IE(aii2) = ai , Var(aii2) = 2a2i , and for µai < 1 ,
IE exp µaii2/2 = (1 - µai)-1/2
yielding (6.3).

Given u , fix µ < 1 . The exponential Markov inequality yields

IP IB 2 > p + u  exp - µ(p + u) IE exp µ IB 2 22

 exp - µu - 1 22

p

µai + log 1 - µai

i=1

.

We start with the case when x1/2  v/3 . Then u = 2x1/2v fulfills u  2v2/3 . Define µ = u/v2  2/3 and use that t + log(1 - t)  -t2 for t  2/3 . This implies

IP IB 2 > p + u

 exp - µu + 1 22

p
µ2a2i

= exp -u2/(4v2) = e-x.

i=1

Next, let x1/2 > v/3 . Set µ = 2/3 . It holds similarly to the above

(6.4)

µai + log 1 - µai  - µ2ai2  -2v2/9  -2x.
I1 I1
Now, for u = 6x and µu/2 = 2x , (6.4) implies

IP IB 2 > p + u  exp - 2x - x = exp(-x)

as required. Below we establish similar bounds for a non-Gaussian vector  obeying (6.1).

6.2 A bound for the 2 -norm
This section presents a general exponential bound for the probability IP  > y under (6.1). Define the value

xc = 0.5 g2 - p log 1 + g2/p .

(6.5)

Theorem 6.4. Let   IRp fulfill (6.1). Then it holds for each x  xc

IP



2

>

p

+

 xp



(x),



2  p + g2

 2 exp(-x),

spokoiny, v.

43

where  = 6.6 . Moreover, for y2  yc2 d=ef p + g2 , it holds with gc = g2(p + g2)-1/2 IP  2 > y2  8.4 exp -gcy/2 - (p/2) log(1 - gc/y)  8.4 exp -xc - gc(y - yc)/2 .

Proof. The main step of the proof is the following exponential bound.

Lemma 6.5. Suppose (6.1). For any µ < 1 with g2 > pµ , it holds

µ2

IE exp

1I

2



2

g2 µ2

-p µ

 2(1 - µ)-p/2.

(6.6)

Proof. Let  be a standard normal vector in IRp and u  IRp . Given r > 0 , define

m(u, r) = IP  - u  r . This value can be rewritten as

m(u, r) = IP  - u 2  r2 = IP  2 - IE  2 - 2u   r2 - u 2 - IE  2 ,

and m(u, r)  1/2 for r2  u 2 + IE  2 = u 2 + p . Let us fix some  with µ  2  g2/µ - p and denote by IP the conditional probability given  . It holds with cp = (2)-p/2

cp

exp   -  2 1I(   g)d 2µ

= cp exp µ  2/2

exp - 1 µ-1/2 - µ1/2 2 1I(µ-1  2  µ-1g2)d 2

= µp/2 exp µ  2/2 IP  + µ1/2 2  µ-1g2

 0.5µp/2 exp µ  2/2 ,

because µ1/2 2 + p  µ-1g2 . This implies in view of p < g2/µ that

exp µ  2/2 1I  2  g2/µ2 - p/µ

 2µ-p/2cp

exp 

-

2 2µ

1I( 

 g)d.

Further, by (6.1)

cpIE

exp 

- 1  2 2µ

1I( 

 g)d

 cp

exp - µ-1 - 1  2 1I(   g)d 2

 cp

exp - µ-1 - 1  2 d 2

 (µ-1 - 1)-p/2

44 Parametric estimation. Finite sample theory

and (6.6) follows.

Due to this result, the scaled squared norm µ  2/2 after a proper truncation possesses the same exponential moments as in the Gaussian case. A straightforward implication is the probability bound IP  2 > p + u for moderate values u . Namely, given u > 0 , define µ = u/(u + p) . This value optimizes the inequality (6.2) in the Gaussian case. Now we can apply a similar bound under the constraints  2  g2/µ2 - p/µ . Therefore, the bound is only meaningful if p + u  g2/µ2 - p/µ with µ = u/(u + p) . One can check that the largest value u for which this constraint is still valid, is given by u = g2 . Hence, (6.6) yields for u  g2

IP  2 > p + u,  2  p + g2

 exp - µ(p + u) IE exp µ  2 22

1I



2



g2 µ2

-

p µ

 2 exp -0.5 µ(p + u) + p log(1 - µ)

= 2 exp -0.5 u - p log(1 + u/p) .

Similarly to the Gaussian case, this implies with  = 6.6 that

IP



2



p

+

 xp



(x),



2  p + g2

 2 exp(-x).

The Gaussian case yields (6.1) with g =  and the result is done. In the nonGaussian case with a finite g , we have to accompany the moderate deviation bound with a large deviation bound IP  > y for y2  p + g2 . This is done by combining the bound (6.6) with the standard slicing arguments.
Lemma 6.6. Let µ0  g2/p . Define y02 = g2/µ02 - p/µ0 and g20 = g2 - µ0p . It holds for y  y0

IP  > y  8.4(1 - g0/y)-p/2 exp -g0y/2  8.4 exp -x0 - g0(y - y0)/2 .

(6.7) (6.8)

with x0 defined by
2x0 = µ0y02 + p log(1 - µ0) = g2/µ0 - p + p log(1 - µ0).
Proof. Consider the growing sequence yk with y1 = y and g0yk+1 = g0y + k . Define also µk = g0/yk . In particular, µk  µ1 = g0/y . Obviously

IP  > y = IP  > yk,   yk+1 .
k=1

spokoiny, v.

45

Now we try to evaluate every slicing probability in this expression. We use that

µk+1yk2

=

(g0y + k - 1)2 g0y + k

 g0y + k - 2,

and also g2/µ2k - p/µk  y2k because y  y0 and

g2/µ2k - p/µk - y2k = µ-k 2(g2 - µkp - g20)  µk-2(g2 - g0p/y - g02)  0.

Hence by (6.6)


IP  > y  IP  > yk,   yk+1

k=1




exp

- µk+1yk2

IE exp

µk+1  2

22

1I

 2  y2k+1

k=1




2 1 - µk+1 -p/2 exp

- µk+1yk2 2

k=1

2

1 - µ1

-p/2



exp

- g0y + k - 2 2

k=1

= 2e1/2(1 - e-1/2)-1(1 - µ1)-p/2 exp -g0y/2

 8.4(1 - µ1)-p/2 exp -g0y/2

and the first assertion follows. For y = y0 , it holds g0y0 + p log(1 - µ0) = µ0y20 + p log(1 - µ0) = 2x0

and (6.7) implies IP  > y0  8.4 exp(-x0) . Now observe that the function f (y) = g0y/2 + (p/2) log 1 - g0/y fulfills f (y0) = x0 and f (y)  g0/2 yielding f (y)  x0 + g0(y - y0)/2 . This implies (6.8).
The statements of the theorem are obtained by applying the lemmas with µ0 = µc = g2/(p + g2) . This also implies y2c = p + g2 , gcyc = g2 , 1 - µc = p/(p + g2) , and xc from (6.5).

The statements of Theorem 6.8 can be represented in the form:

Corollary 6.7. Let  fulfill (6.1). Then it holds for x  xc = 0.5 g2 - p log 1 + g2/p :

IP  2  z(x, p)  2e-x + 8.4e-xc,

 z(x, p) d=ef p + xp,

x  p/,

p + x

p/ < x  xc.

(6.9) (6.10)

46 Parametric estimation. Finite sample theory

For x > xc IP  2  zc(x, p)  8.4e-x,

zc(x, p) d=ef yc + 2(x - xc)/gc 2.

This result implicitly assumes that p  xc which is fulfilled if u0 = g2/p  1 :

xc = 0.5 u0 - log(1 + u0) p  3.3 1 - log(2) p > p.

In the zone x  p/ we obtain sub-Gaussian behavior of the tail of  2 - p , in the zone p/ < x  xc it becomes sub-exponential. Note that the sub-exponential zone is empty if g2 < p .
For x  xc , the function z(x, p) mimics the quantile behavior of the chi-squared distribution p2 with p degrees of freedom. Moreover, increase the dimension p yields growth of the sub-Gaussian zone.
Finally, in the large deviation zone x > xc the deviation probability decays as e-cx1/2 for some fixed c . However, if the constant g in the condition (6.1) is sufficiently large
relative to p , then xc is large as well and the large deviation zone x > xc can be ignored at a small price of 8.4e-xc and one can focus on the deviation bound described by (6.9)
and (6.10).

6.3 A bound for a quadratic form
Now we extend the result to more general bound for IB 2 =  IB2 with a given matrix IB and a vector  obeying the condition (6.1). Similarly to the Gaussian case we assume that IB is symmetric. Define important characteristics of IB

p = tr(IB2),

v2 = 2 tr(IB4),

 d=ef IB2  d=ef max(IB2).

For simplicity of formulation we suppose that  = 1 , otherwise one has to replace p and v2 with p/ and v2/ .
Let g be shown in (6.1). Define similarly to the 2 -case µc = g2/(p + g2) . Further define the values yc, gc , and xc by

y2c d=ef g2/µ2c - p/µc, gc d=ef µcyc = g2 - µcp, 2xc d=ef gcyc + log det Ip - µcIB2 .

(6.11)

Theorem 6.8. Let a random vector  in IRp fulfill (6.1). Then for each x < xc

IP IB 2 > p + (2vx1/2)  (acx), IB 2  yc2  2 exp(-x)

spokoiny, v.

47

with ac d=ef 6  (4µc-1) . Moreover, for y  yc , it holds IP IB 2 > y2  8.4 exp -xc - gc(y - yc)/2 .

Proof. The main steps of the proof are similar to the proof of Theorem 6.4. Lemma 6.9. Suppose (6.1). For any µ < 1 with g2/µ  p , it holds

IE exp µ IB 2/2 1I IB2 2  g2/µ2 - p/µ  2det(Ip - µIB2)-1/2. (6.12)

Proof. With cp(IB) = 2 -p/2 det(IB-1)

cp(IB)

exp   - 1 IB-1 2 1I(   g)d 2µ

µ IB 2

= cp(IB) exp

2

exp - 1 µ1/2IB - µ-1/2IB-1 2 1I(   g)d 2

= µp/2 exp

µ IB 2 2

IP

IB - µ1/2IB2  gµ-1/2 ,

where  denotes a standard normal vector in IRp and IP means the conditional expectation given  . Moreover, for any u  IRp and r  p + u 2 , it holds in view of IE IB 2 = p

IP IB - u  r = IP IB 2 - p - 2u IB  r2 - u 2 - p  IP IB 2 - IE IB 2 - 2u IB  0  1/2.

This implies

exp µ IB 2/2 1I IB2 2  g2/µ2 - p/µ

 2µ-p/2cp(IB)

exp 

 - 1 IB-1 2 2µ

1I( 

 g)d.

Further, by (6.1)

cp(IB)IE

exp   - 1 IB-1 2 1I(   g)d 2µ

 cp(IB)

exp



2
-

1

IB-1 2 d

2 2µ

 det(IB-1) det(µ-1IB-2 - Ip)-1/2 = µp/2 det(Ip - µIB2)-1/2

and (6.12) follows.

Now we evaluate the probability IP IB > y for moderate values of y .

48 Parametric estimation. Finite sample theory

Lemma 6.10. Let µ0 < 1  (g2/p) . With y02 = g2/µ20 - p/µ0 , it holds for any u > 0

IP IB 2 > p + u, IB2  y0  2 exp -0.5µ0(p + u) - 0.5 log det(Ip - µ0IB2) .

(6.13)

In particular, if IB2 is diagonal, that is, IB2 = diag a1, . . . , ap , then

IP IB 2 > p + u, IB2  y0

 2 exp - µ0u - 1 22

p

µ0ai + log 1 - µ0ai

i=1

Proof. The exponential Chebyshev inequality and (6.12) imply

.

(6.14)

IP IB 2 > p + u, IB2  y0

 exp - µ0(p + u) IE exp µ0 IB 2 22

1I

IB2

2



g2 µ20

-p µ0

 2 exp -0.5µ0(p + u) - 0.5 log det(Ip - µ0IB2) .

Moreover, the standard change-of-basis arguments allow us to reduce the problem to the case of a diagonal matrix IB2 = diag a1, . . . , ap where 1 = a1  a2  . . .  ap > 0 . Note that p = a1 + . . . + ap . Then the claim (6.13) can be written in the form (6.14).

Now we evaluate a large deviation probability that IB > y for a large y . Note that the condition IB2   1 implies IB2  IB . So, the bound (6.13) continues to hold when IB2  y0 is replaced by IB  y0 .
Lemma 6.11. Let µ0 < 1 and µ0p < g2 . Define g0 by g02 = g2 - µ0p . For any y  y0 d=ef g0/µ0 , it holds

IP IB > y  8.4 det{Ip - (g0/y)IB2}-1/2 exp -g0y/2 .  8.4 exp -x0 - g0(y - y0)/2 ,

(6.15)

where x0 is defined by 2x0 = g0y0 + log det{Ip - (g0/y0)IB2}.

Proof. The slicing arguments of Lemma 6.6 apply here in the same manner. One has to replace  by IB and (1 - µ1)-p/2 by det{Ip - (g0/y)IB2}-1/2 . We omit the details. In particular, with y = y0 = g0/µ , this yields

IP IB > y0  8.4 exp(-x0).

spokoiny, v.

49

Moreover, for the function f (y) = g0y + log det{Ip - (g0/y)IB2} , it holds f (y)  g0 and hence, f (y)  f (y0) + g0(y - y0) for y > y0 . This implies (6.15).
One important feature of the results of Lemma 6.10 and Lemma 6.11 is that the value µ0 < 1  (g2/p) can be selected arbitrarily. In particular, for y  yc , Lemma 6.11 with µ0 = µc yields the large deviation probability IP IB > y . For bounding the probability IP IB 2 > p + u, IB  yc , we use the inequality log(1 - t)  -t - t2 for t  2/3 . It implies for µ  2/3 that

- log IP IB 2 > p + u, IB  yc
p
 µ(p + u) + log 1 - µai
i=1
p
 µ(p + u) - (µai + µ2ai2)  µu - µ2v2/2.
i=1

(6.16)

Now we distinguish between µc  2/3 and µc < 2/3 starting with µc  2/3 . The bound (6.16) with µ0 = (u/v2)  (2/3)  µc and with u = (2vx1/2)  (6x) yields

IP IB 2 > p + u, IB  yc  2 exp(-x);

see the proof of Theorem 6.2 for the Gaussian case. Now consider µc < 2/3 . For x1/2  µcv/2 , use u = 2vx1/2 and µ0 = u/v2 . It
holds µ0 = u/v2  µc and u2/(4v2) = x yielding the desired bound by (6.16). For x1/2 > µcv/2 , we select again µ0 = µc . It holds with u = 4µ-c 1x that µcu/2 - µ2c v2/4  2x - x = x . This completes the proof.

Now we describe the value z(x, IB) ensuring a small value for the large deviation probability IP IB 2 > z(x, IB) . For ease of formulation, we suppose that g2  2p yielding µc-1  3/2 . The other case can be easily adjusted.
Corollary 6.12. Let  fulfill (6.1) with g2  2p . Then it holds for x  xc with xc from (6.11):

IP IB 2  z(x, IB)  2e-x + 8.4e-xc,

 z(x, IB) d=ef p + 2xv, x  v/18,

p + 6x

v/18 < x  xc.

(6.17)

For x > xc

IP IB 2  zc(x, IB)  8.4e-x,

zc(x, IB) d=ef yc + 2(x - xc)/gc 2.

50 Parametric estimation. Finite sample theory

6.4 Rescaling and regularity condition

The result of Theorem 6.8 can be extended to a more general situation when the condition (6.1) is fulfilled for a vector  rescaled by a matrix V0 . More precisely, let the random p -vector  fulfills for some p ◊ p matrix V0 the condition



sup log IE exp 

 IRp

V0

 022/2,

||  g,

(6.18)

with some constants g > 0 , 0  1 . Again, a simple change of variables reduces the case
of an arbitrary 0  1 to 0 = 1 . Our aim is to bound the squared norm D0-1 2 of a vector D0-1 for another p◊p positive symmetric matrix D02 . Note that condition (6.18) implies (6.1) for the rescaled vector  = V0-1 . This leads to bounding the quadratic form D0-1V0 2 = IB 2 with IB2 = D0-1V02D0-1 . It obviously holds

p = tr(IB2) = tr(D0-2V02).

Now we can apply the result of Corollary 6.12.

Corollary 6.13. Let  fulfill (6.18) with some V0 and g . Given D0 , define IB2 = D0-1V02D0-1 , and let g2  2p . Then it holds for x  xc with xc from (6.11):
IP D0-1 2  z(x, IB)  2e-x + 8.4e-xc ,

with z(x, IB) from (6.17). For x > xc IP D0-1 2  zc(x, IB)  8.4e-x,

zc(x, IB) d=ef yc + 2(x - xc)/gc 2.

Finally we briefly discuss the regular case with D0  aV0 for some a > 0 . This implies IB   a-1 and
v2 = 2 tr(IB4)  2a-2p.

This together with g2  2p yields

y2c d=ef g2/µ2c - p/µc  p/µc2,

gc

d=ef

µcyc



 p,

2xc d=ef gcyc + log det Ip - µcIB2 .

spokoiny, v.

51

7 Some results for empirical processes

This chapter presents some general results of the theory of empirical processes. Under the global conditions from Section 2 one can apply the well developed chaining arguments in Orlic spaces; see e.g. van der Vaart and Wellner (1996), Chapter ???. We follow the more recent approach inspired by the notions of generic chaining and majorizing measures due to M. Talagrand. The chaining arguments are replaced by the pilling device; see e.g. Talagrand (1996, 2001, 2005). The results are close to that of Bednorz (2006). We state the results in a slightly different form and present an independent and self-containing proof.
The first result states a bound for local fluctuations of the process U() given on a metric space  . Then this result will be used for bounding the maximum of the negatively drifted process U() - U(0) - d2(, 0) over a vicinity (r) of the central point 0 . The behavior of U() outside of the local central set (r) is described using the upper function method. Namely, we construct a multiscale deterministic function u(µ, ) ensuring that with probability at least 1 - e-x it holds µU() + u(µ, )  z(x) for all   (r) and µ  M , where z(x) grows linearly in x .

7.1 A bound for local fluctuations
An important step in the whole construction is an exponential bound on the maximum of a random process U() under the exponential moment conditions on its increments. Let d(,  ) be a semi-distance on  . We suppose the following condition to hold:

(Ed) There exist g > 0 , r1 > 0 , 0  1 , such that for any   g and ,    with d(,  )  r1

U() - U( ) log IE exp 
d(,  )

 022/2.

Formulation of the result involves a sigma-finite measure  on the space  which
is often called the majorizing measure and used in the generic chaining device; see Talagrand (2005). A typical example of choosing  is the Lebesgue measure on IRp . Let   be a subset of  , a sequence rk be fixed with r0 = diam( ) and rk = r02-k . Let also Bk() d=ef {    : d(,  )  rk} be the d -ball centered at  of radius rk and k() denote its  -measure:

k() d=ef

(d ) = 1I d(,  )  rk (d ).

Bk ()



52 Parametric estimation. Finite sample theory

Denote also

Mk

d=ef

max
 

( ) k()

k  1.

(7.1)

Finally set c1 = 1/3 , ck = 2-k+2/3 for k  2 , and define the value Q( ) by

Q( ) d=ef

 14 ck log(2Mk) = 3 log(2M1) + 3



2-k log(2Mk).

k=1 k=2

Theorem 7.1. Suppose (Ed) . If   is a central set with the center  and the radius r1 , i.e. d(, )  r1 for all     , then for   g0 d=ef 0g

log IE exp  sup U() - U() 30r1  

 2/2 + Q( ).

(7.2)

Proof. A simple change U(∑) with 0-1U(∑) and g with g0 = 0g allows for reducing the result to the case with 0 = 1 which we assume below. Consider for k  1 the smoothing operator Sk defined as

Skf ()

=

1 k (  )

f ()(d).
Bk ( )

Further, define

S0U()  U()

so that S0U is a constant function and the same holds for SkSk-1 . . . S0U with any k  1 . If f (∑)  g(∑) for two non-negative functions f and g , then Skf (∑)  Skg(∑) . Separability of the process U implies that limk SkU() = U() . We conclude that for each    

U() - U()

=

lim
k

SkU()

-

Sk

.

.

. S0U()

k

 lim
k

Sk . . . Si(I - Si-1)U() 

k .

i=1 i=1

Here k d=ef sup  k() for k  1 with

1()  |S1U() - U()|,

k d=ef |Sk(I - Sk-1)U()|,

k2

For a fixed point  , it holds

k(

)



1 k(

)

1 Bk( ) k-1()

U() - U( ) (d )(d).
Bk-1()

spokoiny, v.

53

For each   Bk-1() , it holds d(,  )  rk-1 = 2rk and U() - U( )
U() - U( )  rk-1 d(,  ) . This implies for each     and k  2 by the Jensen inequality and (7.1)

 exp rk-1 k( )



Bk( )

 Mk


 U() - U( ) (d ) (d)

exp

Bk-1()

d(,  ) k-1() k( )

 U() - U( ) (d ) (d)

exp
Bk-1()

d(,  )

k-1() ( ) .

As the right hand-side does not depend on  , this yields for k d=ef sup  k() by condition (Ed) in view of e|x|  ex + e-x

IE exp

 rk-1

k

 U() - U( ) (d )

 Mk

IE exp

  Bk-1()

d(,  )

k-1()

 2Mk exp(2/2)


(d ) (d) Bk-1() k-1() ( )

= 2Mk exp(2/2).

(d) ( )

Further, the use of d(, )  r1 for all     yields by (Ed)

and thus

IE exp  |U() - U()|  2 exp 2/2 r1

(7.3)

IE exp

 r1

|S1U()

-

U()|

1 1()

IE exp  |U( ) - U()| (d )

B1()

r1



M1 ( )

IE exp


 |U( ) - U()| r1

(d ).

This implies by (7.3) for 1  sup  |S1U() - U()|

IE exp

 r1

1

 2M1 exp 2/2 .

Denote c1 = 1/3 and ck = rk-1/(3r1) = 2-k+2/3 for k  2 . Then holds by the Ho®lder inequality; see Lemma 7.11 below:

 k=1

ck

=1

and it

log IE exp

 3r1

 k=1

k

 c1 log IE exp

 r1

k


+ ck log IE exp
k=2

 rk-1

k


 2/2 + c1 log(2M1) + ck log(2Mk)

k=2

< 2/2 + Q( ).

54 Parametric estimation. Finite sample theory

This implies the result.

7.2 A local central bound

Due to the result of Theorem 7.1, the bound for the maximum of U(, 0) over   Br(0) grows quadratically in r . So, its applications to situations with r2 Q( ) are limited. The next result shows that introducing a negative quadratic drift helps
to state a uniform in r local probability bound. Namely, the bound for the process U(, 0) - d2(, 0)/2 with some positive  over a ball Br(0) around the point 0 only depends on the drift coefficient  but not on r . Here the generic chaining arguments are accomplished with the slicing technique. The idea is for a given r > 1 to split the ball Br(0) into the slices Br+1(0) \ Br(0) and to apply Theorem 7.1 to each slice separately with a proper choice of the parameter  .

Theorem 7.2. Let r be such that (Ed) holds on Br(0) . Let also Q( )  Q for   = Br(0) with r  r . If  > 0 and z are fixed to ensure z  g0 = 0g and (z - 1)  2 , then it holds

log IP

sup
Br (0)

1 30

U(,

0)

-

 d2(, 2

0)

 -(z - 1) + log(4z) + Q.

>z

 Moreover, if z > g0 , then

(7.4)

log IP

sup
Br (0)

1 30

U(,

0)

-

 d2(, 2

0)

 -g0 (z - 1) + g20/2 + log(4z) + Q.

>z

(7.5)

Remark 7.1. Formally the bound applies even with r =  provided that (Ed) is fulfilled on the whole set   .

Proof. Denote

u(r) d=ef

1 sup
30r Br(0)

U() - U(0)

.

Then we have to bound the probability

IP sup r u(r) - r2/2 > z .
rr
For each r  r and   g0 , it follows from (7.2) that

log IE exp u(r)  2/2 + Q.

spokoiny, v.

55

The choice  = z is admissible in view of z  g0 . This implies by the exponential Chebyshev inequality

log IP r u(r) - r2/2  z  -(z/r + r/2) + 2/2 + Q = -z(x + x-1 - 1) + Q,

(7.6)

where u = /(2z) r . We now apply the slicing arguments w.r.t. t = r2/2 = zx2 . By definition, ru(r) increases in r . We use that for any growing function f (∑) and any t  0 , it holds

t+1

f (t) - t 

f (s) - s + 1 ds

t

Therefore, for any t > 0 , it holds by (7.6) in view of dt = 2z x dx

IP sup r u(r) - r2/2 > z
rr

t+1
 IP r u(r) - t  z - 1 dt
0

t+1

 2z

exp -(z - 1)(x + x-1 - 1) + Q x dx

0


 2ze-b+Q exp -b(x + x-1 - 2) x dx

0

with b = (z - 1) and t = r2/2 . This implies for b  2

IP sup r u(r) - r2/2 > z
rr


 2ze-b+Q exp -2(x + x-1 - 2) x dx
0
 4z exp{-(z - 1) + Q}

and (7.4) follows.

If

 z > g0 ,

then

select

 = g0 .

For

r  r

log IP r u(r) - r2/2  z = log IP u(r) > z/r + r/2

 -(z/r + r/2) + 2/2 + Q



-z(x

+

x-1

-

2)/2

-

  z

+

2/2

+

Q,

where u = /z r . This allows to bound in the same way as above

IP sup r u(r) - r2/2 > z  4z exp - (z - 1) + 2/2 + Q
rr
yielding (7.5).

56 Parametric estimation. Finite sample theory

This result can be used for describing the concentration bound for the maximum of (30)-1U(, 0) - d2(, 0)/2 . Namely, it suffices to find z ensuring the prescribed deviation probability. We state the result for a special case with  = 1 and g0  3 which simplifies the notation.

Corollary 7.3. Under the conditions of Theorem 7.2, it holds for x  0 with x+Q  4 :

IP

sup
Br (0)

1 30

U(,

0)

-

1 d2(, 2

0)

> z0(x, Q)

 exp -x ,

where with g0 = 0g  2

z0(x,

Q)

d=ef

  1+
1 +

 x+

Q

2

1 + 2g-0 1 2

g-0 1(x + Q) + g0/2

2

 if 1 + x + Q  g0, otherwise.

(7.7)

Proof. In view of (7.4), it suffices to check that z =

 1+ x+Q

2

ensures

z - 1 - log(4z) - Q  x.

This follows from the inequality

(1 + y)2 - 1 - 2 log(2 + 2y)  y2

with

 y = x+Q  2.

Similarly

z = 1+

1 + 2g0-1 2y2

for

y = g-0 1(x + Q) + g0/2

ensures

g0 z - 1 - g02/2 - log(4z) - Q  x

in

view

of

 z

-

1



y(1

+

2g0-1)

,

4z  (1 + 2y + 4g-0 1y)2 ,

and

y - log(1 + 2y + 4g-0 1y)  0

for y  2 and g0  3 .
 If g Q and x is not too big then z0(x, Q) is of order x + Q . So, the main message of this result is that with a high probability the maximum of (30)-1U(, 0) - d2(, 0)/2 does not significantly exceed the level Q .

7.3 A global upper function and concentration sets
The result of the previous section can be explained as a local upper function for the process U(∑) . Indeed, in a vicinity Br(0) of the central point 0 , it holds (30)-1U()  d2(, 0)/2 + z with a probability exponentially small in z . However, an extension of

spokoiny, v.

57

this result on the whole set  is only possible under some quite restrictive conditions.
This section presents one possible construction of an upper function for the process U(∑) on the complement of the local set (r) . For simplifying the notations assume that U(0)  0 . Then U(, 0) = U() . We say that u(µ, ) is a multiscale upper function for µU(∑) on a subset   of  if

IP sup sup µU() - u(µ, )  z(x)  e-x,
µM  
for some fixed function z(x) . An upper function can be used for describing the concentration sets of the point of maximum  = argmax  U() ; see Section 7.3.1 below.
For constructing such an upper function, the following condition is used which extends condition (Ed) :

(E) For any    there exists µ  M such that N(µ, ) d=ef log IE exp µU() < .

(7.8)

This condition can be used for building a simple pointwise upper function for µU() . Indeed, (7.8) implies

IE exp µU() - N(µ, ) = 1.

(7.9)

The next step is in extending this pointwise result to a uniform one. The standard approach is based on the notion of a -net which is a discrete set   providing that for any point    , there exists a point     with d(, )  . The upper function is first constructed on this discrete set   using (7.9) by increasing the pointwise bound with log N , where the covering number N is the cardinality of   . Then it is extended on the whole set   using stochastic continuity of the process U(∑) .
We apply a slightly different construction usually called pilling. Let the value r1 be fixed. Let also a measure  on  be fixed. By Bµ() we denote the ball of radius r1/µ at    , while µ() denotes its  -measure. In our results the value 1/µ() replaces the covering number N with = r1/µ . Also define the constant 1 describing the local variability of 1(∑) :

1 d=ef sup
µM

sup
 

sup
Bµ()

µ() µ()

.

(7.10)

For any fixed point  , the local maximum of the process µU over the ball Bµ() can be bounded by combining the pointwise result (7.9) and the result of Theorem 7.1 for local fluctuations of the process µU within Bµ() . To get a global bound over   , we introduce the so called penalty function tµ() which accounts for the size of the set

58 Parametric estimation. Finite sample theory

  . In the next result this function is allowed to be µ -dependent. However, in typical situations, one can apply a universal function t() ; cf. the smooth case in Section 7.4.
Remind the definition of the smoothing operator Sµ :

Sµf ()

=

1 µ()

f ()(d).
Bµ()

The next result suggests a construction of the upper function u(µ, ) . The construction involves a constant s which can be selected as the smallest value ensuring the bound
 30r1/s  g  2Q .

Theorem 7.4. Let the process U(∑) fulfill (E) and (Ed) for all ,     with 
d(,  )  r1 . If s is such that 30r1/s  g  2Q , then it holds for any x > 0

IP sup sup µU() - SµN(µ, ) - (1 + s)Sµtµ()  z1(x)  2e-x, (7.11)
µM  

where z1(x) is a linear function in x :

z1(x) d=ef (1 + s) x + log 1T + 2sQ,

and

T d=ef
µM

exp


-tµ()

(d) .
µ()

(7.12)

Proof. We bound µU() in two steps: first we evaluate Sµ µU()-N(µ, ) and then µ U() - SµU() . Convexity of the exp-function implies by the Jensen inequality

exp Sµ µU() - N(µ, ) - tµ()

 Sµ exp µU() - N(µ, ) - tµ()

(d)



exp
Bµ()

µU() - N(µ, ) - tµ()

µ()

(d)

 1

exp µU() - N(µ, ) - tµ()
Bµ()

µ()

(d)

 1

exp


µU() - N(µ, ) - tµ()

. µ()

As the right hand-side does not depend on  , the bound applies to the maximum of this expression over  . This implies in view of IE exp µU() - N(µ, ) = 1

IE exp sup Sµ µU() - N(µ, ) - tµ()
 

 1

exp -tµ()


(d) .
µ()

spokoiny, v.

59

As the sup over µ  M is not larger than the sum of the exponential terms, it holds

IE exp sup sup Sµ µU() - N(µ, ) - tµ()  1T.
  µM
This bound implies for each x > 0 with probability at least 1 - e-x

Sµ µU() - N(µ, ) - tµ()  x + log 1T ,    .

(7.13)

Now define

wµ() d=ef sup µ U() - U() .
Bµ()

With

 d=ef

 (30r1/s)  2Q ,

it

holds

by

Theorem

7.1

in

view

of

2/2  Q

log IE exp s-1wµ()  2Q.

Then

exp s-1µ U() - SµU() - Sµtµ()

= exp

s-1µ U() - U()

- tµ()

(d) µ()



exp
Bµ()

s-1wµ() - tµ()

(d) µ()

 1

exp
Bµ()

s-1wµ() - tµ()

(d) µ()

 1

exp s-1wµ() - tµ()


(d) .
µ()

As the right hand-side does not depend on  , the bound applies to the maximum of this expression over  . This implies

IE exp sup sup s-1µ U() - SµU() - Sµtµ()  1T exp(2Q).
  µM
This implies similarly to (7.13) with probability at least 1 - e-x :

s-1µ U() - SµU() - Sµtµ()  x + 2Q + log 1T ,    . Combining these two bounds yields (7.11) with probability at least 1 - 2e-x .

Remark 7.2. It is interesting to compare the uniform bound (7.11) of Theorem 7.4 and the pointwise bound (7.9): at which prise the pointwise result can be extended to a global one. The proposed construction involves two additional terms. One of them is

60 Parametric estimation. Finite sample theory
proportional to the local entropy Q and it comes from the local bound of Theorem 7.1 as the price for taking the local supremum. The second term is proportional to tµ() + log(T) with T from (7.12) and it is responsible for extending the local maximum into the global one over   .

7.3.1 Hitting probability
Let M () be a deterministic boundary function. We aim at bounding the probability that a process U() on   hits this boundary on the set   . This precisely means the probability that sup  U() - M ()  0 . A particularly interesting problem is to describe for each x > 0 the value z1(x) ensuring that

IP sup U() - M ()  z1(x)  e-x.
 
Let u(µ, ) be the multiscale upper function for U() :

IP sup sup µ U() - u(µ, )  z1(x)  2e-x;
µM  
cf. (7.11). Define

(7.14)

C(µ, ) d=ef -u(µ, ) + µM (), µ() d=ef argmax C(µ, ),
µM
C() d=ef max C(µ, ) = C(µ(), ).
µM
The studied hitting probability can be described via the value

g( ) d=ef inf C() = inf max -u(µ, ) + µM () .

 

  µM

The larger this value is, the smaller is the bound for the hitting probability. More
precisely, let a fixed x and the corresponding z1(x) in (7.14) be fixed. If for each     , the inequality µM ()  u(µ, ) holds with a properly selected µ = µ() , then the hitting probability is bounded by 2e-x .

Theorem 7.5. Suppose (7.14). If g( )  z1(x) , then

IP sup U() - M ()  0  2e-x.
 

spokoiny, v.

Proof. For each     , in view of C()  g( ) , it holds

U() - M ()  0 = µ() U() - M ()  0

 µ() U() - M () + C()  g( )

= µ()U() - u(µ(), )  z1(x)



max µ U() - u(µ, )
µ

 z1(x)

,

and the result follows from (7.14).

61

7.4 Finite-dimensional smooth case
Here we discuss the special case when  is an open subset in IRp , the stochastic process U() is absolutely continuous and its gradient U() d=ef dU()/d has bounded exponential moments.

(ED) There exist g > 0 , 0  1 , and for each    , a symmetric non-negative matrix H() such that for any   g and any unit vector   IRp , it holds

 U() log IE exp 
H ()

 022/2.

A natural candidate for H2() is the covariance matrix Var U() provided that this matrix is well posed. Then the constant 0 can be taken close to one by reducing the value g ; see Lemma 7.12 below.
In what follows we fix a subset   of  and establish a bound for the maximum of the process U(, ) = U() - U() on   for a fixed point  . We will assume existence of a dominating matrix H = H( ) such that H() H for all     . We also
assume that  is the Lebesgue measure on  . First we show that the differentiability condition (ED) implies (Ed) .

Lemma 7.6. Assume that (ED) holds with some g and H() Consider any ,     . Then it holds for ||  g

U(, ) log IE exp  H( - )

 022 . 2

Proof. Denote  =  -  ,  = ( - )/ . Then

H for     .

1

U(, ) = 

U( + t)dt

0

62 Parametric estimation. Finite sample theory

and H( - ) =  H . Now the Ho®lder inequality and (ED) yield

IE exp

U(, )  H( - )

- 022 2

= IE exp

1 0

 

U( + H

t)

-

022 2

dt



1
IE exp
0

 

U( + H

t)

-

022 2

dt  1

as required.

The result of Lemma 7.6 enables us to define d(,  ) = H( - ) so that the corresponding ball coincides with the ellipsoid B(r, ) . Now we bound the value Q( ) for   = B(r1, ) .

Lemma 7.7. Let   = B(r1, ) . Under the conditions of Lemma 7.6, it holds Q( )  c1p , where c1 = 2 for p  2 , and c1 = 2.4 for p = 1 .

Proof. The set   coincides with the ellipsoid B(r1, ) while the d -ball Bk() coincides with the ellipsoid B(rk, ) for each k  2 . By change of variables, the study can be reduced to the case with  = 0 , H  Ip , r1 = 1 , so that B(r, ) is the usual Euclidean ball in IRp of radius r . It is obvious that the measure of the overlap of two balls B(1, 0) and B(2-k+1, ) for   1 is minimized when  = 1 , and this value
is the same for all such  . Define the number ak,p by

akp,p d=ef



 B(1, 0) B(1, 0)  B(2-k+1, )

for any  with  = 1 . It is easy to see that ak,1 = 2k and ak,p decreases with p . So, Mk  2pk and

Q( ) 

1 log(21+p) + 4 33



2-k log(21+kp)

k=2

=

log 2

3 + p + 2p


(k + 1)2-k

3

log 2

= (3 + 7p) 3

 c1p,

k=1

where c1 = 2 for p  2 , and c1 = 2.4 for p = 1 , and the result follows.

7.4.1 Local central bound
Here we specify the local bounds of Theorem 7.1 and the central result of Corollary 7.3 to the smooth case.

spokoiny, v.

63

Theorem 7.8. Suppose (Ed) . For any   0g , r1 > 0 , and   

 log IE exp

sup U() - U()

30r1 B(r1,)

 2/2 + Q,

where Q = c1p .

We consider the local sets of the elliptic form (r) d=ef { : H0( - 0)  r} , where H0 dominates H() on this set: H() H0 .

Theorem 7.9. Let (ED) hold with some g and a matrix H() . Suppose that H() H0 for all   (r) . Then

IP

sup
(r)

1 30

U(,

0)

-

1 2

H0( - 0)

2

 z0(x, p)

 exp(-x),

(7.15)

where z0(x, p) coincides with z0(x, Q) from (7.7) with Q = c1p .

Remark 7.3. An important feature of the established result is that the bound in the right hand-side of (7.15) does not depend on the value r describing the radius of the local vicinity around the central point 0 . In the ideal case one would apply this result with r =  provided that the conditions H()  H0 is fulfilled uniformly over  .

Proof. Lemma 7.7 implies (Ed) with d(, 0) = H0( - 0) 2/2 . Now the result follows from Corollary 7.3.

7.4.2 A global upper function

Now we specify the general result of Theorem 7.4 to the smooth case. To make the
formulation more transparent, the matrix H() from condition (ED) is assumed to be uniformly bounded by a fixed matrix H . Let r be fixed with r2  p/2 . We aim to build un upper function for the process U(∑) on the complement of the central set (r) d=ef { : H0  r} . The penalty function t() is taken independent of µ as a logarithmic function of H0 .

Theorem 7.10. Assume (E) and (ED) be such that det(r1-1H)  det(H0) , and

with H() s be such

H for all that 30r1/s

 

  . Let 
g  2c1p

r1  1 . Given

r  p/2 , define

t() d=ef (p + 2) log H0 ,   .

Then for any x > 0 , it holds with probability at least 1 - 2Me-x for M = µM µp

µU() - SµN(µ, ) - (1 + s)t()  (1 + s)x + 2sc1p,    \ (r). (7.16)

64 Parametric estimation. Finite sample theory

Proof. For the measure Lebesgue measure  on IRp , it holds for Bµ() = B(r1/µ, ) :

1  Bµ()

= det(r1-1µH)  µp det(H0) , p p

where p is the measure of the unit ball in IRp . In particular, this measure does not depend on the location  and thus, 1 = 1 ; see (7.10). The change of variables yields

1 T = exp -t() d()
µM   (Bµ())

 µp
µM

det(H0) IRp p

H0 -p-2 1I

H0

 r d()

M =

u -p-2du  Mp/(2r2)  M.

p u r

Symmetricity arguments imply Sµ H0 = H0 and concavity of the log-function yields Sµt()  t() . Now the result (7.16) follows from Theorem 7.4 in view of Q  c1p .

7.5 Auxiliary facts

Lemma 7.11. For any r.v.'s k and k  0 such that  = k k  1

log IE exp

kk  k log IEek .

kk

Proof. Convexity of ex and concavity of x imply

 IE exp


k k - log IEek

k

 IE exp 1 

k k - log IEek

k

1 


kIE exp k - log IEek = 1.

k

Lemma 7.12. Let a r.v.  fulfill IE = 0 , IE2 = 1 and IE exp(1||) =  <  for some 1 > 0 . Then for any < 1 there is a constant C1 depending on  , 1 and only such that for  < 1
log IEe  C12/2.
Moreover, there is a constant 2 > 0 such that for all   2
log IEe  2/2.

spokoiny, v.

65

Proof. Define h(x) = ( - 1)x + m log(x) for m  0 and  < 1 . It is easy to see by a simple algebra that

Therefore for any x  0

max h(x) = -m + m log

m .

x0 1 - 

mm x + m log(x)  1x + log e(1 - ) .

This implies for all  < 1 IE||m exp(||) 

mm e(1 - ) IE exp(1||).

Suppose now that for some 1 > 0 , it holds IE exp(1||) = (1) <  . Then the function h0() = IE exp() fulfills h0(0) = 1 , h0(0) = IE = 0 , h0(0) = 1 and for  < 1 ,

h0 ()

=

IE2e



IE2e||



(1

1 -

)2 IE

exp(1||).

This implies by the Taylor expansion for  < 1 that

h0()  1 + C12/2

with C1 = (1)/ 12(1 - )2 , and hence, log h0()  C12/2 .

References
Bednorz, W. (2006). A theorem on majorizing measures. Ann. Probab., 34(5):1771≠1781.
Birg¥e, L. (2006). Model selection via testing: an alternative to (penalized) maximum likelihood estimators. Annales de l'Institut Henri Poincare (B) Probability and Statistics.
Birg¥e, L. and Massart, P. (1993). Rates of convergence for minimum contrast estimators. Probab. Theory Relat. Fields, 97(1-2):113≠150.
Birg¥e, L. and Massart, P. (1998). Minimum contrast estimators on sieves: Exponential bounds and rates of convergence. Bernoulli, 4(3):329≠375.
Boucheron, S., Lugosi, G., and Massart, P. (2003). Concentration inequalities using the entropy method. Ann. Probab., 31(3):1583≠1614.

66 Parametric estimation. Finite sample theory
Ibragimov, I. and Khas'minskij, R. (1981). Statistical estimation. Asymptotic theory. Transl. from the Russian by Samuel Kotz. New York - Heidelberg -Berlin: SpringerVerlag .
Le Cam, L. (1960). Locally asymptotically normal families of distributions. Certain approximations to families of distributions and their use in the theory of estimation and testing hypotheses. Univ. California Publ. Stat., 3:37≠98.
Le Cam, L. and Yang, G. L. (2000). Asymptotics in Statistics: Some Basic Concepts. Springer-Verlag, New York.
McCullagh, P. and Nelder, J. (1989). Generalized linear models. 2nd ed. Monographs on Statistics and Applied Probability. 37. London etc.: Chapman and Hall. xix, 511 p. .
Talagrand, M. (1996). Majorizing measures: The generic chaining. Ann. Probab., 24(3):1049≠1103.
Talagrand, M. (2001). Majorizing measures without measures. Ann. Probab., 29(1):411≠ 417.
Talagrand, M. (2005). The generic chaining. Springer Monographs in Mathematics. Springer-Verlag, Berlin. Upper and lower bounds of stochastic processes.
Van de Geer, S. (1993). Hellinger-consistency of certain nonparametric maximum likelihood estimators. Ann. Stat., 21(1):14≠44.
van der Vaart, A. and Wellner, J. A. (1996). Weak convergence and empirical processes. With applications to statistics. Springer Series in Statistics. New York, Springer.

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Localising temperature risk" by Wolfgang Karl H‰rdle, Brenda LÛpez Cabrera, Ostap Okhrin and Weining Wang, January 2011.
002 "A Confidence Corridor for Sparse Longitudinal Data Curves" by Shuzhuan Zheng, Lijian Yang and Wolfgang Karl H‰rdle, January 2011.
003 "Mean Volatility Regressions" by Lu Lin, Feng Li, Lixing Zhu and Wolfgang Karl H‰rdle, January 2011.
004 "A Confidence Corridor for Expectile Functions" by Esra Akdeniz Duran, Mengmeng Guo and Wolfgang Karl H‰rdle, January 2011.
005 "Local Quantile Regression" by Wolfgang Karl H‰rdle, Vladimir Spokoiny and Weining Wang, January 2011.
006 "Sticky Information and Determinacy" by Alexander Meyer-Gohde, January 2011.
007 "Mean-Variance Cointegration and the Expectations Hypothesis" by Till Strohsal and Enzo Weber, February 2011.
008 "Monetary Policy, Trend Inflation and Inflation Persistence" by Fang Yao, February 2011.
009 "Exclusion in the All-Pay Auction: An Experimental Investigation" by Dietmar Fehr and Julia Schmid, February 2011.
010 "Unwillingness to Pay for Privacy: A Field Experiment" by Alastair R. Beresford, Dorothea K¸bler and Sˆren Preibusch, February 2011.
011 "Human Capital Formation on Skill-Specific Labor Markets" by Runli Xie, February 2011.
012 "A strategic mediator who is biased into the same direction as the expert can improve information transmission" by Lydia Mechtenberg and Johannes M¸nster, March 2011.
013 "Spatial Risk Premium on Weather Derivatives and Hedging Weather Exposure in Electricity" by Wolfgang Karl H‰rdle and Maria Osipenko, March 2011.
014 "Difference based Ridge and Liu type Estimators in Semiparametric Regression Models" by Esra Akdeniz Duran, Wolfgang Karl H‰rdle and Maria Osipenko, March 2011.
015 "Short-Term Herding of Institutional Traders: New Evidence from the German Stock Market" by Stephanie Kremer and Dieter Nautz, March 2011.
016 "Oracally Efficient Two-Step Estimation of Generalized Additive Model" by Rong Liu, Lijian Yang and Wolfgang Karl H‰rdle, March 2011.
017 "The Law of Attraction: Bilateral Search and Horizontal Heterogeneity" by Dirk Hofmann and Salmai Qari, March 2011.
018 "Can crop yield risk be globally diversified?" by Xiaoliang Liu, Wei Xu and Martin Odening, March 2011.
019 "What Drives the Relationship Between Inflation and Price Dispersion? Market Power vs. Price Rigidity" by Sascha Becker, March 2011.
020 "How Computational Statistics Became the Backbone of Modern Data Science" by James E. Gentle, Wolfgang H‰rdle and Yuichi Mori, May 2011.
021 "Customer Reactions in Out-of-Stock Situations ≠ Do promotion-induced phantom positions alleviate the similarity substitution hypothesis?" by Jana Luisa Diels and Nicole Wiebach, May 2011.
SFB 649, Spandauer Str. 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
022 "Extreme value models in a conditional duration intensity framework" by Rodrigo Herrera and Bernhard Schipp, May 2011.
023 "Forecasting Corporate Distress in the Asian and Pacific Region" by Russ Moro, Wolfgang H‰rdle, Saeideh Aliakbari and Linda Hoffmann, May 2011.
024 "Identifying the Effect of Temporal Work Flexibility on Parental Time with Children" by Juliane Scheffel, May 2011.
025 "How do Unusual Working Schedules Affect Social Life?" by Juliane Scheffel, May 2011.
026 "Compensation of Unusual Working Schedules" by Juliane Scheffel, May 2011.
027 "Estimation of the characteristics of a LÈvy process observed at arbitrary frequency" by Johanna Kappus and Markus Reiﬂ, May 2011.
028 "Asymptotic equivalence and sufficiency for volatility estimation under microstructure noise" by Markus Reiﬂ, May 2011.
029 "Pointwise adaptive estimation for quantile regression" by Markus Reiﬂ, Yves Rozenholc and Charles A. Cuenod, May 2011.
030 "Developing web-based tools for the teaching of statistics: Our Wikis and the German Wikipedia" by Sigbert Klinke, May 2011.
031 "What Explains the German Labor Market Miracle in the Great Recession?" by Michael C. Burda and Jennifer Hunt, June 2011.
032 "The information content of central bank interest rate projections: Evidence from New Zealand" by Gunda-Alexandra Detmers and Dieter Nautz, June 2011.
033 "Asymptotics of Asynchronicity" by Markus Bibinger, June 2011. 034 "An estimator for the quadratic covariation of asynchronously observed
ItÙ processes with noise: Asymptotic distribution theory" by Markus Bibinger, June 2011. 035 "The economics of TARGET2 balances" by Ulrich Bindseil and Philipp Johann Kˆnig, June 2011. 036 "An Indicator for National Systems of Innovation - Methodology and Application to 17 Industrialized Countries" by Heike Belitz, Marius Clemens, Christian von Hirschhausen, Jens Schmidt-Ehmcke, Axel Werwatz and Petra Zloczysti, June 2011. 037 "Neurobiology of value integration: When value impacts valuation" by Soyoung Q. Park, Thorsten Kahnt, Jˆrg Rieskamp and Hauke R. Heekeren, June 2011. 038 "The Neural Basis of Following Advice" by Guido Biele, Jˆrg Rieskamp, Lea K. Krugel and Hauke R. Heekeren, June 2011. 039 "The Persistence of "Bad" Precedents and the Need for Communication: A Coordination Experiment" by Dietmar Fehr, June 2011. 040 "News-driven Business Cycles in SVARs" by Patrick Bunk, July 2011. 041 "The Basel III framework for liquidity standards and monetary policy implementation" by Ulrich Bindseil and Jeroen Lamoot, July 2011. 042 "Pollution permits, Strategic Trading and Dynamic Technology Adoption" by Santiago Moreno-Bromberg and Luca Taschini, July 2011. 043 "CRRA Utility Maximization under Risk Constraints" by Santiago MorenoBromberg, Traian A. Pirvu and Anthony RÈveillac, July 2011.
SFB 649, Spandauer Str. 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
044 "Predicting Bid-Ask Spreads Using Long Memory Autoregressive Conditional Poisson Models" by Axel Groﬂ-Kluﬂmann and Nikolaus Hautsch, July 2011.
045 "Bayesian Networks and Sex-related Homicides" by Stephan Stahlschmidt, Helmut Tausendteufel and Wolfgang K. H‰rdle, July 2011.
046 "The Regulation of Interdependent Markets", by Raffaele Fiocco and Carlo Scarpa, July 2011.
047 "Bargaining and Collusion in a Regulatory Model", by Raffaele Fiocco and Mario Gilli, July 2011.
048 "Large Vector Auto Regressions", by Song Song and Peter J. Bickel, August 2011.
049 "Monetary Policy, Determinacy, and the Natural Rate Hypothesis", by Alexander Meyer-Gohde, August 2011.
050 "The impact of context and promotion on consumer responses and preferences in out-of-stock situations", by Nicole Wiebach and Jana L. Diels, August 2011.
051 "A Network Model of Financial System Resilience", by Kartik Anand, Prasanna Gai, Sujit Kapadia, Simon Brennan and Matthew Willison, August 2011.
052 "Rollover risk, network structure and systemic financial crises", by Kartik Anand, Prasanna Gai and Matteo Marsili, August 2011.
053 "When to Cross the Spread: Curve Following with Singular Control" by Felix Naujokat and Ulrich Horst, August 2011.
054 "TVICA - Time Varying Independent Component Analysis and Its Application to Financial Data" by Ray-Bing Chen, Ying Chen and Wolfgang K. H‰rdle, August 2011.
055 "Pricing Chinese rain: a multi-site multi-period equilibrium pricing model for rainfall derivatives" by Wolfgang K. H‰rdle and Maria Osipenko, August 2011.
056 "Limit Order Flow, Market Impact and Optimal Order Sizes: Evidence from NASDAQ TotalView-ITCH Data" by Nikolaus Hautsch and Ruihong Huang, August 2011.
057 "Optimal Display of Iceberg Orders" by Gˆkhan Cebirolu and Ulrich Horst, August 2011.
058 "Optimal liquidation in dark pools" by Peter Kratz and Torsten Schˆneborn, September 2011.
059 "The Merit of High-Frequency Data in Portfolio Allocation" by Nikolaus Hautsch, Lada M. Kyj and Peter Malec, September 2011.
060 "On the Continuation of the Great Moderation: New evidence from G7 Countries" by Wenjuan Chen, September 2011.
061 "Forward-backward systems for expected utility maximization" by Ulrich Horst, Ying Hu, Peter Imkeller, Anthony RÈveillac and Jianing Zhang.
062 "On heterogeneous latent class models with applications to the analysis of rating scores" by AurÈlie Bertrand and Christian M. Hafner, October 2011.
063 "Multivariate Volatility Modeling of Electricity Futures" by Luc Bauwens, Christian Hafner and Diane Pierret, October 2011.
SFB 649, Spandauer Str. 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
064 "Semiparametric Estimation with Generated Covariates" by Enno Mammen, Christoph Rothe and Melanie Schienle, October 2011.
065 "Linking corporate reputation and shareholder value using the publication of reputation rankings" by Sven Tischer and Lutz Hildebrandt, October 2011.
066 "Monitoring, Information Technology and the Labor Share" by Dorothee Schneider, October 2011.
067 "Minimal Supersolutions of BSDEs with Lower Semicontinuous Generators" by Gregor Heyne, Michael Kupper and Christoph Mainberger, October 2011.
068 "Bargaining, Openness, and the Labor Share" by Dorothee Schneider, October 2011.
069 "The Labor Share: A Review of Theory and Evidence" by Dorothee Schneider, October 2011.
070 "The Power of Sunspots: An Experimental Analysis" by Dietmar Fehr, Frank Heinemann and Aniol Llorente-Saguer, October 2011.
071 "Econometric analysis of volatile art markets" by Fabian Y. R. P. Bocart and Christian M. Hafner, October 2011.
072 "Financial Network Systemic Risk Contributions" by Nikolaus Hautsch, Julia Schaumburg and Melanie Schienle, October 2011.
073 "Calibration of self-decomposable LÈvy models" by Mathias Trabs, November 2011.
074 "Time-Varying Occupational Contents: An Additional Link between Occupational Task Profiles and Individual Wages" by Alexandra Fedorets, November 2011.
075 "Changes in Occupational Demand Structure and their Impact on Individual Wages" by Alexandra Fedorets, November 2011.
076 "Nonparametric Nonstationary Regression with Many Covariates" by Melanie Schienle, November 2011.
077 "Increasing Weather Risk: Fact or Fiction?" by Weining Wang, Ihtiyor Bobojonov, Wolfgang Karl H‰rdle and Martin Odening, November 2011.
078 "Spatially Adaptive Density Estimation by Localised Haar Projections" by Florian Gach, Richard Nickl and Vladimir Spokoiny, November 2011.
079 "Martingale approach in pricing and hedging European options under regime-switching" by Grigori N. Milstein and Vladimir Spokoiny, November 2011.
080 "Sparse Non Gaussian Component Analysis by Semidefinite Programming" by Elmar Diederichs, Anatoli Juditsky, Arkadi Nemirovski and Vladimir Spokoiny, November 2011.
081 "Parametric estimation. Finite sample theory" by Vladimir Spokoiny, November 2011.
SFB 649, Spandauer Str. 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

