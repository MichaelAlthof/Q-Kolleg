BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2006-041
Forward and reverse representations for
Markov chains
Grigori N. Milstein* John G. M. Schoenmakers**
Vladimir Spokoiny**
* Ural State University, Ekaterinburg, Russia ** Weierstrass Institute for Applied Analysis
and Stochastics, Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Forward and reverse representations for Markov chains
G. N. Milsteinb, J.G.M. Schoenmakersa, V. Spokoinya  May 2, 2006
Keywords: transition density estimation, forward and reverse Markov chains, Monte Carlo simulation, estimation of risk AMS 2000 Subject Classification: 60J05, 60H10, 62G07, 65C05.
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 Economic Risk, and the DFG Research Center matheon in Berlin.
a) Weierstrass Institut fu¨r Angewandte Analysis und Stochastik, Berlin, Germany, {schoenma,spokoiny}@wias-berlin.de b) Ural State University, Ekaterinburg, Russia, grigori.milstein@usu.ru
1

Abstract
In this paper we carry over the concept of reverse probabilistic representations developed in Milstein, Schoenmakers, Spokoiny (2004) for diffusion processes, to discrete time Markov chains. We outline the construction of reverse chains in several situations and apply this to processes which are connected with jump-diffusion models and finite state Markov chains. By combining forward and reverse representations we then construct transition density estimators for chains which have root-N accuracy in any dimension and consider some applications.
1 Introduction
Classical density estimators for random processes such as the Parzen-Rosenblatt estimator (see for instance Silverman (1986)) suffer typically from "curse of dimensionality". I.e., when the dimension of the state space is larger, the efficiency of such estimators is rather poor. In a diffusion setting, Milstein, Schoenmakers, Spokoiny (2004) have developed a new density estimator which is based on forward and reverse simulation of the underlying diffusion process. It is shown that this forward-reverse density estimator (FRE) is basically root-N consistent in any dimension, this in contrast to the Parzen-Rosenblatt estimator which has accuracy N -1/(4+d) for dimension d.
The forward-reverse estimator has turned out to be very useful for practical applications in different areas such as risk analysis and environmental modelling. For example, in van den Berg, Heemink, Lin, Schoenmakers (2003), Spivakovskaya, Heemink, Milstein, Schoenmakers (2005), and Spivakovskaya, Heemink, Schoenmakers (2006), the FRE is applied successfully to the estimation of pollutant concentrations in small coastal water regions, which are caused by a certain calamity at another place. In the latter applications the pollutants are modelled by a diffusion process. In other areas however, for instance finance, the relevant underlying quantities are often modelled by time series, hence discrete-time Markov processes. While the analytical tools for diffusion theory are essentially connected with parabolic partial differential equations, the analytical formalism for discrete-time Markov processes is connected with integral equations. So for such processes one can not relay on diffusion theory and thus a discrete time version of the theory of forward reverse estimation in terms of integral equations is called for. This theory is provided in the present article.
For convenience of the reader we summarize the main results of Milstein, Schoenmakers, Spokoiny (2004) in Section 2. The rest of the paper is organized as follows. In Section 3 we introduce general forward representations for discrete time Markov chains. From the forward chains a family of reverse chains are derived in the spirit of Milstein, Schoenmakers, Spokoiny (2004) in Section 4. Section 5 formalizes general variance reduction for both forward and reverse chains. In Section 6 reverse chains are derived for a special class of autonomous discrete-time Markov chains and some examples are given. In Section 7 we give an application to jump-diffusion models and in Section 8 we deal with Markov
2

processes driven by finite state Markov chains. The forward-reverse density estimator for Markov chains is discussed finally in Section 9.

2 Density estimation for diffusions based on forward reverse representations
Consider a stochastic differential equation (SDE) in the It^o sense
dX = a(s, X)ds + (s, X)dW (s), t0  t  s  T, X(t) = x, (1)
where X = (X1, ..., Xd) , a = (a1, ..., ad) are d-dimensional vectors, W = (W 1, ..., W m) is an m-dimensional standard Wiener process, and  = {ij} is a d × m-matrix, m  d. It is assumed that the d × d-matrix b :=  , b = {bij}, is of full rank for every (s, x), s  [t0, T ], x  Rd. The functions ai(s, x) and ij(s, x) are assumed to be bounded and to have bounded derivatives of any order, which implies existence and uniqueness of the solution Xt,x(s), Xt,x(t) = x, t0  t  s  T , of (1), smoothness of the transition density p(t, x, s, y) of the Markov process X, and existence of all the moments of p(·, ·, ·, y). The solution of SDE (1) may be approximated by different numerical methods, see Kloeden and Platen (1992), Milstein and Tretyakov (2004).

2.1 The Parzen-Rosenblatt forward estimator (FE)

Let X¯ t,x be a numerical approximation of the process Xt,x n = 1, . . . , N , be a sample of independent realizations of

and let X¯ t,x(T

X¯nt,x(T ) , ) . Then

one may estimate the transition density p(t, x, T, y) from this sample by us-

ing standard techniques of non-parametric statistics such as the classical kernel

(Parzen-Rosenblatt) estimator. The kernel (Parzen-Rosenblatt) forward density

estimator with a kernel K and a bandwidth  is given by

p^FE (t, x, T, y)

=

1 N d

N

K

X¯nt,x(T ) - y 

,

n=1

(2)

see Devroye and Gy¨orfi (1985), Silverman (1986). For example, in (2) one could take the Gaussian kernel K(x) = (2)-d/2 exp(-|x|2/2). Here  should decrease to zero as N increases while N d  . It is well known that the quality of
density estimation strongly depends on the bandwidth  and the choice of a
suitable bandwidth is a delicate issue (see e.g. Devroye and Gyrfi (1985)). Even
an optimal choice of the bandwidth  leads to quite poor estimation quality, in
particular for large dimension d. More specifically, if the underlying density is
known to be two times continuously differentiable then the optimal bandwidth  is of order N -1/(4+d) leading to the accuracy of order N -2/(4+d) , see Scott
(1992) or Silverman (1986). For larger d, this would require a huge sample size
N for providing a reasonable accuracy of estimation. In the statistical literature
this problem is referred to as "curse of dimensionality".

3

2.2 The reverse estimator (RE)
In order to proceed with more sophisticated density estimators we introduce a reverse diffusion system for (1). We first introduce a reversed time variable s~ = T + t - s and define

a~(s~, y) = a(T + t - s~, y), ~b(s~, y) = b(T + t - s~, y),
~(s~, y) = (T + t - s~, y).

Then we introduce a vector process Y t,y,1  Rd and a scalar process Yt,y governed by the reverse time stochastic system

dY = (s, Y )ds + ~(s, Y )dW~ (s), Y (t) = y,

dY = c(s, Y )Yds,

Y(t) = 1, t0  t  s  T,

(3)

with W~ being an m-dimensional standard Wiener process and

i =

d

~bij yj

-

a~i,

j=1

c

=

1 2

d

2~bij yiyj

-

d

 

a~i yi

.

i,j=1

i=1

It is possible to construct an alternative density estimator in terms of the reverse system (3). Suppose that (Y¯mt,y, Y¯mt,y,1), m = 1, . . . , M, is an i.i.d. sample of numerical solutions of (3). Then a pure reverse estimator is given by

p^RE (t, x, T, y)

:=

1 M d

M

K

x - Y¯mt,y(T ) 

Y¯mt,y,1(T ).

m=1

(4)

In fact, the reverse estimator (4) can be obtained as a side case from the forwardreverse estimator presented below.

2.3 The forward-reverse estimator (FRE)
By combining the forward (1) and reverse (3) estimators via the Chapman Kolmogorov equation with respect to an intermediate time t, one can construct the forward-reverse estimator (see Milstein, Schoenmakers, Spokoiny (2004)),

p^FRE (t, x, T, y)

=

1 M N d

M

N
K

m=1 n=1

X¯nt,x(t) - Y¯mt,y(T ) 

Y¯mt,y,1(T ).

(5)

It is shown that the forward-reverse estimator (5) has superior properties in
comparison with density estimators based on pure forward (2) or pure reverse (4) representations. Obviously, by taking t = T and t = 0, the estimator

4

(5) collapses to the pure forward estimator (2) and pure reverse estimator (4), respectively.

For estimating a target value p by an estimator p^ we define the accuracy of the estimator by

Accuracy(p^) := (p^) := E(p^ - p)2 = Var(p^) + Bias2(p^).

(6)

Loosely speaking, for a first order kernel applied in (5), any choice of 0 < t < T , and a bandwidth choice N = O(N -1/(4+d)), the FRE has root-N (O(N -1/2)) accuracy for dimension d  4. For d > 4 root-N accuracy is lost but then the FRE accuracy order is still the square of the FE/RE accuracy order (see Table 1). Moreover, it can be shown that root-N accuracy of (5) can also be achieved for d > 4 by using higher order kernels in (5). By definition (6) it is possible to relate the "expected" accuracy of the different density estimators to the number of simulated trajectories involved. However, simulating trajectories is not the only costly issue in the density estimation. For all estimators one has to evaluate a functional of the simulated trajectories. In case of the FE and RE estimators, this functional consists of a single summation, whereas for the FRE estimator a more complicated double summation needs to be evaluated. Therefore, for a proper comparison it is better to consider the complexity of the different estimators which is defined as the required computation cost for reaching a given accuracy . For instance, naive evaluation of the double sum in (5) would require a computational cost of order O(M N ) in contrast to O(N ) for the FE and RE estimators. Clearly, such a naive approach would have a dramatic impact on the complexity of the FRE. Fortunately, smarter procedures for evaluating this double sum exist, which utilize the small support of the kernel K. For details we refer to Milstein, Schoenmakers, Spokoiny (2004) and also van den Berg, Heemink, Lin, Schoenmakers (2006).

Estimator
N Accuracy
Complexity
Compl.{FE/RE} Compl.{FRE}

FE/RE
N -1/(4+d) O(N -2/(4+d)) O( -2-d/2)

FRE d  4 N -1/d log1/d N O(N -1/2) O(| log | -2)
| log |-1 -d/2

FRE d > 4 N -2/(4+d) O(N -4/(4+d)) O(| log | -1-d/4)
| log |-1 -1-d/4

Table 1: Summary of accuracy and complexity of the forward (FE), reverse (RE), and forward-reverse (FRE) estimators.

5

3 Forward probabilistic representations for Markov chains
Consider a discrete-time Markov process (Xn, Fn), n = 0, 1, 2, ..., on a probability space (, F, P) with phase space (S, S), henceforth called Markov chain. In general we assume that S is locally compact and that S is the Borel -algebra on S. For example, S = Rd or a discrete subset of Rd. Let Pn, n  0, denote the one-step transition probabilities defined by
Pn(x, B) := P(Xn+1  B | Xn = x), n = 0, 1, 2, ..., x  S, B  S. (7)
In the case of an autonomous Markov chain all the one-step transition probabilities coincide and are equal to P := P0 = P1 = · · ·.
Let Xmn,x, m  n, be a trajectory of the Markov chain which is at step n in the point x, i.e., Xnn,x = x. The multistep transition probabilities Pn,m are then defined by
Pn,m(x, B) := P(Xmn,x  B), x  S, B  S, m  n.
Due to these definitions, Pn,n(x, B) = x(B) = 1B(x) (Dirac measure), Pn = Pn,n+1, and the Chapman - Kolmogorov equation has the following form:

Pn,m(x, B) = Pn,k(x, dy)Pk,m(y, B), x  S, B  S, n  k  m. (8)

Let us fix N > 0 and consider for 0  n  N the function

un(x) := Pn,N (x, dy)f (y) = E f (XNn,x),

(9)

where f is S-measurable and such that the mathematical expectation in (9) exists; for example, f is bounded. By the Markov property we have for 0  n < N:

un(x)

=

E

f (XNn,x)

=

E

f

(X n+1,Xnn+,x1
N

)

=

E

E

Fn+1

f

(X n+1,Xnn+,x1
N

)

=

E

E f (X )Xnn+,x1

n+1,Xnn+,x1 N

= E un+1(Xnn+,x1) = un+1(y)Pn(x, dy).

Thus, un(x) satisfies the following discrete integral Cauchy problem

un(x) = un+1(y)Pn(x, dy), n < N,

(10)

uN (x) = f (x),

(11)

and (9) is a forward probabilistic representation of its solution. In fact, the probabilistic representation (9) can be used for simulating the solution of (10)(11) by Monte Carlo.

6

We now are going to write the discrete Cauchy problem (10)-(11) in another

form, thus entailing an alternative probabilistic representation for its solution

(9).

Let us consider for n = 0, 1, ..., functions n : S × S  R+ such that the

measures

Qn(x, dy)

:=

Pn(x, dy) n(x, y)

,

x, y  S

(12)

are one-step transition functions as well, and functions hn : S × S  R such that

hn(x, y)Pn(x, dy) = 0, x  S.

(13)

Note that for arbitrary functions n : S × S  R+ the functions n(x, y) := n(x, y) Pn(x, dy )/n(x, y ) satisfy (12), and that for arbitrary functions hn :
S × S  R the functions hn(x, y) := h(x, y)- hn(x, y )Pn(x, dy ) satisfy (13). By (12) and (13), (10)-(11) can be written as

un(x) = (un+1(y) + hn(x, y))n(x, y)Qn(x, dy), n < N, uN (x) = f (x)

(14) (15)

The next theorem provides a forward probabilistic representation for the solution of a class of discrete integral Cauchy problems which covers (14)-(15).

Theorem 1 Let Pn be the one-step transition density of a Markov chain X as in (7) and let the function f : S  R be measurable and bounded. Let further
n : S × S  R and gn : S × S  R be measurable and bounded functions for n = 0, 1, 2, ... Then, the solution of the problem

wn(x) = (wn+1(z) + gn(x, z))n(x, z)Pn(x, dz), n < N, wN (x) = f (x) has the following probabilistic representation:

(16) (17)

wn(x) = E f (XNn,x)XNn,x,1 + XnN,x,1,0 ,

(18)

where (X, X , X) is an extended Markov chain in which X and X are governed by the equations

Xkn+,x1, Xkn+,x1,,

= Xkn,x, k(Xkn,x, Xkn+,x1),

Xnn,x, = ,

= Xkn,x,, + k(Xkn,x, Xkn+,x1)gk(Xkn,x, Xkn+,x1)Xkn,x, ,

(19) Xnn,x,, = ,

where n  k < N.

7

Proof. Note that Xkn,x, = Xkn,x,1 and Xkn,x,, = Xnk,x,1,0 + . Thus, for n < N, (18) may be written as

w (x) = E f (X )X + Xn+1,Xnn+,x1

n+1,Xnn+,x1 ,Xnn+,x1,1

n+1,Xnn+,x1 ,Xnn+,x1,1 ,Xnn,+x1,1,0

n NN

N

= E X E f (X )X + Xn,x,1
n+1

(Xnn+,x1 ,Xnn+,x1,1 ,Xnn,+x1,1,0 )

n+1,Xnn+,x1 N

n+1,Xnn+,x1 ,1 N

n+1,Xnn+,x1 ,1,0 N

+E Xnn,+x1,1,0

= E Xnn+,x1,1wn+1(Xnn+,x1) + Xnn+,x1,1,0

= E n(x, Xnn+,x1)wn+1(Xnn+,x1) + n(x, Xnn+,x1)gn(x, Xnn+,x1)

= (wn+1(z) + gn(x, z))n(x, z)Pn(x, dz),

and (17) is trivially fulfilled for n = N.

4 Reverse probabilistic representations

For definiteness we take S = Rd in this section, hence S = B(Rd), and assume
that the transition probabilities Pn,m(x, dy) have densities pn,m(x, y) with respect to the Lebesgue measure on (S, S). Then the representation (9) can be
written in the form

I(f ) := E f (XNn,x) = pn,N (x, y)f (y)dy.

(20)

Let the initial value  of the chain X at moment n be random with density g(x). Consider the functional

I(g, f ) =

g(x)pn,N (x, y)f (y)dxdy = Ef (XNn,).

(21)

Formally, by taking for g a -function we obtain (20) again, and by taking f to be a -function we obtain the integral

J (g) := g(x)pn,N (x, y)dx,

(22)

We now propose suitable (reverse) probabilistic representations for J(g). From the Chapman - Kolmogorov equation (8) we obtain straightforwardly the Chapman - Kolmogorov equation for densities,

pn,m(x, y) = pn,k(x, z)pk,m(z, y)dz, x, y  S, n  k  m,

(23)

where a "density" pn,n is to be interpreted as a Dirac distribution (-function). Let us fix n and N, n < N, and introduce the functions

vk(y) := g(x)pn,k(x, y)dx, n  k  N,

(24)

8

where g is an arbitrary integrable function on S, not necessarily a density. From (23) we get

vk(y) = vk-1(z)pk-1(z, y)dz, n < k  N,

(25)

vn(y) = g(y),

where pk-1 := pk-1,k denote the one-step densities. We are going to construct a class of reverse Markov chains which give a probabilistic representation for the
solution of (25); hence J(g). For this we introduce for n < k  N a reversed
time variable m = N + n - k and consider functions m : S × S  R+ such that for each m and y the function

qm(y, ·)

:=

pN+n-m-1(·, y) m(y, ·)

(26)

is a density on S. For example, one could take m independent of the second argument, and then obviously

m(y) = pN+n-m-1(z, y)dz.

(27)

We next introduce vm(y) := vN+n-m(y), and transform the problem (25) into

vm(y) = vm+1(z)m(y, z)qm(y, z)dz, n  m < N, vN (y) = g(y).

(28)

Via Theorem 1 we thus obtain a probabilistic representation of the form (18) for the solution of problem (28), hence (25) and J(g). Indeed, by taking in Theorem 1 instead of X a Markov chain Y, where Y is governed by the onestep transition probabilities Qm(y, dz) := qm(y, z)dz (hence Q instead of P ), constructing Y according to

Ykm+,y1,1 = Ykm,y,1k(Ykm,y, Ykm+,1y),

Ymm,y,1 = 1,

and taking gn  0, it follows by Theorem 1 that

m  k < N,

(29)

vm(y) = vN+n-m(y) = E g(YNm,y)YNm,y,1 ,

n  m  N,

(30)

and in particular

J (g) = vN (y) = g(x)pn,N (x, y)dx = E g(YNn,y)YNn,y,1 .

(31)

The representation (31), with Y given by (29), is a reverse probabilistic rep-
resentation due to the time reversed chain Y. Obviously, in general different
choices for the functions m give rise to different reverse representations for J (g).

9

5 Variance reduction

In this section we discuss how to obtain variance reduction in the probabilistic

representations (9) and (31). To this aim we consider the variance of the random

variable

 := f (XNn,x)XNn,x,1 + XnN,x,1,0

(32)

in Theorem 1 and prove the next theorem.

Theorem 2 Let wn, Pn,, f, n, gn as in Theorem 1. Then it holds

wk(Xkn,x)Xkn,x,1+Xnk ,x,1,0 = E(Xkn,x,Xkn,x,1,Xkn,x,1,0) and

wk+1(Xkn+,x1)Xkn+,x1,1 + Xnk+,x1,1,0 , (33)

V ar(Xkn,x,Xkn,x,1,Xnk,x,1,0) wk+1(Xkn+,x1)Xkn+,x1,1 + Xnk+,x1,1,0

(34)

= E(Xkn,x,Xkn,x,1,Xkn,x,1,0) (Xkn,x,1)2(k(Xkn,x, Xkn+,x1)wk+1(Xkn+,x1) +k(Xkn,x, Xkn+,x1)gk(Xkn,x, Xkn+,x1) - wk(Xkn,x))2.

As a consequence, if n, gn, and wk in (16)-(17) are such that

k(x, y)(wk+1(y) + gk(x, y)) = wk(x), x, y  S, n  k < N, (35)

then

wk(Xkn,x)Xkn,x,1 + Xnk,x,1,0 = f (XNn,x)XNn,x,1 + XnN,x,1,0, hence, the random variable (32) is deterministic.

n  k  N, a.s.

Proof. For k < N we may write using the abbreviation Ek := E(Xkn,x,Xkn,x,1,Xkn,x,1,0) :

= Xkn,x,1Ek

wk(Xkn,x)Xkn,x,1 + Xnk,x,1,0

f (XNk,Xkn,x )XNk,Xkn,x,1

+

Xk,Xkn,x ,1,0
N

+ Xnk,x,1,0

= E f (X )X + Xk

k,Xkn,x

k,Xkn,x ,Xkn,x,1

k,Xkn,x ,Xkn,x,1 ,Xkn,x,1,0

NN

N

= E f (X )X + Xk

k+1,Xkn+,x1

k+1,Xkn+,x1 ,Xkn+,x1,1

k+1,Xkn+,x1 ,Xkn+,x1,1 ,Xkn+,x1,1,0

NN

N

= Ek

Xkn+,x1,1 E k+1

f

(X k+1,Xkn+,x1
N

)X k+1,Xkn+,x1
N

,1

+

Xk+1,Xkn+,x1 ,1,0
N

+ Xnk+,x1,1,0

= Ek wk+1(Xkn+,x1)Xkn+,x1,1 + Xnk+,x1,1,0 ,

thus proving (33). Next, by (33) we have

V ar(Xkn,x,Xkn,x,1,Xnk,x,1,0) wk+1(Xkn+,x1)Xkn+,x1,1 + Xnk+,x1,1,0 = Ek wk+1(Xkn+,x1)Xkn+,x1,1 + Xkn+,x1,1,0 - wk(Xkn,x)Xkn,x,1 - Xnk,x,1,0 2 = Ek Xkn,x,1 2 (k(Xkn,x, Xkn+,x1)wk+1(Xkn+,x1)
+k(Xkn,x, Xkn+,x1)gk(Xkn,x, Xkn+,x1) - wk(Xkn,x))2,

10

hence (34). Let us now go back to the with (9) equivalent Cauchy problem (14)-(15).
The solution of this problem has a probabilistic representation according to Theorem 1. Spelling it out, we have

un(x) = E f (XNn,x)XNn,x,1 + XnN,x,1,0 ,

(36)

where X and X are governed by the equations

Xkn+,x1,1 Xnk+,x1,1,0

= Xkn,x,1k(Xkn,x, Xkn+,x1),

Xnn,x,1 = 1,

= Xnk,x,1,0 + k(Xkn,x, Xkn+,x1)hk(Xkn,x, Xkn+,x1)Xkn,x,1,

Xnn,x,1,0 = 0,

n  k < N. By Theorem 2 the variance of this probabilistic representation vanishes if

k(x, y)(uk+1(y) + hk(x, y)) = uk(x), n  k < N.

(37)

In principle, (37) may be satisfied while (12) and (13) hold. Indeed, if the
functions k satisfy (12), then the hk obtained by solving (37) satisfy (13). Vice versa, if the functions hk satisfy (13) then the k obtained by solving (37) satisfy (12). In general the exact solution uk is not known of course, so it will be not possible to choose k and hk such that  in (32) is true deterministic. However, if we have a good approximation uk of uk, n  k  N, at hand, there are possibilities for variance reduction. To formalize the idea of an approximation,
we assume that uk is a known solution of the problem

uk(x) = uk+1(y)Pk(x, dy) + gk(x), n  k < N,

(38)

uN (x) = f (x),

where f - f and the gk are close to zero in some sense. Importance sampling. Let us assume that both f and its approximation f
are positive, and that the approximate solution uk is for all k positive as well. By then taking hk  0 for all k, and

k(x, y)

=

uk

(x) - gk(x) uk+1(y)

,

(39)

(12) holds and we may expect that the variance of (32) will be close to zero. If f does not satisfy f > 0 but is bounded from below we may shift the problem by choosing a constant C such that f + C > 0, and then consider (9) with f replaced by f + C.
Control variates. By taking k  1 for all k, and

hk(x, y) = uk(x) - gk(x) - uk+1(y),

(13) holds and again we may expect that the variance of (32) will be close to zero.

11

Combined variance reduction. Importance sampling and control variates
can be combined in the following way. Assume that functions k > 0 may be identified such that (12) holds. Then by taking

hk(x, y)

=

uk(x) - gk(x) k(x, y)

- uk+1(y),

(13) holds and we may expect that the Monte Carlo estimator for un(x) corresponding to (36) has low variance.
For the reverse probabilistic representation (29)-(31) of the solution of (28)
analogue variance reduction methods apply. To be more specific, let m and hm be such that

Qm(y, dz) := Qm(y, dz)/m(y, z) = qm(y, z)dz/m(y, z) are one step transition probabilities (see (26)), and

(40)

hm(y, z)m(y, z)Qm(y, dz) = 0

(41)

for all y  S. Then with m := mm (28) may be equivalently written as the integral Cauchy problem

vm(y) = (vm+1(z) + hm(y, z))m(y, z)Qm(y, dz), n  m < N,
vN (y) = g(y)
for which Theorem 1 gives a probabilistic representation of its solution. According to Theorem 2 the variance of the corresponding random variable (32) in this probabilistic representation vanishes when
m(y, z)(vm+1(z) + hm(y, z)) = vm(y), y, z  S, n  m < N.
Based on an approximate solution of Cauchy problem (28) of the form,

vm(y) = vm+1(z)m(y, z)Qm(y, dz) + am(y), n  m < N, (42)

vN (y) = g(y),

we may apply importance sampling, control variates, or a combination of both

as for the forward representation above. For example, if m is such that (40)

holds, then

hm(y, z)

=

vm(y) - am(y) m(y, z)

-

vm+1(z)

satisfies (41) and may lead to a variance reduced representation.

Concluding we may say that Theorem 2 provides variance reduction methods as combinations of importance sampling and control variates and thus can be regarded as a discrete time version of Th. 4.2 in Milstein and Schoenmakers (2002) and Th. 2.1 in Milstein, Schoenmakers, Spokoiny (2004).

12

6 Reversing autonomous Markov chains
In this section we study Markov chains in the state space S = Rd with autonomous one step transition density p(x, y) (p does not depend on the step number due to autonomy). If the density p(x, y) and the integral function

(y) := p(x, y)dx, y  Rd,

are known, we can define a Markov chain Y with one step transition density

q(y, z)

=

p(z, y) (y)

(43)

and then give for the solution of (24) the following reverse probabilistic representation (see (29)-(31)):

vk(y) = E g(YNN+n-k,y)YNN+n-k,y,1 ,
YNN++nn--kk,y = y, n  k  N, YrN++1n-k,y,1 = YrN+n-k,y,1(YrN++1 n-k,y), N + n - k  r < N, YNN++nn--kk,y,1 = 1.

(44)

A large class of autonomous Markov chains can be written in the form

Xn+1 = A(Xn, n+1), n = 0, 1, 2, ...,

(45)

where n, n = 1, 2, ..., are i.i.d. random variables with density  in Rd. Let us assume that the map A : Rd × Rd  Rd is continuously differentiable and such that there exists a continuously differentiable inverse  with

A(x, (x, z)) = z.

(46)

For any bounded measurable f the one-step transition density p(x, y) satisfies

f (y)p(x, y)dy = E f (Xnn+,x1) = Ef (A(x, n+1))

(47)

=

f (A(x, ))()d =

f (y)((x, y))

(x, y) y

dy.

Hence, from (47) it follows that

p(x, y) = ((x, y))

det

(x, y

y)

.

(48)

According to Section 2, a reverse chain is identified by choosing a function  : Rd × Rd  R+, such that

q(y, z)

:=

p(z, y) (y, z)

(49)

13

is a density in z for fixed y. Of course there are infinitely many possible possibilities. The choice (43), for instance, gives the one-step density

q(y, z) =

((z, y))

det

(z,y) y

.

((x, y))

det

(x,y) y

dx

When there exists in addition a continuously differentiable inverse  such that

A((z, y), y) = z,

(50)

we may consider the "physically reversed" chain

Yn+1 = (Yn, n+1), n = 0, 1, 2, ...,

(51)

where the sequence n is an i.i.d copy of the sequence n, hence
A(Yn+1, n+1) = A((Yn, n+1), n+1) = Yn.
The chain (51) has a one-step density of the form (49) for a particular choice of . Indeed, let q(y, z) be the one-step transition density for Yn. Then, by noting (y, (z, y)) = z and writing (y, z) := (z, y), we have similar to (48),

q(y, z) = ((y, z))

det

(y, z

z)

= ((z, y))

det

(z, z

y)

=

p(z,

y)

|det |det

(z, (z,

y)/z| y)/y|

= p(z, y)

det

A x

(z,

(z,

y))

.

Thus, by taking for all k,

k(y, z) := (y, z) :=

det

A x

(z,

(z,

y))

-1

(52)

in the construction of Section 2 and applying Theorem 1, we obtain as in (30) a probabilistic representation for the solution of

um(y) = vN+n-m(y) = g(x)pn,N+n-m(x, y)dx, n  m  N,

(hence the integral (24)) of the form

vk(y) = E g(YNN+n-k,y)YNN+n-k,y,1 ,

n  k  N,

(53)

YrN++1 n-k,y = (YrN+n-k,y, r+1), N + n - k  r < N,

YrN++1n-k,y,1

=

YrN +n-k,y,1

det

A x

(YrN++1 n-k,y

,

(YrN++1 n-k,y

,

YrN

+n-k,y

))

-1

.

14

Example 3 Let A in (45) be of the form, A(x, ) = B(x) + C(x), B : Rd  Rd, C : Rd  Rd×d,   Rd.

(54)

In fact, such a form A arises when discretizing a diffusion SDE and then exchang-
ing Wiener increments with some i.i.d. system of random variables (n)n=1,2,..., which reflect certain features (for example heavy tails) of a problem under con-
sideration. As a special case we consider the Markov chain

Xn+1 = A(Xn, n+1) = BXn + Cn+1, B, C  Rd×d,

(55)

which may be regarded as a discrete Ornstein-Uhlenbeck process under an i.i.d.
noise sequence n, n = 1, 2, . . . , of not necessarily Gaussian random variables. Let us suppose that B and C are invertible. Then,  and  in (46) and (50)
exist with

(x, z) = C-1(z - Bx), (z, y) = B-1(z - Cy),

and as a reverse chain we may take

Ykm+,1y = B-1(Ykm,y - Ck+1),

Ymm,y = y,

Ykm+,y1,1

=

Ykm,y,1 |det B|

,

m  k < N,

Ymm,y,1 = 1,

n  m  N.

In the case where the n are Gaussian the chains X and Y are Gaussian as well and all characteristics of X and Y can be computed analytically. However,
this analytical tractability is generally lost when X is governed by (55) with
non-Gaussian n, for example i.i.d. copies of some heavy tailed distribution.

Example 4 Consider a discrete Black-Scholes model

Xni +1 = Ai(Xn, ) := Xni exp µi + (i) n+1 , µi  R, i  Rd, X0i > 0, i = 1, ..., d,
where the i and n are to be interpreted as column vectors in Rd, and the d × d matrix  := [1, . . . , d] is assumed to be invertible with inverse -1. Then,  and  in (46) and (50) exist, where

d

i(x, z) =

(-1)ik (-µk - ln xk + ln zk) ,

k=1

i(z, y) = zi exp -µi - (i) y .

Note that Ai(x, y)/xk = ik exp µi + (i) y , and so after a little algebra

15

we obtain a reverse chain given by

Yki+m1,y = i(Yk, k+1) = Ykim,y exp -µi - (i) k+1 ,

Ymim,y = yi, i = 1,..., d,



Ykm+,y1,1

=

Ykm,y,1 exp -µ

1-

i,j

ij j (Ykm+,1y,

Ykm,y )

=

Ykm,y,1

d i=1

Yki+m1,y Yki m,y

= Ykm,y,1 exp -µ 1 - 1T k+1 ,

Ymm,y,1 = 1,

n  m  N,

m  k < N,

according to (53). For Gaussian noise, both X and Y are log-Gaussian Markov chains, hence analytically tractable like in the previous example. But also here the analytical tractability is lost when non-Gaussian noise is considered.

Example 5 Let us consider the following stochastic volatility model:

Xn+1 = Xn + f (Vn)n+1, Vn+1 = c + g(Xn) + n+1,
where c > 0, f, g : R  R are smooth and invertible functions with continuous non-zero derivatives, and (n, n)n=1,2,... are i.i.d. random variables. Hence,
A(x, v, , ) = [x + f (v), c + g(x) + ]

and according to (46), (50), we solve 1, 2 from

x + f (v)1 = r c + g(x) + 2 = s,

yielding

1(x, v, r, s)

=

r-x f (v)

,

f (v) = 0,

2(x, v, r, s) = s - g(x) - c,

and 1, 2 from

1 + f (2) = y, c + g(1) +  = w,

yielding

1(y, w, , ) = ginv(w -  - c) 2(y, w, , ) = f inv(-1y - -1ginv(w -  - c)),

 = 0.

16

Thus, as a reverse chain we may take

Ykm+,1y,w = ginv(Wkm,y,w - k+1 - c),

Ymm,y,w = y

Wkm+,1y,w = f inv(k-+11Ykm,y,w - k-+11Ykm+,1y,w),

Wmm,y,w = w

Ykm+,y1,w,1 =

g

Ykm,y,w,1 (Ykm+,1y,w)f (Wkm+,1y,w)

,

Ymm,y,w,1 = 1

n  m  N.

Note that the inverse (r, s)  (x, v, r, s) exist only for f (v) = 0. This cor-
responds to the fact that the one step transition density of X does not exist
when f (Vn) = 0. On the other hand the inverse (y, w)  (y, w, , ) exists only for  = 0. This means that the construction of Yk+1 breaks down for a draw k+1 = 0. However, since the random variables (n, n) are assumed to have a density, and f = 0, both k+1 = 0 and f (Vn) = 0 are events of probability zero.

7 Reversing the jump chain of a jump-diffusion

We consider a pure jump process1 Jt, t  0, in R, with jump times 0 < 1 < 2 < · · ·, where k+1 - k are i.i.d. according to an exponential distribution with parameter , 0 := 0, J0 := 0, and where the jumps are i.i.d. according to a density  on R. Hence

Jt =

Jk

k: k+1t

with Jk := Jk+1 - Jk being i.i.d distributed with density . The process Jt is piece-wise constant and is continuous from the right with limits from the left (c.r.l.l.). The solution of the SDE

dXt = µ(Xt-, Jt-)dt + (Xt-, Jt-)dWt + (Xt-, Jt-)dJt,

X0 = x0,

for smooth functions µ,  : R × R  R,  : R × R  R, with a from J indepen-
dent Wiener process Wt, is generally called a jump diffusion. The process Xt is c.r.l.l. as well and at jump times we have

k+1

Xk+1 = Xk +

µ(Xs, Jk )ds + (Xs, Jk )dWs

k

+(Xk+1-, Jk )Jk ,

We now consider the autonomous Markov chain X in R3 defined by Xk =

(k, Xk , tivation,

Jwke

), k = 0, imagine

1, ..., that

and its associated reverse we consider a process of

representations. As a which only its jumps

moand

jump times are of importance. For example, a jump might be connected with

some default event and an insurance company is concerned with the occurrence

of the10th default, since she then has to pay money due to a certain insur-

ance contract. If the actual default time of the 10th default is irrelevant for

1From now on we rather denote time parameters by subscripts.

17

the insurance company, one could only consider the autonomous Markov chain

(Xk , latter

Jchka)i,nkis=les0s,

1, .... In many cases tractable, however.

the

one-step

transition

density

of

the

Let a,x(t, ·) be the transition density in R of the process Uta,x defined as

solution of the SDE

dUt := µ(Ut, a)ds + (Ut, a)dWt, t  0, U0 = x, and let (a, b, y) be the solution of the equation

 + (, a)(b - a) = y.

Then given k = , X = x, J = a, hence Xk = (, x, a), the one-step density of Xk+1 = (k+1, Xk+1 , Jk+1 ) in (, y, b),  < , is given by
p(, x, a, , y, b) = e-(-)a,x( - , (a, b, y))(b - a),  < . (56)

Clearly, the chain X is autonomous. By taking the integral

(, y, b) = =


e-(-)d dx a,x( - , (a, b, y))(b - a)da

0 
e- d

dx

a,x(, (a, b, y))(b - a)da,

(57)

0

we obtain from (43) the one-step transition density

q(, y, b, , z, c)

:=

p(, z, c, , y, b) (, y, b)

=

e-(

-)c,z

( - (

, (c, , y, b)

b,

y))

(b

-

c)

,

 < , (58)

and the corresponding reverse Markov chain (Y, Y) in R3 × R, where Yk := (k, Zk, Ck) is governed by the one-step density (58), with
mm, ,y,b = , Zmm, ,y,b = y, Cmm, ,y,b = b,
and Y satisfies
Ykm+,1,y,b,1 = Ykm, ,y,b,1(km, ,y,b, Zkm, ,y,b, Ckm, ,y,b), Ymm, ,y,b,1 = 1, m  k < N,
(see (29)-(31) and (44)). As a more particular case we consider the autonomous process

(59) (60)

dXt = µ(Xt)dt + (Xt)dWt + dJt,

(61)

where the diffusion component of X has transition density x(t, ·), which is independent of a. Then, Xk := (k, Xk ), k = 0, 1, ..., is an autonomous Markov chain itself, with one step transition density

p(, x, , y) = e-(-) x( - , y - u)(u)du,  < . (62)

18

Thus, according to (57), (58), we obtain a reverse chain Y with one step tran-

sition density

q(, y, , z)

=

p(, z, (,

, y) y)

,

 < ,

(63)

where


(, y) = de- dx x(, y - u)(u)du.
0

(64)

Example 6 Consider the jump-diffusion

dXt = ( - Xt)dt + dWt + dJt, for which the diffusion component

dUt = ( - Ut)dt + dWt

is a mean reverting Gaussian process with transition density

(t, x, u) =

1 exp 2(1 - e-2t)/

-

(

+ (x - )e-t - 2(1 - e-2t)/

u)2

,

and so (62) becomes

p(, x, , y)

=

e-( -)

×

2(1 - e-2(-))/

×

exp

-

(

+

(x - )e-(-) + u - 2(1 - e-2(-))/

y)2

(u)du,

 < .

It is easy to see that x(t, u)dx = et, hence (64) becomes,

(, y) = =


de- dx (, x, y - u)(u)du

0


de- e
0

(u)du

=



e(-) - -

1

,

and the reverse transition density (63) is thus given by

q(, y, , z) =

( - ) e-(-)

×

e(-) - 1 2(1 - e-2(-))/

×

exp

-

(

+

(z - )e-(-) + u - 2(1 - e-2(-))/

y)2

(u)du,

 < .

(65)

The dynamics (58)-(60) thus collapses to the chain Yk = (k, Zk) governed by (65) with

mm, ,y = , Zmm, ,y,b = y,

19

and scalar process Y satisfying

Ykm+,1,y,1 = Ykm, ,y,1(km, ,y, Zkm, ,y),

Ymm, ,y,b,1 = 1,

m  k < N.

Finally we note that for a variety of jump densities , (65) can be expressed in close form (for instance when v is Gaussian).

8 Reversed representations for ODE processes driven by continuous time Markov chains

To complete the picture we describe in this section the reversion of processes
obtained as the solution of an ordinary differential equation which is driven by
a continuous time Markov chain with finite state space. Suppose we are given a
regular Markov chain (Xt)t0 on a probability space (, F , P) with state space S = {x1, ..., xm}. In connection with the chain X we consider a system of ordinary differential equations

dY ds

= a(X, Y ),

(66)

where Y = [Y 1, ..., Y d] , and a : S × Rd (x, y)  [a1(x, y), ..., ad(x, y)]  Rd.

We assume the functions aij(y) := ai(xj, y), i = 1, ..., d, j = 1, ..., m, have bounded continuous derivatives. Let



-q1 q12 ... q1m

Q = 

q21 .

-q2 ... ..

q2m .

 ,

qm1 qm2 ... -qm

be the infinitesimal generator matrix of the chain X with qij  0, i = j, and

qij = qi.
j=i

(67)

It is well known that the infinitesimal generator of the Markov process (X, Y ) generated by the system (66) is given by

Af (xi, y) =

d

ak

(xi

,

y)

f yk

(xi,

y)

-

qif

(xi

,

y)

+

qijf (xj, y),

k=1

j=i

i = 1, ..., m, y  Rd

(68)

(see for example Milstein & Repin (1969)). In particular, for some smooth function f (x, y) with bounded derivative with respect to y, the functions ui(t, y) :=

20

Ef (Xxi (t), Y xi,y(t)) satisfy the hyperbolic system

ui t

(t,

y)

=

d

ak

(xi

,

y)

ui yk

(xi,

y)

-

qiui

+

qij ui,

k=1

j=i

ui(0, y) = f (xi, y) i = 1, ..., m, y  Rd.

By considering the semigroup generated by the extended Markov process (X, Y, Y) on S × Rd × R, where Y is defined by

dY xi,y, dt

= b(X, Y )Yxi,y, ,

Y0xi,y, = ,

for some bounded real valued function b(x, y) in S × Rd with continuous derivatives in y, it follows that the functions

t
vi(t, y) := E f (Xtxi , Ytxi,y) exp( b(Xsxi , Ysxi,y)ds),
0
i = 1, ..., m, y  Rd, t > 0,

(69)

satisfy the more general hyperbolic system

vi t

=

d

ak (xi ,

y)

vi yk

+

b(xi, y)vi

-

qivi

+

qij vj

k=1

j=i

ui(0, y) = f (xi, y), i = 1, ..., m, y  Rd.

(70)

So, (69) is a probabilistic representation for the solution of (70) which may be
used for evaluating the vi(t, y) in (70) by Monte Carlo simulation. Let an initial distribution of (X, Y ) be given by

P {X0 = xi, Y0  H} = i(y)dy
H

(71)

for i = 1, ..., m, with i(y) being a density on Rd. Suppose further that the i(y)ak(xi, y) have continuous partial derivatives (iaik)/yk and that the integrals

Ii =

Rd

d k=1

 (i aik ) yk

(y)dy,

i = 1, ..., m,

exist. Then for each i = 1, ..., m, the function i(t, y) defined by the property

i(t, y)dy = P({Xt = xi, Yt  H}),
H
satisfies the forward Kolmogorov equation

i t

=

-

d

(iaki ) yk

(y)

-

qii

+

qjij

k=1

j=i

(72)

21

with initial condition

i(0, y) = i(y) = (xi, y), i = 1, ..., m.

(73)

In fact i(t, ·)/P({X(t) = xi}) is the conditional density of Y at time t > 0, given X(t) = xi. In order to obtain a probabilistic representation for i(t, y) we will cast the system (72) into the form of (70):

where

i t

=

-

d

ak

(xi,

y)

i yk

+

c(xi, y)i

- qii

+

qij j ,

k=1

j=i

(74)

qij = qji, i = j,

qi =

qij =

qji,

j=i j=i

c(xi, y) = -

d

 ak (xi , yk

y)

+

qi

-

qi.

k=1

Thus, for the solution of (72) we obtain a probabilistic representation

(75)

t
i(t, y) = E(Xtxi , Ytxi,y) exp( c(Xsxi , Ysxi,y)ds),
0

(76)

via a reversed process (X, Y ), where X is a Markov chain with generator matrix Q := {qij} with qii = -qi, and Y  is governed by the equation

dY  ds

=

-a(X, Y ).

(77)

Note that the representation (76) for the solution of the problem (72)-(73) holds for any  with bounded derivatives of first order, hence not only for densities.
Some bibliographical notes. The first probabilistic representation of solutions for hyperbolic equations (for telegraph equation) goes back to M. Kac. Sufficiently general systems of ordinary differential equations driven by a Markov chain were considered in [6]. The detailed description of the process (X, Y ) was done in Milstein & Repin (1969). In a lot of papers such processes were treated in connection with random evolution (see, for instance, Griego & Hersch (1969), Hersch & Papanicolaou (1972) and references therein). In all these papers the process X does not depend on the state of Y. The interaction of general processes X and Y is considered in Milstein (1972). Instead of the system of ordinary differential equations (66) it is possible to examine a system of stochastic differential equations interacting with a Markov chain as well. Both Cauchy problems and boundary value problems for systems of partial differential equations arising in connection with interacting Markov processes are considered in Milstein (1978).

22

9 Forward-reverse transition density estimation with applications

In this section we describe for a Markov Chain (7) an efficient procedure for
estimating the transition density pn,m(x, y) by a forward-reverse probabilistic representation. The procedure below is in fact a discrete-time version of the
method developed in Milstein, Schoenmakers, Spokoiny (2004) for continuous-
time processes given by an Ito SDE. Thus, let us take x, y and n, m fixed and
concentrate on the problem of estimating pn,m(x, y). If m = n + 1 this is a one step transition density which is assumed to be given. Therefore we assume
m - n 1 and, in the spirit of Milstein, Schoenmakers, Spokoiny (2004), for some fixed k with n < k < m we consider the Chapman - Kolmogorov
equation

pn,m(x, y) = pn,k (x, z)pk,m(z, y)dz

(78)

and observe by Section 4 that for hx(z) := pn,k (x, z) (78) has a probabilistic

representation

pn,m(x, y) = hx(z)pk,m(z, y)dz = E hx(Ymk,y)Ymk,y,1 ,

where (Y, Y) is constructed as in (30). We next consider hx to be a ParzenRozenblatt estimator for the density function z  hx(z). Hence,

hx(z)

=

pn,k (x, z)

=

1 Ld

L

K(

Xkn,x(l) 

-

z

),

l=1

where K is some kernel,  is a bandwidth parameter, and Xkn,x(l) are independent realisations of Xkn,x for l = 1, ..., L. By replacing pn,k in (78) by its estimator h we obtain

pn,m(x, y) := hx(z)pk,m(z, y)dz = E hx(Ymk,y)Ymk,y,1 ,

and completely similar to a proof in Milstein, Schoenmakers, Spokoiny (2004) it follows that the estimator

pn,m(x, y)

:=

1 R

R

hx (Ymk(r,y) )Ymk(,ry),1

r=1

=

1R LRd

L

K(

Xkn,x(l)

- 

Ymk(r,y)

)Ymk(,ry),1,

r=1 l=1

(79)

with (Ymk(r,y), Ymk(,ry),1) being independent realisations of (Ymk,y, Ymk,y,1) for r = 1, ..., R, is a root-N consistent estimator for the target density pn,m(x, y). For

23

a detailed study of the properties of the forward-reverse density estimator (79) we refer to Milstein, Schoenmakers, Spokoiny (2004). Below we consider some applications of the forward-reverse estimator (79).

Estimating the probability of visiting a bounded region

Let µG be a probability measure concentrated on G. Let y1, ..., yR be i.i.d. drawings from µG. Then the estimator

pn,m(x, µG)

:=

1 LRd

L

R

K( Xkn,x(l)

- 

Ymk(r,y)r

)Ymk(,ry)r ,1

l=1 r=1

(80)

has expectation



Epn,m(x,

µG)

:=

EE



1 LRd

L

R

K(

Xkn,x(l)

- 

Ymk(r,y)r

)Ymk(,ry)r ,1 |y1 ,

...,

yM



l=1 r=1



=

E

1 R

R

E



1 Ld

L

K(

Xkn,x(l)

- 

Ymk(r,y)r

)Ymk(,ry)r ,1 |yr 

r=1

l=1

=: E

R

1 R

(pn,m

(x,

yr

)

+

(, yr))

r=1

= µG(dy)pn,m(x, y) + µG(dy) (, y)

=: µG(dy)pn,m(x, y) + (, µG),

since the bias of the FRE is independent of L and R. If the kernel K is of sufficient order (for d  4 first order is enough) and the region G is bounded, it follows from Section 6 of Milstein, Schoenmakers, Spokoiny (2004) that (, µG) = O(2), and moreover if (R = L),

Var pn,m(x, µG)



C L

+

o(

1 L

),

L  ,

where C is a constant. Suppose G is some bounded (Borel) region in Rd with small probability to
be visited by the chain X in m - n steps when X starts from x at time n. Hence Pn,m(x, G) := G pn,m(x, y)dy is small. In such a situation one could estimate Pn,m(x, G) in principle with root-N accuracy using a standard Monte Carlo
estimator, IG say, for the probabilistic representation (20), where f is taken to be the indicator of G. However, the relative accuracy VarI^G/Pn,m(x, G) of
this estimator is equal to (Pn-,m1 (x, G) - 1)/N . Hence the relative accuracy
is root-N, but with a large order coefficient when Pn,m(x, G) is small. The FRE estimator (80) mends this problems. Indeed, take µ to be the uniform

24

distribution on G, then (80) yields an estimator for Pn,m(x, G)/(G) (with  denoting the Lebesgue measure on Rd) with accuracy of C/N + 2(, µG).
Hence, (G)pn,m(x, µG) is an estimator for Pn,m(x, G) with relative accuracy (G)Pn-,m1 (x, G) C/N + 2(, µG). Since Pn,m(x, G)/(G) can be interpreted as the average density over the region G, the root-N coefficient does not explode when, for instance, the density pn,m(x, y) is continuous and positive for all y.
The problem of estimating the probability of visiting a critical region at a certain time has many applications in the area of environmental modelling, e.g, see Spivakovskaya et al. (2005), van den Berg et al. (2006).

Probability of visiting an unbounded domain

If the domain G is unbounded but such that it does not intersect with a certain
sphere, we may map G to the inside of this sphere and then work with the image of the chain X under this map. Spelling it out, let G  {x  Rd : |x - x0| > r0} for some x0  Rd and r0 > 0, and let

S

:

x



x0

+

|x

r02 - x0|2

(x

-

x0),

x  Rd,

x = x0,

be the spherical inversion with respect to the sphere of radius r0 with center x0. The map S is a bijective transformation on Rd\{x0} with inverse S-1 = S, which maps {x  Rd : |x - x0| > r0} onto {x  Rd : 0 < |x - x0| < r0}. Let us define Umn,u := S(Xmn,S-1(u)), u = x0. Then, clearly,

Pn,m(x, G) = P (Xmn,x  G) = P (S(Xmn,x)  S(G)) =: P (Umn,S(x)  S(G)), x = x0,

with S(G) := {S(x)  Rd : |x|  G} being bounded. We so can apply our
forward-reverse methodology to the Markov chain U and the region S(G). For
this we need to find the one-step transition density of the chain U, denoted by pUn . It holds,

pUn (u, v)dv := P (Unn+,u1  A) = P (Xnn+,S1-1(u)  S-1(A))
A

=

pn(S-1(u), y)dy =
S -1 (A)

pn(S-1(u), S-1(v))
A

det

S-1 v

dv,

u = x0,

by the standard transformation theorem. We thus yield,

pUn (u, v) = pn(S-1(u), S-1(v))

det

S-1 v

= pn(S(u), S(v))

det

S v

,

u = x0.

Estimating Value at Risk

Let L(z) be the loss (in absolute value) of a portfolio as function of the asset position vector z = Xmn,x, and consider Ga := {z  Rd : L(z) > a} for a > 0.

25

Then Ga is typically unbounded but may be contained in the complement of some sphere, so that we may use the above transformation. The value of a such that Pn,m(x, Ga) = , where  is a given quantile, e.g. 5%, is called the % Value at Risk.
Generally, straightforward Monte Carlo evaluation of the Value at Risk of a large portfolio is very time consuming since one needs many sample trajectories to generate a reliable number in a certain critical region. A forward-reverse approach may therefore be considered as an elegant solution.
References
[1] van den Berg, E., Heemink, A.W., Lin, H.X., Schoenmakers, J.G.M., Probability density estimation in stochastic environmental models using reverse representations. Serra, 20, (1-2) (2006), 126­139.
[2] Bretagnolle, J. and Carol-Huber, C., Estimation des densit´es: risque minimax. Z. Wahrscheinlichkeitstheorie und Verw. Gebiete, 47 (1979), 119­137.
[3] Devroye, L. and Gy¨orfi, L., Nonparametric Density Estimation: The L1 View. Wiley 1985.
[4] Griego, R.J., Hersh, R., Random evolutions, Markov chains and systems of partial differential equations. Proc. Nat. Acad. Sci. USA, 62 , (2) (1969), 305­308.
[5] Hersh, R., Papanicolaou, G., Non-commuting random evolutions and an operator-valued Feynman- Kac formula. Comm.Pure and Appl. Math., 25 , (3) (1972), 337­367.
[6] Kac, I., Ja., Krasovsky, N.N., On stability of systems with random parameters. Appl. Math. Mech., 24 , (5)(1960), 809­823.
[7] Kloeden, P.E., Platen, E., Numerical solution of stochastic differential equations. Springer Verlag Berlin, 1992.
[8] Milstein, G.N., Interaction of Markov processes. Theory Prob. Appl., 17 (1972), 36­45.
[9] Milstein, G.N., Probabilistic solution of linear systems of elliptic and parabolic equations. Theory Prob. Appl., 23 (1978), 851-855.
[10] Milstein, G.N., Repin, Yu.M., The action of a Markov process on a system of differential equations. Differential equations, 5 (1969), 1371-1384.
[11] Milstein, G.N. and Schoenmakers, J.G.M., Monte Carlo construction of hedging strategies against multi-asset European claims. Stochastics and Stochastics Reports, 73, (1-2) (2002), 125­157.
26

[12] Milstein, G.N., Schoenmakers, J.G.M., Spokoiny, V., Transition density estimation for stochastic differential equations via forward-reverse representations. Bernoulli, 10, no. 2 (2004), 281­312.
[13] Milstein, G.N., Tretyakov, M.V., Stochastic Numerics for Mathematical Physics, Springer Verlag Berlin, 2004.
[14] Scott, D.W., Multivariate Density Estimation. Wiley, New York, 1992. [15] Shiryaev A.N. Probability. Springer, 1996. [16] Silverman, B.W., Density Estimation for Statistics and Data Analysis.
Chapman and Hall, London, 1986. [17] Spivakovskaya, D., Heemink, A.W., Milstein, G.N., Schoenmakers, J.G.M.,
Simulation of the transport of particles in coastal waters using forward and reverse time diffusion. Advances in Water Resources, 28 (2005), 927­938.
27

SFB 649 Discussion Paper Series 2006
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Calibration Risk for Exotic Options" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
002 "Calibration Design of Implied Volatility Surfaces" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
003 "On the Appropriateness of Inappropriate VaR Models" by Wolfgang Härdle, Zdenk Hlávka and Gerhard Stahl, January 2006.
004 "Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification" by Harald Uhlig, January/February 2006.
005 "British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis" by Enzo Weber, January 2006.
006 "A Combined Approach for Segment-Specific Analysis of Market Basket Data" by Yasemin Boztu and Thomas Reutterer, January 2006.
007 "Robust utility maximization in a stochastic factor model" by Daniel Hernández­Hernández and Alexander Schied, January 2006.
008 "Economic Growth of Agglomerations and Geographic Concentration of Industries - Evidence for Germany" by Kurt Geppert, Martin Gornig and Axel Werwatz, January 2006.
009 "Institutions, Bargaining Power and Labor Shares" by Benjamin Bental and Dominique Demougin, January 2006.
010 "Common Functional Principal Components" by Michal Benko, Wolfgang Härdle and Alois Kneip, Jauary 2006.
011 "VAR Modeling for Dynamic Semiparametric Factors of Volatility Strings" by Ralf Brüggemann, Wolfgang Härdle, Julius Mungo and Carsten Trenkler, February 2006.
012 "Bootstrapping Systems Cointegration Tests with a Prior Adjustment for Deterministic Terms" by Carsten Trenkler, February 2006.
013 "Penalties and Optimality in Financial Contracts: Taking Stock" by Michel A. Robe, Eva-Maria Steiger and Pierre-Armand Michel, February 2006.
014 "Core Labour Standards and FDI: Friends or Foes? The Case of Child Labour" by Sebastian Braun, February 2006.
015 "Graphical Data Representation in Bankruptcy Analysis" by Wolfgang Härdle, Rouslan Moro and Dorothea Schäfer, February 2006.
016 "Fiscal Policy Effects in the European Union" by Andreas Thams, February 2006.
017 "Estimation with the Nested Logit Model: Specifications and Software Particularities" by Nadja Silberhorn, Yasemin Boztu and Lutz Hildebrandt, March 2006.
018 "The Bologna Process: How student mobility affects multi-cultural skills and educational quality" by Lydia Mechtenberg and Roland Strausz, March 2006.
019 "Cheap Talk in the Classroom" by Lydia Mechtenberg, March 2006. 020 "Time Dependent Relative Risk Aversion" by Enzo Giacomini, Michael
Handel and Wolfgang Härdle, March 2006. 021 "Finite Sample Properties of Impulse Response Intervals in SVECMs with
Long-Run Identifying Restrictions" by Ralf Brüggemann, March 2006. 022 "Barrier Option Hedging under Constraints: A Viscosity Approach" by
Imen Bentahar and Bruno Bouchard, March 2006.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "How Far Are We From The Slippery Slope? The Laffer Curve Revisited" by Mathias Trabandt and Harald Uhlig, April 2006.
024 "e-Learning Statistics ­ A Selective Review" by Wolfgang Härdle, Sigbert Klinke and Uwe Ziegenhagen, April 2006.
025 "Macroeconomic Regime Switches and Speculative Attacks" by Bartosz Makowiak, April 2006.
026 "External Shocks, U.S. Monetary Policy and Macroeconomic Fluctuations in Emerging Markets" by Bartosz Makowiak, April 2006.
027 "Institutional Competition, Political Process and Holdup" by Bruno Deffains and Dominique Demougin, April 2006.
028 "Technological Choice under Organizational Diseconomies of Scale" by Dominique Demougin and Anja Schöttner, April 2006.
029 "Tail Conditional Expectation for vector-valued Risks" by Imen Bentahar, April 2006.
030 "Approximate Solutions to Dynamic Models ­ Linear Methods" by Harald Uhlig, April 2006.
031 "Exploratory Graphics of a Financial Dataset" by Antony Unwin, Martin Theus and Wolfgang Härdle, April 2006.
032 "When did the 2001 recession really start?" by Jörg Polzehl, Vladimir Spokoiny and Ctlin Stric, April 2006.
033 "Varying coefficient GARCH versus local constant volatility modeling. Comparison of the predictive power" by Jörg Polzehl and Vladimir Spokoiny, April 2006.
034 "Spectral calibration of exponential Lévy Models [1]" by Denis Belomestny and Markus Reiß, April 2006.
035 "Spectral calibration of exponential Lévy Models [2]" by Denis Belomestny and Markus Reiß, April 2006.
036 "Spatial aggregation of local likelihood estimates with applications to classification" by Denis Belomestny and Vladimir Spokoiny, April 2006.
037 "A jump-diffusion Libor model and its robust calibration" by Denis Belomestny and John Schoenmakers, April 2006.
038 "Adaptive Simulation Algorithms for Pricing American and Bermudan Options by Local Analysis of Financial Market" by Denis Belomestny and Grigori N. Milstein, April 2006.
039 "Macroeconomic Integration in Asia Pacific: Common Stochastic Trends and Business Cycle Coherence" by Enzo Weber, May 2006.
040 "In Search of Non-Gaussian Components of a High-Dimensional Distribution" by Gilles Blanchard, Motoaki Kawanabe, Masashi Sugiyama, Vladimir Spokoiny and Klaus-Robert Müller, May 2006.
041 "Forward and reverse representations for Markov chains" by Grigori N. Milstein, John G. M. Schoenmakers and Vladimir Spokoiny, May 2006.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

