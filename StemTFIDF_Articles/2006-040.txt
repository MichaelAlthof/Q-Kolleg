BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2006-040
In Search of Non-Gaussian Components of a High-
Dimensional Distribution
Gilles Blanchard 1,2 Motoaki Kawanabe 1 Masashi Sugiyama 1,3 Vladimir Spokoiny 4 Klaus-Robert Müller 1,5
1 Fraunhofer FIRST.IDA, Berlin, Germany 2 CNRS, Université Paris-Sud, Orsay, France 3 Tokyo Institute of Technology, Tokyo, Japan 4 Weierstrass Institute for Applied Analysis
and Stochastics, Berlin, Germany 5 University of Potsdam, Potsdam, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Journal of Machine Learning Research 6 (2005) 1-48

Submitted 4/05; Published 10/05

In Search of Non-Gaussian Components of a High-Dimensional Distribution 

Gilles Blanchard Fraunhofer FIRST.IDA Kekul´estr. 7, 12489, Berlin, Germany and CNRS, Universit´e Paris-Sud, Orsay, France

blanchar@first.fhg.de

Motoaki Kawanabe Fraunhofer FIRST.IDA Kekul´estr. 7, 12489, Berlin, Germany

nabe@first.fhg.de

Masashi Sugiyama Fraunhofer FIRST.IDA Kekul´estr. 7, 12489, Berlin, Germany and Department of Computer Science Tokyo Institute of Technology 2-12-1, O-okayama, Meguro-ku, Tokyo, 152-8552, Japan

sugi@cs.titech.ac.jp

Vladimir Spokoiny

spokoiny@wias-berlin.de

Weierstrass Institute and Humboldt University, Mohrenstr. 39, 10117 Berlin, Germany

Klaus-Robert Mu¨ller Fraunhofer FIRST.IDA Kekul´estr. 7, 12489 Berlin, Germany and Department of Computer Science, University of Potsdam August-Bebel-Str.89, Haus 4, 14482 Potsdam, Germany

klaus@first.fhg.de

Editor:

This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 Economic Risk.

c 2005 Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller.

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller
Abstract
Finding non-Gaussian components of high-dimensional data is an important preprocessing step for efficient information processing. This article proposes a new linear method to identify the "non-Gaussian subspace" within a very general semi-parametric framework. Our proposed method, called NGCA (Non-Gaussian Component Analysis), is essentially based on a linear operator which, to any arbitrary nonlinear (smooth) function, associates a vector which belongs to the low dimensional non-Gaussian target subspace up to an estimation error. By applying this operator to a family of different nonlinear functions, one obtains a family of different vectors lying in a vicinity of the target space. As a final step, the target space itself is estimated by applying PCA to this family of vectors. We show that this procedure is consistent in the sense that the estimaton error tends to zero at a parametric rate, uniformly over the family, Numerical examples demonstrate the usefulness of our method.
1. Introduction
1.1 Setting and general principle
Suppose {Xi}ni=1 are i.i.d. samples in a high dimensional space Rd drawn from an unknown distribution with density p(x) . A general multivariate distribution is typically too complex to analyze from the data, thus dimensionality reduction is useful to decrease the complexity of the model (Cox and Cox, 1994; Scho¨lkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi, 2003). Here our point of departure is the following assumption: the high dimensional data includes low dimensional non-Gaussian components and the other components are Gaussian. This assumption follows the rationale that in most real-world applications the `signal' or `information' contained in the high-dimensional data is essentially non-Gaussian while the `rest' can be interpreted as high dimensional Gaussian noise.
We want to emphasize from the beginning that we do not assume the Gaussian components to be of smaller order of magnitude than the signal components; all components are instead typically of the same order. This setting therefore excludes the use dimensionality reduction methods based on the assumption that the data lies, say, on a lower dimensional manifold, up to some small noise. In fact, this type of methods addresses a different kind of problem altogether.
Under our modeling assumption, therefore, the task is to recover the relevant nonGaussian components. Once such components are identified and extracted, various tasks can be applied in the data analysis process, say, data visualization, clustering, denoising or classification.
If the number of Gaussian components is at most one and all the non-Gaussian components are mutually independent, Independent Component Analysis (ICA) techniques (e.g. Comon, 1994; Hyv¨arinen et al., 2001) are relevant to identify the non-Gaussian subspace. Unfortunately, however, this is often a too strict assumption on the data.
The framework we consider is on the other hand very close to that of Projection Pursuit (PP) algorithms (Friedman and Tukey, 1974; Huber, 1985; Hyv¨arinen et al., 2001). The goal of Projection Pursuit methods is to extract non-Gaussian components in a general setting, i.e., the number of Gaussian components can be more than one and the non-Gaussian components can be dependent.
2

In Search of Non-Gaussian Components of a High-Dimensional Distribution
Projection Pursuit methods typically proceed by fixing a single index which measures the non-Gaussianity (or 'interessingness') of a projection direction. This index is then optimized over all possible directions of projection; the procedure can be repeated iteratively (over directions orthogonal to the first ones already found) to find a higher dimensional projection of the data as needed.
However, it is known that some projection indices are suitable for finding super-Gaussian components (heavy-tailed distribution) while others are suited for identifying sub-Gaussian components (light-tailed distribution) (Hyv¨arinen et al., 2001). Therefore, traditional PP algorithms may not work effectively if the data contains, say, both super- and sub-Gaussian components.
To summarize: existing methods for the setting we consider typically proceed by defining an appropriate interestingness index and then compute a projection that maximizes this index (projection pursuit methods, and some ICA methods). The philosophy that we would like to promote in this paper is in a sense different: in fact we do not specify what we are interested in, but we rather define what is not interesting (Jones and Sibson , 1987). Clearly a multi-dimensional Gaussian subspace is a reasonable candidate for an undesired component (our idea could be generalized by defining, say, a Laplacian subspace to be uninformative). Having defined this uninteresting subspace, its (orthogonal) complement is by definition interesting: this therefore precisely defines our target space.
1.2 Presentation of the method
Technically, our new approach to identifying the non-Gaussian subspace uses a very general semi-parametric framework. The proposed method, called Non-Gaussian Component Analysis (NGCA), is essentially based on a central property stating that there exists a linear mapping h  (h)  Rd which, to any arbitrary (smooth) nonlinear function h : Rd  R , associates a vector  lying in the non-Gaussian target subspace. In practice, the vector (h) has to be estimated from the data, giving rise to an estimation error. However, our main consistency result shows that this estimation error vanishes at a rate log(n)/n with the sample size n . Using a whole family of different nonlinear functions h then yields a family of different vectors (h) which all approximately lie in, and span, the non-Gaussian subspace. We finally perform PCA on this family of vectors to extract the principal directions and estimate the target space.
In practice, we consider functions of the particular form h,a(x) = fa( , x ) where f is a function class parameterized, say, by a parameter a , and  = 1 . Even for a fixed a , it is infeasible to compute values of (h,a) for all possible values of  (say on a discretized net of the unit sphere) because of the cardinality involved. In order to choose a relevant value for  (still for fixed a ), we then opt to use as a heuristic a well-known PP algorithm, FastICA (Hyv¨arinen, 1999). This was suggested by the surprising observation that the mapping   (h,a) is then equivalent to a single iteration of FastICA (although this algorithm was built using different theoretical considerations); hence in this special case FastICA is exactly the same as iterating our mapping. In short, we use a PP method as a proxy to select the most relevant direction  for a fixed a . This results in a particular choice of a , to which we apply the mapping once more, thus yielding a = (ha,a) .
3

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

Finally, we aggregate the different vectors a obtained when varying a by applying PCA as indicated previously.
Thus, apart from the conceptual point, defining uninterestingness as the point of departure instead of interestingness, another way to look at our method is to say that it allows the combination of information coming from different indices: here the above function fa (for fixed a ) plays a role similar to that of a non-Gaussianity index in PP, but we do combine a rich family of such functions (by varying a and even by considering several function classes at the same time). The important point here is while traditional Projection Pursuit does not provide a well-founded justification for combining directions from using different indices, our framework allows to do precisely this ­ thus implicitly selecting, in a given family of indices, the ones which are the most informative for the data at hand.
In the following section we will outline the theoretical cornerstone of the method, a novel semi-parametric theory for linear dimension reduction. Section 3 discusses the algorithmic procedure and is conluded with theoretical results establishing statistical consistence of the method. In Section 4, we study on simulated and real data examples the behavior of the algorithm. A brief conclusion is given in Section 5.

2. Theoretical framework
In this section, we give a theoretical basis for the non-Gaussian component search within a semi-parametric framework. We present a population analysis where expectations can in principle be calculated exactly in order to emphasize the main idea and how the algorithm is built. A more rigorous statistical study of the estimation error will be exposed later in section 3.5.

2.1 Motivation
Before introducing the semi-parametric density model which will be used as a foundation for developing our method, we motivate it by starting from elementary considerations. Suppose we are given a set of observations Xi  Rd, (i = 1, . . . , n) obtained as a sum of a signal S and an independent Gaussian noise component N :

X =S+N,

(1)

where N  N (0, ) . Note that no particular structural assumption is made about the noise covariance matrix  .
Assume the signal S is contained in a lower-dimensional linear subspace E of dimension m < d . Loosely speaking, we would like to project X linearly so as to eliminate as much of the noise as possible while preserving the signal information. An important issue for the analysis of the model (1) is a suitable representation of the density of X which reflects the low dimensional structure of the non-Gaussian signal. The next lemma presents a generic representation of the density p for the model (1).

Lemma 1 The density p(x) for the model (1) with the m -dimensional signal S and an independent Gaussian noise N can be represented as

p(x) = g(T x)(x)

4

In Search of Non-Gaussian Components of a High-Dimensional Distribution

where T is a linear operator from Rd to Rm , g(·) is some function on Rm and (x) is the density of the Gaussian component.
The formal proof of this lemma is given in the Appendix. Note that the above density representation is not unique, as the parameters g, T,  are not identifable from the density p. However, the null suspace (kernel) K(T ) of the linear operator T is an identifiable parameter. In particular, is useful to notice that if the noise N is standard normal, then the operator T can be taken as the projector on the signal space E . In this case, K(T ) coincides with E , the orthogonal complementary subspace to E . In the general situation with "colored" Gaussian noise, the signal space E does not coincide with the orthogonal complementary of the kernel I = K(T ) of the operator T . However, the density representation of Lemma 1 shows that the the subspace K(T ) is non-informative and contains only noise. The original data can then be projected orthogonally onto I , which we call the non-Gaussian subspace, without loss of information. This way, we are preserving the totality of the signal information. This definition implements the general point of view outlined in the introduction, namely: we define what is considered uninteresting; the target space is then defined indirectly as the orthogonal of the uninteresting component.
2.2 Relation to ICA analysis
An equivalent view of the same model is to decompose the noise N appearing in Eq.(1) into a component N1 belonging to the signal space E and an independent component N2 ; it can then be shown that N2 belongs to the subspace K(T ) defined above. In this view the space I is orthogonal to the independent noise component, and projecting the data onto I amounts to cancelling this independent noise component by an orthogonal projection.
In the present paper, we assume that we wish to project the data orthogonally, i.e., that the Euclidean geometry of the input space is meaningful for the data at hand, and that we want to respect it while projecting. An alternative point of view would be to disregard the input space geometry altogether, and to first map the data linearly to a reference space where it has covariance identity ("whitening" transform), which would be closer to a traditional ICA analysis. This would have on the one hand the advantage of resulting in an affine invariant procedure, but, on the other hand, the disadvantage of losing the information of the original space geometry.

2.3 Main model
Based on the above motivation, we assume to be dealing with an unknown probability density function p(x) on Rd which can put under the form

p(x) = g(T x)(x),

(2)

where T is an unknown linear mapping from Rd to Rm with m  d , g is an unknown function on Rm , and  is a centered1 Gaussian density with covariance matrix  .

1. It is possible to handle a more general situation where the Gaussian part has an unknown mean parameter  in addition to the unknown covariance  . For simplicity of exposition we consider here only the case of  = 0 .

5

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

Note that the semi-parametric model (2) includes as particular cases both the pure parametric ( m = 0 ) and purely non-parametric ( m = d ) models. For practical purposes however we are effectively interested in an intermediate case where d is large and m is relatively small. In what follows we denote by I the m -dimensional linear subspace in Rd generated by the adjoint operator T  :
I = K(T ) = (T ) ,

where (·) denotes the range of an operator. We call I the non-Gaussian subspace.
The proposed goal is therefore to estimate I by some subspace I computed from an i.i.d. sample {Xi}in=1 following the distribution with density p(x) . In this paper we assume the effective dimension m to be known or fixed a priori by the user. Note that we do not
estimate  nor g when estimating I . We measure the closeness of the two subspaces I
and I by the following error function:

m

E(I, I) = (2m)-1

I - I

2 F rob

=

m-1

(Id - I )vi 2,

i=1

(3)

where I denotes the orthogonal projection on I , · F rob is the Frobenius norm, {vi}mi=1 is an orthonormal basis of I and Id is the identity matrix.

2.4 Key result
The main idea underlying our approach is summed up in the following Proposition (the proof is given in Appendix A.2). Whenever variable X has covariance2 matrix identity, this result allows, from an arbitrary smooth real function h on Rd , to find a vector (h)  I .

Proposition 2 Let X be a random variable whose density function p(x) satisfies Eq.(2) and suppose that h(x) is a smooth real function on Rd . Assume furthermore that  =
E XX = Id . Then under mild regularity conditions on h , the following vector (h) belongs to the target space I :

(h) = E [Xh(X) - h(X)] .

(4)

In the general case where the covariance matrix  is different from identity, provided

it is nondegenerated, we can apply a whitening operation (also known as Mahalanobis

transform).

Namely, let us put

Y

=

-

1 2

X

the "whitened" data; the covariance matrix of

Y is then identity. Note that if the density function of X is of the form

p(x) = g(T x)(x),
then by change of variable the density function of Z = AX is given by
q(z) = cAg(T A-1z)AA (z),
where cA is a normalization constant depending on A .
2. Here and in the sequel, with some abuse we call  = E XX the covariance matrix even though we do not assume that the non-Gaussian part of the data is centered.

6

In Search of Non-Gaussian Components of a High-Dimensional Distribution

This identity applied to

A

=

-

1 2

and the previous proposition allow to conclude that

Y (h) = E [h(y) - yh(y)]  J =

(

1 2

T



)

and therefore that

(h)

=

-

1 2

Y

(h)



I

=

(T ) ,

where

I

is the non-Gaussian index space for the initial variable

X , and

J

=



1 2

I

the

transformed non-Gaussian space for the whitened variable Y .

3. Procedure

We now use the key proposition established in the previous section to design a practical

algorithm in order to identify the non-Gaussian subspace. The first step is to apply the

whitening transform to the data (where the true covariance matrix  is estimated by the

empirical covariance  ). We then estimate the "whitened" non-Gaussian space J by some

J (this will be described next); this space is then finally pulled back in the original space

by application of

-

1 2

.

To simplify the exposition, in this section we will forget about

the whitening/dewhitening steps and always implicitly assume that we are dealing directly

with the whitened data: every time we refer to the non-Gaussian space it is therefore to be

understood that we refer to

J

=



1 2

I

corresponding to the whitened data

Y

.

3.1 Principle idea

In the previous section we have proved that for an arbitrary function h satisfying mild smoothness conditions, it is possible to construct a vector (h) which lies in the nonGaussian subspace. However, since the unknown density p(x) is used (via the expectation operator) to define  by Eq.(2), one cannot directly use this formula in practice: it is then natural to approximate it by replacing the true expectation by the empirical expectation. This gives rise to the estimated vector

(h)

=

1 n

n i=1

Yih(Yi)

-

h(Yi) ,

(5)

which we expect to be close to the non-Gaussian subspace up to some estimation error. At this point, a natural step is to consider a whole family of functions {hi}in=1 giving rise to an associated vector family of {i}iL=1 all lying close to the target subspace, where i := (hi) . The final step is to recover the non-Gaussian subspace from this set. For this purpose we
suggest to use the principal directions of this family, i.e. to apply PCA (although other
algorithmic options are certainly avalaible for this task). This general idea is illustrated on
Figure 1.

3.2 Normalization of the vectors When extracting information on the target subspace from the set of vectors {i}Li=1 , attention should be paid to how the functions {hi}iL=1 are normalized. As can be seen from its definition, the operator which maps a function h to (h) (and also its empirical counterpart (h) ) is linear. Therefore, if, for example, one of the functions {hi}iL=1 is multiplied
7

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

h(x)

h
1
h
2
h
3
h
4
h
5
x

^ 1

^ 4

^ 2

I ^ 3 ^ 5

Figure 1: The NGCA principle idea: from a varied family of real functions, compute a family of vectors belonging to the target space up to small estimation error.

by an arbitrarily large scalar, the associated (h) could have an arbitrarily large norm: this is likely to influence heavily the procedure of principal direction extraction applied to the whole family.
To prevent this problem, the functions {hi}iL=1 should be normalized in a reasonable way. Several possibilities can come to mind, like using the sup or L2 norm of h or of h . We argue here that a sensible way to normalize functions is such that the average squared deviation (estimation error) of (h) to its mean is of the same order for all functions h considered. This has a first direct intuitive interpretation in terms of making the length of each estimated vector proportional to its associated signal-to-noise ratio. We argue in more detail that the norm of (h) after normalization is directly linked to the amount of information brought by this vector about the target subspace.
Namely, if we measure the information that is brought by a certain vector (h) about the target space J through the angle ((h)) between the vector and the space, we have

(h) - (h)  dist((h), J ) = sin(((h))) (h) .

(6)

Suppose we have ensured by renormalization that (h)2 = E (h) - (h) 2 is constant
and independent of h , and assume that this results in (h) - (h) 2 being bounded by some constant with high probability. It entails that sin(((h))) (h) is bounded independently of h . We expect in this situation that the bigger  , the smaller is sin() , and therefore the more reliable the information about J . This intuition is illustrated in Figure 2, where the estimation error is represented by a confidence ball of equal size for all vectors3.
Therefore, at least at an intuitive level it appears appropriate to use (h) as a renormalization. Note that this is just the square root of the trace of the covariance matrix of (h) , and therefore easy to estimate in practice from its empirical counterpart. In section 3.5, we give actual theoretical confidence bounds for  -  which justify this intuition in a more rigorous manner.
3. Of course, the situation depicted in Figure 2 is idealized: we actually expect (from the Central Limit Theorem) that  - has approximately a Gaussian distribution with some non-spherical variance, giving rise to a confidence ellipsoid rather than a confidence ball. To obtain a spherical error ball, we would have to apply a (linear) error whitening transform separately to each (h) . However, in that case the error whitening transform would be different for each h , and the information of the vector family about the target subspace would then be lost. To preserve this information, only a scalar normalization is adequate, which is why we recommend the normalization scheme explained here.

8

In Search of Non-Gaussian Components of a High-Dimensional Distribution

Figure 2: For the same estimation error represented as a confidence ball of radius  , estimated vectors with higher norm give a more precise information about the true target space.

Finally, to confirm this idea on actual data, we plot in the top row Figure 3 the distribution of  on an illustrative data set using the normalization scheme just described. In order to investigate the relation between the norm of the (normalized)  and the amount of information on the non-Gaussian subspace brought by  , we plot in the right part of Figure 3 the relation between  and J  /  = cos(()) . As expected, the vectors  with highest norm are indeed much closer to the non-Gaussian subspace in general. Furthermore, vectors  with norm close to zero appear to bear almost no information about the non-Gaussian space, which is consistent with the setting depicted in Figure 2: whenever an estimated vector  has norm smaller than the estimation error  , its confidence ball contains the origin, which means that it brings no useable information about the non-Gaussian subspace.
These findings motivate two important points for the algorithm:
1. It should be beneficial to actively search for functions h which yield an estimated (h) with higher norm, since these are more informative about the target space J ;
2. The vectors  with norm below a certain threshold  can be discarded as they are non-informative. So far the theoretical bounds presented below in section 3.5 are not precise enough to give a canonical value for this threshold: we therefore recommend that it be determined by a calibration procedure. For this, we consider independent Gaussian data: in this case  = 0 for any h and thus  represents pure estimation noise. A reasonable choice for the threshold is therefore the 95th percentile (say) of this distribution, which we expect to reject a large majority of the noninformative vectors.

3.3 Using FastICA as preprocessing to find promising functions

When considering a parametrized family of functions {h} , it is a desirable goal to search the parameter space to find indices  such that (h) has a high norm, as proposed in the last section. From now on we will restrict our attention to functions of the form

h(x) = f ( , x ) ,

(7)

where   Rd ,  = 1 , and f is a smooth real function of a real variable. Clearly, it is not feasible to sample the entire parameter space for  as soon as it has more than a few

9

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

1.5 1
0.5 0
-0.5 -1
-1.5

-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 10
8 6 4 2 0 -2 -4 -6 -8 -10
-10 -5 0 5 10

15

10

5

0

-5

-10

-15

-20 -15 -10

-5

0

5 10 15 20

cos()

cos()

cos()

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0
||^ ||0 0.5 1 1.5 2 2.5 3 3.5 4
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0
||^ ||0 5 10 15 20 25
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0
||^ ||0 5 10 15 20 25 30

Figure 3: Illustrative plots of the method, applied to toy data of type (A) (See section 4.1). Left column: Distribution of  projected on a direction belonging to the target space J (abscissa) and a direction orthogonal to it (ordinate). Right column:  (after normalization) vs. cos((, J )) . From top to bottom rows: random draw of functions, after 1 -step, and after 4 -step of FastICA preprocessing.  's are normalized as described in section 3.2.

10

In Search of Non-Gaussian Components of a High-Dimensional Distribution

dimensions, and it is not obvious a priori to find parameters  such that (h) has a high norm. Remember however that we do not need to find an exact maximum of this norm over the parameter space. We merely want to find parameters such that the associated norm is preferably high because they bring more information; this may also involve heuristics. Naturally, good heuristics should be able to find parameters giving rise to vectors with higher norm, bringing more information on the subspace and ultimately better practical results, nevertheless the underlying theoretical motivation stays unchanged regardless of the way the functions are picked.
A particularly relevant heuristic for choosing  is suggested to us when we look at the form of  given by Eq.(5) when we plug in functions of the specific form given by Eq.(7):

 (h )

=

1 n

n

Yif ( , Yi ) - f ( , Yi ) .

i=1

(8)

It is interesting and surprising to notice that this equation precisely coincides with one iteration of a well-known Projection Pursuit algorithm, FastICA (Hyv¨arinen, 1999). More precisely, FastICA consists in iterating the following update rule to form a sequence 1, . . . , T :

t+1



1 n

n i=1

Yif ( t, Yi ) - f ( t, Yi )t

(9)

where the sign  indicates that vector t+1 is renormalized to be of unit norm. Note that the FastICA procedure is derived from quite a different theoretical setting of
what we considered here (see e.g. Hyv¨arinen et al., 2001); its goal is in principle to optimize a non-Gaussianity measure E [F ( , x )] (where F is such that F formally coincides with our f above) and the solution is reached by an approximate Newton method giving rise to the update rule of Eq.(9), repeated until convergence.
This formal identity leads us to adopt the FastICA methodology as a heuristic for our method. Since finding an actual optimum point is not needed, convergence is not an issue, so that we only iterate the update rule of Eq.(9) for a fixed number of iterations T to find a relevant direction T . Finally we apply Eq.(8) one more time to this choice of parameter, so that the procedure finally outputs (hT ) . On Figure 3, we plot the effect of a few iterations of this preprocessing for the method, applied on toy data and see that it leads to a significant improvement.
Paradoxically, if the convergence of this FastICA preprocessing is too good, there is in principle a risk that all vectors  end up in the vicinity of one single "best" direction instead of spanning the whole target space: the preprocessing would then have the opposite effect of what is wished, namely impoverishing the vector family. One possible remedy against this is to apply so-called batch FastICA, which consists in iterating equation (9) on a m dimensional system of vectors, which is orthonormalized anew before each new iteration. In our practical experiments we did not observe any significant change in the results when using this refinement, so we mention this possibility only as a matter of precaution. We suspect two mitigating factors against this possible unwished behavior are that (1) it is known that FastICA does not converge to a global maximum, so that we probably find vectors in the vicinity of different local optima and (2) the "optimal" directions depend on the function f used and we combine a large number of such functions.

11

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

In the next section, we will describe the full algorithm, which consists in applying the procedure just described to different choices of the function f . Since we are using Projection Pursuit as a heuristic to find suitable parameters  for a fixed f , the theoretical setting proposed here can therefore also be seen as a suitable framework for combining Projection Pursuit results when using different index functions f .

3.4 Full procedure

The previous sections have been devoted to detailing some key points of the procedure. We now gather these points and describe the full algorithm. We previously considered the case of a basis function family h(y) = f ( , y ) . We now consider a finite family of possible choices {fk}Lk=1 which are then combined.
In the implementation tested, we have used the following forms of the functions fk :

f(1)(z) = z3 exp

-

z2 22

fb(2)(z) = tanh(bz), fa(3)(z) = exp (iaz) ,

,

(Gauss-Pow3)
(Hyperbolic Tangent) (Fourier4)

More precisely, we consider discretized ranges for   [min, max] , b  [0, B] , and a  [0, A] , giving rise to a finite collection {fk} (which therefore includes simultaneously functions of the three different above families). Note that using z3 and Hyperbolic Tangent functions is inspired by the classical PP algorithms (including FastICA) where these indices are used. We multiplied z3 by a Gaussian factor in order to satisfy the boundedness assumption needed to control the estimation error (see Theorem 3 and 4 below). Furthermore the introduction of the parameter 2 allows for a richer family. Finally, the Fourier functions were introduced as they constitute a rich and important family. A pseudocode for the NGCA algorithm is described in Figure 4.
3.5 Theoretical bounds on the statistical estimation error
In this section we tackle the question of controlling the estimation error when approximating the vectors (h) by their empirical estimations (h) from a rigorous theoretical point of view. These results were derived with the following goals in mind:
· A cornerstone of the algorithm is that we consider a whole family h1, . . . , hL of functions and pick selected members from it. In order to justify this from a statistical point of view, we therefore need to control the estimation error not for a single function h and the associated (h) , but instead uniformly over the function family. For this, a simple control of e.g. the averaged squared deviation E  -  2 for each individual h is not sufficient: we need a stronger result, namely an exponential control of the deviation probability. This allows, by the union bound, to obtain a uniform control over the whole family with a mild (logarithmic) dependence on the cardinality of the family.
4. In practice, separated into real and complex parts (sine and cosine).
12

In Search of Non-Gaussian Components of a High-Dimensional Distribution

Input: Data points (Xi)  Rd , dimension m of target subspace. Parameters: Number T of FastICA iterations; threshold .

Whitening.

The data Xi is recentered by subtracting the empirical mean.

Let  denote the empirical covariance matrix of the data sample (Xi) ;

put

Yi

=

-

1 2

Xi

the empirically whitened data.

Main Procedure.

Loop on k = 1, . . . , L :

Draw 0 at random on the unit sphere of Rd .

Loop on t = 1, . . . , T : [FastICA loop]

Put

t



1 n

n

Yifk( t-1, Yi ) - fk( t-1, Yi )t-1

i=1

Put t  t/ t .

End Loop on t

.

Let Nk be the trace of the empirical covarariance matrix of T :

Nk

=

1 n

n

22
Yifk( T -1, Yi ) - fk( T -1, Yi )T -1 - T .

i=1

Store v(k)  T  n/Nk. [Normalization]

End Loop on k

Thresholding. From the family v(k) , throw away vectors having norm smaller than threshold

PCA step. Perform PCA on the set of remaining v(k) .

Let J be the space spanned by the first m principal directions.

Pull back in original space.

I

=

-

1 2

J

.

.

Output: I .

Figure 4: Pseudocode of the NGCA algorithm.

13

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

· We aim at making the covariance trace 2 directly appear into the main bounding terms of our error control. This provides a more solid justification to the renormalization scheme developed in section 3.2 where we had used arguments based on a non rigorous intuition. The choice to involve directly the empirical covariance in the bound instead of the population one was made to emphasize that estimation error for the covariance itself is also taken into account for the bound.

· While the control of the deviation of an empirical average of the form given in Eq.(5) is a very classical problem, we want to explicitly take into account the effect of the empirical whitening/dewhitening using the empirical covariance matrix  . This complicates matters noticeably since this whitening is itself data-dependent.
· Our goal was not to obtain tight confidence intervals or even exact asymptotical behavior. There is a number of ways in which our results could be substantially refined, for example obtaining uniform bounds over continuous (instead of finite) families of functions using covering number arguments; showing asymptotical uniform central limit properties for a precise study of the typical deviations, etc. Here we tried to obtain simple, while still mathematically rigorous, results, covering essential statistical foundations of our method: consistency and order of the convergence rate.

In the sequel, for a matrix A , we denote A its operator norm.

Analysis of the estimation error with exact whitening. We start by considering an

idealized case where whitening is done using the true covariance matrix

:

Y

=

-

1 2

X

.

In this case we have the following control of the estimation error:

Theorem 3 Let {hk}kL=1 be a family of smooth functions from Rd to R . Assume that supy,k max ( hk(y) , hk(y) ) < B and that X has covariance matrix  with -1  K2 , and is such that for some 0 > 0 the following inequality holds:

E [exp (0 X )]  a0 < .

(10)

Denote h(y) = yh(y) - h(y) . Suppose X1, . . . , Xn are i.i.d. copies of X and let Yi =

-

1 2

Xi

.

If we define

Y (h)

=

1 n

n i=1

h(Yi)

=

1 n

n i=1

Yih(Yi) - h(Yi) ,

(11)

and

Y2

(h)

=

1 n

n

2
h(Yi) - Y (h) ,

(12)

i=1

then for any

<

1 4

,

with

probability

at

least

1 - 4

the following bounds hold simultaneously

for all k  {1, . . . , L} :

dist Y (hk), J

2

Y2

(hk

)

log(L-1) n

+

log

d

+

C

log(nL-1) log(L-1)

n

3 4

,

14

In Search of Non-Gaussian Components of a High-Dimensional Distribution

and

dist

-

1 2

Y

(hk

),

I

 2K

Y2

(hk

)

log(L-1) n

+

log

d

+

C

log(nL-1) log(L-1)

n

3 4

,

where dist(, I) denotes the distance between a vector  and the subspace I , and C, C are constants depending only on the parameters (d, 0, a0, B, K) .

Comments.

1. The above inequality tells us that the rate of convergence of the estimated vectors to the target space is in this case of order n-1/2 (classical "parametric" rate). Furthermore, the theorem gives us an estimation of the relative size of the estimation error for different functions h through the empirical factor Y (h) in the principal term of the bound. As announced in our initial goals, this therefore gives a rigorous foundation to the intuition exposed in section 3.2 for vector renormalization.
2. Also following our goals, we obtained a uniform control of the estimation error over a finite family with a logarithmic dependence in the cardinality. This does not correspond exactly to the continuous families we use in practice but comes close enough if we consider adequate parameter discretization. We will comment on this in more detail after the next theorem.

Whitening using empirical covariance. When  is unknown (which is in general the

case), we use instead the empirical covariance matrix  . Here we will show that under a

somewhat stronger assumption on the distribution of X and on the functions h , we are

still able to obtain a convergence rate of order at most log(n)/n towards the index space

I.

Let

us denote

Yi

=

-

1 2

Xi

the

empirically

whitened datapoints,

h(y) = yh(y) - h(y)

as previously, and

Y

(h)

=

1 n

n

h(Yi)

=

1 n

n

Yih(Yi) - h(Yi) ;

i=1 i=1

(13)

finally, let us denote

(h)

=

-

1 2

Y

(h)

,

and

Y2

(h)

=

1 n

n

2
h(Yi) - Y (h) .

i=1

We then have the following theorem:

Theorem 4 Let us assume the following : (i) There exists 0 > 0, a0 > 0 such that

E exp 0 X 2 = a0 <  ;

(ii) The covariance matrix  of X is such that -1  K2 ; 15

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

(iii) supk,y max ( hk(y) , hk(y) ) < B ;

(iv) The functions hk(y) = hk(y) - yhk(y) are all Lipschitz with constant M .

Then

for

big

enough

n , with

probability at least

1

-

4 n

-

4

the

following

bounds hold

true simultaneously for all k  {1, . . . , L} :

dist(Y (hk), J )  C1

d

log n

n

+

2

2
Y

(hk)

log(L-1) n

+

log

d

+

C2

log(nL-1) log(L-1)

n

3 4

,

and

dist((hk), I)  C1

d

log n

n

+

2K

2
Y

(hk

)

log(L-1) n

+

log

d

+

C2

log(nL-1) log(L

n

3 4

-1

)

,

where C1, C1 are constants depending on parameters (0, a0, B, K, M ) only and C2, C2 on (d, 0, a0, B, K, M ) .

Comments.

1. Theorem 4 implies that the vectors (hk) obtained from any h(x) converge to the unknown non-Gaussian subspace I uniformly at a rate of order log(n)/n .

2. The condition (i) is a restrictive assumption as it excludes some densities with heavy tails. We are considering weakening this assumption in future developments.

3. In the actual algorithm we consider a family of functions of the form h(x) = f ( , x ) , with  on the unit sphere of Rd . Suppose we approximate  by its

nearest neighbor  on a regular grid of scale  . Then we only have to apply the

bound to a discretized family of size L = O(1-d) , giving rise only to an additional

factor in the bound of order d log -1 . Taking for example  = 1/n (the fact that

the function family depends on n is not a problem since the bounds are valid for any

fixed n ), this ensures convergence of the discretized functions to the initial continuous family while introducing only in an additional factor d log n in the bound: this

dodelsongont

change fundamentally term present.

the

order

of

the

bound

since

there

is

already

another

4. For both Theorems 3 and 4 we have given bounds for estimation of both I and J , that is, in terms of the initial data and of the "whitened" data. The result in terms of the initial data ensures the overall consistency of the approach, but the convergence in the whitened space is equally interesting since we use it as the main working space for the algorithm.

5. Comparing to Theorem 3 obtained for exact whitening, we see in the present case that there is an additional term of principal order in n coming from the estimation error of  , with a multiplicative factor which unfortunately is not known accurately. This means that the renormalization scheme is not completely justified in this case, although we feel the idealized situation of Theorem 3 already provides some strong argument in this direction. However, the present result suggests that the accuracy of the normalization could probably be further improved.

16

In Search of Non-Gaussian Components of a High-Dimensional Distribution

4. Numerical results
Parameters used: All of the experiments presented where obtained with exactly the same set of parameters: a  [0, 4] for the Fourier functions; b  [0, 5] for the Hyperbolic Tangent functions; 2  [0.5, 5] for the Gauss-pow3 functions. Each of these ranges was divided into 1000 equispaced values, thus yielding a family {fk} of size 4000 (Fourier functions count twice because of the sine and cosine parts). The preliminary calibration procedure described in the end of section 3.2 suggested to take  = 1.5 as the threshold under which vectors are not informative (strictly speaking, the threshold should be calibrated separately for each function f but we opted here for a single threshold for simplicity). Finally we fixed the number of FastICA iterations T = 10 . With this choice of parameters and 1000 data points in the sample, the computation time is typically of the order of less than 10 seconds on a modern PC under our Matlab implementation.

4.1 Tests in a controlled setting
For testing our algorithm and comparing it with PP, we performed numerical experiments using various synthetic data. Here we report exemplary results using the following 4 data sets. Each data set includes 1000 samples in 10 dimension and each sample consists of 8 -dimensional independent standard Gaussian. Other 2 non-Gaussian components are as follows.

(A) Simple Gaussian Mixture: 2 -dimensional independent Gaussian mixtures with density of each component given by

1 2

-3,1(x)

+

1 2

3,1(x),

(14)

(B) Dependent super-Gaussian: 2 -dimensional isotropic distribution with density proportional to exp(- x ) .
(C) Dependent sub-Gaussian: 2 -dimensional isotropic uniform with constant positive density for x  1 and 0 otherwise.
(D) Dependent super- and sub-Gaussian: 1 -dimensional Laplacian with density proportional to exp(-|xLap|) and 1 -dimensional dependent uniform U (c, c + 1) , where c = 0 for |xLap|  log 2 and c = -1 otherwise.
For each of these situations, the non-Gaussian components are additionally rescaled coordinatewise by a fixed factor so that each coordinate has unit variance. The profiles of the density functions of the non-Gaussian components in the above data sets are described in Figure 5.
We compare the following three methods in the experiments: PP with `pow3' or `tanh' index5 (denoted by PP(pow3) and PP(tanh), respectively), and the proposed NGCA.
5. We used the deflation mode of the FastICA code (Hyv¨arinen et al., 2001) as an implementation of PP. The `pow3' flavor is equivalent to a kurtosis based index, i.e. in this case, FastICA iteratively maximizes the kurtosis. On the other hand, the `tanh' flavor uses a robust index which is appropriate in particular for heavy-tailed data.

17

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

(A) (B) (C) (D)
Figure 5: Densities of non-Gaussian components. The data sets are: (a) 2D independent Gaussian mixtures, (b) 2D isotropic super-Gaussian, (c) 2D isotropic uniform and (d) dependent 1D Laplacian + 1D uniform.

x 10-3 2.5

2

1.5

1

0.5

0 PP(pow3)

PP(tanh)
(A)

NGCA

0.16 0.14 0.12
0.1 0.08 0.06 0.04 0.02
0 PP(pow3)

PP(tanh)
(B)

NGCA

0.03

0.025

0.02

0.015

0.01

0.005

0 PP(pow3)

PP(tanh)
(C)

NGCA

0.03

0.025

0.02

0.015

0.01

0.005

0 PP(pow3)

PP(tanh)
(D)

NGCA

PP (pow3)

x 10-3 2

1.5

1

0.5

0 0
x 10-3 2

0.5

1.5

1

0.5

0 0 0.5

1
NGCA

1.5

1
NGCA

1.5

(A)

Figure 6: Boxplots of the error criterion E(I, I) .

2 x 10-3
2 x 10-3

PP (tanh)

PP (pow3)

0.12 0.1
0.08 0.06 0.04 0.02
0 0

0.02 0.04 0.06 0.08 0.1 0.12
NGCA

0.12
0.1
0.08
0.06
0.04
0.02
0 0 0.02 0.04 0.06 0.08 0.1 0.12
NGCA
(B)

PP (tanh)

PP (pow3)

0.03 0.025
0.02 0.015
0.01 0.005
0 0

0.005

0.01

0.015

0.02

NGCA

0.025

0.03

0.03 0.025
0.02 0.015
0.01 0.005
0 0

0.005

0.01

0.015

0.02

NGCA

0.025

0.03

(C)

PP (tanh)

PP (pow3)

0.014 0.012
0.01 0.008 0.006 0.004 0.002
0 0

0.002

0.004

0.006

0.008

0.01

0.012

0.014

NGCA

0.014 0.012
0.01 0.008 0.006 0.004 0.002
0 0

0.002

0.004

0.006

0.008

0.01

0.012

0.014

NGCA

(D)

PP (tanh)

Figure 7: Performance comparison plots (for error criterion E(I, I) ) of NGCA versus FastICA; top: versus pow3 index; bottom: versus tanh index.

18

In Search of Non-Gaussian Components of a High-Dimensional Distribution
Figure 6 shows boxplots of the error criterion E(I, I) defined in Eq.(3) obtained from 100 runs. Figure 7 shows comparison of the errors obtained by different methods for each individual trial. Because PP tends to get trapped into local optima of the index function it optimizes, we restarted it 10 times with random starting points and took the subspace obtaining the best index value. However, even when it is restarted 10 times, PP (especially with the `pow3' index) still gets caught in local optima in a small percentage of cases (we also tried up to 500 restarts but it led to negligible improvement).
For the simplest data set (A), NGCA is comparable or slightly better than PP methods. It is known that PP(tanh) is suitable for finding super-Gaussian components (heavy-tailed distribution) while PP(pow3) is suitable for finding sub-Gaussian components (light-tailed distribution) (Hyv¨arinen et al., 2001). This can be observed in the data sets (B) and (C): PP(tanh) works well for the data set (B) and PP(pow3) works well for the data set (C), although the upper-quantile is very large for the data set (C) (because of PP getting trapped in local minima). The plots of Figure 7 confirm that NGCA is on average on par or slightly better than PP with `correct' non-Gaussianity index without having to prefix such a non-Gaussianity index. For the data set (C), NGCA appears to be very slightly worse than PP(pow3) (excluding those cases where PP fails due to local minima: the corresponding points are outside the range of the figure), but this appears hardly significant. The superiority of the index adaptation feature of NGCA can be clearly observed in the data set (D), which includes both sub- and super-Gaussian components. Because of this composition, there is no single best non-Gaussianity index for this data set, and the proposed NGCA gives significantly lower error than that of either PP method.
Failure modes. We now try to explore the limits of the method and the conditions under which estimation of the target space will fail. First, we study the behaviour of NGCA again compared with PP as the total dimension of the data increases. We use the same synthetic data sets with 2-dimensional non-Gaussian components, while the number of Gaussian components increases. The averaged errors over 100 experiments are depicted in Figure 8. In all cases we seem to observe a sharp phase transition between a good behaviour regime and a failure mode where the procedure is unable to estimate the correct subspace. In 3 out of 4 cases however we observe that the phase transition to the failure mode occurs for a higher dimension for NGCA than for the PP methods, which indicates better robustness of NGCA.
In the synthetic data sets used so far, the data was always generated with a covariance matrix equal to identity. Another interesting setting to study is the robustness with respect to bad conditioning of the covariance matrix. We consider again a fixed-dimension setting, with 2 non-Gaussian and 8 gaussian dimensions.
While the non-Gaussian coordinates always have variance unity, the standard deviation of the 8 Gaussian dimensions now follows the geometrical progression 10-r, 10-r+2r/7, . . . , 10r. Thus, the higher r , the worse conditioned is the total covariance matrix.
The results are depicted in Figure 9, where we observe again a transition to a failure mode when the covariance matrix is too badly conditioned. Although NGCA still appears as the best method, we observe that on 3 out of 4 data sets the transition to failure mode seems to happen roughly at the same point as for PP methods. This suggests that there is
19

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

Error

Error

1 0.9

NGCA

NGCA

0.9

PP(pow3)

0.8 PP(pow3)

PP(tanh)

PP(tanh)

0.8 0.7

Error

0.7 0.6
0.6 0.5
0.5 0.4
0.4
0.3 0.3

0.2 0.2

0.1 0.1

0 10 15 20 25 30 35 40 45 50
Total Dimension

0 10 15 20 25 30 35 40 45 50
Total Dimension

(A) (B)

1 0.9 NGCA
0.9 0.8 PP(pow3) PP(tanh)
0.8 0.7

Error

0.7 0.6
0.6 0.5
0.5 0.4
0.4
0.3 0.3

0.2

NGCA

0.2

PP(pow3)

0.1

PP(tanh)

0.1

0 10 15 20 25 30 35 40 45 50
Total Dimension
(C)

0 10 15 20 25 30 35 40 45 50
Total Dimension
(D)

Figure 8: Results when the total dimension of the data increases.

20

In Search of Non-Gaussian Components of a High-Dimensional Distribution

Error

0.4 0.35
0.3 0.25
0.2 0.15
0.1 0.05
0 0
0.7 0.6 0.5

NGCA PP(pow3) PP(tanh)
0.5 1 1.5 Log10 noise scaling range
(A)
NGCA PP(pow3) PP(tanh)

0.4

0.3

0.2

0.1

0 0 0.5 1 1.5
Log10 noise scaling range
(C)

Error

Error

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 20
0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 20

NGCA PP(pow3) PP(tanh)
0.5 1 1.5 Log10 noise scaling range
(B)
NGCA PP(pow3) PP(tanh)
0.5 1 1.5 Log10 noise scaling range
(D)

2 2

Error

Figure 9: Results when the Gaussian (noise) components have different scales (the standard deviations follow a geometrical progression on [10-r, 10r] where r is the
parameter on the abscissa).

21

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller
no or only little added robustness of NGCA with respect to PP in this regard. However, this result is not entirely surprising, as we expect this type of failure mode to be caused by a too large estimation error in the covariance matrix and therefore in the whitening/dewhitening steps. Since these steps are common to NGCA and the PP algorithms, it seems logical to expect a parallel evolution of their errors.
4.2 Example of application for realistic data: visualization and clustering
We now give an example of application of our methodology to visualization and clustering of realistic data. We consider here "oil flow" data which has been obtained by numerical simulation of a complex physical model. This data was already used before for testing techniques of dimension reduction (Bishop et al., 1998). The data is 12-dimensional and it is not known a priori if some dimensions are more relevant. Here our goal is to visualize the data and possibly exhibit a clustered structure. Furthermore it is known that the data is divided into 3 classes. We show classes in different colors but the class information is not used in finding the directions (i.e. the process is unsupervised).
We compare the NGCA methodology described in the previous section, and PP ("vanilla" FastICA) using the tanh or the pow3 index. The results are shown on Figure 10. A 3D projection of the data was computed using these methods, which was in turn projected in 2D to draw the figure; this last projection was chosen manually so as to make the cluster structure as visible as possible in each case.
We see that the NGCA methodology gives a much more relevant projection than PP using either tanh or pow3 alone: we can distinguish 10-11 clusters versus at most 5 for the PP methods. Furthermore the classes are clearly separated only on the NGCA projection, on the other ones they are partially confounded in one single cluster. Finally, we confirm, by applying the projection found to held-out test data (i.e. data not used to determine the projection), that the cluster structure is relevant and not due to some overfitting artifact. This, in passing, shows one advantage of a linear projection method, namely that it can be extended to new data in a straightforward way.
Presumably, an important difference between the NGCA projection and the others comes from the Fourier functions, since they are not present in either of the PP methods. It can be confirmed by looking at the vector norms that Fourier functions are more relevant for this data set; they gave rise to estimated vectors with generally higher norms and had consequently a sizable influence of the choice of the projection. One could object that we have been merely lucky for this specific data because Fourier functions happened to be more relevant, and neither PP method uses this index. One could suggest, for fair comparison, to use the PP algorithm with a Fourier index. However, beside the fact that this index is not generally used in classical PP methods, the results would be highly dependent of the specific frequency parameter chosen, so we did not make experiments in that direction (by contrast, the NGCA methodology allows to combine vectors obtained from different frequencies). On the other hand, another route to investigate the relevance of this objection is to look at the results obtained by the NGCA method if Fourier functions are not used ­ thus only considering Gauss-pow3 and tanh. In this case we still hope an improvement over PP because NGCA is combining indices (as well as combining over the parameters ranges 2 and b ). This is confirmed in Figure 10: even without the relevant Fourier functions, NGCA
22

In Search of Non-Gaussian Components of a High-Dimensional Distribution
Figure 10: 2D projection of the "oil flow" (12-dimensional) data obtained by different algorithms. Top: vanilla FastICA methods (left: tanh index, right: pow3 index). Middle: on the left, projection obtained by NGCA (using a combination of Fourier, tanh, Gauss-pow3 indices). On the right is the projection obtained by applying NGCA without the Fourier index. Finally, the bottom panel shows the projection of additional test data using the projection found by NGCA on the middle-left panel. In each case (except the bottom panel), the data was first projected in 3D using the respective methods (without class information), from which a 2D projection was chosen visually so as to yield the clearer cluster structure. For the bottom panel, the 2D projection found from the middle-left panel was used to visualize additional test data. 23

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller
yields a projection where 8 clusters can be distinguished, and the classes are much more clearly separated than with PP methods. Finally, a visual comparison with the results obtained by Bishop et al. (1998) demonstrated that the projection found by our algorithm exhibits a clearer clustered structure; moreover ours is a purely linear projection whereas the latter reference was a nonlinear data representation
Further analysis on clustering performance with additional data sets are given in the Appendix and underline the usefulness of our method.
5. Conclusion
We proposed a new semi-parametric framework for constructing a linear projection to separate an uninteresting multivariate Gaussian `noise' subspace of possibly of large amplitude from the `signal-of-interest' subspace. Our theory provides generic consistency results on how well the non-Gaussian directions can be identified (Theorem 4). To estimate the nonGaussian subspace from the set of vectors obtained, PCA is finally performed after suitable renormalization and elimination of uninformative vectors. The key ingredient of our NGCA method is to make use of the gradient computed for the nonlinear basis function h(x) in Eq.(11) after data whitening. Once the low-dimensional `signal' part is extracted, we can use it for a variety of applications such as data visualization, clustering, denoising or classification.
Numerically we found comparable or superior performance to, e.g., FastICA in deflation mode as a generic representative of the family of PP algorithms. Note that in general PP methods need to pre-specify a projection index with which they search non-Gaussian components. By contrast, an important advantage of our method is that we are able to simultaneously use several families of nonlinear functions; moreover, also inside a same function family we are able to use an entire range of parameters (such as frequency for Fourier functions). Thus, our new method provides higher flexibility, and less restricting assumptions a priori on the data. In a sense, the functional indices that are the most relevant for the data at hand are automatically selected.
Future research will adapt the theory to simultaneously estimate the dimension of the non-Gaussian subspace. Extending the proposed framework to non-linear projection scenarios (Cox and Cox, 1994; Scho¨lkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi, 2003; Harmeling et al., 2003) and to finding the most discriminative directions using labels are examples for which the current theory could be taken as a basis.
Acknowledgements: The authors would like to thank Stefan Harmeling for discussions and J.-F. Cardoso for suggesting us the pre-whitening step for increased efficiency and robustness. We would also like to thank anonymous reviewers for many insightful comments, in particular pointing out the ICA interpretation. We acknowledge partial financial support by DFG, BMBF and the EU NOE PASCAL (EU # 506778). GB and MS also thank Alexander von Humboldt foundation for partial financial support.
24

In Search of Non-Gaussian Components of a High-Dimensional Distribution

Appendix A. Proofs of the theorems
A.1 Proof of Lemma 1
Suppose first that the noise N is standard normal. Denote by E the projector from Rd to Rm which corresponds to the subspace E . Let also E be the subspace complementary to E and E mean the projector on E . The standard normal noise can be decomposed as N = N1 N2 where N1 = EN and N2 = EN are independent noise components. Similarly, the signal X can be decomposed as
X = (ES + N1) N2
where we have used the model assumption that the signal S is concentrated in E and it is independent of N . It is clear that the density of ES + N1 in Rm can be represented as the product g(x1)(x1) for some function g and the standard normal density (x1) , x1  Rm . The independence of N1 and N2 yields the in the similar way for x = (x1, x2) with x1 = Ex and x2 = Ex that p(x) = g(x1)(x1)(x2) = g(x1)(x) . Note that for the linear mapping T = E characterizes the signal subspace E . Namely, E is the image
(T ) of the dual operator T  while E is the null subspace (kernel) of T : E = K(T ) . Next we drop the assumption of the standard normal noise and assume only that the
covariance matrix  of the noise is nondegenerated. Multiplying the both sides of the equation (1) by the matrix -1/2 leads to -1/2X = -1/2S + N where N = -1/2N is standard normal. The transformed signal X = -1/2S belongs to the subspace E = -1/2E . Therefore, the density of X can be represented as p(x) = g(Ex)(x) where E is the projector corresponding to E . Coming back the variable x yields the density of X in the form p(x) = g(T x)(-1/2x) where T = E-1/2 .

A.2 Proof of Proposition 2 For any function (x) , it holds that

(x + u)p(x)dx = (x)p(x - u)dx,

if the integrals exists. Under mild regularity conditions on p(x) and (x) allowing differentiation under the integral sign, differentiating this with respect to u gives

(x)p(x)dx = - (x)p(x)dx.

(15)

Let us take the following function

h(x) := h(x) - x E [Xh(X)] ,

whose gradient is

h(x) = h(x) - E [Xh(X)] .

25

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

The vector (h) is the expectation of -h . From Eq.(15) and using p(x) =  log p(x) p(x) , we have

(h) = h(x) log p(x) p(x)dx.

Applying Eq.(2) to the above equation yields

(h) = h(x) log g(T x) p(x)dx - h(x)-1x p(x)dx = T  h(x)g(T x),(x)dx - -1 xh(x)p(x)dx.

(16)

Under the assumption E XX = Id , we get
E [Xh(X)] = E [Xh(X)] - E XX E [Xh(X)] = 0,
that is, the second term of Eq.(16) vanishes. Since the first term of Eq.(16) belongs to I by the definition of I , we finally have (h)  I .

A.3 Proof of Theorem 3

For a fixed function h, we will essentially apply Lemma 5 stated below for each coordinate

of

Y (h) .

Denoting

the

k -th

coordinate

of

a

vector

v

by

v(k) ,

and

y

=

-

1 2

x

,

we

have

h(k)(x) = [h(y) - yh(y)](k)  B(1 + y )  B (1 + K x ) .

It follows that h(k)(x) is such that

E exp

0 BK

h(k)(x)

 a0 exp

0 K

,

and hence satisfies the assumption of Lemma 5. Denoting by k2 the sample variance of h(k) , we apply the lemma with  = /d , obtaining by the union bound that with probability at least 1 - 4 , for all 1  k  d :

Y - Y

(k)

2  4k2 log

d-1 n

+

C1(0,

a0,

B,

d,

K

)

log2

(n-1)

n

3 2

log2

-1

,

where we have used the inequality (a + b)2  2(a2 + b2) and C1 denotes some function

depending only on the square root and using

indicated a+b

quaan+titiebs.leNadoswtsou: mming

over

the

coordinates,

taking

the

Y - Y

2

Y2

(h) log

-1 + n

log

d

+

C2(0,

a0,

B,

d,

K)

log(n-1) log -1

n

3 4

,

(17)

with probability at least 1 - 4 . To turn this into a uniform bound over the family {hk}kL=1, we simply apply this inequality separately to each function in the family with

26

In Search of Non-Gaussian Components of a High-Dimensional Distribution

 = /L. This leads to the first announced inequality of theorem. We obtain the second

one by multiplying the first by

-

1 2

to the left and using the assumption on

-1 .

Lemma 5 Let X be a real random variable such that for some 0 > 0 :

E [exp (0 |X|)]  a0 < .

Let X1, . . . , Xn denote an i.i.d. sequence of copies of X . Let

and

2 =

1 2n(n-1)

i=j(Xi - Xj)2 be the sample variance.

µ = E [X] ,

µ=

1 n

n i=1

Xi

Then for any



<

1 4

the following holds with probability at least

1 - 4 , where

c

is a

universal constant:

|µ - µ| 

22

log n

-1

+

c-0 1

max

1, log

na0-1

log -1 n

3 4

+

log -1 n

.

Proof For A > 0 denote XA = X1{|X|  A} . We decompose

1 n

n i=1

Xi

-

µ



1n n i=1

Xi - XiA

+

1 n

n i=1

XiA

-

E

XA

+ E X - XA

;

these three terms will be denoted by T1, T2, T3 . By Markov's inequality, it holds that

P [|X| > t]  a0 exp (-0t) ,

Fixing A = log n-1a0 /0 for the rest of the proof, it follows by taking t = A in the above inequality that for any 1  i  n :

P

XiA = Xi



 n

.

By the union bound, we then have XiA = Xi for all i , and therefore T1 = 0 , except for a set A of probability bounded by  .
We now deal with the third term: we have



T3 = |E [X1{|X| > A}]|  E [X1{X > A}] = P [X1{X > A} > t] dt

0 

 AP [X > A] + a0 exp (-0t) dt

A

 a0 A + -0 1 exp (-0A)

=

 n0

1 + log

n-1a0

.

Finally, for the second term, since XA  A = 0-1 log n-1a0 , Bernstein's inequality ensures that with probability as least 1 - 2 the following holds:

1 n

n i=1

XiA

-

E

XA



2Var [XA] log -1 n

+

2

log

n-1a0 0n

log -1 .

27

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

We finally turn to the estimation of Var XA . The sample variance of XA is given by

(A)2

=

1 2n(n -

1)

XiA - XjA 2 ;

i=j

Note that (A)2 is an unbiased estimator of Var XA . Furthermore, replacing XiA by XiA in the above expression changes this quantity at most of 4A2/n since XiA appears only in 2(n - 1) terms. Therefore, application of the bounded difference (a.k.a. McDiarmid's)

inequality (McDiarmid , 1989) to the random variable A yields that with probability 1-

we have

(A)2 - Var XA

 4A2

log -1 n

;

finally, except for samples in the set A which we have already excluded above, we have A =  . Gathering these inequalities lead to the conclusion.

A.4 Proof of Theorem 4
In this proof we will denote by C(·) a factor depending only on the quantities inside the parentheses, and whose exact value can vary from line to line.
From Lemmas 9 and 10 below, we conclude that for big enough n , the following inequality is satisfied with probability 1 - 2/n:

-

1 2

-

-

1 2

 C(a0, 0, K)

d

log n

n

;

(18)

also, it is a a weaker consequence of Lemmas 7 and 8 that the following inequalities hold with probability at least 1 - 1/n each (again for n big enough):

1n n

Xi  C(a0, 0) ,

i=1

1n n

Xi 2  C(a0, 0) .

i=1

(19) (20)

Let us denote  the set of samples where (18), (19) and (20) are satisfied simultaneously; from the above we conclude that for large enough n the set  contains the sample with probability at least 1 - 4/n. For the remainder of the proof we suppose that this condition is satisfied.
For any function h, we have

Y - Y  Y - Y + Y - Y
Note that (up to some changes in the constants) the assumption on the Laplace transform is stronger than the assumption of Theorem 3; hence equation (17) in the proof of this

28

In Search of Non-Gaussian Components of a High-Dimensional Distribution

theorem holds and we have with probability at least 1 - 4 , for any function in the family {hk}kL=1 :

Y - Y

2

Y2

(h) log

(L-1) n

+

log

d

+

C(0, a0,

B,

d, K)

log(nL-1) log L-1

n

3 4

.

(21)

On the other hand, conditions (18) and (19) imply that for any function h in the family,

Y - Y

=

1n n i=1

h(Yi) - h(Yi)



M n

n i=1

Yi - Yi



M n

-

1 2

-

-

1 2

n
Xi
i=1

 C(a0, 0, K)M

d

log n

n

.

where in the first inequality, we have used the Lispchitz assumption on the function h.
One remaining technicality is to replace the term Y (h) (which cannot be evaluated from the data since it depends on the exactly whitened data Yi ) in (21) by Y (h) which can be evaluated from the data. For this use the following, holding for any function h in
the family:

Y2 (h) - Y2 (h)

=

1 2n(n -

1)

22
h(Yi) - h(Yj) - h(Yi) - h(Yj) ;

i=j

let us now focus on one term of the above sum:

22
h(Yi) - h(Yj) - h(Yi) - h(Yj)

= h(Yi) - h(Yi) - h(Yj) + h(Yj) h(Yi) - h(Yj) + h(Yi) - h(Yj)

 M 2 Yi - Yi + Yj - Yj

Yi - Yj + Yi - Yj

 M2

-

1 2

-

-

1 2

-

1 2

+

-

1 2

( Xi + Xj )2

 M 2C(a0, 0, K)

d log n n

Xi 2 + Xj 2 ,

where we have used the Cauchy-Schwarz inequality, the triangular inequality and the Lipschitz assumption on h at the third line. Summing over i = j this expression and using condition (20) we obtain

Y2 (h) - Y2 (h)  M 2C(a0, 0, K)

d

log n

n

,

so that we can effectively replace Y by Y in (21) up to additional lower-order terms. This concludes the proof of the first inequality in the theorem.

29

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

For the second inequality, we additionally write

dist((h), I) 

-

1 2

Y

-

-

1 2

Y



-

1 2

-

-

1 2

Y

+

-

1 2

Y - Y

+

-

1 2

-

-

1 2

Y - Y ;

we now conclude using (18), the previous inequalities controlling Y - Y , the assump-

tion on

-

1 2

and the fact that

Y = E [Xh(X) - h(X)]  B(1 + E [ x ])  C(a0, 0, B) .

Appendix B. Additional proofs and results

We have used Bernstein's inequality which we recall here for completeness under the following form:
Theorem 6 Suppose X1, . . . , Xn are i.i.d. real random variables such that |X|  b and V arX = 2 . Then

P n-1

Xi - E(Xi) >

22

x n

+

2b

x n

 2 exp(-x).

i

The

following

results

concern

the

estimation

of

-

1 2

,

needed

in

the

proof

of

Theorem

4.

We divide this into 4 lemmas.

Lemma 7 Let 1, . . . , n be i.i.d. with E [1] = m and assume log E [exp µ(1 - m)]  cµ2/2 holds for all µ  µ0 , for some positive constants c and µ0 . Then for sufficiently large n

n
P n-1/2 (i - m) > z  e-c-1z2/2.
i=1

Proof This is an application of Chernoff's bounding method:

n
Rn := log P n-1/2 (i - m) > z
i=1
n  -µz n + log E exp µ(i - m)
 i=1 = -µz n + n log E [exp µ(1 - m)] ,

where the above inequality is Markov's. We select µ = zn-1/2c-1 . For n sufficiently large,

it holds that µ  µ0 and by the lemma condition

Rn



 -µz n

+

ncµ2/2

=

-z2c-1/2.

30

In Search of Non-Gaussian Components of a High-Dimensional Distribution

The goal of the following Lemma is merely to replace the assumption about the Laplace transform (in the previous Lemma) by a simpler assumption (existence of some exponential moment). This allows a simpler statement ­ as far as we are not really interested in the precise constants involved.

Lemma 8 Let X be a real random variable such that for some µ0 > 0 :

E [exp(µ0 |X|)] = e0 < .

Then there exists c > 0 (depending only on µ0 and e0 ) such that µ  R |µ|  µ0/2  log E [exp (µ (X - E [X]))]  cµ2/2.

Proof Note that X has finite expectation since |X|  µ0-1 exp µ0 |X| . Taylor's expansion gives that

x



R,

µ



R,

|µ|

<

µ0/2



exp(µx)



1 + µx +

µ2 2

x2

exp(|µ0

|

|x|

/2).

There exists some constant c > 0 (depending on µ0 ) such that

(22)

x  R, x2 exp(|µ0x| /2)  c (exp(|µ0x|)) .

Using this

and the assumption, taking expectation in (22) yields that for

c

=

1 2

ce0

>

0

µ  R, |µ| < µ0/2  E [exp(µX)]  1 + µE [X] + c µ2  exp µE [X] + c µ2 ,

implying

E [exp (µ (X - E [X]))]  exp c µ2 ;

taking logarithms on both sides yields the conclusion.

The next two Lemmas, once combined, provide the confidence bound on

-

1 2

-

-

1 2

which we need for the proof of Theorem 4.

Lemma 9 Let X1, . . . , Xn be i.i.d. vectors in Rd . Assume that, for some µ0 > 0 ,

E exp µ0 X 2 = e0 <  ;

(23)

denote  = E XX and  it empirical counterpart. Then for some constant  depending only on (µ0, e0) , and for big enough n ,

Rn := P

- >

d log n n



2 n

.

31

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

Proof Along this proof C, c will denote constants depending only on µ0, e0 ; their exact value can change from line to line. Note that by definition of  and  ,

-

=

sup
Bd

1 n

n i=1

(Xi )2 - E

2
X

,

where Bd denotes the unit ball of Rd . For  < 1 , let Bd, denote a -packing set of Bd, that is, a discrete  -separated set of points of Bd of maximum cardinality. By the maximality assumption and the triangle inequality, Bd, is also a 2-covering net of Bd. On the other hand, the -balls centered on these points are disjoint and their union is included
in the ball of radius (1 + ) , so that a volume comparison allows us to conclude that #(Bd,)d  (1 + )d  2d . This shows that Bd,2 is a 4-covering set of Bd of cardinality bounded by -d.
Now, if ,   Bd are such that  -   4 , then we have

nn

n

(Xi )2 - (Xi  )2 = (Xi ( -  ))(Xi ( +  ))

i=1 i=1

i=1

n

 8

Xi 2 ,

i=1

where we have applied the Cauchy-Schwarz inequality at the last line.

Now application of Lemmas 7 and 8 yields that for n large enough, with probability at

least 1 - 1/n ,

n
n-1 Xi 2  E
i=1

X2 +

c

log n

n



C

.

The above implies that with probability at least 1 - 1/n ,

sup

n-1/2

n
(Xi )2 -

n
(Xi  )2

  C n.

, Bd: - 2

i=1

i=1

We can also show a similar inequality about the corresponding expectation

sup

n-1/2 E (X )2 - E (X  )2

  C n.

, Bd: - 2

We

now

select



=

n-

1 2

.

Therefore,

approximating

any



 Bd

by

its

nearest

neighbour

in Bd,2 and using the above inequalitites, we obtain that

Rn



1 n

+

P

n

sup n-1/2

Bd,2

i=1

(Xi )2 - E

2
X



1 n

+

n

P n-1/2

(Xi )2 - E

Bd,2

i=1

2
X



1 n

+

#(Bd,2) exp{-0.5c-1(

-

C)d log n}



2 n

> d log n - C > ( - C)d log n

32

In Search of Non-Gaussian Components of a High-Dimensional Distribution

provided that  is chosen so that c-1( - C)d/2 > d/2 + 1 . Here we
have again used Lemmas 7 and 8, noting that for any   Bd it holds that E exp µ0  X  E [exp µ0 X ] < exp(µ0) + e0 by assumption.

Lemma 10 Let A, B be two real positive definite symmetric matrices satisfying A - B   with   (2 A-1 )-1 . Then there exists a constant C such that

A-

1 2

-

B-

1 2

C

A-1

3
2 .

Proof Note that for M < 1 it holds that

(I

-

M

)-

1 2

=

kM k ,

k0

 with (k)  0 the coefficients of the power series development of the function 1/ 1 - x .
Denote max(M ), min(M ) the biggest and smallest eigenvalue of a matrix M . Put K = A = max(A) and L = A-1 = min(A)-1 . Note that LK  1 . Put A = A/K, B = B/K . All eigenvalues of A belong to (0, 1] and therefore

I - A = max(I - A ) = 1 - min(A ) = 1 - (LK)-1 .

By the assumption that   (2L)-1 , it holds that

max(B ) = K-1

B

 K-1 ( A

+

)



1

+

(2LK )-1



3 2

,

and that

min(B )  K-1 (min(A) - )  (2KL)-1 ,

from this we deduce that

I -B

= max(max(B ) - 1, 1 - min(B ))  max

1 2

,

1

-

(2LK

)-1

= 1 - (2LK)-1 .

Putting A = I - A , B = I - B , we have ensured that A < 1 and B < 1 ; we can thus write

A

-

1 2

-

B

-

1 2

=

I

-A

-

1 2

-

I -B

-

1 2

= k(Ak - Bk) .
k1

Noticing that

k-1

Ak - Bk =

Ai(A - B)Bk-1-i  k max

i=0

A,B

k-1 A - B

,

33

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

we obtain

A

-

1 2

-B

-

1 2

 A -B

kk 1 - (2LK)-1 k-1

k1

=

 K

1 2

(2LK

)

3 2

=

C

L

3 2

K

1 2



.

From this we deduce that

A-

1 2

-

B-

1 2

=

K

-

1 2

A

-

1 2

-B

-

1 2



CL

3 2

.

Appendix C. Clustering results
The goal of NGCA is to discover interesting structure in the data. It is naturally a difficult task to quantify this property precisely. In this appendix we try to make this apparent using clustering techniques. We apply a mean distance linkage clustering algorithm to data projected in lower dimension using various techniques: NGCA, FastICA, PCA, LLE, Isomap.
There is no single well-defined performance measure for the performance of clustering. Here we resort to indirect criteria that should however allow a comparative study. We consider the two following criteria:
(1) Label cross-information. We apply clustering to benchmark data for which label information Y is available. Although this information is not used in determining the clustering, we will use it as a yardstick to measure whether the clustering gives rise to relevant structure discovery. We measure this by the scaled mutual information I(C, Y )/H(Y ), where C is the cluster labelling and the normalization ensures that the quantity lies between 0 and 1. Note that there is a priori no mathematical reason why clustering should be related to label information, but this is often the case for real data, so this can be a relevant criterion of structure discovery. A higher score indicates a better match between discovered cluster structure and label structure.
(2) Stability. Recent attempts at formalizing criteria for clustering have proposed that clustering stability should be a relevant criterion for data clustering (e.g. (Meinecke et al. , 2002; Lange et al., 2004)). Again, this is only an indirect criterion as, for example, a trivial clustering algorithm dividing the space without actually looking at the data would be very stable. But with this caveat in mind, it provides a relevant diagnostic tool. Here we measured stability in the following way: the data is divided randomly into 2 groups of equal size on which we apply clustering. Then the cluster labels obtained on group 1 are extended to group 2 by the nearest-neighbor rule and vice-versa. This thus gives rise to two different cluster labellings C1, C2 of the whole data and we measure their agreement through relative mutual information I(C1, C2)/H(C1, C2) . Again, this score lies in the interval [0, 1] and a high score indicates better stability.
We consider the "oil flow" data already presented in section 4.2, and additional data sets from the UCI classification repository, for which the features all take continuous values.
34

In Search of Non-Gaussian Components of a High-Dimensional Distribution

(When there are features taking only discrete values, NGCA is inappropriate since these will generally be picked up as strongly non-Gaussian). Size and dimension of these data sets are given in Table 1.

Data set Oil Wine
Vowel USPS

Table 1: Description of data sets

Nb. of Classes Nb. of samples Total dimension

3 2000 12

3 178 13

11 528

10

10 7291

30

Projection Dim. 3 3 3 10

The results are depicted in Figure 11. On the Oil data set, NGCA works very well for both criteria (as was expected from the good visualization results of section 4.2). On the Wine data set, the different algorithms appear to be divided in two clear groups, with the performance in the first group (NGCA, Isomap, LLE) noticeably better than in the second (PCA, FastICA). NGCA belongs to the better group although the best methods appear to be the non-linear projections LLE and Isomap. The results of the Vowel data set are probably the most difficult to interpret as most methods appear to be relatively close. Isomap appear as the winner method in this case with NGCA quite close in terms of label cross-information and in the middle range for stability. Finally, for the USPS data set we used the 30 first principal components obtained by Kernel-PCA and a polynomial kernel of degree 3. In this case PCA gives better results in terms of label cross-information with NGCA a close second, while NGCA is the clear winner in terms of stability.
To summarize: NGCA performed very well in 2 of the 4 data sets tried (Oil data and USPS), and was in the best group of methods for the Wine Data and had average performance on the last data set. Even when NGCA is outperformed by nonlinear methods LLE and Isomap, it generally achieves a comparable performance though being a linear method, which has other advantages such as clearer geometrical interpretation, direct extension to additional data if needed, possible assessment of variable importance in original space.
References
M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373­1396, 2003.
C.M. Bishop, M. Svensen and C.K.I. Wiliams. GTM: The generative topographic mapping. Neural Computation, 10(1):215­234, 1998.
C.M. Bishop and G.D. James. Analysis of multiphase flow using dual-energy gamma densitometry and neural networks. Nuclear Instruments and Methods in Physics Research, A327:580­593, 1993.
P. Comon. Independent component analysis--a new concept? Signal Processing, 36:287­314, 1994.
T.F. Cox and M.A.A. Cox. Multidimensional Scaling. Chapman & Hall, London, 2001.
L. Devroye, L. Gy¨orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition, Springer, 1996.
35

Blanchard, Kawanabe, Sugiyama, Spokoiny and Mu¨ller

Oil Data: Class Label Information
1

1

Oil Data: stability criterion

0.9 0.9

0.8 0.8

0.7 0.7

0.6 0.6

0.5

0.5

0.4

NGCA

0.3

NGCA PCA

0.4

PCA LLE

0.2

LLE Isomap

0.3

Isomap FastICA-tanh

0.1

FastICA-tanh FastICA-pow3

0.2

FastICA-pow3 Org.Data

Org.Data

0 0.1 2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20

Nb. of clusters

Nb. of clusters

Wine Data: Class Label Information
1

0.8

Wine Data: Stability Criterion

0.9 0.7
0.8 0.6
0.7
0.5 0.6

0.5 0.4

0.4

NGCA

0.3

0.3 PCA

LLE

0.2

0.2 Isomap

FastICA-tanh

0.1

FastICA-pow3

0.1

Org.Data

NGCA PCA LLE Isomap FastICA-tanh FastICA-pow3 Org.Data

00 2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20

Nb. of clusters

Nb. of clusters

Vowel Data: Class Label Information
0.7
NGCA PCA 0.6 LLE Isomap FastICA-tanh 0.5 FastICA-pow3 Org.Data

1 0.9 0.8 0.7

Vowel Data: Stability Criterion

0.4 0.6

0.3 0.5

0.4 NGCA

0.2 PCA

0.3 LLE Isomap

0.1 FastICA-tanh 0.2 FastICA-pow3

Org.Data

0 0.1 2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20

Nb. of clusters

Nb. of clusters

USPS Data: Class Label Information
0.7
NGCA PCA 0.6 LLE Isomap FastICA-tanh 0.5 FastICA-pow3 Org.Data

0.7 0.6 0.5

USPS Data: Stability Criterion

0.4 0.4

0.3 0.3

NGCA 0.2 0.2 PCA
LLE Isomap 0.1 0.1 FastICA-tanh FastICA-pow3 Org.Data

00 2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20

Nb. of clusters

Nb. of clusters

Figure 11: Clustering results

36

In Search of Non-Gaussian Components of a High-Dimensional Distribution
B. Efron. Bootstrap methods: Another look at the jackknife. The Annals of Statistics, 7(1):1­26, 1979.
J.H. Friedman and J.W. Tukey. A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on Computers, 23(9):881­890, 1975.
S. Harmeling, A. Ziehe, M. Kawanabe and K.-R. Mu¨ller. Kernel-based nonlinear blind source separation. Neural Computation, 15(5):1089­1124, 2003.
P.J. Huber. Projection pursuit. The Annals of Statistics, 13:435­475, 1985. A. Hyv¨arinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE
Transactions on Neural Networks, 10(3):626­634, 1999. A. Hyv¨arinen, J. Karhunen and E. Oja. Independent Component Analysis. Wiley, 2001. M. C. Jones and R. Sibson. What is projection pursuit? Journal of the Royal Statistical Society,
series A, 150:1­36, 1987. C. McDiarmid. On the method of bounded differences, Surveys in Combinatorics, London Math.
Soc. Lecture Notes Series 141:148­188, 1989. F. Meinecke, A. Ziehe, M. Kawanabe, and K.-R. Mu¨ller. A resampling approach to estimate the
stability of one-dimensional or multidimensional independent components. IEEE Transactions on Biomedical Engineering, 49:1514­1525, 2002. S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323­2326, 2000. T. Lange, V. Roth, M. L. Braun and J. M. Buhmann. Stability-based validation of clustering solutions. Neural Computation, 16(6):1299-1323, 2004. B. Sch¨olkopf, A.J. Smola and K.­R. Mu¨ller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299­1319, 1998. J.B. Tenenbaum, V. de Silva and J.C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319­2323, 2000.
37

SFB 649 Discussion Paper Series 2006
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Calibration Risk for Exotic Options" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
002 "Calibration Design of Implied Volatility Surfaces" by Kai Detlefsen and Wolfgang K. Härdle, January 2006.
003 "On the Appropriateness of Inappropriate VaR Models" by Wolfgang Härdle, Zdenk Hlávka and Gerhard Stahl, January 2006.
004 "Regional Labor Markets, Network Externalities and Migration: The Case of German Reunification" by Harald Uhlig, January/February 2006.
005 "British Interest Rate Convergence between the US and Europe: A Recursive Cointegration Analysis" by Enzo Weber, January 2006.
006 "A Combined Approach for Segment-Specific Analysis of Market Basket Data" by Yasemin Boztu and Thomas Reutterer, January 2006.
007 "Robust utility maximization in a stochastic factor model" by Daniel Hernández­Hernández and Alexander Schied, January 2006.
008 "Economic Growth of Agglomerations and Geographic Concentration of Industries - Evidence for Germany" by Kurt Geppert, Martin Gornig and Axel Werwatz, January 2006.
009 "Institutions, Bargaining Power and Labor Shares" by Benjamin Bental and Dominique Demougin, January 2006.
010 "Common Functional Principal Components" by Michal Benko, Wolfgang Härdle and Alois Kneip, Jauary 2006.
011 "VAR Modeling for Dynamic Semiparametric Factors of Volatility Strings" by Ralf Brüggemann, Wolfgang Härdle, Julius Mungo and Carsten Trenkler, February 2006.
012 "Bootstrapping Systems Cointegration Tests with a Prior Adjustment for Deterministic Terms" by Carsten Trenkler, February 2006.
013 "Penalties and Optimality in Financial Contracts: Taking Stock" by Michel A. Robe, Eva-Maria Steiger and Pierre-Armand Michel, February 2006.
014 "Core Labour Standards and FDI: Friends or Foes? The Case of Child Labour" by Sebastian Braun, February 2006.
015 "Graphical Data Representation in Bankruptcy Analysis" by Wolfgang Härdle, Rouslan Moro and Dorothea Schäfer, February 2006.
016 "Fiscal Policy Effects in the European Union" by Andreas Thams, February 2006.
017 "Estimation with the Nested Logit Model: Specifications and Software Particularities" by Nadja Silberhorn, Yasemin Boztu and Lutz Hildebrandt, March 2006.
018 "The Bologna Process: How student mobility affects multi-cultural skills and educational quality" by Lydia Mechtenberg and Roland Strausz, March 2006.
019 "Cheap Talk in the Classroom" by Lydia Mechtenberg, March 2006. 020 "Time Dependent Relative Risk Aversion" by Enzo Giacomini, Michael
Handel and Wolfgang Härdle, March 2006. 021 "Finite Sample Properties of Impulse Response Intervals in SVECMs with
Long-Run Identifying Restrictions" by Ralf Brüggemann, March 2006. 022 "Barrier Option Hedging under Constraints: A Viscosity Approach" by
Imen Bentahar and Bruno Bouchard, March 2006.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

023 "How Far Are We From The Slippery Slope? The Laffer Curve Revisited" by Mathias Trabandt and Harald Uhlig, April 2006.
024 "e-Learning Statistics ­ A Selective Review" by Wolfgang Härdle, Sigbert Klinke and Uwe Ziegenhagen, April 2006.
025 "Macroeconomic Regime Switches and Speculative Attacks" by Bartosz Makowiak, April 2006.
026 "External Shocks, U.S. Monetary Policy and Macroeconomic Fluctuations in Emerging Markets" by Bartosz Makowiak, April 2006.
027 "Institutional Competition, Political Process and Holdup" by Bruno Deffains and Dominique Demougin, April 2006.
028 "Technological Choice under Organizational Diseconomies of Scale" by Dominique Demougin and Anja Schöttner, April 2006.
029 "Tail Conditional Expectation for vector-valued Risks" by Imen Bentahar, April 2006.
030 "Approximate Solutions to Dynamic Models ­ Linear Methods" by Harald Uhlig, April 2006.
031 "Exploratory Graphics of a Financial Dataset" by Antony Unwin, Martin Theus and Wolfgang Härdle, April 2006.
032 "When did the 2001 recession really start?" by Jörg Polzehl, Vladimir Spokoiny and Ctlin Stric, April 2006.
033 "Varying coefficient GARCH versus local constant volatility modeling. Comparison of the predictive power" by Jörg Polzehl and Vladimir Spokoiny, April 2006.
034 "Spectral calibration of exponential Lévy Models [1]" by Denis Belomestny and Markus Reiß, April 2006.
035 "Spectral calibration of exponential Lévy Models [2]" by Denis Belomestny and Markus Reiß, April 2006.
036 "Spatial aggregation of local likelihood estimates with applications to classification" by Denis Belomestny and Vladimir Spokoiny, April 2006.
037 "A jump-diffusion Libor model and its robust calibration" by Denis Belomestny and John Schoenmakers, April 2006.
038 "Adaptive Simulation Algorithms for Pricing American and Bermudan Options by Local Analysis of Financial Market" by Denis Belomestny and Grigori N. Milstein, April 2006.
039 "Macroeconomic Integration in Asia Pacific: Common Stochastic Trends and Business Cycle Coherence" by Enzo Weber, May 2006.
040 "In Search of Non-Gaussian Components of a High-Dimensional Distribution" by Gilles Blanchard, Motoaki Kawanabe, Masashi Sugiyama, Vladimir Spokoiny and Klaus-Robert Müller, May 2006.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

