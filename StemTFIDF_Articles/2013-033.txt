BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2013-033
Estimation and Inference for Varyingcoefficient Models with
Nonstationary Regressors using Penalized Splines
Haiqiang Chen* Ying Fang* Yingxing Li*
* Xiamen University, China
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Estimation and Inference for Varying-coefficient Models with
Nonstationary Regressors using Penalized Splines 
Haiqiang Chen, Ying Fang and Yingxing Li Wang Yanan Institute for Studies in Economics, MOE Key Laboratory of Econometrics, and Fujian Key Laboratory of Statistical Science, Xiamen University, Xiamen, Fujian 361005, China
12th July 2013
Abstract This paper considers estimation and inference for varying-coefficient models with nonstationary regressors. We propose a nonparametric estimation method using penalized splines, which achieves the same optimal convergence rate as kernel-based methods, but enjoys computation advantages. Utilizing the mixed model representation of penalized splines, we develop a likelihood ratio test statistic for checking the stability of the regression coefficients. We derive both the exact and the asymptotic null distributions of this test statistic. We also demonstrate its optimality by examining its local power performance. These theoretical findings are well supported by simulation studies. JEL Classification: C12, C14, C22 Keywords: Nonstationary Time Series; Varying-coefficient Model; Likelihood Ratio Test; Penalized Splines A previous version of this paper was presented in the 3rd WISE-Humboldt Workshop in Nonparametric Nonstationary High-dimensional Econometrics in May 2012. The authors are grateful to Zongwu Cai, Jiti Gao, Wolfgang H¨ardle, Yongmiao Hong and all workshop participants for their valuable comments and suggestions. They also acknowledge the financial support received from Chinese National Science Foundation with grant numbers 71201137, 71271179, 71131008 and 11201390. This research was also partially supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
1

1 Introduction
Regression models with nonstationary regressors have received much attention in the literature of theoretical and applied econometrics since the seminal work by Nelson and Plosser (1982). Amongst the popular research in this field has been the study of cointegration. The traditional framework of Engle and Granger (1987), assuming constant cointegrating coefficients, provides an appealing analytical framework to characterize the long-run equilibrium relationship. However, very few empirical studies support the presence of cointegration with constant coefficients. Such an empirical frustration is due to the lack of flexibility of traditional cointegration models in accommodating the time-varying long-run equilibrium relationship.
There are many empirical examples in economics and finance demonstrating time-varying features in cointegrating relationships. For example, Goldfajn and Baig (1998) argue that, during the Asian currency crisis, the cointegrating relation between the spot exchange rate and the interest rate differential is not fixed but depends on the level of the interest rate. Another example is in the literature of stock return predictability, where one of the theoretical and practical issues is to answer whether we could predict the asset return from fundamental variables such as the dividend-price ratio and the earning-price ratio, which are well known nonstationary time series variables (Campbell and Yogo 2006). Although linear prediction models have been extensively used, Lettau and Ludvigsson (2001), Goyal and Welch (2003) and Paye and Timmermann (2006) find empirical evidence that the cointegrating stock return forecasting models might be instable.
Many studies adopt nonparametric methods to capture the time-varying relation with nonstationary variables. The latest research include Wang and Phillips (2009a, 2009b) and Cai, Li and Park (2009), among others. Wang and Phillips (2009b) construct asymptotic theories for the local time density estimation in nonparametric cointegrating regression. They find that the so called ill-posed inverse problem does not exist in nonparametric regression
2

with nonstationary endogenous regressors. Cai et al. (2009) investigate the asymptotic property of local linear estimators in a varying-coefficient model when the smoothing variable is nonstationary but the covariates are either stationary.
Testing the stability of varying coefficients becomes another important issue in this literature. For example, Park and Hahn (1999) construct two residual-based statistics to test the constancy of the cointegrating coefficients based on the series estimation. Kasparis (2008) develops two residual-based statistics for testing the functional form misspecification in cointegrating relations. Bierens and Martins (2010) propose a vector error correction model with cointegration coefficients estimated by Chebyshev polynomials, and conduct a likelihood ratio test on the stability by examining whether all Chebyshev polynomial coefficients are jointly zero. Juhl (2005) and Xiao (2009) further extend the studies to the case where cointegration coefficients are general smooth stochastic functions depending on some stationary covariates. Xiao (2009) considers both kernel and local polynomial estimators and establish their asymptotics. Moreover, he proposes a test statistic by comparing the functional-coefficient estimates to a fixed value estimated under the null. 1
In this paper, we consider varying-coefficient regression models with nonstationary regressors. Our model setting is similar to Cai et al. (2009) and Xiao (2009). However, we propose to estimate the varying coefficients by penalized splines and construct a likelihood ratio test (LRT) for the stability of the varying coefficients. The basic idea of spline methods is to approximate the unknown regression function by splines, which are piecewise polynomials, and then estimate the spline coefficients by the least squares method. In order to maintain a good balance between the goodness of fit and the model variability, a large number of basis func-
1Other contributions to the literature on the stability tests in cointegrating regression includes Hansen (1992), Hao (1996), Quintos (1997), Kuo (1998), Hansen and Johansen (1999), Johansen, Mosconi and Nielsen (2000), Harris, McCabe and Leybourne (2002), and among others. Furthermore, Hong and Phillips (2010) propose a modified RESET test for testing linearity in the cointegration model. Gao, King, Lu and Tjøstheim (2009) consider a nonparametric specification test for a nonparametric time series model with nonstationary variables.
3

tions are employed and a penalty term is imposed to avoid overfitting (Eilers and Marx, 1996). There are several prominent features of such a penalized splines approach. First, this method is simple and easy to implement. Its computation is usually less time-consuming compared to other nonparametric methods such as kernel or local polynomials. Second, it could easily incorporate correlation structure to improve the efficiency of estimator or to account for longitudinal and spatial effects (Ruppert, Wand and Carroll 2003). Third, it has close connections with Ridge regression, Bayesian methods and mixed model representation, thus allowing fitting and testing to be conducted through the paradigm of likelihood (Crainiceanu, Ruppert, Claeskens and Wand 2005). However, theoretical explorations of penalized splines were less well developed until recently. Li and Ruppert (2008) establish the asymptotic normality of the penalized splines estimation. Claeskens, Krivobokova and Opsomer (2009) systematically compare the asymptotics of penalized splines, regression splines and smoothing splines. All these studies are under the univariate nonparametric model assumption yt = (zt) + vt for stationary covariate zt's.
Our studies contribute to the literature through the following aspects. First, we propose the penalized spline estimation method for varying-coefficient models with nonstationary regressors. We establish the consistency as well as the optimal convergence rate of the penalized splines estimators. In our study, the choice of the spline basis is not crucial, but the penalty parameter plays the key role of smoothing. To our best knowledge, this is the first work in establishing the asymptotics of penalized splines estimators for varyingcoefficient models with nonstationary regressors. Second, we consider testing the stability of the regression coefficients. By utilizing the mixed model representation of penalized splines, we relate this problem to testing zero variance component of the spline coefficients. We then adopt the likelihood ratio test (LRT) procedure and derive the exact and the asymptotic null distribution. Since the exact null distribution is non-standard, we provide a fast algorithm to simulate its critical values when the sample size is small. By assuming both the sample size and the number of spline functions grow to infinity, we, for the first time, show that
4

the limiting null distribution of the proposed LRT statistic follows a simple 2 distribution rather than a mixture of 2 distributions. We also study the local power of the proposed LRT by deriving the asymptotic distribution under the local alternative. Simulations show that our method works very well.
The rest of the paper is organized as follows. In Section 2, we introduce the varyingcoefficient regression model with nonstationary regressors and discuss some regularity assumptions. The penalized splines estimation of the varying coefficients and its asymptotics are presented in Section 3. In Section 4, we construct the LRT for the stability of the varying coefficients. Both the exact distribution and the asymptotic null distribution are derived. The local power property is examined as well. Simulations are conducted in Section 5 to demonstrate the finite sample performance of our procedure, while Section 6 concludes.
In matters of notations, =D denotes equality in distribution, =: denotes definition,  denotes convergence in distribution, a.s. denotes almost sure convergence, a  b denotes that a and b has the same order, [T s] denotes the integer part of T s and denotes the integration from 0 to 1.

2 The Model and Assumptions

Consider the following varying-coefficient regression model without intercept and time trends

yt = xt(zt) + ut,

(1)

where (·) is a smooth function of unknown form, yt can be either stationary or nonstationary, zt and ut are stationary, and xt is an integrated process of order one, whose generating mechanism is given by
xt = xt-1 + vt, for t = 1, 2, ...T,

with vt being stationary. We set x0 = 0 to avoid some unnecessary complications in exposi-

tion,

although

x0

=

 oa.s.( T

)

is

sufficient

for

the

asymptotic

results.2

2To save notations, we only consider the simple case when zt is univariate.

5

Compared to traditional varying-coefficient models, which usually deals with independent

and identically distributed (iid) or stationary time series, Model (1) allows the regressors to

be highly persistent variables, such as interest rate, GDP growth rate and unemployment

rates. On the other hand, compared to traditional linear cointegration models, which are

widely used in the literature to capture the long term equilibrium among highly persistent

economic variables, Model (1) affords more flexibility as it allows the relationship to be

varying according to some state variable zt. Before describing our estimating and testing procedures, we first discuss some regularity

assumptions for our Model (1).

Assumption 1: The sequence {vt} is stationary  - mixing with E(|vt|) <  for some

 > 2+r1 with 0 < r1  2 and the mixing coefficients m satisfying

 m=1

m1/(2+r1)-1/

<

.

Assumption 2:

i) The error term ut is a general linear process satisfying


ut = ciet-i = Cu(L)et,
i=0

where {et}t=- are i.i.d N (0, e2) with e2 > 0 and {ci}i=0 satisfies the summability

conditions

 i=0

i|ci|

<



with

Cu(1)

=

0.

ii) ut is independent of zt and vt.

Assumption 3:

i) The sequence {zt} is strictly stationary, ergodic and -mixing with mixing coefficients

m satisfying

 m=1

m1/2-1/r2

<



for

some

r2

>

2.

ii) zt has a marginal density fz(z) on a finite support [0, 1]. fz(z) is continuously differentiable and bounded away from 0.

iii) (z) belongs to the Sobolev space of the m-th order W m[0, 1], i.e. (z) is (m - 1)-th

continuously differentiable and

1 0

{(m)(z)}2dz

<

.

6

Assumption 1 and Assumption 2 i) provide sufficient conditions of strong approximations

for

the

partial

sum

(

1 T

[T s] t=1

ut,

1 T

[T s] t=1

vt

),

sup ||( 1

s[0,1]

T

[T s] t=1

ut,

1 T

[T s]
vt) - {Bu(s), Bv(s)} || a.s. 0, as
t=1

T

 ,

where {Bu(s), Bv(s)} are two Brownian motions defined on D[0, 1], the space of cadlag func-

tions defined in the unit interval [0, 1]. Note that the strong approximation is stronger than

the multivariate invariance principle, but it is commonly used in the literature of nonlinear

regression model with nonstationary regressors, including Park and Hahn (1999), Park and

Phillips (2001), Kasparis (2008), Wang and Phillips (2009a, 2009b), Cai et al. (2009), Shi

and Phillips (2012) among others. Sufficient conditions to derive strong approximations for

dependent random variables are also well established in the literature. For example, Lemma

1 in Park and Hahn (1999) establish conditions of strong approximations for a general linear

process ut and Theorem 4.1 in Shao and Lu (1987) give conditions of strong approximations

for an  - mixing process vt.

Assumption 2 i) also defines ut as an invertible Gaussian moving average process. The

normality assumption is somewhat restrictive but it is for the purpose of employing likelihood

principles. Define the vector u = (u1, ..., uT )T and denote var(u) = 2 u . At the current point, we assume that u is known so that the full likelihood function could be constructed. In practice, u can be posited to be of a particular form u(), where  is a vector of parameters that could be estimated from the data. A simple example is to treat ut as an

AR(1) process. Then u is a function of the first order coefficient . One could apply a two-step procedure to obtain the estimate ^ and replace u by its estimate u(^).
To simplify the derivation of the LRT, Assumption 2 ii) assumes the independence con-

dition between the error term ut and xt, though this might be further relaxed. Following

Saikkonen (1991) and Saikkonen and Choi (2004), we might remove the endogeneity between

ut and xt by adding leads and lags of the term {vt}t=1 in the regression. On the other hand, we assume the independence between ut and zt, which rules out the ill-posed inverse problems

7

in the nonparametric estimation with stationary smoothing variables. Assumption 3 i) guarantees that zt is strictly stationary and imposes some conditions on its
dependency and moments. In this paper, we do not consider the case when zt is nonstationary. Assumption 3 ii) requires that zt has a bounded support. In practice, one can always conduct some transformations, such as the probability integral transform, to satisfy this assumption. We also assume that the marginal density of z is continuously differentiable and bounded away from 0, thus ensuring there are enough observations for estimation. Finally, Assumption 3 iii) imposes some smoothness conditions on the unknown function (z), which is a standard assumption in nonparametric regression analysis.

3 Penalized Splines Estimation

We employ the penalized splines approach to estimate the varying-coefficient regression model

with nonstationary regressors. First, we approximate the unknown varying coefficient by

splines basis. A popular choice is the uniform B-splines family defined by a set of equally

spaced knots k = k/K, for k = 0, · · · , K. The simplest case is the zero degree B-splines,

which are indicator functions between k-1 and k. In general, we could use the iterative

algorithm proposed by de Boor (1978) to calculate the p-th degree B-splines and express (z)

as

K +p

(z) =

k[p](z)k + O(K-1).

k=1

Following the idea of penalized least squares, we could estimate the spline coefficients  by

minimizing the following criterion:

T K+p

2

yt - xt

k[kp](z) + ~-1

t=1 k=1

In a discrete version, this could be written as

{(m)(z)}2dz.

(2)

T t=1

K +p

yt - xt

k k[p] (z )

k=1

2
+ ~-1K-1

K

(

mk K -m

)2,

k=m+1

(3)

8

where  is the differencing operator such that k = k - k-1, m is a positive integer indicating the order of differencing with m = (m-1). Let Y = (y1, · · · , yT )T and X be the diagonal matrix whose (i, i)th element is xi, and  be the matrix3 whose (i, j)-th element is i[p](zj). Define Dm as the differencing matrix such that the j-th element of Dm is mj. Then the above minimization criterion could be written in a matrix form as

(Y - X)T (Y - X) + ~-1K2m-1T DmT Dm.
In general, if we take into account the correlation among ut and the fact that var(u) = 2u, we could incorporate the weighted penalized splines approach and estimate ^ by minimizing

(Y - X)T u-1(Y - X) + ~-1K2m-1T DmT Dm.

(4)

A direct calculation shows that the solution to (4) is

^ = (T Xu-1X + ~-1K2m-1DmT Dm)-1T X-u 1Y.

(5)

Then the penalized spline estimator of (z) for model (1) is defined as

K +p

^(z) =

[kp] (x)^k .

k=1

The methodology and applications of penalized splines are discussed extensively in Ruppert

et al. (2003), but its theoretical studies had been largely absent until recently. For the

univariate nonparametric model, Hall and Opsomer (2005) establish the consistency of the

penalized splines estimators. Li and Ruppert (2008) derive the asymptotic normality and

they were the first to obtain explicit formula for the asymptotic bias and variance. Claeskens

et al. (2009) study the convergence rate of the penalized spline estimation and discussed the

impact of the number of knots. However, all of these results are not directly applicable for

varying-coefficient models with nonstationary regressors.

The following theorem establishes the consistency of the penalized spline estimator. Please

note that all proofs of the theorems are relegated to the appendix.
3The dimensions of  and  both depend on the degree of splines p and the number of knots K. For notation simplicity, we suppress the subscripts p and K.

9

Theorem 3.1. Suppose that Assumptions 1-3 hold. In addition, assume that

i) The pth degree uniform B-splines are used to model (z). The number of knots satisfies

K

 T r1

with

2m 2m+1

< r1

< 1.

ii) The mth order penalty is used and the penalty parameter ~ satisfies that ~  0 and T 2~  .

Then for z  (0, 1), the penalized spline estimator ^(z) satisfies

^(z) - (z) = Op(T -1~-1/2) + Op(T -1+1/(2m)~1/(4m)) + Op(K-1).

(6)

REMARK 3.1. Theorem 3.1 establishes the consistency of the penalized splines estimator for varying-coefficient models with nonstationary regressors. The term Op(T -1~-1/2) reflects the order of asymptotic bias due to smoothing. The term Op(K-1) is the design bias due to

the use of splines in approximating the smooth functions (z). These results are standard.

However, the order of asymptotic variance becomes Op(T -2+1/m~1/(2m)) when xt is an inte-

grated process of order one, compared to a slower rate Op(T -1+1/2m~1/(2m)) for stationary

xt. Correspondingly, when ~ satisfies ~  T -2/(2m+1), our estimator achieves the optimal

convergence

rate

T

,2m
2m+1

faster

than

the

T m/(2m+1)

convergence

rate

with

~



T -1/(2m+1)

for

stationary xt. In particular, when (z)  W 2[0, 1], i.e. m = 2, the optimal convergence rate of ^(z) is T 4/5 when xt is integrated with order one. Such a result is consistent with Cai et

al. (2009) and Xiao (2009).

REMARK 3.2. Penalized splines allow one to flexibly select the degree of splines p, the number of knots K, and the amount of penalty ~-1. Our results have the following implica-

tions. First, the degree of splines p has no impact in the convergence rate of the estimator.

Second, the number of knots K is not crucial as long as it exceeds a certain minimum bound. Third, the penalty parameter ~ could serve as the key smoother and it determines the conver-

gence rate of the estimator. These three conclusions are consistent with the results obtained

10

in Li and Ruppert (2008). However, when xt is stationary, the term (~-1/T )1/(2m) serves as the equivalent bandwidth used in a Nadaraya-Watson kernel estimator. In contrast, when xt is integrated with order 1, the term (~-1/T 2)1/(2m) serves as the equivalent bandwidth used in a Nadaraya-Watson kernel estimator.
Besides the B-spline family, another popular choice of the basis is the p-th degree truncated power polynomial basis (TPS) defined as

{1, z, · · · , zp, (z - 1)p+, · · · , (z - K )p+},

where (z - a)+p = {max(0, z - a)}p. Note that the p-th degree TPS and the p-th degree B-splines span the same linear space. For any given k's, there exists j's and bk's such that

K +p

pK

k(z)k = jzj + (z - k)p+bk.

k=1

j=0 k=1

Moreover, a direct calculation shows that the coefficients of TPS and B-splines satisfy bk+1 = (-K)pp+1k (de Boor, 1978). Hence imposing penalty on k b2k is equivalent to imposing the (p + 1)-th order differencing penalty on the B-splines coefficients k's. In general, we could define the penalty matrix4  such that ~-1K2m-1T DmT Dm = ~-1K2m-2p-1bT b. Equivalently, we could rewrite the minimization criterion (4) for TPS as

(Y - XZ1 - XZ2b)T u-1(Y - XZ1 - XZ2b) + ~-1K2m-2p-1bT b,

(7)

where Z1 and Z2 are matrices whose i-th row are (1, z, · · · , zp) and {(z -1)+p , · · · , (z -K )+p } respectively, and  = (0, · · · , p)T . Because of the equivalence between TPS and B-splines of the same degree, the penalized spline estimator based on TPS could achieve the same optimal convergence rate T 2m/(2m+1) when the penalty parameter satisfies ~  T -2/(2m+1). Since the
choice of p will not affect the convergence rate of the spline estimator, a conventional choice is to let p = 1 for (z)  W 2[0, 1], i.e. m = 2. In this case,  becomes the identity matrix
4The choice of  depends on both the degree of splines p and the order of penalty m. For notation simplicity, we suppress the subscripts p and m.

11

IK. If we denote -1 = ~-1K, then the minimization criterion (7) could be written as

(Y - XZ1 - XZ2b)T -u 1(Y - XZ1 - XZ2b) + -1bT b,

(8)

and the optimal rate of  is of the order T -2/5K-1.

4 Inference using Likelihood Ratio Tests
In this section, we consider testing whether the functional coefficients (z) is time-invariant. The null hypothesis is H0: (z)  0. Under the alternative, (z) is a smooth function of unknown form. Such a stability test is of both theoretical and empirical importance. For example, when a linear cointegration model is misspecified, the resulting estimation of ^0 would not be consistent and neither of the equilibrium residuals. As a result, the traditional cointegration tests might fail to detect the cointegrating relationship.
In the literature of nonparametric regression, there are also lots of discussions on checking whether there is enough evidence to support the use of the general nonparametric method rather than a simple linear cointegration model. In general, traditional approaches often rely on i) comparing the discrepancy measures between the estimates obtained under the null and the alternative, see H¨ardle and Mammen (1993); or ii) constructing the F -test statistic based on the sum of residuals, see Hong and White (1995); or iii) conducting the generalized likelihood ratio test using a reasonable smooth estimate under the alternative, see Fan, Zhang and Zhang (2001). In any of these methods, it is crucial to select the smoothing parameter under the nonparametric alternative. In practice, the power of the test is likely to be affected by the smoothing parameter, especially when it is chosen by some ad hoc methods.
In contrast, we are going to propose a likelihood ratio test procedure that could circumvent this difficulty as we use maximum likelihood principles for both estimation and inference. First, we model (z) by the p-th degree splines in order to define a general nonparametric alternative. As we show in the section above, there is not much difference to estimate (z) by using either the B-splines family or the TPS family. Moreover, the choice of p is not
12

important. Therefore, we mainly focus on using the linear TPS family in this section. Since we could view the spline coefficients bk's, associated with (z - k)+'s, as the deviations from the linear function. Hence testing the stability of (z) is equivalent to testing both the linear coefficient and the spline coefficients being 0, i.e.

H0 : 1 = 0 and b1 = · · · = bK = 0,

against

HA : 1 = 0 or  k, s.t. bk = 0.

Note that this is a multiple testing problem and the number of restrictions under H0 grows

as the sample size does. To circumvent this difficulty, a new idea is to utilize the mixed model

representation for spline estimates based on TPS by treating bk's as random coefficients with

a common variance component, and then relate the null hypothesis above to the significance

test of zero variance. More details are given below.

Note that minimizing (7) is equivalent to solving a system of equations



  



AT1 u-1A1 A2T u-1A1

AT1 u-1A2

 ^ = A1T u-1Y  ,

A2T u-1A2 + -1

^b

A2T -u 1Y

where A1 = XZ1 and A2 = XZ2. The above equation is essentially Henderson's mixed model equations, which motivates us to utilize the mixed model representation to obtain ^ and ^b

as the best linear unbiased predictors (BLUP) in the following model. To be specific, let

Y = A1 + A2b + u, where  is the 2 × 1 vector of fixed effect coefficients and b is the K × 1 vector of random effect coefficients with mean 0 and variance 2-1, with  = IK when

m = 2 and p = 1. The parameter  controls the amount of smoothing and it could be viewed

as the signal to noise ratio. Following Crainiceanu and Ruppert (2004), we could treat Y as

Y |b, A1, A2 =D N (A1 + A2b, 2u), b =D N (0K×1, 2-1).

Note that E(Y ) = A1 and var(Y ) = 2(u + A2-1AT2 ) =: 2. Hence we could define 13

a twice of the log-likelihood of Y as

2l(, , 2) = -(Y - A1)(2)-1(Y - A1) - log |2| - T log(2).

(9)

By maximizing (9), we could estimate the variance components by ^2 and ^. Define ^  = u + ^A2-1A2T . The BLUP of  and b are then obtained as

^ = (A1T ^ -1A1)-1AT1 ^ - 1Y,

and ^b = ^-1A2T ^ -1(Y - A1^),

(10)

and we could estimate (z) by

pK
^(z) = ^kzk + bk(z - k)+p .

k=0

k=1

For the same , minimizing equation (7) yields the same solution as (10). However, the use

of the mixed model representation allows us to adopt the maximum likelihood principle to

make estimation as well as inference on . In particular,  = 0 implies bk = 0 for all k. Hence

the hypothesis test of (z) being constant is equivalent to testing

H0 : 1 = 0 and  = 0.

against

HA : 1 = 0 or  = 0.

Then it is straightforward to rely on the LRT statistic for inference, where

LRTT = sup 2 log l(, , 2) - sup 2 log l(, , 2).
HA H0
Note that the null distribution of the LRT statistic is not standard as the parameter  is always non-negative and it lies on the boundary of the parameter space under H0. Therefore, we derive the exact and the limiting null distributions of our test statistic below.
First we consider the exact case, where both T and K are relatively small and could be treated as fixed. Let P be the projection matrix P = IT - u-1/2A1(A1T u-1A1)-1AT1 u-1/2. Define s,T and s,T as the s-th eigenvalues of the K × K matrices -1/2A2T u-1A2-1/2 and -1/2A2T u-1/2P u-1/2A2-1/2 respectively. We have the following results.
14

Theorem 4.1. Suppose that Assumptions 1-3 hold and the linear TPS with equi-spaced knots

are used. Then under H0 : (z)  0,

LRTT

=D

sup[T
0

log{1 +

NT DT

() ()

}

-

K
log(1 + s,T )] + T
s=1

log{1 +

wT2 -1

T -2 s=1

ws2

},

(11)

where NT () =

K s=1

s,T 1+s,T

ws2,

DT

=

K ws2 s=1 1+s,T

+

T -2 s=K +1

ws2

and

ws

=D

iidN (0, 1).

Theorem 4.1 derives the exact null distribution of the LRT statistic when the sample size

T is finite. Although equation (11) does not have a close form, we could efficiently simulate

this distribution using the following Algorithm A.

Step 1. define a grid 0 = 1 < 2 < · · · < L of possible values for .

Step 2. simulate K independent 12 random variables w12, · · · , wK2 .

Step 3. simulate a random variable 1 that follows T2 -2-K .

Step 4. simulate a random variable 0 that follows 12.

Step

5.

for

every

i,

compute

fT (i) = T log{1 +

NT DT

(i) (i)

}

-

sK=1(1 + s,T ).

Step 6. determine max which maximizes fT (i) over i's.

Step 7. compute fT (max) + T log{1 +

}.0

K s=1

ws2

+1

Step 8. repeat steps 2­7.

If we treat K as fixed and let T grow to infinity, we have the following results.

Theorem 4.2. Suppose that Assumptions 1-3 hold and the linear TPS with equi-spaced knots are used. Then there exist s and s, for s = 1, · · · , K, such that

T -2s,T  s,

T -2s,T  s,

as T  .

(12)

Moreover, under H0 : (z)  0,

LRTT



K
sup{
d0 s=1

1

ds + ds

ws2

-

K s=1

log(1

+

ds)}

+

12.

(13)

15

REMARK 4.1. In Theorem 4.2, we have explicitly derived that the convergence rate

of the eigenvalues s,T and s,T is T 2, which is faster than the T convergence rate due

to nonstationarity. When K is fixed, the part corresponding to testing  = 0 converges

to the term supd0{

K s=1

ds 1+ds

ws2

-

K s=1

log(1

+

ds)}.

Since this limiting distribution is

nonstandard, one could simulate it by modifying Algorithm A described above.

Furthermore, if we assume K and T both grow to infinity, the null distribution of LRTT approaches to a simple 2 distribution.

Theorem 4.3. Suppose that Assumptions 1-3 hold and the linear TPS with equi-spaced knots

are used. Let the number of knots K satisfying K  T r with 4/5 < r < 1. Then there exist ¯ and ¯ such that

K
K-1T -2 k,T  ¯,
k=1
Under H0 : (z)  0, we have,

K
K-1T -2 k,T  ¯.
k=1

(14)

LRTT  21.

(15)

REMARK 4.2. Theorem 4.3 assumes that K grows as T does. Compared to the fixed K

case, the amount of penalty -1 is expected to be larger, and the probability of obtaining

the maximum likelihood estimate (MLE) of  at its actual value 0 approaches to 1 provided

that H0 is true. Therefore, the part corresponding to testing  = 0 degenerates and we have a simple 2 distribution.

For the local alternatives, we assume that (z) = 0 + T -1(z), where 1(z) is a nonzero

smooth function that belongs to W 2[0, 1]. Suppose we span 1(z) with the first degree TPS

as ¯0 + ¯1z +

K k=1

¯bk

(z

-

k )+ .

Utilizing the mixed model representation, we treat the

spline coefficients ¯b = (¯b1, · · · , ¯bK)T as random with mean 0 and variance ¯02-1. It has

been shown in Section 3 that ¯0 converges to 0 at the rate of T 2/5K. Therefore, we denote

¯0 = d¯0T -2/5K-1 for some constant d¯0  0. Recall that our LRT test will examine both the

fixed part ¯1 and the variance part d¯0. Therefore, we will consider two different cases in the

16

local alternatives. In Case 1, 1(z) is a linear function with nonzero slope. i.e. ¯1 = 0 but d¯0 = 0. The local alternative is then set as H01 : (z) = 0 + T -11(z). In Case 2, 1(z) has ¯1 = 0 but d¯0 = 0. The local alternative is set as H02 : (z) = 0 + T -4/51(z).

Theorem 4.4. Suppose that Assumptions 1-3 hold and the linear TPS with equi-spaced knots are used. Let the number of knots K satisfying K  T r with 4/5 < r < 1.
Under the local alternative H01, the LRT statistic converges to a noncentral 12, i.e.

LRTT  (w1 + ¯1¯2)2,

where w1 =D N (0, 1) and ¯2 is defined right before equation (52). Under the local alternative H02,

LRTT



max
d[0,d¯0

{d¯0
]

¯

+

g¯3(d)

-

g¯2(d)}

+

(1

+

d¯0

)w12,

where w1 =D N (0, 1), ¯ and g¯2(d) are defined as in Theorem 4.3, g¯3(d) is defined right after

equation (53) and is defined right before equation (55).

REMARK 4.3. Strictly speaking, we should also consider Case 3 where neither ¯1 or d¯0 equals 0. For this case, the local alternative could be set as H03 : (z) = 0 + T -11(z). Notice that such a local alternative converges with a rate faster than T 4/5. The nonzero variance component will not affect the asymptotic distribution. Hence in Case 3, the LRT statistic still converges to a noncentral 2 distribution as in Case 1. To save the length of this paper, we omit the detailed discussions of Case 3.
REMARK 4.4. Under H02, the asymptotic distribution of LRTT has two components, where the first part is nonnegative and the second part is a scaled 12. In summary, our penalized spline estimator of (z) has the T 4/5 convergence rate, while our test statistics could detect an alternative whose convergence rate is not faster than T 4/5. On the other hand, for any sequence such that H0A : (z)  0 + T -1(z) and  < 4/5, LRTT diverges and the power function satisfies

P (LRTT > 12,/2)  1, 17

where 12,/2 is the upper /2 quantile of 12 distribution. Hence the proposed likelihood ratio test could achieve the optimality.
5 Finite Sample Performance
Monte Carlo simulations are conducted in this section to examine the finite sample performance of the proposed LRT test. The data generating process is
yt = (zt)xt + ut,
where xt = xt-1 + vt, ut = ut-1 + t, vt's and t's are iid N (0, 1), and they are independent of each other. The initial values are set to be zero. In particular, four cases for the parameter values are considered: i) (zt) = 0.25,  = 0; ii) (zt) = 0.25,  = 0.5; iii) (zt) = (zt - 0.5)2,  = 0; iv) (zt) = (zt -0.5)2,  = 0.5, where the first two cases are related to calculating the size of the test and the last two are related to calculating the power of the test. The simulation designs above are similar to those in Xiao (2009). The sample sizes we consider are T = 100 and 300. In particular, we would like to examine the impact of the number of knots K. Hence when T = 100, we consider three situations, K = 10, K = 20 and K = 40; when T = 300, we consider K = 20, K = 40, and K = 80. All reported results are based on 2000 replications.
Table 1 report the size of the proposed likelihood ratio test when  is given, i.e. the true covariance matrix u is known. The five columns on the left use the critical values based on the finite distribution derived in Theorem 4.1, while the five columns on the right use the critical values based on the asymptotic distribution 21 as indicated in Theorem 4.3. From Table 1, we find that both the finite distribution and the 2 limiting distribution work very well. For example, consider Panel A1 with K chosen as 10. Even though the sample size is just 100, the actual rejection rates based on the asymptotic 2 distribution are 0.1995, 0.1490, 0.0995, 0.0465 and 0.0100, very close to the nominal sizes 0.2, 0.15, 0.10, 0.05 and 0.01 respectively. Moreover, we find that the number of knots does not have much impact on
18

the size performance. For a given nominal level in any reported panel, the absolute differences in the rejection rates associated with different K are not greater than 0.005. This is consistent with the empirical conclusions that the number of knots is not important, provided that it is above some minimum threshold (Ruppert, 2002).
Table 2 repeats all designs in Table 1, except that the covariance matrix u is treated as unknown and replaced by an estimate. We find that the our LRT procedure still performs well and is less likely to be affected by the fact that the covariance is unknown.
Table 3 reports the power of our test statistic. Once again, we find that the choice of the number of knots K is not important and the procedure is robust against the use of an estimated covariance. When the sample size increase from 100 to 300, the rejection rates are all greater than 0.98, implying very good power performance of our testing procedure.
6 Conclusions
Varying-coefficient regression models with nonstationary regressors have received heated interests in recent years. This paper proposes a penalized splines approach to estimate the varying coefficients. Compared to kernel-based methods, penalized splines estimation not only achieves the same optimal convergence rate, but also enjoys the advantage of fast computation. Utilizing the mixed model representation of penalized splines, we construct a likelihood ratio test for the stability of the varying coefficient. We derive the exact and limiting distributions of the proposed test statistic. When the number of knots is treated as fixed, the null distribution is non-standard, but could be simulated via a proposed algorithm using spectral decomposition. When the number of knots grows as the sample size does, the limiting null distribution converges to a simple 2 distribution. Our test is less likely to be suffered from the mis-selection of the smoothing parameters. Simulations show that the asymptotic distribution works very well even for the finite sample case.
19

There are some issues worth of future studies. One potential analysis is to extend the current setting to the case allowing for dependence between ut and vt. Another natural extension is to consider a more general varying-coefficient cointegrating regression model which includes both the stochastic and the deterministic functional coefficients in the cointegrating relationship.

Appendix A: Proofs

Proof of Theorem 3.1: Note that our model could be written as Y = X + u. If u = IT , we could always multiply u-1/2 and consider instead Y~ = X~  + u~, where Y~ = -u 1/2Y , X~ = u-1/2X is an integrated process and the elements of u~ = u-1/2u are uncorrelated.
Hence without loss of generality, we only need to show that equation (6) holds when ut's are
uncorrelated, i.e. u is the identity matrix. Recall that the penalized spline estimator ^(z) could be written as in equation (5), i.e.

^(z) = z(T X2 + ~-1K2m-1DmT Dm)-1T XY,

(16)

where z = {[1p](z), · · · , K[p]+p(z)}. First consider the (i, j)th element of the term T X2.

Define R0 =: T -2

T t=1

xt2

[[ip](zt

)j[p](zt

)

-

E{i[p](zt)j[p](zt)}].

By

subtracting

and

adding

the mean, we have,

TT

T -2(T X2)i,j = T -2

xt2i[p](zt)j[p](zt) = R0 + T -2

E{[ip](zt)j[p](zt)}xt2.

t=1 t=1

Recall that i[p](z) is nonzero only in a small interval of length (p + 1)/K. For example, when zero degree splines are used, i[0](z) is the indicator function I(i-1)/K<zi/K . Hence E{i[p](zt)[jp](zt)} = O(K-1) and var{i[p](zt)[jp](zt)} = O(K-1). Moreover,

T -2

T t=1

E{[ip](zt)j[p](zt)}xt2

=

E{[ip](zt)[jp](zt)}T -1

T ( xt t=1 T

)2

=

Op (K -1 ),

and hence

R0

T
= T -1
t=1

xt T

2
[[ip](zt)[jp](zt) - E{i[p](zt)j[p](zt)}] = Op{(T K)-1/2} = op(K-1),

20

where the last equality holds as K = o(T ). Denote qij as the limit of KE{i[p](zt)j[p](zt)}.

Then

T
KT -2(T XX)ij = K[T -2 x2t E{i[p](zt)j[p](zt)} + R0]  qij
t=1
Therefore,

Bv2(s)ds. (17)

^(z) = z(T X2 + ~-1K2m-1DmT Dm)-1T XY, = z(T X2 + ~-1K2m-1DmT Dm)-1T X2 +z(T X2 + ~-1K2m-1DmT Dm)-1T Xu + Op(K-1) =: R1 + R2 + Op(K-1),

(18)

where the term Op(K-1) comes from the bias due to splines approximation,

R1

=: z

K T2

(T

X

2



+

~-1K

2m-1DmT

Dm)

-1

K T2

T

X 2 

=

z

(

K T2

T

X

2

)

+

¯K

2mDmT

Dm

-1

(

K T2

T

X

2

)

,

R2

=: z

K T2

(T

X

2



+

~-1K

2m-1DmT

Dm)

-1

K T2

T

Xu

=

1 T

z

(

K T2

T

X

2)

+

¯K

2m

DmT Dm

-1

T (T -1/2Xu) T /K

,

and

¯

=

~-1/T 2.

By

equation

(17),

the

term

K T2

T

X

2



converges.

Using

a

similar

technique

as in Li and Ruppert (2008), we can show that the term ¯1/(2m) serves equivalently as the

bandwidth h used in a Nadaraya-Watson kernel estimator. Therefore,

R1 - z = Op(hm) = Op{¯m/(2m)} = Op(T -1~-1/2).

(19)

Now consider the second term R2. Note that ER2 = 0 and the i-th element of T Xu satisfies

T (T -1/2Xu)i T /K

=

T t=1

{i[p]

(zt

)

xt T

T /K

ut}

=

Op(

1 ). T /K

(20)

By the fact that ¯ = ~-1/T 2 and ¯1/(2m) serves as the bandwidth used in Nadaraya-Watson

kernel estimate, we have

 var( T

R2)

=

O(

1 Kh

T

1 /K

)

=

O(

1 Th

)

=

O(

T

1 ¯1/(2m)

)

=

O(T

-1+1/m~1/(2m)),

21

and hence

R2

=

1 T

Op(T -1/2+1/(2m)~1/(4m))

=

Op(T -1+1/(2m)~1/(4m)).

Together with equation (18), (19) and (21),

^(z) - (z) = Op(T -1~-1/2) + Op(T -1+1/(2m)~1/(4m)) + Op(K-1),

and Theorem 3.1 holds.

(21)

Note that the proofs of Theorem 4.1­4.4 share lots of similarity. Thus we first provide a general description and four useful propositions which could be applied to all these theorems.

Recall that twice of the log-likelihood of Y is defined as in equation (9), i.e.

2l(, , 2) = -(Y - A1)(2)-1(Y - A1) - log |2| - T log(2).

where  = u + A2-1A2T and 2 = var(Y ). Instead of maximizing 2l(, , 2) over the parameter space (, , 2), we consider maximizing the profile log-likelihood function
2 log L() = 2l(^, , ^2) over the parameter space   0, where ^ and ^2 are the profile maximum likelihood estimates (MLE) for  and 2 when  is given. Specifically, they satisfy

^ = (AT1 - 1A1)-1AT1 -1Y

and ^2 = T -1(Y - A1^)T - 1(Y - A1^).

By plugging ^ and ^2 into equation (9), we could simplify the profile log-likelihood as

2 log L() = -T - log |^2| - T log(2).

(22)

Denote log L0 as the maximum log-likelihood under the null hypothesis. Then we can decompose 2LRTT into two parts by adding and subtracting 2 log L(0), i.e.

2LRTT = sup{2 log L() - 2 log L(0)} + {2 log L(0) - 2 log L0},
0
where the first part corresponds to testing  = 0 and the second part corresponds to testing the linear coefficient 1 = 0 given that  = 0. The following propositions establish some

22

useful preliminary results. In particular, Proposition 6.1­Proposition 6.3 studies the property related to the first part, while Proposition 6.4 studies the property of the second part.

Proposition 6.1. Denote Y~ = -u 1/2Y , A~1 = u-1/2A1, A~2 = u-1/2A2. We have

2

log

L()

-

2

log

L(0)

=

T

log(1

+

R3/2 T ^2/2

)

-

K k=1

log(1

+

k,T

).

(23)

where R3 is given right below equation (29), and k,T 's are defined right before Theorem 4.1.

Proof of Proposition 6.1: Define V =: IT + A~2-1A~2T . With the notations of Y~ , A~1 and A~2, we have  = u1/2V1u/2. Correspondingly, the profile MLE can be rewritten as

^ = (A~T1 V-1A~1)-1A~1V-1Y~ and ^2 = T -1(Y~ - A~1^)T V-1(Y~ - A~1^).

By Patterson and Thompson (1971), it is well-known that there exists a matrix W satisfying

W W T = P = IT - A~1(A~1T A~1)-1A~T1 ,

W T W = IT -2,

(24)

and that T ^2 = (Y~ - A~1^)T V-1(Y~ - A~1^) = Y~ T W (W T VW )-1W T Y~ .
By equation (22) and that || = |1u/2||V||1u/2| = |V||u|, we have

-2 log L() = log |^2| + T {1 + log(2)}

= T log(^2) + log |V| + log |u| + T {1 + log(2)}

=

T

log(

T ^2 2

)

+

log

|V|

+

C0.

(25)

where C0 = log |u| + T {1 + log(2)} - T log(T /2).
Note that for any p1 × p2 matrices A and B, it holds that |Ip1 + AT B| = |Ip2 + BT A|.
Moreover, |A| equals the product of its eigenvalues. Hence
K
log |V| = log |IT + A~2-1A~T2 | = log |IK + -1/2A~2T A~2-1/2| = log(1 + k,T ).
k=1
Together with equation (25), we have

-2

log

L()

=

T

log(

T ^2 2

)

+

K

log(1 + k,T ) + C0.

k=1

(26)

23

When  = 0, we have V0 = IT , W T V0W = W T W = IT -2 and T ^02 = Y~ T W W T Y~ . Thus,

-2

log

L(0)

=

T

log( Y~

T

WWT 2

Y~

)

+

C0.

(27)

It follows that

2 log L() - 2 log L(0)

=

T

log{1

+

(Y~

T

W

W T Y~ - T T ^2/2

^2 )/2

}

-

K
(1
k=1

+

k,T

).

(28)

Note that (W T VW )-1 = (IT -2 + W T A~2~ -1AT2 W )-1 = IT -2 - 1, where, by Woodbury

Matrix Identity,

1 =: W T A~2~ -1/2(IK + -1/2A~2T W W T A~2-1/2)-1~ -1/2A~T2 W.

(29)

Hence T ^2 = Y~ T W (W T VW )-1W T Y~ = Y~ T W W T Y~ - R3, where R3 = Y~ T W 1W T Y~ . Together with equation (28), we conclude that equation (23) holds.

Proposition 6.2. For any number of knots K, the following results hold.

Under H0 or H01, R3/2 =

K k=1

k,T 1+k,T

wk21

=D

NT ();

Under H02, R3/2 = 

K k=1

k,T +d¯0(KT 2)-1k2,T 1+k,T

wk22

=D

NT ();

where NT () is defined in Theorem 4.1, NT () =: 

K k=1

wk,T +d¯0(KT 2)-1k2,T
1+k,T

2 k

with

wk

=D

iidN (0, 1), and k,T 's are defined right before Theorem 4.1.

Proof of Proposition 6.2: Under H0 or H01, the spline coefficients are all 0. Therefore, Y~ =D N (A~1, 2IT ). Recall that the matrix W satisfies equation (24). Hence

W T A~1 = (W T W )W T A~1 = W T {IT - A~1(A~1T A~1)-1A~T1 }A~1 = 0(T -2)×2,

and W T Y~ / =D N (0T -2, IT -2), where 0T -2 is the (T - 2) × 1 vector whose components are 0.

Recall that the matrix 1 is defined as in equation (29). Suppose its eigen decomposition is

U1S1U1, where S1 is the diagonal matrix whose (i, i)th element i1 is also the ith eigenvalue

of 1. Then

R3 2

=

(

WT 

Y~

)T

1(

WT 

Y~

)

=

(

U1T

W 

T

Y~

)T

S1(

U1T

W 

T

Y~

)

=

T -2
i1wi21,
i=1

(30)

24

where wi1's are the elements of U1T W T Y~ /. Now we show that the eigenvalues i1's satisfy i1 = i,T /(1 + i,T ) for = 1, · · · , K and
i1 = 0 for i > K. Note that for any p1 × p2 matrices A and B, AB and BA shares the same nonzero eigenvalues. Hence the nonzero eigenvalues of 1 are the same as those of the matrix (IK + -1/2A~T2 W W T A~2-1/2)-1-1/2A~2T W W T A~2-1/2. Suppose the eigen decomposition of the matrix -1/2A~2T W W T A~2-1/2 is U2S2U2T , where S2 is the K × K diagonal matrix whose (k, k)th element is the kth eigenvalue k,T . Then

(IK + -1/2A~2T W W T A~2-1/2)-1-1/2A~T2 W W T A~2-1/2 = (U2U2T + U2S2U2T )-1U2S2U2T = U2{(IK + S2)-1S2}U2T .

Note that the (k, k)th element in the diagonal matrix (IK + S2)-1)S2 is k,T /(1 + k,T ),

which equals the first K eigenvalues k1's. Since the rest T - 2 - K eigenvalues are 0, we have

R3/2 =

K k=1

k,T 1+k,T

wk21.

Note

that

W T Y~ /

=D

N (0T -2, IT -2).

Moreover,

U1

is

an

orthog-

onal matrix. Hence we conclude that U1T W T Y~ / =D N (0T -2, IT -2), i.e. wk1 =D iidN (0, 1).

Therefore, R3/2 =D NT ().

Under H02, (z) = 0 + T -4/51(z), where 1(z) has an associated ¯0 satisfying ¯0 = d¯0T -2/5K-1 for some positive constant d¯0. Let ¯b be the spline coefficients of 1(z). Then var(T -4/5¯b) = T -8/5d¯0T -2/5K-12-1 = d¯0(KT 2)-12-1. Since W T A~1 = 0(T -2)×2, we have W T Y~ = T -4/5W T A~2¯b + W T u~. Equivalently, we could write that W T Y~ / =D N 0T -2, IT -2 + d¯0(KT 2)-1W T A~2-1A~T2 W . Denote

2 = {IT -2 + d¯0(KT 2)-1W T A~2-1A~2T W }1/21{IT -2 + d¯0(KT 2)-1W T A~2-1A~T2 W }1/2.

Similar

as

equation

(30),

we

conclude

that

R3 2

=

T -2 i=1

i2wi22,

where

i2

is

the

ith

eigenvalue

of 2 and wi2 is the ith element of U1T {IT -2 +d¯0(KT 2)-1W T A~2-1A~T2 W }-1/2W T Y~ /. Note

the nonzero eigenvalues of 2 are the same as those of

(IK +-1A~T2 W W T A~2-1/2)-1-1/2A~2T W {IT -2+d¯0(KT 2)-1W T A~2-1A~T2 W }W T A~2-1/2.

25

With the eigen decomposition -1/2A~2T W W T A~2-1/2 = U2S2U2T , the above matrix equals

(IK + U2S2U2T )-1{U2S2U2T + d¯0(KT 2)-1U2S2U2T U2S2U2T } = U2(IK + S2)-1{S2 + d¯0(KT 2)-1S22}U2T .

Since the (k, k)th element in the diagonal matrix (IK + S2)-1{S2 + d¯0(KT 2)-1S22} is

,k,T +d¯0(KT 2)-1k2,T
1+k,T

we have

R3 2

=

K k=1

k,T

+d¯0(KT 2)-1k2,T 1+k,T

wk22.

Note

that

wk2

=D

iidN (0, 1)

because var[U1T {IT -2 + d¯0(KT 2)-1W T A~2-1A~2T W }-1/2W T Y~ ] = 2IT -2 under H02. There-

fore, Proposition 6.2 is proved.

Proposition 6.3. Assume K = o(T ). Then under H0, H01 or H02, Y~ T W W T Y~ /T  .

Proof of Proposition 6.3: Under H0 or H01, W T Y~ / =D N (0T -2, IT -2). Therefore,

Y~ T W W T Y~ T 2

=

T

- T

2

+

op(1)

=

1

+

op(1).

Under H02 W T Y~ / =D N 0T -2, IT -2 + d¯0(KT 2)-1W T A~2-1A~2T W . Therefore,

T -2

T -2

Y~ T W W T Y~ /(T 2) = T -1 i2wi22 = T -1 i2 + op(1),

i=1 i=1

(31)

where i2 is the ith eigenvalue of the matrix IT -2 + d¯0(KT 2)-1W T A~2-1A~2T W and wi2 is defined as in the proof of Proposition 6.2 and satisfying wi2 =D iidN (0, 1). Since W T A~2-1A~T2 W and -1/2A~T2 W W T A~2-1/2 share the same nonzero eigenvalues,

T -2

T -1

i2

=

T

-2 T

+ T -1

K

d¯0(KT 2)-1i,T

= 1 + op(1).

i=1 i=1

With equation (31), Y~ T W W T Y~ /T  2. Hence, Proposition 6.3 is proved.

Proposition 6.4. Let A~0c1 be defined right before equation (33). It holds that

2

log

L(0)

-

2

log

L0

=

T

log{1

+

Y~

T A~c01(A~0c1)T Y~ /2 Y~ T W W T Y~ /2

}.

26

(32)

Proof of Proposition 6.4: By equation (27), 2 log L(0) = T log(Y~ T W W T Y~ /2) + C0, where W is the T × (T - 2) matrix satisfying equation (24). Now we show that there exists a T × (T - 1) matrix W0 such that 2 log L0 = T log(Y~ T W0W0T Y~ /2) + C0.
Partition A1 as A1 = (A01, Ar1), where A01 is the T × 1 vector whose elements are xt's and Ar1 is the T ×1 vector whose elements are xtzt's. Define A~01 = u-1/2A01 and A~r1 = -u 1/2Ar1. Note that the maximum likelihood estimates associated with log L0 satisfies that

^0 = (A~0T1A~01)-1A~0T1Y~ ,

^02 = T -1(Y~ - A~01^0)T (Y~ - A~01^0).

Similar as equation (24), there exists W0 such that (Y~ - A~01^0)T (Y~ - A~01^0) = Y~ T W0W0T Y~ ,

W0T W0 = IT -1 and W0W0T = P0 =: IT - A~01(A~T01A~01)-1A~T01. Thus

-2 log

L0

=

T

log(^02)

+

(Y~

-

A~01^0)T (Y~

-

A~01^0)

+

T

log(2)

=

T

log(

Y~

T

W0W0T 2

Y~

)

+

C0,

and

2 log L(0) - 2 log L0

=T

log{1 +

(Y~

T

W0

W0T Y~ T

Y~ -Y~ T W W W W T Y~ /2

T

Y~

)/2

}.

We

could

project

A~r1

onto

the

unit

direction

A~01 ||A~01||

and

the

unit

direction

orthogonal

to

A~01,

i.e.

A~r1

=

1

A~01 ||A~01||

+

2A~c01.

(33)

By standard linear algebra, W0W0T - W W T = A~0c1(A~0c1)T . Hence equation (32) holds.

Now we study Theorem 4.1.

Proof of Theorem 4.1: By Proposition 6.2, R3/2 =

Y~ T W W T Y~ /2 =

T -2 k=1

wk21

.

Hence

K k=1

k,T 1+k,T

wk21

=D NT ().

Note

that

T ^2 2

=

Y~ T W W T Y~ 2

- R3

=

K k=1

1

wk21 + k,T

T -2
+ wk21
k=K +1

=D DT ().

Together with Proposition 6.1, we have

K

sup{2 log L() - 2 log L(0)} =D sup[T log{1 + NT ()/DT ()} - log(1 + k,T )].

0

0

k=1

Denote (A~0c1)T Y~ / = wT -1. Proposition 6.4 yields

2

log

L(0)

-

2

log

L0

=

T

log{1

+

Y~

T A~c01(A~c01)T Y~ /2 Y~ T W W T Y~ /2

}

=

T

log{1

+

wT2 -1

T -2 k=1

wk21

}.

(34)

27

It remains to show that under H0, wT -1 =D N (0, 1) and it is independent of wk1's, or equivalently, the vector U1T W T Y~ . Under H0, Y~ = A~1 = A~010 + u~. Recall that A~c01 is the unit direction that is orthogonal to both A~01 and W . Since (A~c01)T Y~ = (A~c01)T u~ =D N (0, 2) and
cov{(A~c01)T Y~ , U1T W T Y~ } = 2(A~0c1)T W U1 = 20T -2U1 = 0T -2,
we conclude Theorem 4.1 holds.

Now we study Theorem 4.2.
Proof of Theorem 4.2: First we show equation (12) holds. Recall that s,T and s,T are the sth eigenvalues of -1/2AT2 -u 1A2-1/2 and -1/2A2T u-1/2P u-1/2A2-1/2 respectively. Note that -1/2 and u-1/2 are bounded deterministic matrices. By continuous mapping theorem, it suffices to show that T -2AT2 A2 and T -2AT2 P A2 converge, where P = IT -A2(AT2 A2)-1A2T .
Define that  = E(zt), (i) = E{zt(zt - i)+}, and (i, j) = E{(zt - i)+(zt - j)+}, for l = 0 or 1, i = 1, · · · , K and j = 1, · · · , K. Let 1 be the 2 × 2 matrices with (i, j)-th element i+j-2, 2 be the K × K matrix with (i, j)-th element (i, j) and 3 be the 2 × K matrix with (i, j)-th element i-1(j). We first show that

T -2AT1 A1  1 Bv2(s)ds, T -2A2T A2  2 Bv2(s)ds, T -2AT1 A2  3 Bv2(s)ds.

(35) (36) (37)

Take the proof of equation (35) as an example. Note that the (i, j)th element of A1T A1 satisfying (A1T A1)i,j = xt2zti+j-2. By subtracting and adding the mean, we have

T -2

xt2zti+j-2 = T -2

xt2 zti+j-2 - E(zti+j-2) + T -2E(zti+j-2)

= op(1) + T -2E(zti+j-2) x2t .

x2t

Hence T -2(AT1 A1)i,j = T -2 x2t zti+j-2  i+j-2 Bv2(s)ds = (1)i,j Bv2(s)ds. Similarly,

28

we could show that equation (36) and (37) are true. Moreover, T -2AT2 P A2 = T -2A2T A2 - T -2AT2 A1(T -2AT1 A1)-1T -2AT1 A2  (2 - 3T -1 13) Bv2(s)ds.
Therefore, we conclude that equation (12) is true.

Next we prove that equation (13) is valid. Recall that equation (34) holds for any T and

K. Let d = T 2. Then we have

sup{2
0

log

L()

-

2

log

L(0)}

=D

sup[T
d0

log{1

+

NT DT

(dT (dT

-2) -2)

}

-

K k=1

log(1

+

dT -2k,T

)].

Define the right hand side in equation (38) as supd0 fT (d). We want to show that

(38)

sup
d0

fT

(d)



sup
d0

f (d)

=:

K
sup{
d0 s=1

1

ds + ds

ws2

-

K s=1

log(1

+

ds)}.

(39)

We first establish the finite dimensional convergence of fT (d). Since T -2s,t  s, we have

NT (dT -2) converges to

K s=1

ds 1+ds

ws2

for

every

fixed

d.

By

Proposition

6.3,

T -1DT (dT -2) = T -1{Y~ T W W T Y~ /2 - NT (dT -2)} = 1 + op(1).

Therefore, NT (dT -2)/DT (dT -2) = Op(T -1) and we have

T

log{1

+

NT DT

(dT (dT

-2) -2)

}

=

T

[

NT T {1

(dT -2) + op(1)}

+

Op(T -2)]

=

K s=1

1

ds + ds

ws2

+

op(1).

Together with the fact T -2s,T  s, we conclude fT (d) converges to f (d) for every fixed d.

Lemma 6.1 below shows that fT (d) form a tight sequence and hence fT (d) converges to f (d)

weakly. Lemma 6.2 further shows that supd0 fT (d)  supd0 f (d) by proving a continuous

mapping theorem type results holds. Therefore, equation (38) holds.

By

Proposition

6.4,

2

log

L(0)-2

log

L0

=

T

log(1+

Y~

T

wT2 -1 W W T Y~

/2

),

where

wT -1

=

(A~0c1)T

Y~ /.

Under H0, wT -1 =D N (0, 1). Recall that W T Y~ / =D N (0T -2, IT -2). Hence

log(1 +

Y~

T

wT2 -1 W W T Y~

/2

)

=

wT2 -1 T {1 + op(1)}

+ Op(T -2)

=

wT2 -1 T

+ op(T -1).

29

Therefore, 2 log L(0) - 2 log L0 = wT2 -1 + op(1) and Theorem 4.2 is proved.

Lemma 6.1. Under the assumptions of Theorem 4.2, fT (d)  f (d), where fT (d) and f (d) are defined between equation (38) and equation (39).

Proof of Lemma 6.1: We have already established the finite dimensional convergence of fT (d) to f (d). It suffices to show that fT (d) form a tight sequence, i.e. for every and  strictly positive, there exist  and T0 such that for T  T0,

-1P{ sup |fT (u) - fT (t)|  }  .
tut+

By

the

definition

of

fT

(·),

we

have

|fT

(u)

-

fT (t)|

=

T

log{

DT (T -2t) DT (T -2u)

}

+

Since log(1 + x) < x for every x > 0, it holds that

K s=1

log

1+tT -2s,T 1+uT -s,T

.

log{

DT (T -2t) DT (T -2u)

}



DT (T -2t) - DT (T -2u) DT (T -2u)



(u

-

t)

K s=1

T

-2s,T

ws2

T -2 s=K

+1

ws2

.

Since T -2s,T  s, there exists a constant C1 such that T -2s,T ws2  ws2 for all s and

T . Denote RK,T =

.K
s=1

ws2

/K

T -2 s=K+1

ws2/(T

-K -2)

Then

T

log{

DT DT

(T -2t) (T -2u)

}



(u

-

t)C1KRK,T .

Since

T -2s,T  s, there exists a constant C2 such that

K s=1

log(

1 1

+ +

uT -2s,T tT -2s,T

)



(u

-

t)

K s=1

s,T T -2



(u

-

t)C2K.

Let C3 = max(C1, C2). Then P{suptut+ |fT (u) - fT (t)|  }  P(RK,T  C3K - 1), and it reduces to show the cumulative distribution function (c.d.f.) HK,T of RK,T satisfies

1 - HK,T ( C3K - 1)  .

(40)

Note that RK,T follows the F -distribution with degrees of freedom K and T - 2 - K. For

every x, limT  HK,T (x) = HK (Kx), where HK is the c.d.f of K2 random variables. Using

L'Hospital

rules,

we

have

lim0+{1

-

HK ( C3

-

K

)}/{

 2

}

=

0.

Therefore,

we

could

find



=

(

, ),

with



<

1

and

C

-K

>

0,

such

that

1 - HK ( C3

- K)



 2

.

Because

of

the

convergence of HK,T (x) to HK(Kx), we could find T0 such that for T  T0, it holds that

30

|HK,T ( C3K

- 1) - HK ( C3

- K)|



 2

.

Thus

equation

(40)

holds

and

we

conclude

that

it

converges to f (d) weakly.

Lemma 6.2. Under the assumptions of Theorem 4.2, supd0 fT (d)  supd0 f (d).
Proof of Lemma 6.2: Lemma 6.1 shows that fT (d) weakly converges to f (d). Similar as Crainiceanu and Ruppert (2004), we first find a random variable FK,T such that

sup fT (d) = max fT (d).

d0

d[0,FK,T ]

(41)

Note that fT (0) = 0 for all T . It suffices to find FK,T such that fT (d) < 0 when d > FK,T .

Recall that log(1 + x)  x when x  0. By definition, DT (dT -2) 

T -2 s=K +1

ws2,

and

NT (dT -2) 

K s=1

ws2

for

all

d.

Hence

T

log{1

+

NT DT

(dT (dT

-2) -2)

}



T

NT (dT -2) DT (dT -2)



T

K s=1

ws2

T -2 s=K

+1

ws2

.

(42)

Let m0 be the positive constant such that all nonzero s,T 's satisfy T -2s,T  m0. Then

K
- log(1 + T -2s,T )  -K log(1 + dm0).
s=1

(43)

With equations (42) and (43), we establish that

fT (d)  T

K s=1

ws2

T -2 s=K

+1

ws2

-

K

log(1

+

dm0).

(44)

Let

FK,T

=:

m0-1{exp(

T

T -2-K

K RK,T

)

-

1}.

Mind

that

fT (FK,T )

= 0.

Since

the

right

hand

side of equation (44) is monotonic decreasing in d, FK,T has the desired property (41).

For any fixed M > 0 and t  0, we have maxd[0,M] fT (d)  supd0 fT (d). Hence

P{sup
d0

fT

(d)



t}



P{ max
d[0,M ]

fT

(d)



t}.

Taking lim sup for T   and applying the Continuous Mapping Theorem,

lim sup P(sup fT (d)  t)  lim sup P( max fT (d)  t) = P( max f (d)  t).

T  d0

T  d[0,M ]

d[0,M ]

31

Using the fact that

we have

lim P{ max f (d)  t} = P{sup f (d)  t},

M  d[0,M ]

d0

(45)

lim sup P(sup fT (d)  t)  P{sup f (d)  t}.

T  d0

d0

Since P(AB)  P(A) - P(Bc),

(46)

P(sup fT (d)  t)  P(sup fT (d)  t, FK,T < M )
d0 d0

= P( max fT (d)  t, FK,T < M )
d[0,M ]



P( max
d[0,M

]

fT

(d)



t) - P(FK,T

>

M ).

Note that P(FK,T > M )  P(FK > KM ), where FK is a 2K distributed random variable.

Taking lim inf, lim infT  P(supd0 fT (d)  t)  P(maxd[0,M] f (d)  t) - P(FK  KM ).

Using equation (45) and that limM P(FK  KM ) = 0, we conclude

lim inf
T 

P(sup
d0

fT

(d)



t)



P{sup
d0

f

(d)



t}.

Together with equation (46), the limit of P{supd0 fT (d)} exists and satisfying

lim
T 

P{sup
d0

fT

(d)



t}

=

P{sup
d0

f

(d)



t}.

Therefore, supd0 fT (d)  supd0 f (d).

Now we study Theorem 4.3.

Proof of Theorem 4.3: First we show that equation (14) holds. Note that for any matrix A,

its trace equals the sum of its eigenvalues. Since -1/2 and u-1/2 are bounded determinis-

tic matrices, it suffices to show both (KT 2)-1tr(AT2 A2) and (KT 2)-1tr(AT2 P A2) converge,

where P is defined the same as in the proof of Theorem 4.2.

Take the term (KT 2)-1tr(AT2 A2) as an example. Let

 2

=

2

Bv2(s)ds. From Equation

(36), the (i, j)-th element of T -2A2T A2 satisfies T -2(A2T A2)ij  ( 2)ij. Note that

KK
K-1 (2)ii = K-1 [E{(zt - i)2+}
i=1 i=1

Bv2(s)ds]  C

Bv2(s)ds,

32

where the last inequality holds as E{(zt - i)2+} is bounded. Therefore, (KT 2)-1tr(A2T A2) converges. Similarly, (KT 2)-1tr(A2T P A2) converges and thus equation (14) is true.

Next we prove equation (15). Let d = KT 2. By equation (34), we could conclude that

sup0{2 log L() - 2 log L(0)} =D supd0 gT (d), where

sup gT (d)
d0

=:

sup[T
d0

log{1

+

NT DT

d(KT 2)-1 (d(KT 2)-1)

}

-

K k=1

log{1

+

d(KT 2)-1k,T }].

(47)

Let g1(d) and g2(d) be continuous functions defined respectively as the following limits:

g¯1(d)

=

lim
T 

K k=1

1

dK-1T -2k,T + dK-1T -2k,T

,

K

g¯2(d)

=

lim
T 

log(1 + dK-1T -2k,T ).

k=1

(48)

Lemma 6.3 shows that g¯1(d) and g¯2(d) exist for every fixed d and established the finite

dimensional convergence of gT (d) to g¯1(d) - g¯2(d). Similar as Lemma 6.1, we could show that

gT (d) form a tight sequence and hence gT (d) converges to g¯1(d) - g¯2(d) weakly. Similar as

Lemma 6.2, we could establish a continuous mapping theorem type result and conclude that

sup gT (d)  sup{g¯1(d) - g¯2(d)},
d0 d0
Next we want to prove that

(49)

sup{g¯1(d) - g¯2(d)} = 0,
d0

(50)

To prove equation (50), we will show that g¯1(d) - g¯2(d)  0 for all d. Note that the first

derivative of the partial sum induced by g¯1(d) - g¯2(d) satisfies

K dK-1T -2k,T k=1 1 + dK-1T -2k,T

K
- log(1 + dK-1T -2k,T )
k=1

(1)

=

K k=1

(1

K-1T -2k,T + dK-1T -2k,T )2

-

K k=1

1

K-1T -2k,T + dK-1T -2k,T

=: Q1,T + Q2,T ,

where

Q1,T

=

K k=1

K-1T -2k,T (1 + dK-1T -2k,T )2

K
-
k=1

1

K-1T -2k,T + dK-1T -2k,T

,

33

and

Q2,T

=

K k=1

1

K-1T -2k,T + dK-1T -2k,T

-

K k=1

1

K-1T -2k,T + dK-1T -2k,T

.

Since (1 + dK-1T -2k,T )  1, we have Q1,T  0 for all T . Moreover, Q2,T  0 as we

explained below. Recall that k,T and k,T are the k-th eigenvalues of -1/2A~T2 A~2-1/2 and

-1/2A~T2 P A~2-1/2 respectively. Moreover,

-1/2A~T2 A~2-1/2 - -1/2A~2T P A~2-1/2 = -1/2A~2T A~1(A~T1 A~1)-1A~T1 A~2-1/2,

which

is

a

semi-positive

definite

matrix.

Hence

k,T

 k,T

for

all

k

and

T.

Since

x 1+dx

is

an

increasing function of x, we have

Q2,T

=:

K k=1

1

K-1T -2k,T + dK-1T -2k,T

-

K k=1

1

K-1T -2k,T + dK-1T -2k,T

 0.

(51)

Because g¯1(d) and g¯2(d) are both absolutely summable, we could change the order between

summation and derivative. Since Q1,T  0 and Q2,T  0, we conclude that the first derivative of g¯1(d) - g¯2(d) satisfies

g¯1(d) - g¯2(d) = Tlim(Q1,T + Q2,T )  0.
Recall that g¯1(0) - g¯2(0) = 0. For d  0, g¯1(d) - g¯2(d) = 0 + 0d{g¯1(x) - g¯2(x)}dx  0. Therefore, equation (50) holds and thus sup0{2 log L() - 2 log L(0)} = supd0 gT (d)  0.
Similarly as in Theorem 4.2, we conclude that

2 log L(0) - 2 log L0 = wT2 -1 + op(1), where wT -1 =D N (0, 1) under H0. Therefore, LRTT  21 under H0.

Lemma 6.3. Assume the conditions in Theorem 4.3 and define gT (d) as in equation (47). Then gT (d) converges to g¯1(d) - g¯2(d) for every fixed d, where g¯1(d) and g¯2(d) are defined
in equation (48).

34

Proof of Lemma 6.3: First, we prove that g¯1(d) and g¯2(d) exist for any fixed d  0. Since

1 + x  exp(x) for x  0, we have

KK

K

1 + d K-1T -2k,T  (1 + dK-1T -2k,T )  exp{d K-1T -2k,T }.

k=1

k=1

k=1

If d

K k=1

K

-1

T

-2k,T

converges,

so

does

log{

Kk=1(1 + dK-1T -2k,T )}. We have already

proved equation (14), i.e.

K k=1

K

-1T

-2

k,T



¯.

Hence

K k=1

log(1

+

dK-1T -2k,T )

con-

verges and its limit g¯2(d) exists. Note that

0

dK-1T -2k,T 1 + dK-1T -2k,T

 dK-1T -2k,T

 dK-1T -2k,T .

Since

K k=1

dK

-1

T

-2k,T

converges,

the

limit

of

K dK-1T -2k,T k=1 1+dK-1T -2k,T

exists and we could

denote it as g¯1(d).

Next we establish the finite dimensional convergence of gT (d) to g¯1(d) - g¯2(d). Lemma 6.4

below shows NT (dK-1T -2) converges to g¯1(d) for every fixed d. By Proposition 6.3,

T -1DT d(KT 2)-1 = T -1{Y~ T W W T Y~ /2 - NT d(KT 2)-1 } = 1 + op(1).

Correspondingly NT d(KT 2)-1 /DT d(KT 2)-1 = Op(T -1) and

T

log{1

+

NT DT

d(K T (d(K T

2)-1 2)-1)

}

=

T

[

NT T

d(KT 2)-1 {1 + op(1)}

+ Op(T -2)] = g¯1(d) + op(1).

With the fact that

K k=1

log{1

+

d(K T

2)-1k,T

}

converges

to

g¯2(d)

for

every

fixed

d,

Lemma

6.3 holds.

Lemma 6.4. Assume the conditions in Theorem 4.3. Then NT (dK-1T -2) converges to g¯1(d) for every fixed d.

Proof of Lemma 6.4: Notice that wk =D iidN (0, 1). It suffices to consider show that

K k=1

dK-1T -2k,T 1+dK-1T -2k,T

wk2

- g¯1(d)

=

K k=1

dK-1T -2k,T 1+dK-1T -2k,T

(wk2

- 1)

converges

to

0

in

finite

dimen-

sion. For every fixed d,

E

K k=1

1

dK-1T -2k,T + dK-1T -2k,T

(wk2

-

1)

=

0.

35

Moreover,

K
var{
k=1

1

dK-1T -2k,T + dK-1T -2k,T

(wk2

-

1)}

=

2

K
(1
k=1

dK-1T -2k,T + dK-1T -2k,T

)2

=

O(K -1 )

=

o(1).

Hence Lemma 6.4 is valid.

Now we consider Theorem 4.4.

Proof of Theorem 4.4: First we consider the local alternative H01. Note that all spline

coefficients are 0 under H01. Therefore, it still holds sup0{2 log L() - 2 log L(0)}  0.

It suffices to show 2 log L(0) - 2 log L0 converges to a noncentral 12 with parameter ¯1¯2.

As W T A~1 = 0(T -2)×2, we have W T Y~

= W T u~.

By

the

fact

that

W0T A~r1

=

W0T

(1

A~01 ||A~01||

+

2A~0c1) = 2W0T A~c01, it holds W0T Y~ = (T -1¯1)2W0T A~0c1 + W0T u~. Hence

Y~ T W0W0T Y~ - Y~ T W W T Y~

2

=

T -2¯1222(A~0c1)T W0W0T A~0c1 2

+

2T -1¯12(A~0c1)T W0W0T u~ 2

+

u~T (W0W0T - 2

W W T )u~

=

T -2¯1222 2

+

2T -1¯12(A~c01)T u~ 2

+

u~T A~c01(A~c01)T u~ 2

=D (w1 + T -12¯1)2,

where w1 = (A~0c1)T u~/ =D N (0, 1), and the second equation holds as any unit direction orthogonal to A~01 is the eigenvector of the projection matrix W0W0T , i.e. (A~0c1)T W0W0T = (A~0c1)T . Now we show the limit of T -12 exists. Note that

(T -12)2 = (T -12)2(A~c01)T W0W0T A~0c1 = T -2A~rT1W0W0T A~r1 = T -2A~Tr1P0A~r1.

Using the same technique as in Theorem 4.2, we conclude all elements in T -2ATr1Ar1 and T -2ATr1P0Ar1 converge, where P0 = IT - A01(AT01A01)-1AT01. Since u-1/2 and -1/2 are bounded deterministic matrices, T -2A~Tr1A~r1 and T -2A~rT1P0A~r1 converge. Equivalently, T -12
converges and we could denote its limit as ¯2. Hence

LRTT = 2 log L(0) - 2 log L0 + op(1) = (w1 + ¯1¯2)2 + op(1),

(52)

36

i.e. the asymptotic distribution is a non-central chi-square distribution with parameter ¯1¯2.

Next consider H02. Let d = KT 2. By Proposition 6.1 and Proposition 6.2, we have

sup0{2 log L()

-

2 log L(0)}

=D

supd0 hT (d),

where

hT (d)

=:

T

log{1

+

NT DT

(d(KT
(d(K T

2 2

)-1 )-1

)
)

}

-

K k=1

log{1

+

d(K

T

2

)-1k,T

}.

We

want

to

show

sup hT (d)  sup h(d) =: sup{d¯0¯ + g¯3(d) - g¯2(d)},
d0 d0 d0

(53)

where g¯2(d) and ¯ are defined as in Theorem 4.3 and g¯3(d) is the limit of

K k=1

(d-d¯0)(KT 2)-1k,T 1+d(KT 2)-1k,T

.

Note that g¯3(d) exists because the term

K k=1

(d-d¯0)(KT 2)-1k,T 1+d(KT 2)-1k,T

is bounded by -d¯0¯ and g¯1(d).

To prove equation (53), we first establish the finite dimensional convergence of hT (d) to

h(d). For every fixed d, we could simplify NT d(KT 2)-1 as

NT d(KT 2)-1

=

d¯0

K
(K T
k=1

2)-1k,T

wk2

+

K k=1

(d - d¯0)(KT 2)-1k,T 1 + d(KT 2)-1k,T

wk2.

Apply the same technique as Lemma 6.4, we could show that Kk=1(KT 2)-1k,T (wk2 - 1)  0

and thus

kK=1(KT 2)-1k,T wk2  ¯. Similarly,

K k=1

(d-d¯0)(KT 2)-1k,T 1+d(KT 2)-1k,T

wk2

converges

to

g¯3(d)

for every fixed d. Therefore, NT d(KT 2)-1 does so to d¯0¯ + g¯3(d). Using Proposition 6.3,

T -1DT d(KT 2)-1 = T -1{Y~ T W W T Y~ /2 - NT d(KT 2)-1 } = 1 + op(1). Correspondingly,

NT d(KT 2)-1 /DT d(KT 2)-1 = Op(T -1). Hence

T

log{1

+

NT DT

d(K T (d(K T

2)-1 2)-1)

}

=

T

[

NT T

d(K T {1 + op

2)-1 (1)}

+ Op(T -2)] = d¯0¯ + g¯3(d) + op(1).

Since

K k=1

log{1

+ d(K T

2)-1k,T

}

converges

to

g¯2(d)

for

every

fixed

d,

so

does

hT

(d)

to

h(d).

Similar as Lemma 6.1 and Lemma 6.2, we could further show that hT (d) weakly converges

to h(d) and a continuous mapping theorem type results holds. Thus equation (53) holds.

Next we want to show

sup
d0

h(d)

=

dm[0a,dx¯0]{d¯0¯

+

g¯3(d)

-

g¯2(d)}.

(54)

37

Note that the first derivative of the partial sum induced by g¯3(d) - g¯2(d) satisfies

lT (d)

=

K {1 + d0(KT 2)-1k,T }(KT 2)-1k,T k=1 {1 + d(KT 2)-1k,T }2

-

K k=1

1

(KT 2)-1k,T + d(KT 2)-1k,T



K (KT 2)-1k,T k=1 1 + d(KT 2)-1k,T

-

K k=1

1

(KT 2)-1k,T + d(KT 2)-1k,T

=

K (KT 2)-1k,T k=1 1 + d(KT 2)-1k,T

-

K k=1

1

(KT 2)-1k,T + d(KT 2)-1k,T

,

where the inequality holds when d  d¯0. By equation (51),

K (KT 2)-1k,T k=1 1 + d(KT 2)-1k,T



K k=1

1

(KT 2)-1k,T + d(KT 2)-1k,T

.

When d  d¯0, lT (d)  0 and hence h(d)  h(d¯0) for d  d¯0. Thus equation (54) holds.

Finally, we will show that 2 log L(0) - 2 log L0  (1 + d¯0 )w12. Under H02, we have that

W T Y~ = W T (T -4/5A~2¯b + u~) and W0T Y~ = W0T (T -4/5A~2¯b + u~). Hence

Y~ T W0W0T Y~ - Y~ T W W T Y~ 2

=

(T -4/5A~2¯b + u~)T A~c01(A~c01)T (T -4/5A~2¯b + u~) 2

T
=D i4wi2,
i=1

where wi =D iidN (0, 1) and i4 is the ith eigenvalue of

4 = {IT + d¯0(KT 2)-1A~2-1A~2T }1/2A~0c1(A~c01)T {IT + d¯0(KT 2)-1A~2-1A~T2 }1/2.

Note that 4 share the same nonzero eigenvalue as

(A~0c1)T {IT

+

d¯0(KT 2)-1A~2-1A~2T }A~0c1

=

1

+

d¯0

(A~c01)T

A~2-1A~T2 KT 2

A~c01

.

Moreover,

we

could

show

that

the

limit

of

(A~c01)T A~2-1A~2T A~c01 KT 2

exists.

Using the same tech-

nique as in Theorem 4.2, we conclude that each element in T -2A2T A01(A0c1)T A2 converges.

Since -u 1/2 and -1/2 are bounded deterministic matrices, each element of the matrix

T -2-1/2A~2T A~0c1(A~c01)T A~2-1/2 converges and K-1T -2tr{-1/2A~2T A~c01(A~0c1)T A~2-1/2} con-

verges. By the fact that (A~c01)T A~2-1A~2c A~c01 = tr{-1/2A~T2 A~c01(A~c01)T A~2-1/2}, the limit of

(KT 2)-1(A~0c1)T A~2-1A~2T A~c01 exists and we could denote it as . Therefore,

Y~ T W0W0T Y~ - Y~ T W W T Y~ 2

=D (1 + d¯0

)w12,

(55)

and we could further conclude 2 log L(0) - 2 log L0  (1 + d¯0 )w12. Theorem 4.4 is proved.

38

Appendix B: Tables

Table 1: The empirical size of the proposed LRT when u is known

Finite dist.

Asymptotic dist.

20% 15% 10% 5% 1% 20% 15% 10% 5% 1%

Panel A1: T = 100,  = 0

K = 10 0.1915 0.1445 0.0930 0.0430 0.0095 0.1995 0.1490 0.0995 0.0465 0.0100

K = 20 0.1940 0.1425 0.0935 0.0425 0.0100 0.1985 0.1480 0.0990 0.0465 0.0100

K = 40 0.1920 0.1425 0.0925 0.0415 0.0085 0.1985 0.1480 0.0990 0.0465 0.0100

Panel A2: T = 100,  = 0.5

K = 10 0.1940 0.1435 0.0975 0.0550 0.0095 0.1975 0.1495 0.1025 0.0585 0.0105

K = 20 0.1930 0.1440 0.0990 0.0550 0.0095 0.1970 0.1495 0.1035 0.0580 0.0105

K = 40 0.1945 0.1435 0.0975 0.0555 0.0090 0.1985 0.1500 0.1035 0.0585 0.0105

Panel B1: T = 300,  = 0

K = 20 0.2035 0.1560 0.0975 0.0540 0.0160 0.2050 0.1565 0.0995 0.0535 0.0165

K = 40 0.2035 0.1550 0.0990 0.0530 0.0160 0.2050 0.1565 0.0995 0.0535 0.0165

K = 80 0.2025 0.1560 0.0980 0.0530 0.0160 0.2045 0.1560 0.0995 0.0535 0.0165

Panel B2: T = 300,  = 0.5

K = 20 0.1985 0.1550 0.0975 0.0485 0.0105 0.1995 0.1555 0.1010 0.0495 0.0110

K = 40 0.1985 0.1520 0.1000 0.0485 0.0120 0.1980 0.1550 0.1005 0.0490 0.0110

K = 80 0.1975 0.1540 0.0975 0.0485 0.0105 0.1975 0.1550 0.1005 0.0495 0.0110

Note: The model is yt = (zt)xt + ut with xt = xt-1 + vt and ut = ut-1 + t, where vt's and t's are iid N (0, 1) and are independent with each other. The initial values are set to be zero. In particular, (zt) = 0.25 and the true covariance matrix is used. The rejection frequencies are calculated based on 2000 replications.

39

Table 2: The empirical size of the proposed LRT when u is unknown

Finite dist.

Asymptotic dist.

20% 15% 10% 5% 1% 20% 15% 10% 5% 1%

Panel A1: T = 100,  = 0

K = 10 0.2170 0.1595 0.1065 0.0500 0.0150 0.2245 0.1635 0.1120 0.0565 0.0155

K = 20 0.2190 0.1610 0.1075 0.0530 0.0125 0.2245 0.1630 0.1115 0.0565 0.0150

K = 40 0.2190 0.1585 0.1060 0.0530 0.0135 0.2245 0.1630 0.1115 0.0565 0.0150

Panel A2: T = 100,  = 0.5

K = 10 0.2065 0.1565 0.1120 0.0635 0.0125 0.2110 0.1610 0.1155 0.0650 0.0145

K = 20 0.2045 0.1545 0.1080 0.0630 0.0130 0.2105 0.1610 0.1135 0.0650 0.0145

K = 40 0.2060 0.1555 0.1105 0.0625 0.0125 0.2105 0.1605 0.1135 0.0650 0.0145

Panel B1: T = 300,  = 0

K = 20 0.2110 0.1640 0.1075 0.0575 0.0165 0.2125 0.1645 0.1075 0.0585 0.0175

K = 40 0.2110 0.1635 0.1065 0.0570 0.0170 0.2125 0.1645 0.1075 0.0585 0.0175

K = 80 0.2105 0.1635 0.1060 0.0565 0.0165 0.2120 0.1640 0.1075 0.0585 0.0175

Panel B2: T = 300,  = 0.5

K = 20 0.2075 0.1550 0.1000 0.0500 0.0115 0.2090 0.1595 0.1025 0.0500 0.0115

K = 40 0.2065 0.1555 0.0995 0.0480 0.0115 0.2075 0.1590 0.1020 0.0500 0.0110

K = 80 0.2065 0.1565 0.1000 0.0485 0.0105 0.2070 0.1590 0.1020 0.0500 0.0110

Note: The model is yt = (zt)xt + ut with xt = xt-1 + vt and ut = ut-1 + t, where vt's and t's are iid N (0, 1) and are independent with each other. The initial values are set to be zero. In particular, (zt) = 0.25. The true covariance matrix is unknown and is replaced by its estimate. The rejection frequencies are calculated based on 2000 replications.

40

Table 3 The power of the proposed LRT

Finite dist.

Asymptotic dist.

u is known Panel A1: T = 100,  = 0

u is unknown

u is known

u is unknown

K = 10 0.6220 (0.5255) 0.6330 (0.5405) 0.6255 (0.5300) 0.6365 (0.5455)

K = 20 0.6175 (0.5195) 0.6275 (0.5325) 0.6210 (0.5240) 0.6330 (0.5390)

K = 40 0.6140 (0.5150) 0.6265 (0.5290) 0.6195 (0.5205) 0.6295 (0.5350)

Panel A2: T = 100,  = 0.5

K = 10 0.7085 (0.6050) 0.6930 (0.5905) 0.7125 (0.6110) 0.6975 (0.5960)

K = 20 0.7050 (0.6010) 0.6855 (0.5890) 0.7080 (0.6055) 0.6890 (0.5925)

K = 40 0.7025 (0.5960) 0.6855 (0.5875) 0.7055 (0.6025) 0.6875 (0.5900)

Panel B1: T = 300,  = 0

K = 20 0.9920 (0.9855) 0.9915 (0.9855) 0.9920 (0.9855) 0.9915 (0.9850)

K = 40 0.9915 (0.9855)) 0.9905 (0.9850) 0.9915 (0.9855) 0.9910 (0.9850)

K = 80 0.9910 (0.9855) 0.9910 (0.9850) 0.9910 (0.9855) 0.9910 (0.9850)

Panel B2: T = 300,  = 0

K = 20 0.994 (0.99)

0.9935 (0.9910) 0.994 (0.99)

0.9940 (0.9910)

K = 40 0.994 (0.99)

0.9940 (0.9905) 0.994 (0.99)

0.9935 (0.9910)

K = 80 0.994 (0.99)

0.9935 (0.9905) 0.994 (0.99)

0.9935 (0.9910)

Note: The model is yt = (zt)xt + ut with xt = xt-1 + vt and ut = ut-1 + t, where vt's and t's are iid N (0, 1) and are independent with each other. The initial values are set to be zero. In particular, (zt) = (zt - 0.5)2. The rejection frequencies are calculated using the critical values associated with  = 0.05 or  = 0.01 as indicated inside the parenthesis. All results are based on 2000 replications.

41

References
Bierens, H. J. & L. Martins (2010) Time-varying cointegration. Econometric Theory 26, 1453-1490.
Cai, Z., Q. Li, & J. Y. Park (2009) Functional-coefficient models for nonstationary time series data. Journal of Econometrics 148, 101-113.
Campbell, J. & M. Yogo (2006). Efficient tests of stock return predictability. Journal of Financial Econometrics 81, 27-60.
Claeskens, G., T. Krivobokova, & J. Opsomer (2009) Asymptotical properties of penalized spline estimators. Biometrika 96, 529-544.
Crainiceanu, C. M. & D. Ruppert (2004) Likelihood ratio tests in linear mixed models with one variance component. Journal of the Royal Statistical Society, Series B 66, 165-185.
Crainiceanu, C. M., D. Ruppert, G. Claeskens & M. P. Wand (2005) Exact likelihood ratio tests for penalized splines. Biometrika 92, 91­103.
de Boor, C. (1978) A practical guide to splines. Springer-Verlag, New York.
Eilers, P. H. C. & B. D. Marx (1996) Flexible smoothing with B-splines and penalties. Statistical Science 11, 89-121.
Engle, R. F. & C. W. J. Granger (1987) Cointegration and error correction: Representation, estimation, and testing. Econometrica 55, 251-276.
Fan, J., C. Zhang, & J. Zhang (2001) Generalized likelihood ratio statistics and Wilks phenomenon. Annals of Statistics 29, 153-193.
Gao, J., M. King, Z. Lu, & D. Tjøstheim (2009) Nonparametric specification testing for nonlinear time series with nonstationarity. Econometric Theory 25, 1869-1892.
Goldfajn, I. & T. Baig (1998) Monetary policy in the aftermath of currency crises: The case of Asia. Working paper, IMF.
Goyal, A. & I. Welch (2003) Predicting the equity premium with dividend ratios. Management Science 49, 639-654.
Hall, P. & J. D. Opsomer (2005) Theory for penalised spline regression. Biometrika 92, 105-118.
Hansen B. E. (1992) Tests for parameter instability in regressions with I(1) processes. Journal of Business and Economic Statistics 10, 321-335.
Hansen, H. & S. Johansen (1999) Some tests for parameter constancy in cointegrated VARmodels. Econometrics Journal 2, 306-333.
Hao, K. (1996) Testing for structural change in cointegrated regression models: Some comparisons and generalizations. Econometric Reviews 15, 401-429.
42

H¨ardle, W. & E. Mammen (1993) Comparison nonparametric versus parametric regression fits. Annals of Statistics 21, 1926-1947.
Harris, D., B. McCabe, & S. Leybourne (2002) Stochastic cointegration: estimation and inference. Journal of Econometrics 111, 363-384.
Hong, Y. & H. White (1995) Consistent specification testing via nonparametric series regression. Econometrica 63, 1133-1159.
Hong, S. & P. C. B. Phillips (2010) Testing linearity in cointegrating relations with an application to purchasing power parity. Journal of Business and Economic Statistics 28, 96-113.
Johansen S., R. Mosconi, & B. Nielsen (2000) Cointegration analysis in the presence of structural breaks in the deterministic trend. Econometrics Journal 3, 216-249.
Juhl, T. (2005) Functional-coefficient models under unit root behavior. Econometrics Journal 8, 197-213.
Kasparis, I. (2008) Detection of functional form misspecification in cointegrating relations. Econometric Theory 24, 1373-1404.
Kuo, B. S. (1998) Test for partial parameter instability in regressions with I(1) processes. Journal of Econometrics 86, 337-368.
Lettau, M. & S. Ludvigsson (2001) Consumption, aggregate wealth, and expected stock returns. Journal of Finance 56, 815-849.
Li, Y. & D. Ruppert (2008) On the asymptotics of penalized splines. Biometrika 95, 415-436.
Nelson, R. & C.I. Plosser (1982) Trends and random walks in macroeconomic time series: some evidence and implications. Journal of Monetary Economics 10, 139-162.
Park, J. & S. Hahn (1999) Cointegrating regressions with time varying coefficients. Econometric Theory 15, 664-703.
Park, J. & P. C. B. Phillips (2001) Nonlinear regressions with integrated time series. Econometrica 69, 117-161.
Patterson, H. D. & R. Thompson (1971) Recovery of inter-block information when block sizes are unequal. Biometrika 58, 545­554.
Paye, B. S. & A. Timmermann (2006) Instability of return prediction models. Journal of Empirical Finance 13, 274-315.
Quintos, C. E. (1997) Stability tests in error correction models. Journal of Econometrics 82, 289-315.
Ruppert, D. (2002) Selecting the number of knots for penalized splines. Journal of the Computational and Graphical Statistics 11, 735-757.
43

Ruppert, D., M. P. Wand & R. J. Carroll (2003) Semiparametric regression. Cambridge, UK: Cambridge University Press.
Saikkonen, P. (1991) Asymptotically efficient estimation of cointegration regressions. Econometric Theory 7, 1-20.
Saikkonen, P. & I. Choi (2004) Cointegrating smooth transition regressions. Econometric Theory 20, 201-340.
Shao, Q. & C. Lu (1987) Strong approximation for partial sums of weakly dependent random variables. Scientia Sinica 15, 576-587.
Shi, X. & P. C. B. Phillips (2012) Nonlinear cointegrating regression under weak identification. Econometric Theory 28, 509-547.
Wang Q. & P. C. B. Phillips (2009a) Structural Nonparametric Cointegrating regression. Econometrica 77, 1901-1948.
Wang Q. & P. C. B. Phillips (2009b) Asymptotic theory for local time density estimation and nonparametric cointegrating regression. Econometric Theory 25, 710-738.
Xiao, Z. (2009) Functional-coefficient cointegration models. Journal of Econometrics 152, 81-92.
44

SFB 649 Discussion Paper Series 2013
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.

001
002 003 004 005 006
007
008 009
010 011 012
013 014
015 016
017
018 019

"Functional Data Analysis of Generalized Quantile Regressions" by
Mengmeng Guo, Lhan Zhou, Jianhua Z. Huang and Wolfgang Karl Härdle, January 2013. "Statistical properties and stability of ratings in a subset of US firms" by
Alexander B. Matthies, January 2013. "Empirical Research on Corporate Credit-Ratings: A Literature Review" by Alexander B. Matthies, January 2013. "Preference for Randomization: Empirical and Experimental Evidence" by
Nadja Dwenger, Dorothea Kübler and Georg Weizsäcker, January 2013. "Pricing Rainfall Derivatives at the CME" by Brenda López Cabrera, Martin Odening and Matthias Ritter, January 2013. "Inference for Multi-Dimensional High-Frequency Data: Equivalence of
Methods, Central Limit Theorems, and an Application to Conditional Independence Testing" by Markus Bibinger and Per A. Mykland, January 2013.
"Crossing Network versus Dealer Market: Unique Equilibrium in the Allocation of Order Flow" by Jutta Dönges, Frank Heinemann and Tijmen R. Daniëls, January 2013. "Forecasting systemic impact in financial networks" by Nikolaus Hautsch,
Julia Schaumburg and Melanie Schienle, January 2013. "`I'll do it by myself as I knew it all along': On the failure of hindsightbiased principals to delegate optimally" by David Danz, Frank Hüber,
Dorothea Kübler, Lydia Mechtenberg and Julia Schmid, January 2013. "Composite Quantile Regression for the Single-Index Model" by Yan Fan, Wolfgang Karl Härdle, Weining Wang and Lixing Zhu, February 2013. "The Real Consequences of Financial Stress" by Stefan Mittnik and Willi
Semmler, February 2013. "Are There Bubbles in the Sterling-dollar Exchange Rate? New Evidence from Sequential ADF Tests" by Timo Bettendorf and Wenjuan Chen, February 2013.
"A Transfer Mechanism for a Monetary Union" by Philipp Engler and Simon Voigts, March 2013. "Do High-Frequency Data Improve High-Dimensional Portfolio
Allocations?" by Nikolaus Hautsch, Lada M. Kyj and Peter Malec, March 2013. "Cyclical Variation in Labor Hours and Productivity Using the ATUS" by Michael C. Burda, Daniel S. Hamermesh and Jay Stewart, March 2013.
"Quantitative forward guidance and the predictability of monetary policy ­ A wavelet based jump detection approach ­" by Lars Winkelmann, April 2013.
"Estimating the Quadratic Covariation Matrix from Noisy Observations: Local Method of Moments and Efficiency" by Markus Bibinger, Nikolaus Hautsch, Peter Malec and Markus Reiss, April 2013. "Fair re-valuation of wine as an investment" by Fabian Y.R.P. Bocart
and Christian M. Hafner, April 2013. "The European Debt Crisis: How did we get into this mess? How can we get out of it?" by Michael C. Burda, April 2013.

SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2013
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
020 "Disaster Risk in a New Keynesian Model" by Maren Brede, April 2013. 021 "Econometrics of co-jumps in high-frequency data with noise" by Markus
Bibinger and Lars Winkelmann, May 2013. 022 "Decomposing Risk in Dynamic Stochastic General Equilibrium" by Hong
Lan and Alexander Meyer-Gohde, May 2013. 023 "Reference Dependent Preferences and the EPK Puzzle" by Maria Grith,
Wolfgang Karl Härdle and Volker Krätschmer, May 2013. 024 "Pruning in Perturbation DSGE Models - Guidance from Nonlinear Moving
Average Approximations" by Hong Lan and Alexander Meyer-Gohde, May 2013. 025 "The `Celtic Crisis': Guarantees, transparency, and systemic liquidity risk" by Philipp König, Kartik Anand and Frank Heinemann, May 2013. 026 "State Price Densities implied from weather derivatives" by Wolfgang Karl Härdle, Brenda López-Cabrera and Huei-Wen Teng, May 2013. 027 "Bank Lending Relationships and the Use of Performance-Sensitive Debt" by Tim R. Adam and Daniel Streitz, May 2013. 028 "Analysis of Deviance in Generalized Partial Linear Models" by Wolfgang Karl Härdle and Li-Shan Huang, May 2013. 029 "Estimating the quadratic covariation of an asynchronously observed semimartingale with jumps" by Markus Bibinger and Mathias Vetter, May 2013. 030 "Can expert knowledge compensate for data scarcity in crop insurance pricing?" by Zhiwei Shen, Martin Odening and Ostap Okhrin, May 2013. 031 "Comparison of Methods for Constructing Joint Confidence Bands for Impulse Response Functions" by Helmut Lütkepohl, Anna StaszewskaBystrova and Peter Winker, May 2013. 032 "CDO Surfaces Dynamics" by Barbara Choro-Tomczyk, Wolfgang Karl Härdle and Ostap Okhrin, July 2013. 033 "Estimation and Inference for Varying-coefficient Models with Nonstationary Regressors using Penalized Splines" by Haiqiang Chen, Ying Fang and Yingxing Li, July 2013.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

