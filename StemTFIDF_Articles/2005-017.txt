BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2005-017
A Two State Model for Noise-Induced
Resonance in Bistable Systems with Delay
Markus Fischer* Peter Imkeller**
* Weierstraß-Institut für Angewandte Analysis und Stochastik (WIAS), Berlin, Germany ** Department of Mathematics,
Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

A two state model for noise-induced resonance in bistable systems with delay

Markus Fischer Weierstraß-Institut für Angewandte
Analysis und Stochastik (WIAS) Mohrenstr. 39 10117 Berlin Germany

Peter Imkeller Institut für Mathematik Humboldt-Universität zu Berlin
Unter den Linden 6 10099 Berlin Germany

March 24, 2005

Abstract
The subject of the present paper is a simplified model for a symmetric bistable system with memory or delay, the reference model, which in the presence of noise exhibits a phenomenon similar to what is known as stochastic resonance. The reference model is given by a one dimensional parametrized stochastic differential equation with point delay, basic properties whereof we check.
With a view to capturing the effective dynamics and, in particular, the resonance-like behaviour of the reference model we construct a simplified or reduced model, the two state model, first in discrete time, then in the limit of discrete time tending to continuous time. The main advantage of the reduced model is that it enables us to explicitly calculate the distribution of residence times which in turn can be used to characterize the phenomenon of noise-induced resonance.
Drawing on what has been proposed in the physics literature, we outline a heuristic method for establishing the link between the two state model and the reference model. The resonance characteristics developed for the reduced model can thus be applied to the original model.
2000 AMS subject classifications: primary 34K50, 60H10; secondary 60G17, 34K11, 34K13, 34K18, 60G10.
Key words and phrases: stochastic differential equation; delay differential equation; stochastic resonance; effective dynamics; Markov chain; stationary process; stochastic synchronization.
1 Introduction
Stochastic resonance in a narrower sense is the random amplification of a weak periodic signal induced by the presence of noise of low intensity such that the signal amplification is maximal at a certain optimal non-zero level of noise. In addition to weak additive noise and a weak periodic input signal there is a third ingredient in systems where stochastic resonance can occur, namely a threshold or a barrier that induces several (in our case two) macroscopic states in the output signal.
Consider a basic, yet fundamental example. Let V be a symmetric one dimensional double well potential. A common choice for V is the standard quartic potential, see Fig. 1 a). The barrier mentioned
This work was partially supported by the DFG research center Matheon (FZT 86) in Berlin and the DFG Sonderforschungsbereich 649 "Economic Risk".
1

above is in this case the potential barrier of V separating the two local minima. Assume that the periodic input signal is sinusoidal and the noise white. The output of such a system is described by the stochastic differential equation (SDE)

(1)

dX(t) = - V

X (t)

+ a · sin

2 T

t

dt +  · dW (t),

t  0,

where W is a standard one dimensional Wiener process,   0 a noise parameter, V the first order derivative of the double well potential V , a  0 the amplitude and T > 0 the period of the input signal.
As an alternative to the system view, equation (1) can be understood as describing the overdamped motion of a small particle in the potential landscape V in the presence of noise and under the influence of an exterior periodic force. It was originally proposed by Benzi et al. (1981, 1982) and Nicolis (1982) as an energy balance model designed to explain the succession of ice and warm ages in paleoclimatic records as a phenomenon of quasi periodicity in the average global temperature on Earth.

If a = 0, i. e. in the absence of a periodic signal, equation (1) reduces to an autonomous SDE which has two metastable states corresponding to the two local minima of V . With  > 0 sufficiently small, the diffusion will spend most of its time near the positions of these minima. In the presence of weak noise, there are two distinct time scales, a short one corresponding to the quadratic variation of the Wiener process, and a long one proportional to the average time it takes the diffusion to travel from one of the metastable states to the other.
The fact that the time scale induced by the noise process is small in comparison with the mean residence time as  tends to zero should allow us to disregard small intrawell fluctuations when we are interested in the interwell transition behaviour.

Suppose a > 0 small enough so that there are no interwell transitions in case  = 0, i. e. in the deterministic

case. The input signal then slightly and periodically tilts the double well potential V . We now have two

different mean residence times, namely the average time the particle stays in the shallow well and the

average time of residence in the deep well. Of course, both time scales also depend on the noise intensity.

Notice

that

deep

and

shallow

well

change

roles

every

half

period

T 2

.

Given

a

sufficiently

long

period

T , the noise intensity can now be tuned in such a way as to render the occurence

of transitions from the shallow to the deep well probable within one half period, while this time span

is too short for the occurrence of transitions in the opposite direction. At a certain noise level the output

signal will exhibit quasi periodic transition behaviour, thereby inducing an amplification of the input

signal.

For a more comprehensive description of stochastic resonance and its wide field of applications in many fields of science and engineering see Gammaitoni et al. (1998) or Anishchenko et al. (1999). Very recently it has been used in economics related models designed for the explanation of non-linear market phenomena such as crashes and bubbles, see Krawiecki and Holyst (2003). In their model, the external periodic force corresponds to a weak external information carrying signal. What models that exhibit stochastic resonance have in common is the quasi periodicity of the output at a certain non-zero noise level. More generally, stochastic resonance is an instance of noise-induced order.

In view of the fact that the system given by equation (1) can work as a random amplifier it seems natural to take the frequency spectrum of the output signal as basis for a measure of resonance. The most common measure of this kind is the spectral power amplification (SPA) coefficient. Another measure of resonance based on the frequency spectrum is the signal to noise ratio (SNR). For a detailed analysis see Pavlyukevich (2002).
In general, when measuring stochastic resonance, it is assumed that the solution is in a "stationary regime". Since equation (1) is time dependent for a > 0 we cannot expect (X(t))t0 to be a stationary

2

process. Transforming the non-autonomous SDE (1) into an autonomous SDE with state space R × S1, one can recover the time homogeneous Markov property and a unique invariant probability measure exists, cf. Imkeller and Pavlyukevich (2002). In section 3 we will make use of the same idea of appropriately enlarging the state space in order to regain a time homogeneous Markov model.
A different starting point for a measure of resonance ­ the one that will be adopted here ­ is the distribution of intrawell residence times. Observe that the roles of the two potential wells are interchangeable.
A third class of measures of resonance is provided by methods of quantifying (un)certainty, in particular by the entropy of a distribution. This agrees well with the view of stochastic resonance as an instance of noise-induced order.
The fact that with  > 0 and a > 0 small a typical solution to (1) spends most of its time near the positions of the two minima of the double well potential V suggests to identify the two potential wells with their respective minima. The state space R of the non-autonomous SDE thus gets reduced to two states, say -1 and 1, corresponding to the left and the right well, respectively.
According to an idea of McNamara and Wiesenfeld (1989) the effective dynamics of equation (1) can be captured in the two state model by constructing a {-1, 1}-valued time inhomogeneous Markov chain with certain (time dependent) transition rates. These rates are determined as the rates of escape from the potential well of the tilted double well potential which corresponds to the reduced state in question. An approximation of the rate of escape from a parabolic potential well is given in the limit of small noise by the Kramers formula, cf. section 5.
In the physics literature, a standard ansatz for calculating the two state process given time dependent transition rates is to solve an associated differential equation for the probabilities of occupying state ±1 at time t, a so-called master equation (cf. Gammaitoni et al., 1998).
An advantage of the reduced model is its simplicity. It should be especially useful in systems with more than two meta-stable states. Although it is intuitively plausible to apply a two state filter, there is possibly a problem with the measure of resonance, for it might happen that with the same notion of tuning stochastic resonance would be detected in the two state model, while no optimal noise level, i. e. no point of stochastic resonance, exists in the continuous case. This is, indeed, a problem for the SPA coefficient and related measures, see Pavlyukevich (2002). The reason is that in passing to the reduced model small intrawell fluctuations are "filtered out", while they decisively contribute to the SPA coefficient in the original model.
Measures of resonance based on the distribution of intrawell residence times, however, do not have this limitation, that is they are robust under model reduction as Herrmann et al. (2003) show.
In equation (1) replace the term that represents the periodic input signal with a term that corresponds to a force dependent on the state of the solution path a fixed amount of time into the past, that is replace the periodic signal with a point delay. This yields what will be our reference model, see equation (2).
The idea to study such equations with regard to noise-induced resonance seems to originate with Ohira and Sato (1999). Their analysis, though, is of limited use, because they make too strong assumptions on independence between the components of the reduced model which they consider in discrete time only.
A better analysis of the reduced model for an important special choice of equation (2) can be found in Tsimring and Pikovsky (2001). The same model is the object of recent studies by Masoller (2003), Houlihan et al. (2004) and Curtin et al. (2004), and it will be our standard example, too.
While the measure of resonance applied by Tsimring and Pikovsky (2001) is essentially the first peak in the frequency spectrum, in the other articles focus is laid on the residence time distribution in the reduced model, which is compared with numerical simulations of the original dynamics. Under certain
3

simplifying assumptions, approximative analytical results are obtained via a master equation approach, where the master equation is a DDE instead of an ODE.
In section 5 we follow Tsimring and Pikovsky (2001) in establishing the link between the reduced and the reference model. Results by Masoller (2003) show that the density of the residence time distribution has a characteristic jump. She proposes to take the height of this jump as a measure of resonance, and we will follow her proposal, supplementing it by an alternative.
Our approach is different, though, in that we do not use any kind of master equation. Instead, we construct a reduced model with enlarged state space, which has the Markov property and which allows us to explicitly calculate the stationary distributions as well as the residence time distributions.

2 The reference model

Consider the one dimensional motion of a small particle in the presence of large friction and additive

white noise subject to the influence of two additional forces: one dependent on the current position of the

particle and corresponding to a symmetric double well potential V , the other dependent on the position of

the particle a certain amount of time r in the past and corresponding to a symmetric single well potential

U , where the position of the extremum of U coincides with the position of the saddle point of V .

Without loss of generality we may assume that the saddle point of the potential V is at the origin and

the extrema are located at (-1, -L) and (1, -L) respectively, where L > 0 is the height of the potential

barrier. A standard choice for V is the quartic potential x  L(x4 - 2x2).

Instead of U we will consider  · U , where  is a scalar, that serves to "adjust" explicitly the strength

of the delay force.

An admissible function for U

is the parabola x 

1 2

x2

.

In fact, with this choice of

U

and taking as potential V

the quartic potential with L =

1 4

we find ourselves

in

the

setting that was

studied by Tsimring and Pikovsky (2001).1 Another reasonable choice for U would be a function whose

first derivative equals the sign function outside a small symmetric interval around 0 and is smoothly

continued on this interval (see Fig. 1).

-2 a) -1 21 1 2............................................................................................................................................................................................. -1

.. b)
. ...
... ...

2

. ... ... ...

... ... ..
... ..

1

..
... ... ... ...

.....................................................................

-2 -1

12

-1

.. c) ...

2

.....

.. .....

..... ..

.....

... ..

1.....

... ..

......

... ..

...................................................

-2 -1

12

-1

Figure 1: Graphs on the interval [-2, 2] of a) quartic double well potential V , b) quadratic delay potential

U

:

x



1 2

x2

,

c)

absolute

value

delay

potential

U

:

x



|x|, x



R

\

(-, ),

smoothly

continued

on

(-, ).

2.1 The underlying SDDE
The dynamics that govern the motion of a Brownian particle as described above can be expressed by the following stochastic delay differential equation determining our reference model:
(2) dX(t) = - V X(t) +  · U X(t - r) dt +  · dW (t), t  0,
1Our notation is slightly different from that of equation (1) in Tsimring and Pikovsky (2001: p. 1). In particular, their parameter , indicating the "strength of the feedback", corresponds to -, here.

4

where W (.) is a standard one dimensional Wiener process on a probability space (, F, P) adapted to a filtration (Ft)t0 satisfying the usual conditions, r > 0 is the time delay, V , U are the first derivatives of V and U , respectively,   R is a parameter regulating the intensity of the delay force and   0 a noise parameter. In the special case  = 0 equation (2) becomes a DDE, while in case  = 0 we have an ordinary SDE.
As initial condition at time zero one prescribes an F0-measurable C([-r, 0])-valued random variable  such that E(  2 ) <  and X0 =  P-almost surely. Here, X0 denotes the segment of X at time zero. More generally,
Xt := s  X(t + s), s  [-r, 0]
is the segment at time t  0, and (Xt)t0 the segment process associated with X, provided a solution (X(t))t-r to equation (2) exists. The initial segment may, of course, be deterministic, i. e. X0 = f for some function f  C([-r, 0]).

The above description of the two potentials is compatible with the following conditions on V and U :

(3a) V, U  C2(R),

(3b) V (x) = V (-x) for all x  R,

U (x) = U (-x) for all x  R,

(3c) V (x) = 0 iff x  {-1, 0, 1},

U (x) = 0 iff x = 0,

(3d) V (-1) = V (1) > 0,

(3e) sup{V (x) | x  (-, -1)  (0, 1)}  0, sup{U (x) | x  (-, 0)}  0.

If V and U are bounded or satisfy a linear growth condition, then results from the literature ensure existence of a unique strong solution for every F0-measurable C([-r, 0])-valued square integrable random variable and a (weakly) unique weak solution for every probability measure on B(C([-r, 0])) as initial condition. The segment processes associated with those solutions enjoy the strong Markov property, see Mohammed (1984, 1996).
Let R > 0 and let VR, UR be functions such that VR and UR have bounded derivatives, while agreeing with V and U , respectively, on the interval [-R, R]. By considering equation (2) with V , U replaced by VR, UR we see that unique solutions exist up to an explosion time.
In order to prevent explosion of solutions to equation (2) we need growth conditions on V and U . We choose them in a way such as to give us control over the impact of the delay potential U in terms of the potential function V . In addition to conditions (3) let us assume that for some positive constants R^0, ^ and  we have

(4a) U (x) > 0 for all x  (0, ), (4b) U (x)  0 for all x  R,

V (x)  0 for all x  R \ [-R^0, R^0],

(4c)

V (x) U (3x)



^ · |x|1+

for all x  R \ [-R^0, R^0].

Without loss of generality we may assume   (0, 1]. Henceforth, whenever the reference model is concerned, we will suppose that conditions (3) and (4) are satisfied. These conditions not only guarantee existence and uniqueness of solutions, but also the existence of a stationary distribution as will be shown next.

Both, non-explosion as well as stationarity, can be checked by the "one step method". Let b : [0, r]×R  R be a continuous function, locally Lipschitz in the second variable (uniformly in the first) and such that for some positive constants R0,  we have

(5) x · b(t, x)  - · |x| for all t  [0, r], x  R \ [-R0, R0].

5

Growth condition (5) guarantees existence of a unique strong solution to the non-autonomous SDE

(6) dY (t) = b t, Y (t) dt +  · dW (t), t  [0, r],

for every F0-measurable real valued square integrable random variable as initial condition for Y (0), see Durrett (1996: pp. 190-192).2 Moreover, E(|Y (0)|2) <  implies E( Yr 2 ) < .
Let y  R and let Y y be a solution to equation (6) with deterministic initial condition Y (0) = y. For
R > 0 denote by R the time of first exit of Y y from the interval (-R, R), that is we set

R := inf t  [0, r] |Y y(t)|  R ,

with inf  =  by convention. A one dimensional version of proposition 1.4 in Herrmann et al. (2005) yields the following estimate for the probability that Y y leaves the interval (-R, R) within time r provided R is big enough. For any  > 1 it holds that

(7)

P R  r

6

r 2

2

+

2

exp

-

2( - 2

1)



R

for all R  (R0  |y|).

For f  C([-r, 0]) define the drift coefficient bf by

bf (t, x) := -V (x) -  · U f (t - r) , With (t, x)  [0, r] × R we have

(t, x)  [0, r] × R.

x · bf (t, x) = - x V (x) -  x U f (t - r)

=

- |x| U

3|x|

V (x) U (3x)

-

xU

f (t - r)



- |x| U

3|x|

V (x) U (3x)

+

|| |x| U

f .

apply (3b) apply (4b)

Set KV := sup{-x · V (x) | x  [-R^0, R^0]}. Because of (4c) we find that  KV + || R^0 U f 
x · bf (t, x)  -^ U 3|x| |x|2+ + || |x| U f 

if |x| < R^0, if |x|  R^0.

If

f

3

R^0



2|| ^



1

, then

 KV + || R^0 U

f

if |x| < R^0,

(8)

x · bf (t, x)



-13 |1^8|

f f

U

1+ 

U

f f ) |x|

if if

R^0



|x|

<

1 3

f

,

|x|



1 3

f

.

In particular, provided that

f





3

R^0 

2|| ^

1

,

we

have

(9)

x · bf (t, x)  -f |x|

for all

t



[0, r],

|x|



1 3

f

,

where

f

:=

^ 18

f

1+ 

U

(

f

).

Clearly, inequality (9) implies growth condition (5) if we replace b with bf

and R0 with

1 3

f

.

Notice

that (5) is also satisfied for bf when f  is small. In this case we would estimate the product x · bf (t, x)

2Alternatively, we could invoke theorem 10.2.2 in Stroock and Varadhan (1979) and the fact that pathwise uniqueness holds.

6

by applying (4c) and observing that U (x) is greater than some positive constant for all positive x big
enough.
Let (X(t)t[-r,^) be a strong solution to equation (2) for some admissible initial condition  up to explosion time ^. We have to show that ^ =  P-a. s.
Suppose ^ > n · r P-almost surely for some n  N0. This will certainly be the case if we take n = 0. Set s := n · r. By hypothesis, (X(t))t[-r,s] is well defined as a solution to equation (2) up to time s. Let   , and set f := Xs(). Observe that Xs(~) = f for almost all ~   with respect to the probability measure P(.|Fs)().
In equation (6) replace the drift b with bf and prescribe f (0) as initial condition. Notice that bf fulfills a growth condition like (5). The fact that there is a unique strong solution to equation (6) now
implies that, P(.|Fs)()-almost surely, (X(s + t))t[0,r] exists and satisfies (6). Consequently, ^ > s + r with probability one under P(.|Fs)().
Since P(A) = E(P(A|Fs)) for any event A  F , we see that ^ > n · r entails ^ > (n + 1)r. Proceeding by induction, we conclude that solutions to our reference equation (2) cannot explode in finite time.

Stationarity is studied in Scheutzow (1983, 1984) for equations of a special form. Let F be a real valued Borel-measurable and locally bounded functional on C([-1, 0]), where locally bounded means bounded on bounded subsets. Consider the SDDE

(10) dX~ (t) = F (X~t)dt + dW (t), t  0.

By appropriately scaling time and space one can bring SDDEs with delay length r = 1 or noise parameter

 = 1, as long as both are positive, into the form of equation (10). Let us specify F as

(11)

F (g)

:=

-

r 

·

V

r · g(0) +  · U

r · g(-1)

,

g  C([-1, 0]).

Since the coordinate projections are measurable and V , U are locally bounded continuous functions

because of (3a), F is Borel-measurable and locally bounded.

is

Let g a weak

 C([-1, 0]) and assume that X~ together with a solution to (10) with F as just defined and X~0 =

gWP~ie-anlemr opsrtoscuersesly(.W~S,e(tF~ft)(tt)0:)=on(r~ ,·

F~, g(

P~ )

t r

),

t  [-r, 0]. that P(X0

Let =f

X together with ) = 1. Then the

a Wiener process (W, processes (X(t))t-r

(aFntd)t(0)orn·

(, X~ (

F, P) be

t r

))t-r

a weak solution of (2) such have the same distribution

(cf. Scheutzow, 1983: p. 31). Therefore, if  > 0, then our reference equation can be transformed into an

instance of equation (10).

Theorem 1 cites part of theorem 3 and theorem 4 from Scheutzow (1984: pp. 47-48, 54). Strong existence and pathwise uniqueness clearly imply weak existence and uniqueness in distribution. Notice that in order to have a weak solution for any initial distribution it is sufficient to check existence of solutions for all deterministic initial conditions.

Theorem 1 (Scheutzow). Let F be a Borel measurable and locally bounded functional on C([-1, 0]). Assume that weak existence and uniqueness hold for equation (10). Let L : C([-1, 0])  [0, ) be Borel measurable, and set for R  0

AR := - sup E~g L(X~1) - L(g) g  C([-1, 0]), g   R .

Here, E~g means expectation with respect to the probability measure of a weak solution for (10) with deterministic initial condition g.

1. Let ((X~ , W~ ), (~ , F~, P~), (F~t)) be a weak solution to (10) with arbitrary initial distribution . There

is at most one invariant probability measure  for equation (10). If an invariant probability measure

 exists, then

P~X~t t  in total variation.

7

2.

If

A0  0 < 

and

0  limR

A0 0 AR

< 1,

then

(10)

possesses

an

invariant

probability

measure.

As Lyapunov functional L we choose

L(g) := g  + |g(0)|, g  C([-1, 0]).

Notice that beside conditions (4) only properties (3a) and (3b) are needed in order to derive growth

estimates (8) and (9). For g  C([-1, 0]) define the rescaled drift condition ~bg by

~bg(t, x) :=

r 

·

bfg

r · t, r · x ,

(t, x)  [0, 1] × R,

where

fg(t) :=

r · g

t r

,

t  [-r, 0].

In particular,

fg  = r ·

g

.

Set R~0 :=

3 r

R^0



2|| ^



1

.

If

g   R~0 then inequality (9)

entails that

(12)

x · ~bg(t, x)  -~g |x|

for all

t  [0, 1],

|x|



1 3

g

,

where

~g :=

r 

fg

=

^ 18

· 

·

r1+

 2

·

g

1+ 

·

U

  r·

g.

In analogy to equation (6), we consider non-autonomous SDEs of the form

(13) dY~ (t) = ~b t, Y~ (t) dt + dW (t), t  [0, 1].

The drift coefficient ~b has to be chosen in accordance with the deterministic initial condition of equation (10). For g  C([-1, 0]) denote by Y~ g the (strongly) unique solution to equation (13) with ~bg in place of ~b and initial condition g(0). Let ~R denote the time of first exit of Y~ g from the interval (-R, R), where
R > 0, that is we set ~R := inf t  [0, 1] |Y~ g(t)|  R ,
with inf  =  by convention. As a consequence of (7), provided that g   R~0, with  > 1 we have

(14)

P ~R  1

 6 ~g2 + 2 exp

-

2( - 

1)

~g

R

for all

R





1 3

g

  |g(0)|

.

First, we turn to estimating the expected supremum of |Y~ g|.

Set yg :=

1 3

g

  |g(0)|.

For

g   R~0

and any  > 1 it holds that

E Y~1g 


= P sup |Y~ g(t)| > R dR   yg + P (~R  1) dR
t[0,1] 0 yg



  yg + 6 ~g2 + 2 ·

exp

-

2( - 

1)

~g

R

dR

yg

apply (14)

=

 yg

+

3(~g2 + 2) ( - 1)~g

·

exp

-

2( - 

1)

~g

yg





1 3

g

  |g(0)|

+

3 -1

~g

+

2 ~g

· exp

-

2( - 3

1)

~g

g

.

Therefore, given a > 1 we find R(a) > 0 such that for all g  C([-1, 0]) with g   R(a) it holds that



(15)

E Y~1g 



a
3

g

if

|g(0)|



1 3

g

,

a|g(0)|

if

|g(0)|

>

1 3

g

.

8

Second, we try to find an upper bound for the expected value of |Y~ g|.

Clearly, if |g(0)| 

1 3

g

, then

inequality

(15)

implies

E(|Y

g (1)|)



a 3

g

.

Assume that |g(0)| >

1 3

g

.

Girsanov's theorem implies

that the stochastic equation

t
(16) Z(t) = x -  · sgn Z(s) ds + W~ (t), t  [0, 1],

0

where W~ is a standard Wiener process, possesses a (weakly) unique weak solution for any choice of the

parameters x  R,  > 0.

Set

x

:=

|g(0)|

-

1 3

g

,  := ~g, and let Zg

be a solution process satisfying (16) with x,  on an

appropriate stochastic basis. By symmetry and because of growth inequality (12), we find that

E |Y~ g(t)|



1 3

g



+

E~

|Z g (t)|

for all t  [0, 1].

Notice that Zg

is

a

Brownian

motion

with

two-valued

drift

started

at

x

=

|g(0)|

-

1 3

g

, and |Zg| is a

reflected Brownian motion with drift -~g. The transition probabilities of Zg can be computed explicitly,

see Karatzas and Shreve (1991: pp. 437-441). It holds that

E~ Zg(1) 



=

z 2

exp

-

1 2

(x

-

z

-

)2

+ exp

2x

-

1 2

(x

+

z

+

)2

0



+ 2z exp -2z 1 2

exp

-

1 2

(v

-

)2

dv dz.

0 x+z

dz

Observe that

1 3

g

x

g , that is x is of order

g , while  = ~g is of order at least

g

1+ 

.

Therefore, E~( Zg(1) ) goes to zero as g  tends to infinity. We conlude that, given a > 1, there is

R~(a) > 0 such that for all g  C([-1, 0]) with g   R~(a) it holds that

(17)

E |Y~ g(1)|



a 3

g .

Observe that A0 as defined in theorem 1 is finite. Estimates (15) and (17) imply that AR tends to infinity as R goes to infinity. Theorem 1 thus guarantees existence of an invariant probability measure. Let us summarize our findings.

Proposition 1. Suppose that V , U satisfy conditions (3) and (4). Let   0,   R be given. Then the following holds for equation (2):

1. For every F0-measurable C([-r, 0])-valued random variable as initial condition there is a (pathwise) unique strong solution.

2. The segment solution processes enjoy the strong Markov property.

3. If  > 0, then there is a unique invariant probability measure  for the segment process, which converges in total variation to  for every initial distribution.

The additional conditions (4) are rather crude and could be varied in many ways. For example, one might

relax condition (4c) by requiring a ratio between V and U of order one instead of order 1+ provided

the growth of U is of polynomial order .

A different restriction on the geometry of V and U would be the following: Assume that a constant

Rpot greater than the positive root of V exists such that V and U are linear on R \ [-Rpot, Rpot]. Clearly, this condition would not allow V to be a parabolic or quartic potential. In this setting, however, the

delay parameter  becomes important. By appealing to theorem 5 in Scheutzow (1984: pp. 55-56) we find

that

a

stationary

distribution

exists

if



>

0

and



>

-

V U

,(Rpot )
(Rpot )

while

no

invariant

measure

exists

in

case



<

-

V U

.(Rpot )
(Rpot )

9

2.2 Basic parameter settings
Let us have a look at basic parameter settings for equation (2). The simplest and least interesting choice of parameters is  = 0 and  = 0, i. e. no noise and no delay. In this case, (2) reduces to a one dimensional ordinary differential equation with two stable solutions, namely -1 and 1, and an instable trivial solution.
The dynamics of the general deterministic delay equation, i. e.  = 0,  = 0, is not obvious for all combinations   R, r > 0. In Redmonda et al. (2002) stabilization of the trivial solution and the corresponding bifurcation points are studied. The parameter region such that the zero solution is stable is contained in   1, r  [0, 1].3 This is not the parameter region we are interested in, here. Recall from section 1 that stochastic resonance is a phenomenon concerned with an increase of order in the presence of weak non-zero noise. For large || the delay force would be predominant. Similarly, with r small the noise would not have enough time to influence the dynamics.
Indeed, we must be careful in our choice of  lest we end up with a randomly perturbed deterministic oscillator. Solutions to equation (2) exhibit periodic behaviour even for  > 0 comparatively small.
If  = 0 and  > 0, then our SDDE (2) reduces to an ordinary SDE. Of interest is again the case of small noise. A Brownian particle moving along a solution trajectory spends most of its time fluctuating near the position of the minimum of one or the other potential well, while interwell transitions only occasionally occur.
Now, let  > 0 and || be small enough so that the corresponding deterministic system does not exhibit oscillations. Let us suppose first that  is positive. Then the effect of the delay force should be that of favouring interwell transitions whenever the Brownian particle is currently in the same potential well it was in r units of time in the past, while transitions should become less likely whenever the particle is currently in the well opposite to the one it was in before. Notice that the influence of the delay force alone is insufficient to trigger interwell transitions. In fact, with  > 0 not too big, transitions are rare and a typical solution trajectory will still be found near the position of one or the other minimum of V with high probability.
Consider what happens if the noise intensity increases. Of course, interwell transitions become more frequent, while at the same time the intrawell fluctuations increase in strength. But there is an additional effect: As we let the noise grow stronger interwell transitions occur at time intervals of approximately the same length, namely at intervals between r and 2r, with high probability. The solution trajectories exhibit quasi-periodic switching behaviour at a non-zero noise level. This is what we may call an instance of stochastic resonance.
Further increasing the noise intensity leads to ever growing intrawell fluctuations which eventually destroy the quasi-periodicity of the interwell transitions. When the noise is too strong, the potential barrier of V has no substantial impact anymore and random fluctuations easily crossing the barrier are predominant.
Suppose  is negative. The effect of the delay force, now, is that of pushing the Brownian particle out of the potential well it is currently in whenever the particle's current position is on the side of the potential barrier opposite to the one remembered in the past. Sojourns of duration longer than r, on the other hand, become prolonged due to the influence of the delay which in this case renders transitions less likely.
In order to obtain some kind of regular transition behaviour a higher noise level as compared to the case of positive  is necessary. Of course, one could change time scales by increasing the delay time r, thereby allowing for lower noise intensities. In section 5 we will state more precisely what regular
3Equation (1.3) in Redmonda et al. (2002) is our standard example with V the quartic potential, where - corresponds to our parameter .
10

transition behaviour means in case  < 0, yet we will not subsume it under the heading of stochastic resonance.

3 The two state model in discrete time
Applying the ideas sketched in section 1, we develop a reduced model with the aim of capturing the effective dynamics of the reference model. To simplify things further we start with discrete time. As the segment process associated with the unique solution to (2), the reference model equation, enjoys the strong Markov property, it is reasonable to approximate the transition behaviour of that solution by a sequence of Markov chains. One unit of time in the discrete case corresponds to r/M time units in the original model, where the delay interval [-r, 0] is divided into M  N equally spaced subintervals.
After defining the approximating Markov chains we obtain an explicit formula for their stationary distributions which will be useful in calculating, for each M  N, the residence time distribution in the stationary regime and deriving its density function in the limit of discrete time tending to continuous time. Finally, based on the residence time distributions, we introduce two simple measures of resonance.
The results on Markov chains we need are elementary and can be found, for example, in Brémaud (1999), which will be our standard reference.

3.1 A sequence of Markov chains and stationary distributions

Let M  N be the discretization degree, that is the number of subintervals of [-r, 0]. The current state of the process we have to construct can attain only two values, say -1 and 1, corresponding to the positions of the two minima of the double-well potential V . Now, there are M + 1 lattice points in [-r, 0] that delimit the M equally spaced subintervals, giving rise to 2M+1 possible states in the enlarged state space.
Let SM := {-1, 1}M+1 denote the state space of the Markov chain with time unit r/M . Elements of SM will be written as (M +1)-tuples having {-1, 1}-valued entries indexed (from left to right) from -M to 0. This choice of the index range serves as a mnemonic device to recall how we have discretized the delay interval [-r, 0]. Thus, l  {-M, . . . , 0} corresponds to the point l · r/M in continuous time.
To embed the discrete into the time continuous model, let ,  be positive real numbers. If X(.) is the unique solution to (2) in the case of "interesting" noise parameter  and delay parameter , one may think of  as the escape rate of X(.) from one of the two potential wells under the condition X(t)  X(t - r) and of  as the escape rate of X(.) under the condition X(t)  -X(t - r). All of the parameters of the reference model, including the delay length and the geometry of the potentials U and V , will enter the discrete model through the transition rates  and , cf. section 5.
In the discrete model of degree M , instead of two different transition rates we have two different transition probabilities M and M with M = Rsc(, M ), M = Rsc(, M ), where Rsc is an appropriate scaling function. In analogy to the time discretization of a Markov process we set

(18)

Rsc : {, } × N

(, N )



 +

·

(1

-

e-

+ N

)



(0, 1).

Let Z = (Z(-M), . . . , Z(0)), Z~ = (Z~(-M), . . . , Z~(0)) be elements of SM . A transition from Z to Z~ shall have positive probability only if the following shift condition holds:
(19)  l  {-M, . . . , -1} : Z~(l) = Z(l+1).

Example. Take the element (-1, 1, -1)  S2. According to the shift condition, starting from (-1, 1, -1) there are at most two transitions with positive probability, namely to the elements (1, -1, 1) and

(1, -1, -1).



11

If (19) holds for Z and Z~ then there are two cases to distinguish which correspond to the conditions

X(t)  X(t - r) and X(t)  -X(t - r), respectively. to state Z~. Under condition (19) we must have

Denote

by

pMZZ~

the

probability

to

get

from

state

Z

(20)



Z(0) = Z(-M)

then

pZMZ~

=

M 1 -

M

if if

Z~(0) = Z(0), Z~(0) = Z(0),



Z(0) = Z(-M)

then

pZMZ~

=

M 1 -

M

if if

Z~(0) = Z(0), Z~(0) = Z(0).

The fact that ­ because of (18) ­ we always have M , M  (0, 1), implies

(21) pZMZ~ = 0 iff shift condition (19) is satisfied.

Set PM := (pMZZ~)Z,Z~SM . Clearly, PM is a 2M+1 × 2M+1 transition matrix. For every M  N choose an SM -valued discrete process (XnM )nN0 on some measurable space (M , FM ) and probability measures PMZ , Z  SM , on FM such that under PZM the process XM is a homogeneous Markov chain with transition matrix PM and initial condition PMZ (X0M = Z) = 1.
If  is a probability measure on the power set (SM ) then, as usual, let PM denote the probability measure on FM such that XM is a Markov chain with transition matrix PM and initial distribution  with respect to PM . Write P instead of PM , when there is no ambiguity about the measure PM .
From relation (21), characterizing the non-zero entries of PM , it follows that PM and the associated Markov chains are irreducible. They are also aperiodic, because the time of residence in state
(-1, . . . , -1), for example, has positive probability for any finite number of steps. Since the state space
SM is finite, irreducibility implies positive recurrence, and these two properties together are equivalent to the existence of a uniquely determined stationary distribution on the state space, cf. Brémaud
(1999: pp. 104-105). Therefore, for every M  N, we have a uniquely determined probability measure M on (SM ) such that4

(22)

M (Z~) =

M (Z) pMZZ~

for all Z~  SM .

Z SM

There is a simple characterization of the stationary distribution M in terms of the number of "jumps" of the elements of SM .5 Let Z = (Z(-M), . . . , Z(0)) be an element of SM , and define the number of jumps
of Z as J (Z) := # j  {-M +1, . . . , 0} Z(j) = Z(j-1) .

The global balance equations (22) then lead to

Proposition

2

(Number of jumps formula).

Let M

 N.

Set ~M

:=

M (1-M

)

,

~M

:=

M (1-M

)

,

~M

:=

~M · ~M . Then for all Z  SM the following formula holds

(23)

M (Z) =

1 cM

J (Z)+1
~M 2

J (Z)
~M 2

=

1 cM

~MJ (Z) mod 2

J (Z)
~M 2

,

M
where cM := 2 ·

M j

~Mj mod

2

j
~M2

.

j=0

4For the probability of a singleton {Z} under a discrete measure  we just write (Z). 5At the moment, "number of changes of sign" would be a label more precise for J (Z), but cf. section 4.

12

Proof. The right-hand part of equation (23) is just a rearrangement of the middle part. For Z  SM

define

J (Z)+1

J (Z)

M (Z) := ~M 2 ~M 2 .

We then have cM = ZSM M (Z), because J (Z)  {0, . . . , M } for every Z  SM , and with j 

{0, . . . , M } there are exactly 2 ·

M j

elements in SM having j jumps.

Let Z = (Z(-M), . . . , Z(0)) be an element of SM . Define elements Z~, Z^ of SM as

Z~ := Z(-1), Z(-M), . . . , Z(-1) ,

Z^ := -Z(-1), Z(-M), . . . , Z(-1) .

Because of (20) and (21) the global balance equations (22) reduce to



(24)

M (Z)

=

(1 - M ) · M (Z~) + (1 - M ) · M (Z^) if Z(-1) = Z(0),

M · M (Z~) + M · M (Z^)

if Z(-1) = -Z(0).

Equations (24) determine M up to a multiplicative constant. Of course, ZSM M (Z) = 1, and we

have already satisfy (24).

LseetenZt,hZ~a,tZ^c1Mbe

ZSM M (Z) = 1. It is therefore elements of SM as above. Then

sufficient

to

show

that

M (Z),

Z



SM ,



J (Z)

if Z(-M) = Z(0),

J (Z~)

=

JJ

(Z (Z

) )

+ -

1 1

if if

Z(-M) = -Z(-1) = -Z(0), Z(-M) = Z(-1) = -Z(0),



J (Z)

if Z(-M) = -Z(0),

J (Z^)

=

JJ

(Z (Z

) )

+ -

1 1

if if

Z(-M) = Z(-1) = Z(0), Z(-M) = Z(0) = -Z(-1),

and M (Z~), M (Z^) can now be calculated. This yields the assertion.

Let cM be the normalizing constant from proposition 2. By splitting up the sum in the binomial formula we see that

(25)

cM =

1+

~M ~M

1+

~M M + 1 -

~M ~M

1-

~M M .

3.2 Residence time distributions

Let Y M be the {-1, 1}-valued sequence of current states of XM , that is 6



YnM

:=

(XnM )(0) (X0M )(n)

if if

n  N, n  {-M, . . . , 0}.

Denote by LM (k) the probability to remain exactly k units of time in the same state conditional on the occurrence of a jump, that is

(26)

LM (k) = PM (YnM = 1, . . . , YnM+k-1 = 1, YnM+k = -1 | YnM-1 = -1, YnM = 1), k  N,

where n  N is arbitrary. The above conditional probability is well defined, because

PM Y-M1 = -1, Y0M = 1 = M {(, . . . , , -1, 1)} > 0.
6Recall the tuple notation for elements of SM .

13

Here, {(, . . . , , -1, 1)} denotes the set {Z  SM | Z(-1) = -1, Z(0) = 1}. By symmetry the roles of -1 and 1 in (26) are interchangeable. Under PM not only XM is a stationary process, but ­ as a coordinate projection ­ Y M is stationary, too, although it does not, in general, enjoy the Markov property. We note
that LM (k), k  N, gives the residence time distribution of the sequence of current states of XM in the
stationary regime.

Observe that LM (.) has a "geometric tail". To make this statement precise set

(27) KM := PM Y0M = -1, Y1M = 1, . . . , YMM = 1 Y0M = -1, Y1M = 1 .
In view of the "extended Markov property" of Y M , that is the Markov property of the segment chain XM , we have

(28)

LM (k) = (1 - M ) · KM · M · (1 - M )k-M-1,

k  M + 1,

where (1 - M ) · KM is the probability mass of the geometric tail. Stationarity of PM implies

KM

=

M (-1, 1, . . . , 1) . M {(, . . . , , -1, 1)}

From proposition 2 we see that

M (-1, 1, . . . , 1)

=

~M cM

,

and arranging the elements of {(, . . . , , -1, 1)} according to their number of jumps we obtain

M {(, . . . , , -1, 1)} We therefore have (29) KM =

=

~M 2cM

·

1+

~M ~M

1+

~M ~M

1+

~M M-1 + 1 -

~M ~M

1-

2

 1 + ~M

M-1 +

1-

~M ~M

 1 - ~M

.
M -1

~M M-1 .

In a similar fashion we can calculate LM (k) for k  {1, . . . , M }. We obtain

(30a)

LM (M )

=

PM Y0M = -1, Y1M = 1, . . . , YMM = 1, YMM+1 = -1 M {(, . . . , , -1, 1)}

= M · KM ,

and for k  {1, . . . , M - 1}

(30b)



LM (k) =

~M 2

· KM

·

+

~M (1 + ~M (1 +

~M )M-1-k + (1 - ~M )M-1-k - (1 -

~M )M-1-k ~M )M-1-k

.

More interesting than the residence time distribution in the case of discrete time is to know the behaviour of this distribution in the limit of discretization degree M tending to infinity.
Recall the definition of scaling function Rsc according to equation (18) for some numbers ,  > 0. If M = Rsc(, M ) and M = Rsc(, M ) for all M  N, then ­ with the usual notation O(.) for the order of convergence ­ we have

(31)

M

=

 M

+

O(

1 M

2

),

M

=

 M

+

O(

1 M

2

).

Indeed, if condition (31) holds between the transition probabilities M , M , M  N, and some positive transition rates , , then we can calculate the normalizing constant cM , the "tail constant" KM and the density function of the residence time distribution in the limit M  .

14

Proposition 3. Let M , M  (0, 1), M  N. Suppose that the sequences (M )MN, (M )MN satisfy relation (31) for some positive real numbers , . Then cM and KM converge to c and K, respectively, as M  , where

(32) (33)

c

:=

lim
M 

cM

=

1+

K

:=

lim
M 

KM

=

1+

 

e +

1-

2

 

e +

1-

 

e-

=

2·



1 k!

k

mod

2

()

k 2

,

k=0

 

e-

=



 cosh() + 

sinh(

)

.

Define

a

function

fL

:

(0, )



R

by

q



fL(q)

:=

lim M
M 

·

LM

qM

. Then

 · K ·  cosh (1 - q) +  sinh (1 - q)

if

(34) fL(q) = K ·  · exp -(q - 1)

if

q  (0, 1], q > 1.

Proof. If relation (31) holds, then in order to derive (32) and (33) from (25) and (29), respectively, it is

sufficient

to

observe

that

(1 +

a N

+

O(

1 N2

))N

N

ea

for

every

a



R.

The

last

part

of

(32)

is

obtained

by series expansion. Similarly, expression (34) for fL follows from equations (30a), (30b) and (28).

Observe that fL as defined in proposition 3 is indeed the density of a probability measure on (0, ). In case  =  this probability measure is just an exponential distribution with parameter  (= ). If  =  then fL has a discontinuity at position 1, where the height of the jump is

(35) fL(1+) - fL(1-) = K · ( - ).

Clearly, the restrictions of fL to (0, 1] and (1, ), respectively, are still strictly decreasing functions,

and fL(q), q  (1, ), is again the density of an exponential distribution, this time with parameter 

("h=ype)rabnodlict"otdaisltprirboubtaiboinlistywimthastshKe ge.omTehterifcunmcetaionnfL(q)o,f mass 1 - K. The ratio between the hyperbolic cosine and

q  (0, 1), is the density of  and  as parameter and the hyperbolic sine density

a mixture of two tios talptroobabi.lity

Recall how at the beginning of this section we interpreted the discretization degree M as the number of subintervals of [-r, 0], where r > 0 is the length of the delay that appears in equation (2). Let us assume that the numbers ,  are functions of the parameters of our reference model, in particular of the noise parameter  and the length of the delay r. Then we should interpret the density fL as being defined on normalized time, that is one unit of time corresponds to r units of time in the reference model. The density of the residence time distribution for the two state model in continuous time should therefore read

(36)

f~L(t)

:=

1 r

fL

t r

,

t  (0, ).

Before we may call f~L the density of a residence time distribution, we have to justify the passage to the limit M   at the level of distributions of the Markov chains XM , which underlie the definition of
LM . We return to this issue in section 4.

3.3 Two measures of resonance
Drawing on the residence time distribution of the Markov chain XM we introduce simple characteristics that provide us with a notion of quality of tuning for the reduced model in discrete time.

15

We consider XM and the resonance characteristics to be defined in the stationary regime only, because by doing so we can guarantee that an eventual resonance behaviour of the trajectories of XM is independent of transitory behaviour. We know that PM is a positive recurrent, irreducible and aperiodic transition matrix and, therefore, the distribution of XnM converges to M in total variation as n   for every initial distribution of X0 (Brémaud, 1999: p. 130). In section 2 we saw an analogous result for the segment process of a solution to equation (2).
Assume that the transition probabilities M , M are related to some transition rates ,  by means of a smooth scaling function like (18), for example, such that condition (31) is satisfied. Under this assumption we let the discretization degree M tend to infinity. Assume further that ,  are functions of the parameters of the reference model, in particular, that  = (),  = () are C2-functions of the noise parameter   (0, ). The resonance characteristics can then be understood as functions of .
Recall that the residence time distribution LM has a geometric tail in the sense that LM (k), k  M +1, renormalized by the factor (1 - M ) · KM is equivalent to a geometric distribution on N \ {1, . . . , M } with KM as defined by (27). The distribution which LM induces on {1, . . . , M } is given ­ up to a renormalizing factor ­ by equations (30b) and (30a). A natural characteristic seems to be the jump in the density of the residence time distribution fL obtained above. In discrete time, i. e. with discretization degree M  N, we set

(37) M := M · LM (M +1) - LM (M ) . Because of (28), (30a) and (35) we have

(38)

M = M · KM · (1 - M ) · M - M ,



:=

lim
M 

M

=

K · ( - ).

To consider the height of the discontinuity of fL as a measure of resonance has already been proposed by Masoller (2003). Following her proposal we define what stochastic resonance means according to the jump characteristic.

Definition 1. Let M  N  {}, and suppose that the following conditions hold:

(i) M as a function of the noise parameter  is twice continuously differentiable,

(ii)

lim
0

M

()

=

0,

(iii) M has a smallest root opt  (0, ).

If M has a global maximum at opt, then let us say that the Markov chain XM or, in case M = , the reduced model defined by the family (XN )NN exhibits stochastic resonance and call opt the resonance point. If M has a global minimum at opt, then let us say that the Markov chain XM (or, in case
M = , the reduced model) exhibits pseudo-resonance and call opt the pseudo-resonance point.

Alternatively, we may take the probability of transitions in a certain time window as characteristic of the resonance effect. For M  N and q  (0, 1] define

(39)

M
^M := LM (k),
k=1

(q+1)M

M(q) :=

LM (k).

k=M +1

By summation over k we see from (28) that

^M = 1 - (1 - M ) · KM ,

(Mq) = (1 - M ) · KM · 1 - (1 - M ) qM ,

16

and letting M tend to infinity we get

(40)

^

:=

lim
M 

^M

=

1 - K,

(q)

:=

lim
M 

(Mq)

=

K · (1 - e-q·).

Recall that M steps in time of the chain XM or the {-1, 1}-valued process Y M correspond to an amount of time r in the reference model. Thus, ^M corresponds to the probability of remaining at most time r in one and the same state, while (Mq) approximates the probability of state transitions occuring in a time window corresponding to (r, (q+1)r] of length q · r given a transition at time zero.
In (39) we could have allowed for a "window width" q > 1. The interesting case, however, is a small time window, because then (Mq) measures the probability of transitions within the second delay interval. For q = 1 the two components of our resonance measure correspond to time windows of equal length, that is ^M gives the probability of transitions within the first delay interval, while (M1) is the probability of hopping events occurring in the second delay interval. Since LM is geometrically distributed on N \ {1, . . . , M }, (M1) majorizes the transition probability for all time windows of the same length starting after the end of the first delay interval. Let us write M for M(1).
The idea of the following definition is to maximize quasi-periodicity by finding a noise level such that sojourns in the same state become neither too long nor too short. Here, short sojourns are those that last less than the length of one delay interval, long sojourns those that last longer than the length of two delay intervals. Observe that if the current state of XM remains the same for more than M steps in discrete time, then the influence of the delay will be constant until a transition occurs.
Definition 2. Let M  N  {}, and suppose that the following conditions hold:
(i) M as a function of the noise parameter  is twice continuously differentiable with values in the unit interval,
(ii) lim0(^M + M )() = 0,
(iii) M has a unique global maximum at opt  (0, ).
If M (opt) > ^M , then let us say that the Markov chain XM or, in case M = , the reduced model defined by the family (XN )NN exhibits stochastic resonance of strength M (opt), and call opt the resonance point, else let us speak of pseudo-resonance and call opt the pseudo-resonance point.

In the above definition we might have taken a shorter time window than the second delay interval. A natural choice would have been the probability of transitions occuring in a time window corresponding to (r, (1 + q)r] normalized by the window width. In the limit M   we obtain

(41)

lim
q0

1 q

·

(q)

=

K · 

=

fL(1+).

Here, fL is the density of the residence time distribution from proposition 3 and fL(1+) is the right-hand limit appearing in equation (35), which gives the height of the discontinuity of fL.

Of course, definition 2 could be modified in other ways, most importantly by allowing the time window that corresponds to M to float. This would be necessary for a distributed delay. Suppose that in the reference model instead of the point delay we had a delay supported on [-r, -] for some  > 0. Then a reasonable starting point for a measure of resonance could be a time window of length r with its left boundary floating from  to r. Notice that a distributed delay (in the reference or in the reduced model) can be chosen in such a way as to render continuous the density fL.

17

4 The two state model in continuous time

Our aim in this chapter is to justify the passage from time discretization degree M to the limit M  

as undertaken in sections 3.2 and 3.3. To this end we will look for a process in continuous time that is the

limit in distribution of the Markov chains XM , M  N, in the stationary regime. We can then consider

the distribution of residence times for this new process and show that it coincides with the limit of the

residence time distributions in discrete time which was calculated in section 3.2. Since the measures of

resonance introduced in section 3.3 were defined over the (discrete) residence time distributions, we may

conclude that in this case, too, the passage to the limit M   is admissible.

For M  N the Markov chain XM takes its values in the finite space SM with cardinality 2M+1. The first thing to be done, therefore, is to choose a common state space for the Markov chains. This

will be D0 := D{-1,1}([-r, 0]), the space of all {-1, 1}-valued cadlag functions, i. e. right-continuous functions with left limits, on the interval [-r, 0], endowed with the Skorokhod topology. This simplest of

all Skorokhod spaces is introduced in detail in appendix A.3, while in appendix A.5 we present D := D{-1,1}([-r, )), the space of all {-1, 1}-valued cadlag functions on the infinite interval [-r, ).

Recall how in section 3.1 we partitioned the delay interval [-r, 0]. Time step n  {-M, -M +1, . . .}

with respect

to the

chain

XM

was said to

correspond

to

point

n

·

r M

in continuous

time.

Keeping in

mind

this correspondence, in section 4.1 we embed the spaces SM , M  N, into D0, which allows us to look

upon the stationary distributions M as being probability measures on B(D0) and to view the random

sequences XM as being D0-valued Markov chains.

Now, because of shift condition (19) from section 3.1 one may regard XM as being a process with

trajectories in D. If the discretization of time is taken into account, then the chain XM induces a probability measure on B(D) for every initial distribution over SM  D0.

Weak convergence of the stationary distributions or, equivalently, convergence of the M with respect to the Prohorov metric induced by the Skorokhod topology on D0 will be established in section 4.2. Weak convergence of the distributions on B(D) is the object of section 4.3.

Finally, in section 4.4, we return to the question of identity between the residence time distribution

for the limit process and the one we obtained above as the limit of discrete distributions.

4.1 Embedding of the discrete-time chains
First we interpret the finite enlarged state space SM as a subset of D0. After that, we change philosophy and regard a chain XM as being equivalent to a {-1, 1}-valued cadlag process.
The embedding of SM , the state space of the Markov chain XM , into D0 is in a sense the reverse of what one does when approximating solutions to stochastic delay differential equations by Markov chains in discrete time.7 Approximation results of this kind were obtained for the multi-dimensional version of equation (10) by Scheutzow (1983, 1984). The method is more powerful, though, as Lorenz (2003) shows, where weak convergence of the approximating processes to solutions of multi-dimensional SDDEs is related to a martingale problem that can be associated with the coefficients of the target equation.
Of course, D0 is a toy space compared to C([-r, 0], Rd). Notice, however, that linear interpolation as in the case of C([-r, 0], Rd) is excluded, because the only continuous functions in D0 are the two constant functions -1 and 1.
Let M  N, Z  SM , and associate with Z = (Z(-M), . . . , Z(0)) a function fZ : [-r, 0]  {-1, 1} defined
7Under suitable conditions the approximating time series converge in distribution to the (weakly unique) solution of the SDDE.

18

by

-1

fZ (t) := Z(0) · 1{0}(t) +

Z (i)

·

1[i

r M

,(i+1)

r M

)(t),

t  [-r, 0].

i=-M

Clearly, fZ  D0. Hence, ~M : Z  fZ defines a natural injection SM  D0, which induces the following embedding of probability measures on (SM ) into the set of probability measures on B(D0).

M+1 (SM ) µ  µ~ :=

µ(Z) · fZ  M1+(D0),

Z SM

where f is the Dirac or point measure concentrated on f  D0. Denote by ~M the probability measure on B(D0) associated with the stationary distribution M for
the chain XM , and write X~ M for the corresponding D0-valued Markov chain. Since all we have done so far is a reinterpretation of the state space the results obtained in chapter 3 regarding XM are also valid for X~ M .
Although the embedding ~M given above is natural in view of how the delay interval [-r, 0] should be partitioned according to section 3.1, it is not the only one possible. Indeed, one could select different
interpolation points in the definition of fZ. As the degree of discretization M increases the complete Skorokhod distance between the different functions fZ, Z  SM being fixed, tends to zero, and the convergence results stated in 4.2 and 4.3 still hold true.

Following the notation of appendix A.3, for Z  SM we write

J(Z) := J(fZ ),

J(Z) := J(fZ ),

Z := fZ ,

thereby denoting the sets of discontinuities or jumps of Z, and the minimal distance between two discontinuities. Notice that our new definition of J(Z) agrees with the number of jumps J (Z) defined in section 3.1 in the sense that #J(Z) = J (Z).

Recall the notation of section 3.1. Let  be a distribution on (SM ) and denote by PM the probability

measure on FM such that XM is a Markov chain with transition matrix PM and initial distribution

X0M d . For a "point distribution" on Z  SM write PZM .

For f

 D0

let Z(f ) be the element of SM

such

that

Z (i)

=

f

(

r M

· i)

for

all

i



{-M, . . . , 0}.

Let

(YnM )n{-M,-M+1,...} be the sequence of current states of XM as defined at the beginning of section 3.2.

Write

Y~ M (t) :=

Y

M

t r

M

,

t  -r.

For A  B(D) set

P~ fM (A) := PMZ(f) Y~ M  A ,

P~M (A) := PMM Y~ M  A ,

thereby defining probability measures on B(D). Note that P~Mf , P~M are well defined and correspond to the distribution of XM with X0M = Z(f ) PM -almost surely and X0M d M , respectively.

4.2 Convergence of the stationary distributions on D0
The aim of this section is to prove that the sequence (~M )MN of probability measures on B(D0) induced by the sequence (M ) of stationary distributions converges weakly to a probability measure ~. Since (D0, dS) is separable, theorem 3 says that weak convergence of (~M ) to ~ is equivalent to convergence under the Prohorov metric induced by dS .
The proof follows the usual strategy for this kind of convergence. First, we check that the closure of {~M | M  N} is compact in M+1 (D0) with respect to the Prohorov topology. Now, (D0, dS ) is also

19

complete. According to the Prohorov compactness criterion, cited as theorem 4 in the appendix, it is
therefore sufficient to show that the set {~M | M  N} is tight. For the second step, choose a limit point ~  M+1 (D0) of {~M | M  N}, which exists according to
the first step. It remains to show that any convergent subsequence of (~M ) actually converges to ~. Before embarking on the actual proof of convergence we need some technical preparation, which
consists in defining suitable approximation sets and estimating their probability under the measures M .

Let N  N, Z  SN , and set for M  N \ {1, . . . , N -1}

UNM (Z) :=

Z~  SM

#J(Z~) = #J(Z) 

   :

sup |(s) - s|
s[-r,0]



r 2N +1



fZ~   = fZ

.

For N big enough in comparison to r, UNM (Z)  SM is the set of elements Z~  SM such that dS(fZ , fZ~) 

r 2N +1

.

Furthermore,

#J (fZ~)

=

#J (fZ )

for

all

Z~



UMN (Z).

Notice that fZ~ is not necessarily an approximation of fZ with respect to the complete metric dS ,

because the slope of  can be of order N for all admissible time transformations.

Recall from proposition 2 that the probability M (Z) of an element Z  SM under the stationary distribution M depends only on the number of jumps of Z. It will be useful to partition SM into subsets of elements which have equal number of inner jumps. Set

SM (i) := Z  SM #J(fZ ) = i ,

i  {0, . . . , M -1}.

Clearly, SM = SM (0)  . . .  SM (M -1) is a pairwise disjoint union. For Z, Z~  SM (i) we have |#J(Z) - #J(Z~)|  {0, 1}. Notice that we prescribed #J(Z) = #J(Z~) instead of #J(Z) = #J(Z~) in the definition
of SM (i). Elements Z  SM such that fZ jumps at position 0 play a special role, as their accumulated probability under M tends to zero as M tends to infinity.
Before establishing this point, we need two lemmata, see appendix B. Lemma 1 estimates the number of elements of SM (i) and UNM (Z), respectively. Lemma 2 shows that for M  N large most of the probability mass of M is concentrated on elements of SM which have a number of jumps small in comparison to M .
It is even sufficient to restrict attention to elements of UNM (Z), where Z  SN is such that the number of jumps of Z is small in comparison to N which in turn must be small against M . We also see that the probability of a set UMN (Z) under M gives a good approximation of the probability which the "generating" element Z  SN receives under N .
If we compare probabilities with respect to probability measures M for different indices M  N, we have to assume that an appropriate relation holds between the corresponding transition probabilities M , M as M varies. We assume scaling relation (31) as in section 3.2, where we considered convergence of the residence time distributions.
For M  N let ~M  M+1 (D0) be the probability measure which corresponds to the stationary distribution M , if we embed SM into D0 as was done in section 4.1.

Proposition 4. Suppose the sequences of transition probabilities (M )MN, (M )MN satisfy relation (31) for some transition rates ,  > 0. Then there is a probability measure ~ on B(D0) such that (~M ) converges weakly to ~ as M tends to infinity.

Proof. Lemma 2 from appendix B.1 must be applied several times yields, see appendix B.2.

For some special sets we can calculate their probability with respect to ~.

20

Proposition 5. Let ~ be the weak limit of (M )MN according to proposition 4. For i  N0 set

Hi := {f  D0 | #J(f ) = #J(f ) = i},

H^i := {f  D0 | #J(f ) = #J(f )+1 = i+1}.

Then for all i  N0

~(Hi)

=

2 c ·

i!

·



i+1 2



i 2

,

~(H^i) = 0.

Proof. Observe that Hi, H^i, i  N0, are disjoint closed subsets of D0, because convergence with respect to the Skorokhod topology on D0 preserves the number of inner jumps.8 Indeed, Hi, H^i, i  N0, are the connected components of D0, and they are also open sets, because dS(f, g) = 2 for all f, g  D0 such that f (0) = g(0) or #J(f ) = #J(g).
The assertion now follows from theorem 3 in appendix A.1, proposition 2, equations (33) and (60) of
proposition 3 and lemma 1 in appendix B.1, respectively, under scaling condition (31).

4.3 Convergence of the chain distributions on D
Let the notation be that of section 4.1, let us write D := D{-1,1}([0, )), DR := DR([0, )) and recall D0 = D{-1,1}([-r, 0]), DR0 = DR([-r, 0]), D = D{-1,1}([-r, )), DR = DR([-r, )). All spaces come with their respective Skorokhod topology, and D  DR, D0  DR0 are closed subsets.
We sketch a proof for weak convergence of the sequence (P~M ) in M+1 (D) applying results from semimartingale theory as developed in Jacod and Shiryaev (1987).

A semimartingale with values in DR is described in terms of its characteristics, a triplet (B, C, ), where B is a truncated predictable process ("drift"), C the quadratic variation process of the continuous martingale part and  a random measure, namely the compensator of the jump measure of the semimartingale (Jacod and Shiryaev, 1987: pp. 75-76).
Any probability measure Q on B(D) gives the distribution of a {-1, 1}-valued jump process. The characteristics (B, C, ) of such a process take on a special form. One may choose a continuous truncation function with support contained in (-2, 2), thereby eliminating the contribution of B. The quadratic variation process C of the continuous martingale part disappears, because the only continuous functions in D are the two constant functions -1 and 1. The important characteristic is therefore the compensator measure . If Q corresponds to a {-1, 1}-valued process in discrete time, then the compensator can be calculated explicitly (Jacod and Shiryaev, 1987: pp. 93-94).

Let M section

 N, Z  4.1. Recall

SM and that P~ZM

let P~ZM be the corresponding probability measure on is the distribution of the {-1, 1}-valued cadlag process

B(D) as defined in (Y~ M (t))t-r induced

by the sequence Y M of current states of XM when time discretization is taken into account. Denote by

(Y (t))t-r the canonical process on D and by (Ft)t-r the canonical filtration in B(D).

Notice that (Y (t))t-r under jumps of (Y (t))t0 under P~MZ are

P~ZM is equivalent concentrated on {

to the

r M

k

|

k

process (Y~  N}. We

M (t))t-r under PZM can now calculate the

and that the compensator

measure ~M,Z : D × [0, ) × R  [0, ] of (Y (t))t0 under P~ZM in terms of the increment process of Y .

Observe that ~M,Z is determined by the integral processes (  ~M,Z )t0,  any bounded Borel function.9

Set

s(k)

:=

r M

·k,

k



{-M, -M+1, . . .}.

According

to

II.3.11

in

Jacod

and

Shiryaev

(1987: p. 94)

it

holds

8Skorokhod convergence in D does not necessarily preserve the number of jumps. 9See Jacod and Shiryaev (1987: p. 66) for a definition of the integral process w. r. t. a random measure.

21

for all functions , all t  0, ~  D (all probabilities with respect to P~ZM )

(  ~M,Z )t(~) =

t r

M

E  Y (s(k)) - Y (s(k -1)) · 1Y (s(k))=Y (s(k-1)) Fs(k-1) (~)

k=1

t r

M

= E . . .  Y (s(k-M -1)), . . . , Y (s(k-1)) (~)

k=1

t r

M

= 1(~(s(-M)),...,~(s(0)))=Z (~ ) ·

k=1

(2) · P Y (s(k)) = 1 Y (s(k-M -1)) = -1, Y (s(k-1)) = -1 · 1~(s(k-M-1)=-1,~(s(k-1))=-1(~)

+ (2) · P Y (s(k)) = 1 Y (s(k-M -1)) = 1, Y (s(k-1)) = -1 · 1~(s(k-M-1)=1,~(s(k-1))=-1(~)

+ (-2) · P Y (s(k)) = -1 Y (s(k-M -1)) = 1, Y (s(k-1)) = 1 · 1~(s(k-M-1)=1,~(s(k-1))=1(~)

+ (-2) · P Y (s(k)) = -1 Y (s(k-M -1)) = -1, Y (s(k-1)) = 1 · 1~(s(k-M-1)=-1,~(s(k-1))=1(~)

t r

M

= 1...(~) ·

(2) · M · 1~(s(k-M-1)=-1,~(s(k-1))=-1(~ ) + M · 1~(s(k-M-1)=1,~(s(k-1))=-1(~ )

k=1

+ (-2) · M · 1~(s(k-M-1)=1,~(s(k-1))=1(~ ) + M · 1~(s(k-M-1)=-1,~(s(k-1))=1(~ ) ,

where Bayes' formula has been applied.

Let f  D0 with f (0) = f (0-), and write Z(f ) = Z(f, M ) for the element of SM such that fZ(f) = f . The compensator measure ~M,Z(f) then induces a random measure

M,f : DR × [0, ) × R  [0, ], f : DR  DR

M,f () := ~M,Z(f)(f ()), where f ()(t) := f (t) · 1[-r,0)(t) + (t) · 1[0,)(t).

Assume that scaling relation (31) is satisfied for some positive transition rates , . Then for all functions  : R  R,   DR, t  0 it holds that

lim
M 

  tM,f ()

=

+

t
(2) ·  · 1~(s-r)=-1,~(s)=-1(f ()) +  · 1~(s-r)=1,~(s)=-1(f ()) ds
0 t
(-2) ·  · 1~(s-r)=1,~(s)=1(f ()) +  · 1~(s-r)=-1,~(s)=1(f ()) ds,
0

which defines a random measure f : DR × [0, ) × R  [0, ].

Let µ : DR × [0, ) × R  [0, ] be the jump measure associated to the D-valued process (Y (t))t0, cf. Jacod and Shiryaev (1987: pp. 68-69). We have for all functions  : R  R,   DR, t  0

(  µ)t() =

1(s)=(s-)() · (2) · 1(s)=1,(s-)=-1() + (-2) · 1(s)=-1,(s-)=1() .

0<st

Theorem IX.2.31 in Jacod and Shiryaev (1987: p. 495) guarantees the existence of a probability measure Qf on B(DR) such that
· Qf {  DR | (0) = f (0)} = 1,
· the canonical process is a semimartingale under Qf with characteristics (0, 0, f ).

22

We notice that Qf (D) = 1. Let us interpret Qf as a probability measure on B(D). According to theorem II.2.21 in Jacod and Shiryaev (1987: p. 80) the second property implies that
· (  µ -   f )t0 is a local martingale under Qf for every function  : R  R.
Observe that theorem IX.2.31 does not guarantee uniqueness of the probability measure Qf . Here, however, uniqueness can be established by considering sequences of stopping times 1, 2, . . . which exhaust the jump positions. The above local martingale property must then be applied to show that any two solution measures to the semimartingale problem coincide on the sets {n  t} for all t  0, n  N. Recall that every element   D is determined by its value (0) and the positions of its discontinuities. By the uniqueness theorem of measure theory we see that Qf is uniquely determined.
Let p : D  D be the natural projection. Then theorem IX.3.21 in Jacod and Shiryaev (1987: p. 505) implies that P~MZ(f,M)  p-1 w Qf in M+1 (D). Define a probability measure P~f  M+1 (D) by P~f := Qf  f-1. We have Z(f,M) w f in M1+(D0). In view of Qf ({  DR | (0) = f (0)}) = 1 we conclude that P~ MZ(f,M) w P~ f . The last step is to show that (P~M ) converges weakly, that is in place of a deterministic initial condition f  D0 we have ~M  M1+(D0) as initial distribution. Let ~ be the weak limit of (~M ) according to proposition 4. As a consequence of proposition 5 we have ~({f  D0 | f (0) = f (0-)}) = 1. Define P~  M1+(D) by
P~(A) := P~f (A) d~(f ), A  B(D).
D0
If f1, . . . , fn  D0 with fi(0) = fi(0-), i  {1, . . . , n}, then any convex combination of the sequences (P~MZ(f1,M)), . . . , (P~ZM(fn,M)) converges weakly to the corresponding convex combination of the measures P~f1 , . . . , P~fn . An approximation argument analoguous to that in the proof of proposition 4 leads to
Proposition 6. Suppose scaling relation (31) holds. Let P~M , M  N, be defined as in section 4.1, and let ~ be the weak limit of (M )MN according to proposition 4. Then there is a probabilty measure P~  M+1 (D) such that P~M w P~.

4.4 Residence times revisited

In section 3.2 we calculated the residence time distribution for the two state model of discretization degree
M for each M  N. We then let M tend to infinity in order to obtain the residence time distribution
and its density function in the "continuous time" limit.
At that stage, however, we had not yet established the existence of a corresponding limit process. This was done in section 4.3, where we saw that (P~M ), the sequence of distributions induced by the two state chains in discrete time, converges weakly to a probability measure P~ on B(D). We are now in a position to show that any process with distribution P~ has the same residence time distribution as the one
obtained in section 3.2. On the probability space (D, B(D), P~) a process with distribution P~ is, of course, given by the
canonical process of coordinate projections pt : D  {-1, 1}, because pt is Borel measurable for all t  -r, cf. appendix A.4. We continue to work directly on the canonical space. Define a mapping

(42)  : D  [0, ],

 (f ) := inf{t  0 | f (t) = -1}.

Then  is Borel measurable as we have

n

 -1[0, t]

=

p-0 1{-1}



n=0

k=1

p0-1{1}



.

.

.



p-1 k-1 n

{1}
t



p-nk 1t {-1}

for all t  0,

23

where the cadlag property of the elements of D has been exploited. Because of this property the infimum in (42) is really a minimum provided  < . We notice that  is a stopping time with respect to the natural filtration in B(D) and that  is finite P~-almost surely.
For each   (0, 1) denote by A~ the event that in the time interval [-r, 0] there is exactly one jump, that jump going from -1 to 1. This means we set
(43) A~ := f  D  ~  [0, ) : f (t) = -1  t  [-r, -~r)  f (t) = 1  t  [-~r, 0] .

Observe that A~  B(D) and P~(A~) > 0 for all   (0, 1). The distribution function of  conditional on the event of exactly one jump from -1 to 1 "just before" time zero can be approximated by functions of the form
(44) F(t) := P~   t A~ , t  [0, ),
where  > 0 must be small. Since  is P~-almost surely finite and A~ has positive probability under P~, the function F indeed determines a probability distribution on [0, ).
Let f~L be the residence time distribution density in the limit of discretization degree M tending to infinity as given by (36). Set
t
(45) F (t) := f~L(s)ds, t  [0, ).
0
We have to show that F(t) tends to F (t) as  goes to zero for each t  [0, ). In (42) and (43), the definitions of  and A~, respectively, instead of time zero we could have chosen any starting time t0  0, see appendix B.3, which gives a proof of

Proposition 7. Suppose scaling relation (31) holds. Let the distribution functions F,   (0, 1), and F be defined by (44) and (45), respectively. Then

lim
0

F (t)

=

F

(t)

for all t  [0, ).

5 Connection between the reduced and the reference model

The aim of this section is to provide a heuristic way of establishing the missing link between our original model, which is given by equation (2), and the
reduced model developed in section 3. The situation here is quite similar to the one that was studied by Tsimring and Pikovsky (2001), and we will closely follow their approach in deriving a relation between the transition rates ,  and the parameters of the original model.
The main ingredient in finding such a relation is the so-called Kramers rate, which gives an asymptotic approximation of the time a Brownian particle needs in order to escape from a parabolic potential well in the presence of white noise only as the noise intensity tends to zero. By means of the Kramers rate we calculate escape rates from potentials that should mirror the "effective dynamics" of solutions to equation (2). The resonance characteristics defined in subsection 3.3 can then be written down explicitly as functions of the noise parameter , which allows us to numerically calculate the resonance point and to compare the optimal noise intensity according to the two state model with the behaviour of the original model.
Let U be a smooth double well potential with the positions of the two local minima at xleft and xright, respectively, xleft < xright, the position of the saddle point at xmax  (xleft, xright) and such that

24

U(x)   as |x|  . An example for U is the double well potential V from sections 1 and 2. Consider the SDE
(46) dX(t) = -U X(t) dt +  · dW (t), t  0,
where W (.) is a standard one dimensional Wiener process with respect to a probability measure P and  > 0 is a noise parameter. Denote by Xx, a solution of equation (46) starting in Xx,(0) = x, x  R. With y  R let y(Xx,) be the first time Xx, reaches y, that is we set
y(Xx,) := inf{t  0 | Xx, = y}.

As we are interested in the transition behaviour of the diffusion, we need estimates for the distribution of y(Xx,) when x and y belong to different potential wells.
In the limit of small noise the Freidlin-Wentzell theory of large deviations (Freidlin and Wentzell, 1998) allows to determine the exponential order of y(Xx,) by means of the so-called quasipotential Q(x, y) associated with the double well potential U . One may think of Q(x, y) as measuring the work a
Brownian particle has to do in order to get from position x to position y. The following transition law
holds.

Theorem 2 (Freidlin-Wentzell). Let Q be the quasipotential associated with U , let x  (-, xmax), y  (xmax, xright]. Set ql := Q(xleft, xmax). Then

(47a)

lim 2 · ln
0

EP

y (X x,

= ql,

(47b)

lim P
0

exp

ql -  2

< y(Xx,) < exp

ql +  2

= 1 for all  > 0.

Moreover, Q(xleft, xmax) = 2 U (xmax) - U (xleft) . If x  (xmax, ), y  [xleft, xmax) then ql has to be replaced with qr := Q(xright, xmax).

We notice that in travelling from position x in the left potential well to y  (xmax, xright], a position in the downhill part of the right well, the transition time in the limit of small noise is determined exclusively

by the way up from position xleft of the left minimum to position xmax of the potential barrier. A typical path of Xx,, if  > 0 is small, will spend most of its time near the positions of the two

minima of the double well potential. Typically, the diffusion will reach the minimum of the potential well

where it started, before it can cross the potential barrier at xmax and enter the opposite well. Theorem 2 implies the existence of different time scales for equation (46). On the one hand, there is

the

time

scale

induced

by

the

Wiener

process,

where

one

unit

of

time

can

be

chosen

as

1 2

,

that

is

the

time it takes the quadratic variation process associated with W (.) to reach 1. On the other hand, there

is the mean escape time given by (47a), which is proportional to exp

2L 2

, where L > 0 is the height of

the potential barrier. Clearly, with  > 0 small, the time scale induced by the white noise is negligible in

comparison with the escape time scale.

Moreover, if U (xleft) = U (xright), then there are two different heights Ll and Lr for the potential

barrier depending on where the diffusion starts. Suppose, for instance, that Ll < Lr. According to (47b),

waiting a time of order exp

2Ll + 2

with 0 <  < 2(Lr - Ll) one would witness transitions from the left

well to the right well, but no transition in the opposite direction. If the waiting time was of an exponential

order less than exp

2Ll 2

, there would be no interwell transitions at all, where "no transitions" means that

the probability of a transition occurring tends to zero as   0. Thus, by slightly and periodically tilting

a symmetric double well potential quasi-periodic transitions can be enforced provided the tilting period

is of the right exponential order. This is the mechanism underlying stochastic resonance.

25

Now, let us suppose that y(Xx,), where x < xmax and y  (xmax, xright], is exponentially distributed with rate rK > 0 such that

(48)

rK  exp

-2

U (xmax) - U (xleft) 2

.

Equations (47a) and (47b) of theorem 2 would be fulfilled. In the physics literature it is generally assumed that y(Xx,) obeys an exponential distribution with rate rK provided  > 0 is sufficiently small. This is known as Kramers' law, and rK is accordingly called the Kramers rate of the respective potential well. It is, moreover, assumed that the proportionality factor missing in (48) can be specified as a function of the curvature of U at the positions of the minimum and the potential barrier, respectively. The Kramers rate thus reads

(49)

rK = rK(, U ) =

|U

(xleft) U 2

(xmax) |

exp

-

2|

U

(xlef

t) - 2

U

(xmax)

|

.

Observe that both the assumption of exponentially distributed interwell transition times and formula (49) for the Kramers rate are empirical approximations, where the noise parameter  is supposed to be sufficiently small.
Well known results for one-dimensional diffusions, extended to the multi-dimensional framework in recent papers by Bovier et al. (2002a,b), show that in the limit of small noise the distribution of the interwell transition time indeed approaches an exponential distribution with a noise-dependent rate that asymptotically satisfies relation (48). The order of the approximation error can also be quantified. For our purposes, however, Kramers' law and the Kramers rate as given by equation (49) will be good enough.
In subsection 3.1 we introduced the transition rates ,  as being switching rates in the two state model conditional on whether or not the current state agrees with the last remembered state. The idea, now, is to find two "effective" potentials U, U such that  is proportional to the Kramers rate describing the escape time distribution from potential U, while  is proportional to the Kramers rate for potential U, where the Kramers rate is given by formula (49). More precisely, we must have
(50)  = () = r · rK(, U),  = () = r · rK(, U).
Note that the inclusion of the delay time r as a proportionality factor is necessary, because in the construction of our two state model we took one unit of time as equivalent to the length of the interval [-r, 0].
There is an important point to be made here. In the discussion of section 3 we assumed that X(t)  1 or X(t)  -1. The error of this approximation is of first order in , and its contribution to the delay force is proportional to 2 U (1) + O(3), i. e. of the second order in . As long as we content ourselves with an approximation of first order in , two states corresponding to the positions of the minima around -1 and 1 should be enough in order to model the effective dynamics of the reference equation. If we wanted to capture the influence of second order terms in the delay force, we would have to build up a model of four states corresponding to the positions ± x, ± x of the minima of the distorted potential V .10
The problem disappears, of course, if U is constant except on a small symmetric interval (-, ) around the origin (see Fig. 1 c), for in this case the delay force would not depend on the particular value of X(t - r) provided |X(t - r)|  .
10Cf. also the numerical results in Curtin et al. (2004).

26

Let L := V (0) - V (1) be the height of the potential barrier of V . Set  := |V

(0)V

(1)|,



:=

V

(1) U (1) (V (1))2

,

~ :=

U

(1) L

.

Neglecting

terms

of

order

higher

than

one,

from

(49)

we

obtain

(51a)

 = ()  r ·

 (1 -  ) 2

exp

-

2L

(1 - 2

~

)

,

(51b)

 = ()  r ·

 (1 +  ) 2

exp

-

2L

(1 + 2

~

)

.

Recall that the Kramers rate is exact only in the small noise limit. Thus, for the formulae (51a) to

become the actual rates of escape it is necessary that  tends to zero. If the rates ,  as functions of

r

and



are

to

converge

to

some

finite

non-zero

values,

we

must

have





0

and

r





such

that

1 2

and ln(r) are of the same order. There remain errors due to the first order approximations of V , V and

U , which make sense only if V , U are sufficiently regular and the delay parameter  is of small absolute

value.

In subsection 3.3 we defined two measures of resonance, namely the jump height M of the residence time distribution density and the probabilities ^M , M of transitions within the first and second delay interval, respectively.11 Recall that M  N  {} is the degree of discretization, where M =  denotes
the limit M  . We restrict attention to the case M = , that is to the two state model in continuous
time.

Suppose the transition rates ,  are functions of the reference model parameters as given by (51a) read as equalities. In particular, ,  are functions of the delay length r and the noise parameter . Let us further suppose that the delay parameter  is of small absolute value, r > 0 is big enough so that the critical parameter region for  lies within the scope of formula (51a), and that the remaining parameters are sufficiently nice.

As a consequence of the exponential form the Kramers rate possesses, we notice that



=

r

·

 2

·

4

1 - 22 exp

-

2L 2



r·

 2

exp

-

2L 2

.

In first order of , the geometric mean  of ,  coincides with the transition rate arising in case

 = 0, that is when there is no delay. Compare this with proposition 3, which states that the residence tdiimsteribduentisointywf~iLthispadriastmriebtuerted on. the first delay interval according to a mixed hyperbolic sine - cosine

The conditions of definitions 1 and 2 are satisfied. If  > 0, then the reduced model exhibits stochastic resonance according to both definitions. According to the jump height measure there is no effect in case  = 0 and pseudo-resonance in case  < 0, while the time window measure does not distinguish between  = 0 and  < 0, classifying both cases as pseudo-resonance.

Let us specify the potentials V and U according to the model studied by Tsimring and Pikovsky (2001), that is V is the standard quartic potential and U a parabola, see Fig. 1. For the constants appearing in formula (51a) we have

L

=

1 4

,

 = 2,



=

3 2

,

~ = 4.

With r = 500,  = 0.1, for example, we obtain the resonance point   0.32 according to the jump height measure, while the time window measure yields   0.29 with probability ()  0.88 for transitions occurring in the second delay interval.
11The jump height measure corresponds to a measure of resonance proposed by Masoller (2003).

27

Assume  is negative. Again, both measures yield an optimal noise level. With  = -0.1 we have   0.30 as the noise level that maximizes the jump height in f~L. According to the time window
measure optimal noise level is at   0.34, but ()  0.02, that is sojourns of duration between r
and 2r are rare.

3 a)...................... 12 ............................................................................................... ..............................................................................................
012

3 b)
2
1 ...............................................................................................
...............................................................................................
012

3 c)
21 ...............................................................................................
...............................................................................................
012

Figure 2: Graphs on [0, 2] of the density fL of the residence time distribution in normalized time. Parameters of
the original model: r = 500, a)  = 0.30,  = 0.1, b)  = 0.30,  = -0.1, c)  = 0.35,  = -0.1.

There seems to be a discrepancy, now, between the predicted optimal noise level and the level of "most regular" transition behaviour which one would expect from numerical simulation. This is true especially with regard to the jump height measure, the pseudo-resonance point  being too low.
The problem is that the expected residence time at the level of optimal noise in case  < 0 is long compared with r. In spite of the fact that long residence times are rare, there is a high probability of finding a solution path remaining in one and the same state for the length of many delay intervals or of witnessing a quasi-periodic transition behaviour break down.
For example, let  = 0.30,  = 0.1. The expected residence time is then about 1.16r, while with  = 0.30 and  = -0.1 the expected residence time is around 4.62r. More importantly, with  negative the exponential part of the residence time distribution has a "heavy tail" in the sense that long sojourns receive a relatively high probability, cf. Fig. 2.
These properties of the residence time distribution support the distinction made in definitions 1 and 2 between stochastic resonance and pseudo-resonance.

6 Conclusions and open questions
The main advantage of the two state model which has been our concern for most of this work is that it provides a tool for the analysis of the phenomenon of noise-induced resonance in systems with delay.
The reference model introduced in section 2 is a more elaborate system exhibiting stochastic resonance. Basic features of this model are the extended Markov property and the existence of an invariant probability measure. Both properties carry over to the two state model.
By first studying the two state model in discrete time we obtained an explicit characterization of its stationary distribution. It was thus possible to calculate the residence time distribution which in turn served as starting point for the definition of two simple measures of resonance. The characterization of the stationary distributions in discrete time together with the passage to the time limit also allows to calculate measures of resonance different from those considered here, for example the entropy of a distribution.
In section 5 a heuristic link between the reference and the two state model was outlined. The two state model seems to reliably mirror those aspects of the reference model that are responsible for the

28

phenomenon of stochastic resonance. Observe that we did not show whether the dynamics of the original model in the limit of small noise is reducible to the two state model nor whether the resonance measures considered here are indeed robust under model reduction.
There are different ways in which to proceed. The reference model could be modified, for example, by substituting a distributed delay for the point delay. Clearly, the white noise could be replaced with noise of different type, and higher dimensional equations may be considered.
Lastly, the passage to continuous time as addressed in section 4 should be a special case of more general convergence results for continuous time Markov chains with delay.

A Skorokhod spaces and weak convergence

A.1 Weak convergence in separable metric spaces

The results summarized in this section are taken from Ethier and Kurtz (1986: ch. 3 §§ 1-3) and Billingsley (1999: § 2). Let (S, d) be a separable metric space, and denote by M1+(S) the set of probability measures on the Borel--algebra B(S). Define the Prohorov metric  by

(P, P^) := inf > 0 | P(G)  P^(G ) + for all closed G  S , P, P^  M+1 (S),

where G := {x  S | infyG d(x, y) < }. Then  is indeed a metric, and (M1+(S), ) is a separable metric space. If, in addition, (S, d) is complete, then (M1+(S), ) is complete, too (Ethier and Kurtz,
1986: p. 101).

Denote by Cb(S) the space of all bounded continuous functions on (S, d), topologized with the supremum norm. A sequence (Pn)nN of probability measures on B(S) is said to converge weakly to a probability measure P  M+1 (S), in symbols Pn w P, iff

 f  Cb(S) :

f d Pn n- f d P .
SS

The next theorem gives different characterizations of weak convergence and states that weak convergence is equivalent to convergence in the Prohorov metric (Ethier and Kurtz, 1986: p. 108). Recall that we assume (S, d) to be separable. In an arbitrary metric space convergence under  would still imply weak convergence and its characterizations, but the converse would not necessarily hold.
Let P  M+1 (S). A set A  S is called a P-continuity set iff A  B(S) and P(A) = 0, i. e. A is Borel-measurable and its boundary is a P-null set.

Theorem 3. Let (Pn)nN  M+1 (S), P  M1+(S). The following conditions are equivalent:

(i)

lim
n

(Pn,

P)

=

0,

(ii) Pn w P,

(iii) f d Pn n- f d P for all uniformly continuous functions f  Cb(S),
SS

(iv) lim sup Pn(A)  P(A) for all closed sets A  S,
n

(v)

lim inf
n

Pn(A)



P(A)

for

all

open

sets

A  S,

(vi)

lim
n

Pn(A)

=

P(A)

for

all

P-continuity

sets

A  S.

29

Useful in proving convergence in M+1 (S) is the Prohorov criterion for compactness, provided the underlying metric space is complete (Ethier and Kurtz, 1986: p. 104).
Theorem 4 (Prohorov). Let   M1+(S), and suppose that (S, d) is complete. Then the following conditions are equivalent:
(i)  is tight, i. e.  > 0  A  S compact : infP P(A)  1 - ,
(ii)  > 0  A  S compact : infP P(A )  1 - ,
(iii) the closure of  is compact in the Prohorov topology.

The mapping theorem states that under a measurable map weak convergence carries over to the sequence of image measures if the set of discontinuities of the mapping is negligible with respect to the original limit measure (Billingsley, 1999: p. 21).
Theorem 5. Let (Pn)nN  M1+(S), P  M+1 (S). Let (S , d ) be a second metric space and  : S  S be a B(S )-B(S)-measurable map. Denote by J the set of discontinuities of .
If Pn w P and P(J) = 0 then Pn -1 w P -1.

A.2 The Skorokhod space DR0
Here, we gather results and definitions from Billingsley (1999: §§ 12-13) on the nature of DR0 := DR([-r, 0]), the Skorokhod space of all real valued cadlag functions on the finite interval [-r, 0], i. e. of functions f : [-r, 0]  R such that

f (t+) := lim f (s) = f (t) for all t  [-r, 0), f (t-) := lim f (s) exists for each t  (-r, 0].
st st

It is possible to define Skorokhod spaces of functions with values in more general spaces than R. In fact, the theory can be developed for DE([-r, 0]) essentially in the same way as for DR0 as long as E is a Polish space, i. e. a complete and separable metric space.
Let f be any real valued function on [-r, 0]. Define the modulus of cadlag continuity as

(52)

w~(f, ) := inf max w(f, [ti-1, ti)) n  N, -r = t0 < . . . < tn = 0,
i{1,...,n}
min (ti - ti-1) >  ,   (0, r),
i{1,...,n}

where w(., .) is the modulus of uniform continuity defined as

(53) w(f, I) := sup |f (s) - f (t)|, I  [-r, 0] an interval.
s,tI

A function f : [-r, 0]  R is in DR0 if and only if lim0 w~(f, ) = 0, cf. Billingsley (1999: p. 123).

Denote by  := { : [-r, 0]  [-r, 0] |  bijective and strictly increasing} the set of "time transforma-

tions" on [-r, 0]. For all    we have (-r) = -r, (0) = 0, and  is continuous. On  define a

pseudo-norm

  := sup ln
s,t[-r,0], s=t

(t) - (s) t-s

,

  .

Let f , g be elements of DR0 , and define the distances dS, dS as

(54) dS(f, g) := inf > 0    : sup |(t) - t|   sup |f (t) - g((t))|  ,

t[-r,0]

t[-r,0]

(55) dS (f, g) := inf > 0    :     sup |f (t) - g((t))|  .
t[-r,0]

30

Both functionals, dS and dS, measure the distance between f and g in terms of the supremum norm f - g   . In addition, dS requires that time transformations  differ as little as possible from the identy on [-r, 0], while dS puts an extra restriction on the slope of the transformations.
The most important difference between dS and dS lies in the fact that they give rise to different sets of Cauchy sequences. Theorem 6 is a summary of Billingsley (1999: pp. 125-129).
Theorem 6. Let dS, dS be defined as above. Then DR0 is a separable metric space under dS as well as under dS. Both metrics generate the same topology, called the Skorokhod topology.
Equipped with the Skorokhod topology, DR0 is a Polish space, and dS is a complete metric, while DR0 is not complete under dS.

The example below illustrates why dS does not define a complete metric on DR0 . The sequence to be constructed is a Cauchy sequence with respect to dS, the only possible limit point of which lies outside the space of cadlag functions. The same sequence is not Cauchy under dS.

Example. Choose t0  [-r, 0). For n  N big enough set fn := 2 · 1In - 1, where In := [t0, t0 + 2-n).

Then (fn)  DR0 is a Cauchy sequence of {-1, 1}-valued functions with respect to the metric dS, and fn(t) n -1 for all t = t0, but dS(fn, -1) = 2 for all n  N, and f := 2 · 1{t0} - 1 is no cadlag

function.



The following criterion, which is theorem 12.3 in Billingsley (1999: p. 130), is an analogue of the ArzelàAscoli theorem for compactness in spaces of continuous functions.
Theorem 7. Let A  DR0 . Then the closure of A is compact in the Skorokhod topology if and only if the following two conditions hold:
(i) sup sup |f (t)| < ,
f A t[-r,0]
(ii) lim sup w~(f, ) = 0.
0 f A
A.3 The Skorokhod space D0
There are two equivalent ways of topologizing D0 := D{-1,1}([-r, 0]), the space of all {-1, 1}-valued cadlag functions on the finite interval [-r, 0]. The first is to define metrics dS, dS in analogy to appendix A.2, where |. - .| should be interpreted as a metric on {-1, 1}. In fact, if (E, d) is a metric space, one can define the Skorokhod space DE([-r, 0] with its accompanying metrics. If, in addition, (E, d) is complete and separable, then an analogue of theorem 6 holds.12
The second option is to restrict the metrics dS, dS and the Skorokhod topology of DR0 to D0. This works, because D0 is a closed subset of DR0 with respect to the Skorokhod topology. Theorem 6 now implies that D0 is a separable metric space under dS, and complete and separable under dS, as is the case for DR0 . Define the moduli of continuity w, w~ by restriction or in analogy to (52) and (53), respectively.
For f  D0 define J(f ), the set of discontinuities or jumps, and f , the minimal distance between two discontinuities or an inner discontinuity and one of the boundary points of [-r, 0], as
J(f ) := t  (-r, 0] f (t) = f (t-) ,
f := min |t - s| t, s  J(f )  {-r, 0} ,
where f (t-) is the left-hand limit of f at t. Set J(f ) := J(f )  (-r, 0), the set of inner discontinuities of f . Notice that the only possible discontinuity of f not in J(f ) is 0, the right boundary of [-r, 0].
12Skorokhod spaces for E-valued functions on the infinite interval [0, ) are defined in Ethier and Kurtz (1986).

31

Proposition 8. Let f  D0,   (0, r), and let I  [-r, 0] be an interval. Then

(56) w(f, I)  {0, 2}, (57) #J(f )  N0, (58) w~(f, )  {0, 2},

w(f, I) = 0  f is constant on the interval I, w~(f, ) = 0  f > .

Proof. Obviously, |f (s) - f (t)|  {0, 2} for all s, t  [-r, 0], and (56) is a consequence of (53), the definition of w.
If there were an f  D0 with #J(f ) = , one could choose a sequence (tn)nN  J(f ) such that tn n t and tn = t for all n  N. Since f is a cadlag function, there would be l, r > 0 such that f is constant on the intervals (t - l, t), (t, t + r), except if t were a boundary point of [-r, 0], in which case only one of the constants l, r could be chosen appropriately. In any case, tn  (t- l, t) or tn  (t, t +r) for n big enough, a contradiction, because f cannot be constant on an open interval and at the same time have a discontinuity in it.
Clearly, w~(f, )  {0, 2}. Suppose w~ = 0. Then there are m  N and a partition -r = t0 < . . . < tm = 0 such that ti - ti-1 >  and w(f, [ti-1, ti)) = 0 for all i  {0, . . . , m}. Hence, f is constant on each interval [ti-1, ti), and the minimal distance between two discontinuities or an inner discontinuity and the boundary of [-r, 0] is at least min{(ti - ti-1) | i  {0, . . . , m}}.
Conversely, f >  implies w~(f, ) = 0, because -r = t0 < . . . < tm = 0 forms a suitable partition of [-r, 0], if one chooses m = #J(f )+1 and takes as t1, . . . , tm-1 the inner discontinuities of f .
Theorem 7, which states necessary and sufficient conditions for compactness in DR0 , takes on a simple form in the present context.
Proposition 9. Let A  D0. Then the closure of A is compact in the Skorokhod topology if and only if inf{f | f  A} > 0.

Proof. Condition (i) of theorem 7 is satisfied for any A  D0. Hence, we must show that condition (ii) of 7 is equivalent to inffA f > 0.
Let f  D0, then w~(f, )  {0, 2} for all   (0, r). Therefore, lim0 supfA w~(f, ) = 0 if and only if there exists 0  (0, r) such that for all   (0, 0) and all f  A we have w~(f, ) = 0. According to (58) the latter condition is equivalent to the existence of 0  (0, r) such that f  0 for all f  A, which in turn is just inffA f > 0.
Condition inffA f > 0 implies supfA #J(f ) < , but the converse implication does not hold, as can be seen by considering the sequence (fn) defined in the example of appendix A.2.
A.4 The space DR
Denote by DR := DR([-r, )) the space of all real valued cadlag functions on the interval [-r, ). Observe that a cadlag function on [-r, ) has at most countably many points of discontinuity (Ethier and Kurtz, 1986: p. 116). It is possible to define Skorokhod metrics on DR in a way similar to that of appendix A.2, cf. Ethier and Kurtz (1986: ch. 3 § 5).
There are two noteworthy differences, though, because the interval the elements of DR live on is no longer bounded to the right. In the definitions which correspond to (54) and (55) one needs a "fading function" or a "fading series" to guarantee finiteness of the metrics. More importantly, the special role

32

which the right boundary plays in (53) and in the definition of the set of time transformations  has no counterpart with DR.
An alternative is provided by Billingsley (1999: § 16), where a connection is established between DR and Skorokhod spaces over finite intervals. Observe that all definitions and properties of DR0 = DR([-r, 0]) carry over to the space DR([-r, t]) for any t  (-r, ). Set
t : DR f  f|[-r,t·r]  DR([-r, t · r]), t > -1.

Let m  N0. Set DRm := DR([-r, m · r]). Write dm, dm for the corresponding Skorokhod metrics, and define a function hm and a "continuous restriction" m by

 1 if t  [-r, (m-1)r),

hm(t) :=

m0

-

t r

if if

t  [(m-1)r, m · r), t  m · r,

m : DR f  m(f · hm)  DRm.

Define a Skorokhod metric d on DR by

(59)



d (f, g) :=

2-m 1  dm m(f ), m(g) .

m=0

An equivalent (but incomplete) Skorokhod metric d can be defined as in (59) by replacing the metrics dm with dm (Billingsley, 1999: p. 168). Now define a metric on the product of the spaces DRm, m  N0, that is one sets



D :=

DRm,

m=0



d~ (fm)mN0 , (gm)mN0 :=

2-m 1  dm (fm, gm) .

m=0

We have the following embedding theorem (Billingsley, 1999: p. 170).
Theorem 8. Define  : DR  D by f  (m(f · hm))mN0 . Then 1.  is an isometry with respect to d and d~,
2. (DR)  D is closed, 3. (D, d~ ) is a Polish space, and so is (DR, d ).
The natural projection pt : DR f  f (t)  R is Borel-measurable for each t  -r, and pt is continuous at f  DR if and only if f is continuous at t. The projections generate B(DR) and form a canonical process (Billingsley, 1999: p. 172). Notice that the restrictions t, t > -1, are measurable, too. The following proposition characterizes convergence in DR in terms of convergence of the restricted sequences (Billingsley, 1999: p. 169).
Proposition 10. Let fn, n  N, f be elements of DR. Then fn n f w. r. t. d if and only if t(fn) n t(f ) in DR([-r, t · r]) for every continuity point t · r of f .

33

A.5 The Skorokhod space D
First observe that for any t > -r the space D{-1,1}([-r, t]) of all {-1, 1}-valued cadlag functions on [-r, t] can be defined in analogy to the space D0.
Denote by D := D{-1,1}([-r, )) the set of all {-1, 1}-valued cadlag functions on [-r, ). Clearly, D is a subset of DR, but it is also closed with respect to the Skorokhod topology of DR as can be seen from proposition 10 and appendix A.3. We may therefore restrict the topology of DR and its metrics d, d , thus topologizing D.

B Convergence to continuous time

B.1 Approximating sets in SM
Lemma 1.

(60)

 M  N  i  {0, . . . , M - 1} :

#SM (i)

=

4·

M -1 i

,

(61)  N  N  Z1, Z2  SN  M  N : Z1 = Z2  UMN (Z1)  UMN (Z2) = ,



 N  N  i  {0, . . . , N - 1}  M  N  Z  SN (i) :

(62)

2M 2N +1

i  #UMN (Z) 

2M 2N +1

i
+1 .

Proof. Any element f  ~M SM (i) can be described as follows. Choose i out of M -1 possible positions for the inner discontinuities, and decide on the binary values of f (-r), f (0). This determines f , and (60) follows.

Let Z1 = (Z1(-N), . . . , Z1(0)), Z2 = (Z2(-N), . . . , Z2(0)) be elements of SN . By definition of UNM (.), for UMN (Z1)  UMN (Z2) =  we must have #J(Z1) = #J(Z2) as well as Z1(-N) = Z2(-N) and Z1(0) = Z2(0).

Suppose that Z1, Z2 have the same number of inner discontinuities and agree at -N and 0, but still

Z1 = Z2. Then Z1, Z2 differ in the position of at least one inner discontinuity, that is to say there is s1 

(-r, 0) such that s1  J(Z1) \ J(Z2); by symmetry, there is also s2  (-r, 0) such that s2  J(Z2) \ J(Z1).

Select

such an

s2, then

|s - s2| 

r N

for all s  J(Z1).

Let Z~1 be an element of UMN (Z1). Then for any inner discontinuity s  J(Z1) there is exactly one

s~ 

J(Z~1)

such

that

|s~ - s|



r 2N +1

,

and

vice

versa.

The

same

holds

true

for

any

element

Z~2

of

UMN (Z2)

with respect to Z2.

In particular, there is s~2  J(Z~2) such that |s~2 - s2| 

r 2N +1

.

But s~2  J (Z~1),

because

|s~2

- s~|



r N

-

2r 2N +1

>

0

for

all

s~ 

J (Z~1).

Since

Z~1,

Z~2

were

arbitrary,

this

establishes

(61).

An element Z~  UMN (Z) is determined by the positions of its #J(Z) inner discontinuities, where

k

·

r M

-r

|

k



{1, . . . , M -1}

is

the

set

of

possible

such

positions.

If

s



J(Z ),

then

there

is

k



{1, . . . , N -1}

with

s

=

k·

r N

- r,

and

it

exists

exactly

one

s~ 

J(Z~)

such

that

s~ 

[s-

r 2N +1

,

s+

r 2N +1

].

Equation

(62)

is

now

a

consequence

of

2M 2N +1

#

k

·

r M

-r

k  {1, . . . , M -1}



[s

-

r 2N +1

,

s

+

r 2N +1

]



2M 2N +1

+ 1,

for

all

s



{k

·

r N

-

r

|

k



{1, . . . , N -1}},

and

the

fact

that

#J(Z )

=

i

for

Z



SN (i).

34

Lemma 2. Let M , N , N0 be natural numbers such that N0 < N  M , let > 0, and define the expressions 1, 2 and 3 as

N0
1 := M

UMN (Z)  1 - ,

i=0 ZSN (i)

2 :=  i  {0, . . . , N0}  Z  SN (i) :

N -1

3 :=

N SN (i)  .

M UNM (Z) - N (Z)  N i ,

i=N0 +1

Suppose that the sequences of transition probabilities (M )MN, (M )MN satisfy relation (31) for some transition rates ,  > 0. Then for all > 0

(63)  N~0  N  N0  N~0  N~  N  N  N~  M~  N  M  M~ : 1,

(64)  N0  N  N~  N  N  N~  M~  N  M  M~ : 2,

(65)  N~0  N  N0  N~0  N~  N  N  N~ : 3.

Finally, it holds that

(66)  > 0  N0, N~  N  N  N~  M~  N  M  M~ : 1  2  3.

Proof. Formula (66) follows by "putting together" (63), (64) and (65), where N0 = N0( ) can be chosen as the maximum of N~0 according to (63) and N~0 according to (65), N~ = N~ ( , N0) as the maximum of the respective variables N~ , and in the same way for M~ = M~ ( , N0, N~ , N ).
The remaining formulae will be established one by one. Let > 0, without loss of generality < 1. Recall proposition 2, where the normalizing constant cM for the probability measure M was defined, and equation (32) of proposition 3, where we obtained an explicit expression for c = limM cM . In analogy to propositions 2 and 3, respectively, we set

M0
cM,M0 := 2 ·
k=0

M k

M k mod 2 1 - M

M0
c,M0 := 2 ·

1 k!

k

mod

2

()

k 2

,

k=0

M · M (1 - M )(1 - M )

k
2,

M  N, M0  {1, . . . , M }. M0  N,

Because of relation (31) it holds that  ~  (0, 1)  M0  N  M~  N  M  M~  {0, . . . , M0}:

(67)

(1 + ~) · ()k 

M 2k ·

M ·M

k

(1-M )(1-M )

 (1 - ~) · ()k

 (1 + ~) 

M

·

M 1-M

 (1 - ~)



1



M ·...·(M -2k+1) M 2k



M ·...·(M -2k) M 2k+1

 1 - ~.

In view of the above we have

 ~  (0, 1)  M~ 0  N  M0  M~0  M~  N  M  M~ :

(68)

1

c,M0 c

 1-~



M  M0



cM,M0 c

-1

+

c cM,M0

-1

 ~.

35

To conclude the preparations, recall that for Z  SM (i), where M  N, i  {0, . . . , M - 1}, we have

#J(Z)  {i, i+1}, and that exactly half of the elements of SM (i) has a discontinuity at 0.

Let ~ > 0. Choose N~

Choose N~0  N such  N such that  N 

that N~ 

c,N0 c
i  {0,

 ..

1 .,

- ~ for N0}:

all

N0



N~0.

Let

N0



N

with

N0



N~0

=

N~0(~).

2 2N +1

i



1-~ Ni

and

1 N -1 Ni i

 1-i! ~.

Let N  N with N  N~ = N~ (~, N0). Choose M~ such that  M  M~  i  {0, . . . , N0}:

2M 2N +1

i

2M -(2N +1) 2N +1

i



(1 - ~)

2M -(2N +1) 2N

i



(1 - ~)2

M N

i



c cM



1-~



M i

M i mod 2

M ·M

1-M

(1-M )(1-M )

i 2



1-~ i!

i

mod

2

()

i 2

,

where (67) has been applied. For N0  N~0(~), N  N~ (~, N0), M  M~ (~, N0, N ) we have

N0
M

UMN (Z)

i=0 ZSN (i)

N0
=

+

M UNM (Z)

i=0 ZSN (i)  J(Z)=i ZSN (i)  J(Z)=i+1

N0



1 2cM

#SN (i)

i=0

2M i 2N +1

M i mod 2

M ·M

1-M

(1-M )(1-M )

+i
2

M (i+1) mod 2

M ·M

1-M

(1-M )(1-M )

i+1 2

as a consequence of proposition 2. According to the choice of N0, N , M and because of (60) it holds that

#SN (i)



4

·

(1

-

~)

·

Ni i!

,

2M 2N +1

i



(1 - ~)2

M N

i,

c cM

 1 - ~.

We therefore have

N0
M

UNM (Z)

i=0 ZSN (i)

N0



2 c

(1

-

~)4

M -1 i

i=0

M i mod 2

M ·M

1-M

(1-M )(1-M )

+i
2

M (i+1) mod 2

M ·M

1-M

(1-M )(1-M )

i+1 2



N0

2 c

(1

-

~)4

M i

M i mod 2

M ·M

1-M

(1-M )(1-M )

i 2

i=0

N0



2 c

(1

-

~)5

1 i!

i

mod

2

()

i 2

i=0

=

(1

-

~)5

c,N0 c

 (1 - ~)6  1 - 6~.

Since ~  (0, 1) was arbitrary, we may set ~ := 6 , thereby establishing (63).

Let ~ > 0, N0  N. Choose N~  N such that  N  N~  i  {0, . . . , N0 +1}:



N  2N0 + 3



1 N



~



2N 2N +1

i



1-~



1+~ 

c cN

 1-~



1+~ 

N 

·

N 1-N

 1-~



1+~ 

N2 

·

N ·N (1-N )(1-N )

i 2

 1 - ~,

which is possible because of (67). Let N  N with N  N~ = N~ (~, N0). Choose M~ such that  M  M~  i  {0, . . . , N0}:

M N 

2M 2N +1

i



(1 - ~)

2M -(N +1) 2N

i



(1 - ~)2

M N

i



2M 2N +1

+

1

i



(1 + ~)

M N

i.

Let N  N~ (~, N0), M  M~ (~, N0, N ), i  {0, . . . , N0}, Z  SN (i). We have to distinguish two cases. In each case the first step will be an application of proposition 2.

36

Case 1. #J(Z) = i, that is fZ has no discontinuity at 0. Then M UNM (Z) - N (Z)

=

1 cM

#UMN (Z)

M i mod 2

M ·M

1-M

(1-M )(1-M )

-i
2

1 N i mod 2

N ·N

cN 1-N

(1-N )(1-N )

i 2



1 cM

2M 2N +1

i(1 - ~)2

 M

i mod 2

 M2

i 2

-

1 cN

(1

+

~)2

 N

i mod 2  N2

i 2



1 c

i mod 2 ()

i 2

(1 - ~)5

M N

iM -(i mod 2+2

i 2

) - (1 + ~)3N -(i mod 2+2

i 2

)

=

1 c

1 Ni

i

mod

2

()

i 2

(1 - ~)5 - (1 + ~)3



1 c

1 Ni

(, , N0) (1 - 5~ - 1 - 7~)

=

-

12 c

~ Ni

(, , N0)



-

12 c

~ Ni

(, , N0 +1),

where

(, , n) := max{k mod 2 ()

k 2

| k  {0, . . . , n}}. On the other hand,

M UMN (Z) - N (Z)



1 cM

2M 2N +1

+1

i(1

+

~)2

1 M

i

i mod 2 ()

i 2

-

1 cN

(1

-

~)2

1 Ni

i mod 2 ()

i 2



1 c

1 Ni

i mod 2 ()

i 2

(1 + ~)4 - (1 - ~)3

Case 2. #J(Z) = i + 1, that is fZ jumps at 0. Then



18 c

~ Ni

(, , N0 +1).

M UMN (Z) - N (Z)

=

1 cM

#UNM (Z)

M (i+1) mod 2

M ·M

1-M

(1-M )(1-M )

-i+1 2

1 N (i+1) mod 2

N ·N

cN 1-N

(1-N )(1-N )

i+1 2



1 c

1 Ni

(i+1) mod 2 ()

i+1 2

(1-~)5 M

-

(1+~)3 N



1 c

1 Ni

(,

,

N0

+

1)

1+7~ N



8 c

~ Ni

(, , N0 +1).

In the same way one obtains

M

UNM (Z)

- N (Z)



16 c

~ Ni

(, , N0 +1).

Set

~ :=

min{

,

c 18(,,N0+1)

},

and

the

proof

of

(64)

is

finished.

Let ~ > 0. Choose N~0  N according to (68) such that

 N0  N~0 N~  N  N  N~ :

1+~ 

cN c



cN,N0 c

 1 - ~.

Making again use of proposition 2 we have for N0  N~0, N  N^ = N^ (~, N0)

N -1
N SN (i)
i=N0 +1

N -1

=

2 cN

N -1 i

i=N0 +1

N i mod 2

N ·N

1-N

(1-N )(1-N )

+i
2

N (i+1) mod 2

N ·N

1-N

(1-N )(1-N )

i+1 2



1 cN

(cN

-

cN,N0 )

=

1 - c cN,N0
cN c



~ 2

.

This establishes (65).

37

B.2 Proof of proposition 4

The first step is to show that the closure of {~M | M  N} is compact in the Prohorov topology of M1+(D0). According to theorem 4 it is sufficient to prove tightness of {~M | M  N}, that is
 > 0  K~  D0 compact : inf{~M (K~ ) | M  N}  1 - ,

where compactness means compactness with respect to the Skorokhod topology of D0. Recall from section 4.1 the definition of ~M . For all natural numbers N0 < N  M we have

~M ~M UMN

 M

N0
UMN (Z)
i=0 ZSN (i)

,

where UNM :=

UNM (Z).

Z SN

Let > 0. According to (63) we can find natural numbers N0 < N  M~ such that for all M  M~ :

N0
M

UMN (Z)

i=0 ZSN (i)

 1- .

Fix N , M~ . In analogy to the definition of UMN we set

A~ :=

f  D0 #J(f ) = #J(Z)      :

sup

|(s) - s|



r 2N +1



f   = fZ

.

Z SN

s[-r,0]

Then

#J(f )



N

and

f



2r N (2N +1)

for

all

f



A~,

and

by

lemma

9

we

see

that

cl(A~),

the

closure

of

A~,

is compact with respect to the Skorokhod topology. By definition we have UMN  A~ for all M  M~  N .

Define

M~ -1

K~ :=

~M (SM )  cl(A~).

M =1

Then K~ is compact in the Skorokhod topology, and with M  N it holds that



~M

(K~ )



~M ~M

~M (SM ) ~M (UNM )

=1 1-

if M  {1, . . . , M~ - 1}, if M  M~ .

Hence, inf{~M (K~ ) | M  N}  1 - . Since > 0 was arbitrary, we now know that {~M | M  N} is relatively compact.

Let (~M(j))jN be a weakly convergent subsequence of (~M )MN. Denote by ~ the limit of (~M(j)) in the Prohorov topology. We have to check that ~M w ~ as M  . Because of theorem 3 it is sufficient to show that
 d~M M-  d~    Cb(D0) uniformly continuous.
D0 D0
Let  be a bounded and uniformly continuous real function on D0 and set K := sup{ |(f )| | f  D0}. With M  N it holds that

 d~M -  d~   d~M -  d~M(j) +  d~M(j) -  d~ for all j  N.

The convergence ~M(j) w ~ implies |  d~M(j) -  d~|  0 as j  . We therefore have to show

that

 > 0  j0  N  j  j0  M~  N  M  M~ :

 d~M -  d~M(j)  .

38

Let > 0, j0  N. Choose natural numbers N0 = N0( ), N~ = N~ ( ) according to (66). Choose

 = ( , ) > 0 such that |(f ) - (g)|  for all f, g  D0 with dS(f, g)  . Let j  N be big enough

so

that

j



j0,

M (j)



N~

and

r 2M (j)+1

 .

Set

N

:=

M (j).

Recalling the definition of our approximation sets13 we see that dS(fZ , fZ~)   for all Z  SN (i) and

Z~  UNM (Z) if i  {0, . . . , N0} and M  N . By the choice of  this implies that |(fZ ) - (fZ~)|  for

all such Z, Z~.

Finally, choose a natural number M~ = M~ ( , N0, N~ , N ) according to (66). Then for M  M~

 d~M -  d~N

N0
 2K · +

(fZ ) N (Z) -

(fZ~) M (Z~)

i=0 ZSN (i)

Z~UMN (Z)

N0
 2K · +

(fZ ) · M UNM (Z) - N (Z) +

(fZ~) - (fZ ) M (Z~)

i=0 ZSN (i)

Z~UNM (Z)

N0
 2K · + ·

M UMN (Z)

i=0 ZSN (i)

+ K · ·

N0
N -i #SN (i)
i=0

N0

 2K · + · M SM + 4K · ·

N -i

N -1 i

i=0

 2K · +

+ 4K · ·


1 i!
i=0

= (2K + 1 + 4Ke) · .

B.3 Proof of proposition 7
Clearly, F(0) = 0 = F (0) for all   (0, 1). With M  N let P~M be the probability measure on B(D) as defined in section 4.1. Recall that P~M is the measure induced by the sequence of current states of the Markov chain XM under PM , i. e. in the stationary regime. For   (0, 1), M  N set
FM (t) := P~M   t A~ , t  [0, ).
From proposition 6 we know that P~M w P~ as M tends to infinity. Check that for   (0, 1), t  (0, ) the events A~, A~  {  t} are P~-continuity sets of B(D). An application of theorem 3 yields
(69) P~M   t | A~ M- P~   t | A~ , i. e. FM (t) M- F(t) for all   (0, 1), t > 0.
For all t > 0,   (0, 1), M  N we have
F(t) - F (t)  F(t) - FM (t) + FM (t) - F (t) .
In view of (69) it is sufficient to show that for each t > 0 and each  > 0 there are 0  (0, 1), M0  N such that
(70) FM (t) - F (t)   for all   (0, 0), M  M0.
13The sets UMN (Z) were defined at the beginning of section 4.2.

39

As in section 3.2, let (YnM )n{-M,-M+1,...} be the random sequence of current states on (, F ) at discretization degree M  N. Let   (0, 1) and let M  N be such that  · M  1. Set

Aj,M := Y-MM = -1, . . . , Y-Mj-1 = -1, Y-Mj = 1, . . . , Y0M = 1 , j  {0, . . . , M -1}.

Notice that Aj,M is an event in F . The corresponding event in B(D) is given by

A~j,M :=

f  D

 l  {j +1, . . . ,

M }: f

-

l M

r

= -1



 l  {0, . . . , j} :

f

-

l M

r

=1

.

For all   (0, 1) and all M  N such that  · M  1 it holds that

PM Aj,M = P~ M A~j,M A~ = A~0,M  . . .  A~,MM -1

for all j  {0, . . . , M - 1}, P~M -almost surely.

In analogy to (26), the definition of the residence time distribution LM (.) of discretization degree M , we
set Lj,M (k) := PM Y0M = 1, . . . , YkM-1 = 1, YkM = -1 Aj,M , k  N.

Then, by construction of FM , for all   (0, 1) and all M  N such that  · M  1 we have

FM (t) =

t r

M

k=1

M -1
P~ M A~j,M
j=0

A~ · Lj,M (k),

t > 0.

It is not necessary to calculate the probabilities P~M (A~j,M | A~). Instead, proceeding in a way very much as in section 3.2, we will estimate limes inferior and limes superior of M · LjM,M ( qM ) as M tends to infinity, where q > 0 and (jM )  N0 is any sequence such that jM  {0, . . . , M -1} for all M  N. The estimates will be uniform in   (0, 0] for any small 0 > 0.
In analogy to (27), the definition of the tail constant KM , we set for   (0, 1) and M big enough

Kj,M := PM Y-MM = 1, . . . , Y-Mj-1 = 1, Y-Mj = 1, . . . , YMM- M = 1 Aj,M .

Because of the shift invariance of Y M under PM , the above definition of Kj,M is really analogue to that of KM . Exploiting the stationarity of XM under PM , we obtain

M -j M- M +j+1

Kj,M

=

M (-1, . . . , -1, 1, . . . , 1)

.

M {(, . . . , , -1, . . . , -1, 1, . . . , 1)}

M- M

M -j

j+1

As a consequence of proposition 2, the formula for the stationary distributions M , we see that Kj,M is the same for all j  {0, . . . , M -1}. Proceeding as in the derivation of (29) we find that

(71) Kj,M =

2

1+

~M ~M

1 + ~M M- M + 1 -

~M ~M

1 - ~M M- M

=: K,M .

In order to calculate Lj,M we apply proposition 2 again in a way similar to that of section 3.2. Let   (0, 1), let M  M be such that M  1, and j  {0, . . . , M -1}. Then for k  {1, . . . , M- M }

(72a)

Lj,M (k)

=

~M 2

· K,M

·

~M (1 + ~M )M- M -k + (1 - ~M )M- M -k

+ ~M (1 + ~M )M- M -k - (1 - ~M )M- M -k .

40

While Lj,M (k) in (72a) does not vary with j as long as k  M - M , for k  {M - M +1, . . . , M -j} it holds that

(72b)

Lj,M (k) = K,M · M · (1 - M )k-M+ M -1,

and for k  M -j +1 we have

(72c)

Lj,M (k) = K,M · M · (1 - M ) M -j · (1 - M )k-M+j-1.

Now, let the discretization degree M tend to infinity, where we assume that scaling relation (31) holds for some rates , . From (71) we see that

(73)

K, :=

lim
M 

KM,

=

2

1+

 

e(1-) +

1-

 

e-(1-)

=

 cosh

(1 - )

 +  sinh

(1 - )

.

Let q > 0 and let (jM )  N0 be any sequence such that jM  {0, . . . , M - 1} for all M  N. If q  (0, 1-], then from (72a) we find that

(74a)

lim
M 

M

·

LjM,M

qM

=

 · K, ·

 cosh (1--q)

 +

sinh

 (1 -  - q)

.

If q  (1-, 1), then a rough estimate of (72b) and (72c), respectively, yields

(74b)

lim sup M · LjM,M
M 

lim inf
M 

M

· LjM,M

qM qM

 max{, } · K,,  min{, } · K, · e- · e-(q+-1).

On the other hand, if q  1, then by (72c) we have

(74c)

lim sup M · LjM,M
M 

lim inf
M 

M

· LjM,M

qM qM

  · K, · e-(q-1),   · K, · e- · e-(q+-1).

Notice that convergence in (73) as well as in (74) is uniform in   (0, 0] for arbitrary 0  (0, 1). If we
let  tend to zero, we recover the residence time distribution density fL of proposition 3. Taking the time discretization into account, we obtain f~L as given by (36) instead of fL.
Given t > 0,  > 0, uniform convergence of (Lj,M ) in  and dominated convergence of the corresponding residence time distribution densities over the interval (0, t] imply that we can find 0  (0, 1) and
M0  N such that inequality (70) is fulfilled. The assertion of proposition 7 then follows.

41

References
V. S. Anishchenko, A. B. Neiman, F. Moss, and L. Schimansky-Geier. Stochastic resonance: noiseenhanced order. Physics ­ Uspekhi, 42(1):7­36, 1999.
R. Benzi, G. Parisi, A. Sutera, and A. Vulpiani. The mechanism of stochastic resonance. Journal of Physics A, 14:453­457, 1981.
R. Benzi, G. Parisi, A. Sutera, and A. Vulpiani. Stochastic resonance in climatic change. Tellus, 34: 10­16, 1982.
Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, New York, 2nd edition, 1999.
A. Bovier, M. Eckhoff, V. Gayrard, and M. Klein. Metastability in reversible diffusion processes I. sharp asymptotics for capacities and exit times. Technical Report 767, Weierstrass-Institut für angewandte Analysis und Stochastik (WIAS), 2002a.
A. Bovier, M. Eckhoff, V. Gayrard, and M. Klein. Metastability in reversible diffusion processes II. precise asymptotics for small eigenvalues. Technical Report 768, Weierstrass-Institut für angewandte Analysis und Stochastik (WIAS), 2002b.
Pierre Brémaud. Markov chains: Gibbs fields, Monte Carlo simulation, and queues. Springer-Verlag, New York, 1999.
D. Curtin, P. Hegarty, S. D. Goulding, J. Houlihan, Th. Busch, C. Masoller, and G. Huyet. Distribution of residence times in bistable noisy systems with time-delayed feedback. Physical Review E, 70(3): 031103, 2004.
Richard Durrett. Stochastic Calculus. A Practical Introduction. Probability and Stochastics Series. CRC Press, Boca Raton (FL), 1996.
Stewart N. Ethier and Thomas G. Kurtz. Markov Processes. Characterization and Convergence. John Wiley & Sons, New York, 1986.
M. I. Freidlin and A. D. Wentzell. Random perturbations of dynamical systems, volume 260 of Grundlehren der mathematischen Wissenschaften. Springer-Verlag, New York, 1998.
Luca Gammaitoni, Peter Hänggi, Peter Jung, and Fabio Marchesoni. Stochastic resonance. Reviews of Modern Physics, 70(1):223­287, 1998.
Samuel Herrmann, Peter Imkeller, and Ilya Pavlyukevich. Stochastic resonance: non-robust and robust tuning notions. February 2003.
Samuel Herrmann, Peter Imkeller, and Dierk Peithmann. Large deviations for diffusions with time periodic drift and stochastic resonance. 2005.
J. Houlihan, D. Goulding, Th. Busch, C. Masoller, and G. Huyet. Experimental investigation of a bistable system in the presence of noise and delay. Physical Review Letters, 92(5):050601, 2004.
Peter Imkeller and Ilya Pavlyukevich. Model reduction and stochastic resonance. Stochastics and Dynamics, 2(4):463­506, 2002.
Jean Jacod and Albert N. Shiryaev. Limit Theorems for Stochastic Processes, volume 288 of Grundlehren der mathematischen Wissenschaften. Springer-Verlag, Berlin - Heidelberg - New York, 1987.
Ioannis Karatzas and Steven E. Shreve. Brownian Motion and Stochastic Calculus. Springer-Verlag, New York, 2nd edition, 1991.
A. Krawiecki and J.A. Holyst. Stochastic resonance as a model for financial market crashes and bubbles. Physica A, 317(3-4):597­608, 2003.
Robert Lorenz. Weak approximation of stochastic delay differential equations with time series. 2003.
42

Cristina Masoller. Distribution of residence times of time-delayed bistable systems driven by noise. Physical Review Letters, 90(2):020601, 2003.
B. McNamara and K. Wiesenfeld. Theory of stochastic resonance. Physical Review A, 39(9):4854­4869, 1989.
Salah-Eldin A. Mohammed. Stochastic functional differential equations. Pitman Publishing, London, 1984.
Salah-Eldin A. Mohammed. Stochastic differential systems with memory: Theory, examples and applications. In L. Decreusefond, Jon Gjerde, B. Øksendal, and A. S. Üstünel, editors, Stochastic Analysis and Related Topics VI. Papers from the Sixth Oslo-Silivri Workshop on Stochastic Analysis held in Geilo, Boston, 1996. Birkhäuser.
C. Nicolis. Stochastic aspects of climatic transitions ­ responses to periodic forcing. Tellus, 34:1­9, 1982. Toru Ohira and Yuzuru Sato. Resonance with noise and delay. Physical Review Letters, 82:2811­2815,
1999. Ilya E. Pavlyukevich. Stochastic Resonance. PhD thesis, Humboldt-Universität zu Berlin, 2002. Brian F. Redmonda, Victor G. LeBlanc, and André Longtin. Bifurcation analysis of a class of first-order
nonlinear delay-differential equations with reflectional symmetry. Physica D, 166:131­146, 2002. Michael Scheutzow. Qualitatives Verhalten der Lösungen von eindimensionalen nichtlinearen stochas-
tischen Differentialgleichungen mit Gedächtnis. PhD thesis, Fachbereich Mathematik der Universität Kaiserslautern, 1983. Michael Scheutzow. Qualitative behaviour of stochastic delay equations with a bounded memory. Stochastics, 12(1):41­80, 1984. Daniel W. Stroock and S. R. Srinivasa Varadhan. Multidimensional diffusion processes, volume 233 of Grundlehren der mathematischen Wissenschaften. Springer Verlag, Berlin - Göttingen - Heidelberg, 1979. L. S. Tsimring and A. Pikovsky. Noise-induced dynamics in bistable systems with delay. Physical Review Letters, 87(25):250602, 2001.
43

SFB 649 Discussion Paper Series
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Nonparametric Risk Management with Generalized Hyperbolic Distributions" by Ying Chen, Wolfgang Härdle and Seok-Oh Jeong, January 2005.
002 "Selecting Comparables for the Valuation of the European Firms" by Ingolf Dittmann and Christian Weiner, February 2005.
003 "Competitive Risk Sharing Contracts with One-sided Commitment" by Dirk Krueger and Harald Uhlig, February 2005.
004 "Value-at-Risk Calculations with Time Varying Copulae" by Enzo Giacomini and Wolfgang Härdle, February 2005.
005 "An Optimal Stopping Problem in a Diffusion-type Model with Delay" by Pavel V. Gapeev and Markus Reiß, February 2005.
006 "Conditional and Dynamic Convex Risk Measures" by Kai Detlefsen and Giacomo Scandolo, February 2005.
007 "Implied Trinomial Trees" by Pavel Cízek and Karel Komorád, February 2005.
008 "Stable Distributions" by Szymon Borak, Wolfgang Härdle and Rafal Weron, February 2005.
009 "Predicting Bankruptcy with Support Vector Machines" by Wolfgang Härdle, Rouslan A. Moro and Dorothea Schäfer, February 2005.
010 "Working with the XQC" by Wolfgang Härdle and Heiko Lehmann, February 2005.
011 "FFT Based Option Pricing" by Szymon Borak, Kai Detlefsen and Wolfgang Härdle, February 2005.
012 "Common Functional Implied Volatility Analysis" by Michal Benko and Wolfgang Härdle, February 2005.
013 "Nonparametric Productivity Analysis" by Wolfgang Härdle and Seok-Oh Jeong, March 2005.
014 "Are Eastern European Countries Catching Up? Time Series Evidence for Czech Republic, Hungary, and Poland" by Ralf Brüggemann and Carsten Trenkler, March 2005.
015 "Robust Estimation of Dimension Reduction Space" by Pavel Cízek and Wolfgang Härdle, March 2005.
016 "Common Functional Component Modelling" by Alois Kneip and Michal Benko, March 2005.
017 "A Two State Model for Noise-induced Resonance in Bistable Systems with Delay" by Markus Fischer and Peter Imkeller, March 2005.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

