BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2011-016
Oracally Efficient Two-Step Estimation of Generalized
Additive Model
Rong Liu* Lijian Yang** Wolfgang Karl Härdle***
* University of Toledo, USA ** Michigan State University, USA *** Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Submitted to the Annals of Statistics arXiv: math.PR/0000000
ORACALLY EFFICIENT TWO-STEP ESTIMATION OF GENERALIZED ADDITIVE MODEL 
By Rong Liu1, Lijian Yang2,3 and Wolfgang K. Ha¨rdle4
1University of Toledo, 2Soochow University, 3Michigan State University, 4Humboldt-Universita¨t zu Berlin
Generalized additive models (GAM) are multivariate nonparametric regressions for non-Gaussian responses including binary and count data. We propose a spline-backfitted kernel (SBK) estimator for the component functions. Our results are for weakly dependent data and we prove oracle efficiency. The SBK techniques is both computational expedient and theoretically reliable, thus usable for analyzing high-dimensional time series. Inference can be made on component functions based on asymptotic normality. Simulation evidence strongly corroborates with the asymptotic theory.

1. Introduction. An effective semiparametric regression tool for high dimensional data is the additive model introduced by Hastie and Tibshirani (1990), which stipulates that

(1.1)

E

(

X)

=



(X)

,



(X)

=



+


=1



()

for set

a{re,sXpoT n}se=1

and a predictor vector X = = {, 1, ..., }=1 of size

(1, ..., )T.  is observed

When which

a data follows

model (1.1), unknown component functions { ()}=1 can be estimated

via kernel, B spline and smoothing spline with a univariate convergence

rate. This fact together with the interpretability of the functions has not

only led to a remedy of the "curse of dimensionality", but also led to in-

creased practical applications of additive models. A list of articles on addi-

tive models and related works include, among others, Stone (1985), Stone

(1994), Huang (2004) and Xue and Yang (2006a) for B spline methods;

Tjøstheim and Auestad (1994), Linton and Nielsen (1995), Linton (1997),

Fan et al. (1998), Yang et al. (1999), Xue and Yang (2006b) and Yang et al.

Supported in part by the Deutsche Forschungsgemeinschaft through the CRC 649 "Economic Risk", by NSF awards DMS 0706518, 1007594 and by a Credit Rating Grant from the National University of Singapore Risk Management Institute.
AMS 2000 subject classifications: Primary 62M10; secondary 62G08 JEL classification: C00, C14, J01, J31 Keywords and phrases: Bandwidths, B spline, knots, link function, mixing, NadarayaWatson estimator
1

2 R. LIU, L. YANG AND W. K. HA¨ RDLE

(2006) for kernel methods; and more recently, spline-backfitted kernel (SBK)

smoothing methods of Wang and Yang (2007), Wang and Yang (2009),

Liu and Yang (2010) and Ma and Yang (2011), the spline-backfitted spline

(SBS) smoothing method of Song and Yang (2010).

Certain types of responses  , however, such as binary or Poisson re-

sponses, are much m{ore framework, the data ,

XapTp}ro=p1riaarteelgyendeersactreibdeadccboyrdGinAgMtos.

In

the

GAM

(1.2)

E ( X = x) =  { (x)} ,

with  (x) of additive structure as in (1.1), and a given function  which relates  (x) to the conditional variance function 2 (x) = Var ( X = x) via the equation 2 (x) =  ()  { (x)}, in which  () is a nuisance parameter that quantifies overdispersion. The inverse of  is called the link function. For binary responses, one commonly takes ()-1 () = log {/ (1 - )},
the logistic link to conduct logistic regression, while for Poisson regression, ()-1 () = log , the log link. If one takes ()-1 () = , the identity link,
model (1.2) becomes model (1.1).
Model (1.2) has its origin in the special case where the probability density
function of  conditional on X with respect to a fixed -finite measure forms an exponential family

 ( X,  ) = exp [{ (X) -  { (X)}} / () +  (, )] .

For the theoretical to assume that the

ddeavtealo{pm,eXntT i}n=t1hicsopmaepserf,rohmowseuvcehr,eixtpiosnneonttinalecfeasmsailryy,

but only that the conditional variance and conditional mean are linked by

the following equation

Var

(

X

=

x)

=



()



[()-1

{E

(

X

=

] x)}

.

We can also write model (1.2) in the usual regression form

(1.3)

 =  { (X)} +  (X) 

for conditional white noise  that satisfies E (X) = 0, E (2 X) = 1. For identifiability, one requires that

(1.4)

E { ()} = 0, 1    

for

unique

additive

representations

of



(x)

=



+


=1



().

As

in

most

works on nonparametric smoothing, estimation of the functions { ()}=1

GENERALIZED ADDITIVE MODEL

3

is conducted on compact sets. Without lose of generality, let the compact set be  = [0, 1].

Methods for the generalized additive model (1.2) are much less developed

in comparison to the additive model (1.1), see for instance, the B spline

method of Stone (1986) and Xue and Liang (2010), the kernel method of

Linton and H¨ardle (1996) and Yang et al. (2003), and the two-stage meth-

ods of Horowitz and Mammen (2004) and Horowitz et al. (2006). Generally

speaking, the proposed kernel methods are too computationally intensive

for high dimension , thus limiting their applicability to a small number of

predictors. On the other hand, B spline methods provide only convergence

rates but no asymptotic distributions, so no measures of confidence can be

assigned to the estimators. In the case of the additive model (1.1), the SBK

method of Wang and Yang (2007) combines the advantages of both kernel

and spline methods and the result is balanced in terms of theory, computa-

tion, and interpretation. The basic idea of the SBK method for the additive

model (1.1) is to first project the data with B-splines into a space of func-

tions with additive structure and then to apply kernel smoothing to the

projected objects.

In this paper we extend the SBK method to model (1.2). The desired aim

is to achieve orcale efficiency. If all the nonparametric functions of the last  - 1 variables, { ()}=2 and the constant  were known by an "oracle", one could simply plug these in and estimate the only unknown functions

1 (1) by maximizing the log-likelihood function with kernel weights com-

puted from variable 1. This estimator of 1 (1) is called "oracle smoother"

or "infeasible estimator", and it does not suffer from the "curse of dimen-

sionality" since the smoothing operation involves w.l.o.g. only 1. The pro-

posed SBK method pre-estimates functions { ()}=2 and constant 

by linear splines and then use these estimates as proxies for the unknown

functions { ()}=2 and constant . The main contribution is proving

that ..

t(he-e1/r2rolorgcau).seCdobnysetqhuiesnatplyp,rtohxeimSaBtKionesitsimunaitfoorrmislyunniefgolrimgilbyle(oovfeorrtdheer

data range) asymptotically equivalent to the "oracle smoother", automati-

cally inheriting all oracle efficiency properties of the latter. Our proof relies

on "reducing bias by undersmoothing" and "averaging out the variance",

accomplished with the joint asymptotics of kernel and spline functions for

realizations of geometrically strongly mixing time series. These results are es-

tablished under substantially greater technical difficulty than existng works

on additive model such as Wang and Yang (2007), Wang and Yang (2009),

Liu and Yang (2010), Ma and Yang (2011), and Song and Yang (2010). The

additional complication is due to the lack of decomposition of spline esti-

4 R. LIU, L. YANG AND W. K. HA¨ RDLE

mation error into the sum of a bias and a noise term when the link function ()-1 is nonlinear.

A similar result was proved in Horowitz and Mammen (2004) for i.i.d.

rather than dependent data and only pointwise rates instead of uniform rates

were derived. It is also worth emphasizing that although Horowitz and Mammen

(2004) had used the B-spline estimator for the first stage in simulation, their

proof is valid only for using the orthogonal series estimator in stage one.

Another major contribution of this paper is establishing that the spline-

obfacokrfidtetrede s(tim-1a/t2o)r

of of

the the

additive constant  infeasible estimator

is within an negligible error and thus also oracally effi-

cient. As far as we know, our estimator of the additive constant  is the only

one which has an asymptotic distribution with -1/2 rate.

The paper is organized as follows. In Section 2 we discuss the assump-

tions of the model (1.2). In Section 3, we introduce the oracle smoother or

infeasible estimator for 1 (1) and for , and state their asymptotics. In

Section 4 we introduce the SBK estimator for 1 (1) and spline-backfitted

estimator for  and present their asymptotic oracle efficiencies by showing

that they differ from their infeasible counterparts only negligibly. In Section

5 we describe implementation steps of the estimators. In Section 6 we apply

the methods to simulated and real examples. All technical proofs are given

in the Appendix.

2. Model assumptions. Following Stone (1985), p. 693, the space of -centered square integrable functions on [0, 1] is

0

=

{ 

:

E

{

()}

=

0,

E

{2

} ()

<

} +

.

Next define the model space , a collection of functions on  as



=

{ 

(x)

=



+


=1



(x)

;





} 0

,

in which  is finite constant. The constraints that E { ()} = 0, 1    

ensure unique additive representation of  as expressed in (1.4), but are

not the

necessary empirical

for the definition expectation, E 

of =

space
=1

. In what follows, denote by E  (X) /. We introduce two inner

products on . For functions 1, 2  , the theoretical and empirical inner

products are E {1 (X) 2 122, = E

defined respectively as 1, 2 = E {1 (X) 2 (X)}. The corresponding induced norms are 12 (X). More generally, we define  = E 

(X)}, 122 (X) .

1, 2 = E 12

 = (X),

of

T-hthroourgdheorustmtohoetphafpuenrc,tfioornaansyc(om)[p,ac]t=int{erva(l)[,

], 

we d}enote the space [, ] , and the class

GENERALIZED ADDITIVE MODEL

5

of Lipschitz continuous functions for constant  > 0 as Lip ([, ] , ) =

{  () -  ()    -  , ,   [, ]}. We mean by "" both sides

having the same order as   . For any vector x = (1, 2,    , )T,

we denote

(
=1



the supremum and  )1/
. In particular, we

norms as x = max1  and x use x to denote the Euclidean norm.

=

We need the following Assumptions on the data generating process.

(A1) The additive component functions   (1) [0, 1] , 1     with 1  (2) [0, 1],   Lip ([0, 1] , ) = 2     for some constant  > 0.
(A2) The inverse link function  satisfies:   2 () ,  () > (0,   ) while for a compact interval  whose interior contains  [0, 1] ,
 > max  ()  min  () >  for constants  >  > 0. (A3) The conditional variance function 2 (x) is (measura)ble and bounded.
The errors {}=1 satisfy E () = 0, E 2+   for some
  (1/2, 1] and the sequence of -fields  =  {(X) ,   ; ,    - 1} for  = 1, . . . , . (A4) The density function  (x) of (1, ..., ) is continuous and

0 <   infx  (x)  supx  (x)   < .

The marginal densities  () of  have continuous derivatives on

[0, 1] as well as the uniform upper bound  and lower bound  .

(A5)

Constants 0, all , with the

0  (0, +) exist such tha{t  -mixing coefficients for Z

(=)(XT,0)-}0=1

holds for defined

as

 () = sup P (  ) - P () P () ,   1.
 {Z ,}, {Z ,+}

Assumptions (A1), (A2) and (A4) are standard in the GAM literature,

see Stone (1986), Xue and Liang (2010), while Assumptions (A3) and (A5)

are the same for weakly dependent data as in Wang and Yang (2007),

Liu and Yang (2010). Assumption (A2) implies that a compact interval 

exists +

1w(h[o0s,e1]in-t1e)riowrhceornetain1s(x11)([=0,1+])and=2that

's ()

interior contains with  1 = (2, ...,

).

3. Oracle smoothers. We now introduce what is known as the oracle
smoother in Wang and Yang (2007) as a benchmark for evaluating the estimators. If the last -1 components { ()}=2 were w.l.o.g. known by an "oracle", then the only unknown component 1 (1) may be estimated by

6 R. LIU, L. YANG AND W. K. HA¨ RDLE

the following procedure. Define for each 1  [, 1 - ] a local log-likelihood function ~ () = ~ (, 1) ,    as

(3.1)

-1


=1

[

{

+



1

(X

1)}

-



{

+



1

(X

1)}]



(1

-

1)

with



1 (X 1)

=



+


=2



(X)

and

define

the

oracle

smoother

of

1 (1) as

(3.2)

~K,1 (1) = argmax ~ (, 1) .

in which  () =  (/) / for a kernel function  and bandwidth  that satisfy

(A6) The kernel function  is a symmetric probability density, supported on

[-1, 1] and   Lip ([-1, 1] , ) for some positive constant  > 0.

A 

=cons(tan-t1/5)

,

>0 -1

=exist(s1s/u5c(hlotghat)th)e.

bandwidth



=



satisfies

In what follows, we denote 22 =  2 () , 2 () =   () 2. Denote the higher order error of ~K,1 (1) as

K,1 (1)

=

-~K-,11(1)=-1 1((11)

- -

bias1 1) 

(1) 2/1 (X) /1

(1) (1)

,

with the scale function 1 (1) and bias function bias1 (1) defined as

(3.3)

1

(1)

=

1

(1)

E

{

{

(X)}

1

=

} 1

,

bias1 (1)

=

2

( )

[1

(1)



(1)

E

[

{

(X)}

1

=

] 1

(3.4)

+1 (1)  (1) - {1 (1)}2 

 1
(1)

E E

[ [

{ (X)} { (X)}

1 1

= =

] 1
]] 1

.

THEOREM 1. Under Assumptions (A1)-(A6), as    ()
sup K,1 (1) = .. -1/21/2 log  .
1[,1-]
( ) In particular, sup1[,1-] ~K,1 (1) - 1 (1) = .. log /  .

GENERALIZED ADDITIVE MODEL

7

THEOREM 2. Under Assumptions (A1)-(A6), for any 1  [, 1 - ], as   , the oracle kernel smoother ~K,1 (1) given in (3.2) satisfies

 

{ ~K,1

(1)

-

1

(1

)

-

bias1

(1

)

2

/1

} (1)

()   0, 1 (1)-1 12 (1) 1 (1)-1

in which (3.5)

12

(1)

=

1

(1)

E

{2

(X)

1

=

} 1

 22

.

The same oracle idea applies to the constant as well. Define the loglikelihood function

~

()

=

-1


=1

[

{

+





(X)}

-



{

+





(X)}]

,

where   (X) = ~ = argmax ~

().=C1 lear(ly,~) .(~T)h=e

infeasible 0.

estimator

of



is

defined

as

THEOREM 3. Under Assumptions (A1)-(A5), as   

~

-



=

[ E



{

(X)}]-1

-1


=1



(X)



+

.

( -1

(log

) )2

.

Although the oracle smoother ~K,1 (1) enjoys the desirable theoretical properties in Theorems 1 and 2, it not useful statistics as its computation is based on the knowledge of unavailable functions { ()}=2 and the unknown constant , the same can be said of ~. These benchmarks, however,
motivate the spline-backfitted estimators that we will introduce in the next
section.

4. Spline-backfitted kernel estimators. In this section we describe how the unknown functions { ()}=2 and constants  can be preestimated by linear splines and how the estimates are used to construct

the SBK estimator. First, we introduce the space of linear splines defined

in Liu and Yang (2010). Let 0 = 0 < 1 <    <  < +1 = 1 denote a

sequence of equally spaced points, note by  = ( + 1)-1 the width

called interior knots[, on of each subinterval  ,

inter]val +1 , 0

[0, 

1]. De  

and denote the degenerate knots -1 = 0, +2 = 1. Assume that

(A7) The number of interior knots satisfies:   1/4 log , i.e.,  1/4 log      1/4 log  for some positive constants  , .

8 R. LIU, L. YANG AND W. K. HA¨ RDLE

For  = 0, . . . ,  + 1, define the linear B spline basis as



 ( + 1)  -  + 1 , -1    



()

=

(1 -  -   /)+

=





+ 1 - ( 0

+ 1) 

, ,

    +1 otherwise

,

the space of -empirically centered linear spline functions on [0, 1] as

0,

=

{ 

:



()

=

 +1
 =0

 

()

,

E

{

()}

=

} 0

,

1







,

and the space of additive spline functions on  as

0

=

{ 

(x)

=



+


=1



() ;

}   ,   0, ,

which is equipped with the likelihood function ^ () =

empirical

-1


=1

inner product , 2,. Define the log[ (X) -  { (X)}] ,   0, which

according to Lemma 14 of Stone (1986), has a unique maximizer with prob-

ability approaching 1. The multivariate function  (x) is then estimated by

the additive spline function

^ (x) = argmax0 ^ () .

Since

^ (x)



0,

one

can

write

^ (x)

=

^ +


=1

^

()

for

^





and

^ ()  0,. Next define the log-likelihood function

(4.1)

^ ()

=

1 

 [ {
=1

+

^

1

(X

1)}

-

 {

+

^

1

(X

1)}] 

(1

-

1)

where

^

1

(X

1)

=

^

+


=2

^

().

Define

the

SBK

estimator

as:

(4.2)

^SBK,1 (1) = argmax ^ () .

THEOREM 4. Under Assumptions (A1)-(A7), as   , ^SBK,1 (1) is oracally efficient,
() sup ^SBK,1 (1) - ~K,1 (1) = .. -1/2 log  .
1[0,1]

Theorem 4 follows from (A.29), Lemmas A.15 and A.16. The following corollary is a consequence of Theorems 1, 2 and 4.

GENERALIZED ADDITIVE MODEL

9

COROLLARY 1. Under Assumptions (A1)-(A7), as   , the SBK estimator ^SBK,1 (1) given in (4.2) satisfies
( ) sup ^SBK,1 (1) - 1 (1) = .. log / 
1[,1-]

and for any 1  [, 1 - ], with bias1 (1) as in (3.4) and 1 (1) in (3.3)

 

{ ^SBK,1

(1)

-

1

(1)

-

bias1

(1)

2/1

} (1)

()   0, 1 (1)-1 12 (1) 1 (1)-1 .

^ (D=)e1fi=n^e-(n1ext)=.th1Se[imsip{lalirn+teo-b^Tahcke(Xfiortet)em}d-4e,stth{ime m+atao^irn^r(eXs=ul)t}as]rhginmowwasxhtichhat^^t h(e()Xdiwff)iet=rhence between ^ and its infeasible counterpart ~ is asymptotically negligible.

is

THEOREM 5. oracally efficient,

iU.en.,derA (s^su-m~p)tio n0s

(A1)-(A5) and hence

and

(A7),

as





,

^

 

(^

-

)





( 0,



()1/2

[ E



{

(X)}]-1/2)

.

5. Implementation. We implement our procedures with the following

rule-of-thumb number of interior knots

( 

)

 =  = min 1/4 log  + 1, /4 - 1/ - 1

which satisfies (A8), i.e.  =   1/4 log , and ensures that the number

of parameters in the linear least squares problem is less than /4, i.e., 1 +

 ( + 1)  /4. For more discussion, see Portnoy (1997).

According to Corollary 1, the asymptotic distribution of the estimator

^SBK, () depends not only on the functions bias () / () and  ()-1 2 ()  ()-1, but also crucially on the choice of bandwidths . Define the optimal bandwidth of , denoted by ,opt, as the minimizer

of the asymptotic mean integrated squared errors (AMISE) of

{^(),  = 1, . . . , }:

AMISE (^)

=



[{ bias

()

2/

()}2

] + ()-1 2 ()  ()-1 / ()  () .

10 R. LIU, L. YANG AND W. K. HA¨ RDLE

By letting  AMISE (^) / = 0, one obtains an optimal bandwidth ,opt:

,opt

=

{ -1

 4

{b(ias)-(1)2/()()(}2)- (1)()  }1/5

,

which is approximated by

^ ,opt

=

{ -1

4 =1=1{ b(ias)(-12)

()  ()-1 / ()}2

}1/5

,

where



()

=



()

E

[

{

(X)}



=

] 

and

2 () bias ()

= =

 2

(()){E{2((X))(=) E[} {

22 , (X)}



=

] 

+-{(())}(2)()

E [ E [

{ {

(X)} (X)}

 

= =

] 
]} 

.

E

{Th2e(Xfo)llowin=g

esti}m, aEt[ion {met(hXo)d}sfor

the terms  (),  (), = ], E [ { (X)}  =

 (), ] and

 

E [ { (X)} 

=

]

are

proposed.

The

final

bandwidth

is

denoted

as ^,opt.

1).33==T21he^(de,r-,iv1a)ti^-v1e,+,fu 3nct-io2=n++s43 6^,(,=+(43 )^1a-,n,d(,-13(-)2an),da-r3e)

estimated as where {^,,

}=+03

maximize:


=1

[ 

{3
=0

,, 

+

 +3
=4

,,

(

-

} ,-3)3

-

{3
=0

,, 

+

 +3
=4

,,

(

-

}] ,-3)3

where min  = ,0 <    < ,+1 = max .

2).3=E0[^{,,(X +)}=+4=3 ^,],is(est-imat,e-d3a)3s by minimizing


=1

[ 

{^

(X)}

-

{3
=0

,, 

+

 +3
=4

,,

(

-

-3)3}]2

,

GENERALIZED ADDITIVE MODEL

11

33=== 110E[^[^,,{,{,^((X+X-)1)}+}-3=+4{3=^=+,34,=3]0^(an,,d,-,(E[,-+-{3)3,(-bX=+y34))3}2mainn,i,dm =(izing-] are-e3s)3ti}m]a2t.ed by 3).3=E0{^2,(,X)+ ==+43}^is,,e s(tim -ated,b-y3)3 by minimizing


=1

([ 

-



{^

(X)}]2

-

{3
=0

,, 

+

 +3
=4

,,

(

-

-3)3})2

.

4).

The

density

function

 ()

is

estimated

by

-1


=1



(

- )

with the rule-of-the-thumb bandwidth .

6. Examples. We have applied the estimation procedure described in the previous section to both simulated (Example 1 and 2) and real (Example 3) data.

6.1. Example 1. The data are generated from the model

P(

=

1X

=

x)

=



{ 

+


=1



} ()

,



()

=

 1 + 

with  = 5,  = 0, 1 () = sin (), 2 () =  (3) and 3 () = 4 () = 5 () = , where  is the standard normal distribution function. The predictors are generated by transforming the following vector autoregression
(VAR) equation for 0  ,  < 1,

( )  =  1 - 2 , 2    , 1    

Z = Z-1 + ,    (0, ) , 2    ,  = (1 - ) I× + 11T ,

with

stationary

Z

=

(1, ..., )T





{ 0,

( 1

-

2)-1

} 

,

1

=

(1, ..., 1)T

and I× is the  ×  identity matrix. Higher values of  correspond to

stronger dependence among the observations, and in particular, if  = 0,

the data are i.i.d. The  controls the correlation of the 1 and 2. In this

study, we have experimented with two cases:  = 0,  = 0;  = 0.5,  = 0.5

to cover various scenarios. For  = 1, ..., , let ,min, ,max denote the smallest and largest observations of the variable  in the  -th replication. The component functions {}=1 are estimated on sample values.

12 R. LIU, L. YANG AND W. K. HA¨ RDLE

Denoting the estimator of  in the -th sample as ^SBK,, and , accordingly. We define the (mean) integrated squared error (ISE and MISE):

ISE(^SBK,, )

=

-1


=1

{^SBK,,

(,

)

-

(,

)}2

,

MISE(^SBK,)

=

1 100

100
=1

ISE(^SBK,, ).

In order to show the SBK estimator's efficiency relative to the "oracle smoother" ~K, (), define the empirical relative efficiency of ^SBK, () with respect to ~K, () as
EFF = [ =1={1{^~SBKK,,(()-) -(()})2}2 ]1/2 .

Tables 1 and 2 show EFF () and std {EFF ()}, which are the means and standard deviations of the MISEs and EFFs of ^SBK, and ~K, for  = 1, 2. It is apparent that the SBK estimator performs as good as the oracle estimator, see Theorem 4.
(Insert Table 1 about here) (Insert Table 2 about here)

6.2. Example 2. Using the same model in Example 1 but with a higher dimension  = 10, where  () = sin (),  = 1, ..., 10 and data are generated the same way. We have run 100 replications for sample size  = 500, 1000, 1500, 2000. The MISEs of EFFs of ^SBK,1 and ~K,1 are shown in Table 3. As expected, increases in sample size reduce MISE for both estimators and across all combinations of  and  values.
(Insert Table 3 about here) The convergence properties are displayed in Figure 1 (a) showing the kernel density estimator of the simulated efficiencies for  = 1 and sample sizes  = 500, 1000, 1500, 2000 for  = 0,  = 0. The vertical line at efficiency = 1 is the standard line for the comparison of ^SBK,1 and ~K,1. One can clearly see that the center of the density plots is moving towards the standard line 1.0 with a narrower spread when sample size increases, which confirms the result of Theorem 4. The basic graphic pattern of Figure 1 (b) with  = 0.5,  = 0.5 is similar to that for the i.i.d case, though with slightly slower convergent and slightly poorer efficient.
(Insert Figure 1 about here) To have an impression of the actual function estimates, for  = 0,  = 0 and  = 0.5,  = 0.5 with sample size  = 500, 1000, 1500, 2000, we have plotted the SBK estimators and their 95% pointwise confidence intervals

GENERALIZED ADDITIVE MODEL

13

(three dotted lines), oracle estimators (dashed lines) for the true functions 1 (solid lines) in Figures 2 and 3. The results are satisfactory and show that the theory works in practice, and that performance improves with increasing sample size.
(Insert Figure 2 about here) (Insert Figure 3 about here)

6.3. Example 3. We have applied the estimation to the dataset comes

from the credit reform database provided by the Research Data Center

(RDC) of the Humboldt Universita¨t zu Berlin. After we exclude the miss-

ing values, it contains financial information from 18610 solvent ( = 0) and

1000 insolvent ( = 1) German companies. The time period ranges from

1997 to 2002 and in the case of the insolvent companies the information

was gathered 2 years before the insolvency took place. For more details, see

Ha¨rdle et al. (2010). The financial ratios we use are showed in Table 4.

(Insert Table 4 about here)

In order to satisfy (A4), we make following transformation:  =  (),  = 1, ..., 8, where  is the empirical cdf for the data {}=1 . We measure the quality of the estimation by Accuracy Ratio (AR), which is the

ratio of two areas. The first one is the area between the Cumulative Accu-

racy Profile (CAP) curve and the diagonal line, and the second one is the

area area

between the perfect model CAP is close to 1/2 in this example, so

cwuervheavaendARthed2iag01oCnaAlP. T(h)esec-on1d.

As a result, our model has the AR value 62.66%. We can also estimate

the functions  () for . For example, if we are interested in the effects

of Ebit/Total Assets and log (Total Assets), we can obtain the estimations

for 3 () and 8 (), which are showed in Figure 4.

(Insert Figure 4 about here)

It is not a surprise that the estimation for 8 () decreases as  value in-

creases. It means that a company with more Total Assets has smaller prob-

ability of insolvent. While as  value increases, the estimation for 3 ()

increases for most part but decreases at the end. So generally, those compa-

nies with higher Ebit/Total Assets ratio have more probability of insolvent.

But it looks like that those companies with extremely high Ebit/Total Assets

ratio have less probability of insolvent. It is an interesting topic to figure out

the reason.

APPENDIX A: APPENDIX SECTION
A.1. Preliminaries. In the proofs that follow, we use " " and "" to denote sequences of random variables that are uniformly "" and " " of

14 R. LIU, L. YANG AND W. K. HA¨ RDLE

certain order.

m iLxiEng1M, sMeqAue=nAc.e1.wi=t(1hSuEn, klo2d=ads=e0f(.1E9D8e42n)o,teThe0or=efmomr 1as)xo1mLeet{0{E}=(102,+b+e}a,n)0.

<
If

 ()  0 exp (-0), 0 > 0, 0 > 0, then 1 = 1 (0, ), 2 = 2 (0, )

exist such that

(A.1)

 = sup


P

{- 1

<

} 

-



()



1

 0

{ log

() /01/2

}1+ /

for any  with 1    2, where

1

=

2

{ log

( /10/2

)}

/,



>

2

(1

+

)

/;

2

=

4

(2

+

)

-1

log

() /10/2

.

LEMMA A.2. (Bernstein's inequality, Bosq (1998), Theorem 1.4) Let
{} be a zero mean real valued process, and suppose that there exists  > 0 such that for  = 1,    , ,   3, E   -2! E 2 < +,  = max1  ,   2. Then for each  > 1, integer   [1, /2], each  > 0 and   3

{  P =1 

>

} 



1

exp

( -

2 2522 +

5

)

+

2

()



([



 +

1

])

2 2+1

where

1

=

2

 

+

2

( 1

+

2 2522 +

) 5

,

2

()

=

11

( 1

+

52/(2+1) 

)

.

Denote the theoretical inner product of  and 1with respect to the -th marginal density  () as , =  () , 1 =  ()  ()  and define the centered B spline basis , () and the standardized B spline basis , () as

, ()

=



() -

,  -1,



-1

() , ,

()

=

, () ,2

,

1









+ 1,

so that E , () = 0, E 2, () = 1.

LEMMA A.3. (Wang and Yang (2007), Theorem A.2) Under Assumptions (A1)-(A5) and (A7), one has:

GENERALIZED ADDITIVE MODEL

15

(i) Constants 0 ( ), 0( ), 1 ( ) and 1( ) exist depending on the marginal densities  () , 1    , such that 0 ( )   ,  0 ( )  and

(A.2)

1 ( )   ,22  1( ).

(ii) uniformly for ,  = 1, ...,  + 1



{} E , () , ()

  

1 -1/3
1/6 0

  =   -  = 1  -  = 2  -  > 2

E

, () , ()



{ 

 1- 0

 -   2  -  > 2

,   1.

LEMMA A.4. (De Boor (2001), p.149) A constant  > 0 exists such that for any   1 [0, 1] with   Lip ([0, 1] , ), there is a function   (0) [0, 1] such that  -   2.

LEMMA A.5. (Wang and Yang (2007), Lemma A.2) Constants 0, 0 > 0 exist such that for any  = (0, ,)1T+1,1  1+(+1),

0

( 02

+

2
,

) 2,



 0 + , ,,

2 2



0

( 02

+

2
,

) 2, .

LEMMA A.6. (Xue and Yang (2006a), Lemma A.4) Under Assumptions (A2), (A4) and (A6), as   , the uniform supremum of the rescaled difference between 1, 22, and 1, 22 is

 = sup
1,2(0)[0,1]

1, 22, - 1, 22 12 22

()

= ..

log  1/2  1/2

.

A.2. Oracle smoothers.

LEMMA A.7. Under Assumptions (A1)-(A6), as   ,

sup

~

{1

(1)}

-

bias1

(1)

2

-

-1


=1



(1

-

1)



(X)



1[,1-]

() = .. -1/21/2 log 

where bias1 (1) is defined in (3.4).

16 R. LIU, L. YANG AND W. K. HA¨ RDLE

Proof. According to (3.1) and (1.3), ~ {1 (1)} is

(A.3) =

-1 -1

 =1
=1

[[{-(X{)1}

(1) - 

+  {1

] 1 (X 1)}  (1 (1) +  1 (X 1)}

- +

1)  (X)



]

 (1 - 1)

Let , = , (1) = ,,1 + ,,2 in which

,,1 (1)

=

[-E{[[(X{)}(X-)} {-1

(1) {1

+  (1)

] 1 (X 1)} +  1 (X

 (]1 1)} 

- 1 (1

) -

] 1)

,

(A.4)

,,2 = ,,2 (1) =  (X)  (1 - 1) .

Then according to (A.3), one can rewrite  {1 (1)} as

-1


=1

,

+

E

[

{

(X)}

-



{1

(1)

+



1

(X

] 1)}



(1

-

1)

.

The deterministic term is

= =

E 

[ { (X)} [ { (u)}

 [ { (1,

- -

 

{1 {1

(1) (1)

+ +

 

1 1

]

(X 1)} ]
(u 1)}

 ((1 - 1)

-1

1 - 1 

u 1)} {1 (1) - 1 (1)}

)



(u)

u

=

+[-021,11]-(1{[1-(-1,11],[u1 )1){} ({(1,11u(,u11))1)-}1{u1 (11+1)1}(2(+21))+((2)2]1)2

1

(1)

+



} (2)

+ 

1 2



(1)

{ {


(1, (1,

u u

{ 1)} 11 1) + 1 

(1) + (1)2 1 (1) +  (2)}2]

(1, u 1

1)

+



(2)}

1u

1+

(2

)

which equals

2


[-1,1]

12

(1)

1

{ 1

(1) 1 2

(1)


[0,1]-1



{

(1,

u

1)}



(u1)

u

1

+1

(1)


[0,1]-1



{

(1,

u

1)}



(1, u 1

1) u

}
1

+

(2)

.

GENERALIZED ADDITIVE MODEL

=

22

( )

{1

(1)



(1)

E

[

{

(X)}

1

=

] 1

+-{1(1(1)1)}12

[ 

(1)

E

[

{

(X)}

1

=

]] 1



(1)

E

[

{

(X)}

1

=

]} 1

+

(2)

= bias1 (1) 2+ (2) .

17

Using the above

E 2,,1

=

 -2

[ { (u)} -  {1 (1) +  1 (u 1)}]2

[0,1]

=

(  1
 -1

- 

1

)2 



(u) u+ (4) [ { (1 + 1,

u

1)}

-



{1

(1)

+



1

(u

1)}]2

=

[0,1]-1 [-1,1]

 (1)2 -1



(1+

1, u [

[0,1]-1 [-1,1]

 (1)2 { (1, u 1) +

1) 1u 1+ (4) { (1, u-1)} {11 (1
 ()} 1u 1+ (4) =

) +   ()

(2 .

)}]2

Note that sup1  { (X)} -  {1 (1) +  1 (X 1)}   whenever

 (1 - 1) = A.2 implies that

0, hence E  sup1[,1-]

,,-1 1=(21,,)1-=2 E2.,.,1{so1/a2p-pl1y/i2nlgogLe}m.ma

LEMMA A.8. Under Assumptions (A2), (A4)-(A6), as   

sup

~ (1 (1)) + 1 (1)

( ) = .. log /  ,

1[,1-]

where 1 (1) is defined in (3.3).

Proof. See Liu et al. (2011).



LEMMA A.9. Under Assumptions (A1) to (A3), (A5) and (A7), a constant  exists such that, as   

sup

() Cov ,, ,





-

1+ 2+



(

-


) 2+

for  = 

1[,1-]

18 R. LIU, L. YANG AND W. K. HA¨ RDLE

Proof.

According

to

Davydov's

inequality,

for

1 

+

1 

+

1 

=

1,

() Cov ,, ,

is bounded by

2

{2

(

-

)}1/

 (

,,1

+

,,2





,),1

+ (



,,2



)

 2 {2 ( - )}1/ ,,1  + ,,2 

,,1  + ,,2 

Let has

 =  ,,1

=


2+ = 

(,-=2+11 )+a2n/d,

w,he,1re=take(s-va21l++ue)i.nCtohve((A,3,),,

then ,,

)one 



-

1+ 2+



(

-


) 2+

for some constant .



Proof of Theorem 1 and Theorem 2. The Mean Value Theorem

ensures the existence of a ¯1 (1) between ~K,1 (1) and 1 (1) such that

~ {~K,1 (1)} - ~ {1 (1)} = ~ {~1 (1)} {~K,1 (1) - 1 (1)}

Note that ~ {~K,1 (1)} = 0 yielding

(A.5)

~K,1

(1)

-

1

(1)

=

-

~ ~

(1 (1)) (¯1 (1))

.

Lemma A.8, Let  =

Lemma  (1)

=A.7 a=n1d(A,.,5w) hthereen

imply , is

E  = 0 and ~ {1 (1)} = / +  (1) 2 +

Theorem 1. d (efi2n)e.d as (A.4).

Note

that

()  () =  (, 1) = Cov ,, +,

2

= =
=

E V=a2r1=((VaV,ra())r(+,))+=1Va=r( C-o1v=((11,-,,),))  ()

=  Var , + ,

where

Var

() ,

=

-11

(1)

E

{2

(X)

1

=

} 1

 22

+



(4)

.

According to Lemma A.9, one has

 () =

() Cov ,, +,





-

1+ 2+


 () 2+

.

GENERALIZED ADDITIVE MODEL

19

Hence



 =

1-1  ()



( 1-1 1

-

)



-

1+ 2+



{0


exp (-0)} 2+



0

-

1+ 2+


1-1

exp

{-0/

(2

+

)}

,

so a constant 1 exists such that 0 as   . Since 2   Var

( ) ,

1-

1+ 2+

,

and

therefore

/

Var

( ,

)

 0 when  is large, according to



(A.1) in Lemma A.1, constants 1 and 2 exist such that for some 0 <   1

(A.6)

 = sup


P

{- 1

<

} 

-



()



1

 0

{ log

() /10/2

}1+ /

for any  with 1    2, where

1

=

2

{ log

( /10/2

)}

/,



>

2

(1

+

)

/;

2

=

4

(2

+

)

-1

log

() /10/2

.

() For  in (A3), set  = 4 (2 + ) -1 log /10/2 , then by (A6), the  in (A.6) is



=

( max E
1

[

{

(X)}

-



{1

(1)

+



1

(X

1)}

+



(X)

] 

)

 (1 - 1)2+

{}

= 

max
1
  

E  +  (X) 2+

{ E



(1

-

1)

2+

 }
=

(1 - 1) 2+ {}
 -(1+) ,

i.e.,  =  {-(1+)/} =  {(1+/2)/5-/2} =  (1/5-2/5)  0 when 1/2 <   1. So /   (0, 1), then



[

{1

(1)}

-

bias1

(1)

2]

 / -112

(1)





(0,

1)

,

where 12 (1) is defined in (3.5). According to Theorem 1, one has as   , sup1[,1-] ~ {1 (1)} - ~ {¯1 (1)}  0 because

sup1[,1-] 1 (1) - ¯1 (1)  0. Then according to Slutsky's theorem:

 

[ {~K,1

(1)

-

1

(1)}

1

(1)

-

bias1

(1)

2]





( 0,

12

) (1)

.

20 R. LIU, L. YANG AND W. K. HA¨ RDLE

where 1 (1) is defined in (3.3).



Proof of Theorem 3. According to the Mean Value Theorem, a con-

saw~tn=hadenratwer¯gh-bmeer~atexw(¯e)en=(~X a()n-)d=1.~Celxe=ias1=rtls1ys,u{~c¯h(+(~t)h=a)t0(a(~Xna-dn)d}th) e~>n (¯t)h e=>i~n0f(e~aa)sc-icbo~lre d(ein)stg=imt-oat~(oAr(2i)s),

~ ()

= =

-1 -1


=1

=1

[ 

-



{

+



] ((X)}

 (X)  = . -1/2

log

) 

by

Bernstein's

Inequality.

Similarly,

~ ()

=

--1


=1



{

+



 (X)}

cimonpvlyertgheas tto~--E= {(.X. ()}-a1l/m2 olosgt

su)rely at the rate  and plugging it

of -1/2 into

log

.

These

(~ - ) = -~ () /~ (¯), Theorem 3 is proved.



A.3. Spline backfitted kernel estimators. In this section, we present the proof of Theorem 4. We write any   0 as  = TB (X) with vector  = (0, ,)T1+1,1   where  = ( + 1)  + 1 is the dimension of the additive spline space 0, and
B (x) = {1, 1,1 (1) , ..., +1, ()}T ,

its standardized ^ () = ^ () =

ba-si1s. W=e1

d[enotTeBw(iXth)a-slig{htTaBbu(sXeo)}f ]n,owtahtiicohn

yields

the

gradient and Hessian formulae

^ ()

=

-1


=1

[ B

(X)

-



{ T

B

} (X)

B

] (X)

,

2^ ()

=

--1


=1



{ TB

} (X)

B

(X

)

B

(X

)T

.

The multivariate function  (x) is estimated by an additive spline func-

tion

^ (x) ^

= =

(^^00+, ^,)=T11^(

) = ^TB (x) , = argmax ^

()

.

1 +1

Lemma 14 of Stone (1986) en(sur)es that with probability approaching 1, ^ exists uniquely and that ^ ^ = 0. In addition, Lemma A.4 and (A1) provide a vector ¯ and an additive spline function ¯ such that

(A.7)

¯ (x) = ¯TB (x) , ¯ -   2.

We first establish technical lemmas before proving Theorems 4 and 5.

GENERALIZED ADDITIVE MODEL

21

LEMMA A.10. Under Assumptions (A1)-(A5) and (A7), as   

^ (¯) ^ (¯)

() = .. 2 + -1/2 log  ,
() = .. 3/2 + -1/2-1/2 log  .

Proof.

^ (¯)

=

-1


=1

[ B

(X)

-



{ ¯TB

} (X)

B

] (X)

=

-1


=1

[

{

(X)}

-



{¯

(X

)}

+



(X)

] 

B

(X)

T1 he fi=r1s[t[el{eme(nXt o)}f

the above vector -  {¯ (X)}] +

is  (X)

],

which

is

..

( 

2

+

-1/2

log

) 

according to Lemmas A.4 and A.2. The other elements can be written as

-1


=1

[ ,,,

+

E

[

{

()}

-



{¯

] ()}

, () +  (X) , ()] ,

where

,,,

=

[-E{[[({)(}-)} {-¯({¯)(}])},]

() , (

] )

.

According to (A.2) and (A.7), one has

E

[

{

()}

-



{¯

] ()}

,

()



E

 { ()} -  {¯ ()}

, () ,2







-

¯

max
1 +1

,2-1

max
1 +1

E

,

(

)

(

1 )

( 1)

=  2 × -1/2 ×  =  5/2 ,

for some constant  and likewise for any   2

E  { ()} -  {¯ ()}  , ()



-2



-

¯-2

max
1 +1

,

-2 (-2)

max
1 +1

E

,-2 ()

1

1

×E



{ ()}

-



{¯

()}

2

2, () ,22



( 

5/2)-2

E



{ ()}

-



{¯ ()}

2

2, () ,22

22 R. LIU, L. YANG AND W. K. HA¨ RDLE

and



E [ { ()} -  {¯ ()}]2 2, ()





-

¯2

max
1 +1

,

-2 2

max
1 +1

E

2, ()

=



( 

4)

.

1

1

Usi.n.g(th2ese-1b/o2ulongds)anadndapplying Lemma A.2, one has

-1


=1

,,,

=

-1

=1



(X)

,

()

=

..

( -1/2

log

) 

.

The lemma is then proved. Define the following matrices:



V = E B (X) B (X)T , S = V-1,

V

=

-1


=1

B

(X)

B

(X)T

,

S

=

V- 1

and similar matrices

[

V = E  { (X)} B (X) B (X)T =

,00 ,0,  ,

] ,0,, ,,,, ×

(A.8)

[

S = V- 1 =

,00 ,0,  ,

]

,0,, ,,,  ,

,
 ×

For any vector   , denote

{}

V () = E  TB (X) B (X) B (X)T

[]

=

,00 () ,0,, ()
[

,0,, () ,,,, ()

,
 ×

]

S () = V- 1 () =

,00 ()

,0,, ()

,0,, () ,,,, () ×

(A.9)

V, () = -2^ () , S, () = V-,1 () .

LEMMA A.11. Under Assumptions (A2) and (A4)

(A.10) (A.11)

VI  V  VI , SI  S  SI , V,I  V  V,I , S,I  S  S,I .

GENERALIZED ADDITIVE MODEL

23

Under Assumption (A2), (A4), (A5) and (A7), as    with probability increasing to 1

(A.12)

VI  V ()  VI , SI  S ()  SI

(A.13)

V,I  V, ()  V,I , S,I  S, ()  S,I .

Proof. For (A.10), see Lemma A.9 in Wang and Yang (2007), while
(A.12) follows from Lemma A.6. The statements (A.12) and (A.13) follow from (A.10) and (A.12), together with the boundedness of  in (A2). 
Define three vectors , ,  as

(A.14)  = (,0, ,,)T1+1

=

-S-1


=1

1[{

(X)}

-



{¯

(X

] )}

B

(X

)

,

(A.15)  = (,0, ,,)T1+1

=

-S-1


=1

1
[ (X)

]

B

(X)

,

(A.16)  = (,0, ,,)1T+1
1

= ^ - ¯ -  - .

LEMMA A.12. Under Assumptions (A1)-(A5) and (A7), as   

(A.17) (A.18) (A.19)

()

^ - ¯ = .. 2 + -1/2-1/2 log  ,

()

 = .. -3/2-1 log  ,



=

..

( 

2)

,

 

=

..

(  -1/2 -1/2

log

) 

.

Proof. The matrix t exists

Mean whose

Value Theorem implies that an  diagonal elements are in [0, 1], such

×  that

diagonal for ^ =

t^+ (I - t) ¯

^

() ^

-

^

(¯)

=

2^

(^

)

( ^

-

) ¯

.

() Since, as noted before, that ^ ^ = 0, the above equation becomes

^

-

¯

=

-

( 2^

(^))-1

^

(¯)

.

24 R. LIU, L. YANG AND W. K. HA¨ RDLE

According to (A.9),

-2^ ()

=

-1


=1



{ TB

} (X)

B

(X)

B

(X)T

=

V,

()

,

Lemma A.11 implies that with probability approaching 1 V,I  -2^ (^)  V,I .

Then (A.17) ^ - ¯ =

follow( s . 

Lemma 3/2 + 

-A1/.120.-F1/u2rltohgerm) oarse,well

according

to

^'s

defi-

nition. Note that Taylor expansion ensures that for any vector a  

aT

{ 2

^

(^)

-

2^

(¯)}

a







max
1

^ B (X) - ¯TB (X)

aTVa

while by Cauchy Schwartz inequality

max
1

^ B (X) - ¯TB (X)

 ^ - ¯ sup B (x)

( x[0,1]

)(

)

= . 3/2 + -1/2-1/2 log  ×  -1/2

()

= .  + -1-1/2 log  .

Consequently, one has the following bound on the difference of two Hessian

matrices

sup

( 2

^

(^)

-

2^

(¯))

a

() a-1 = .  + -1-1/2 log  .

a

Denote next

^a

=

-

{ 2^

(^)}-1

^

(¯)

=

^

-

¯

¯a

=

-

{ 2^

(¯)}-1

^

(¯)

then

^a

=

..

( 

3/2

+

 -1/2 -1/2

) log 

and

so

is

¯a

by

similar

argu-

ments. Furthermore,

2^

(^)

(^a

-

¯a)

=

{ 2

^

(¯)

-

2

^

(^)}

¯a

GENERALIZED ADDITIVE MODEL

25

entails that ^a - ¯a =
=

(

)(

)

.. 3/2 + -1/2-1/2 log  × O.  + -1-1/2 log 

()

.. 5/2 + -3/2-1 log2  .

Using similar tricks, one can show that

~a - ¯a

=

..

(  3/2

+



-1/2-1/2

log

) 

×

O.

( 

2

)

()

= .. 7/2 + 3/2-1/2 log  ,

(

)(

)

~a -  -  = .. 3/2 + -1/2-1/2 log  ×. -1/2-1/2 log 

() = .. -1/2 log  + -1-1 log2  ,

in which

~a

=

[ -1


=1



{

(X)}

B

(X)

B

(X)T]-1

^

(¯)

.

Putting together the above proves (A.18). Lastly, almost surely



= 

SS, -1-1 =1=[1[{ {(X(X)}

- )}

 -

] {¯{(¯X(X)})}B]

(X) B (X)

=

..

( 

2)

and

 

= 

S-1


=1

[

(X

)

]

B

(X)

S,

-1


=1

[

(X)

]

B

(X)

() = .. -1/2-1/2 log2  ,

which completes the proof of the lemma.



LEMMA A.13. Under Assumptions (A1)-(A5) and (A7), as   

 ^ - ¯ + =1 ^ - ¯

=

() .. 3/2 + -1-1/2 log  ,
()

^ - ¯2, + ^ - ¯2 = .. 2 + -1/2-1/2 log  ,

 ^ -  + =1 ^ - 

=

() .. 3/2 + -1-1/2 log  ,
()

^ - 2, + ^ - 2 = .. 2 + -1/2-1/2 log  .

26 R. LIU, L. YANG AND W. K. HA¨ RDLE

Proof. According to (A.17) and the definition of ^ and ¯

^ - ¯ = sup ^TB (x) - ¯TB (x)  ^ - ¯ sup B (x)

x[0,1](

) ( x[0,)1]

 .. 2 + -1/2-1/2 log  ×  -1/2

()

= .. 3/2 + -1-1/2 log  .

The bound on ^ - ¯ is similarly obtained. Next, Lemma A.11 implies

^ - ¯2, + ^ - ¯2  2V ^ - ¯ ()
= .. 2 + -1/2-1/2 log  .

Since

¯

-



+

¯

-

2

+

¯

-

2,

=



( 

2)

by

the

definition

in

(A.7), the Lemma follows.



In the following denote



(1)

=

{,

(1)}=+11,,=2

,

,

(1)

=

-1


=1

,

()



(1

-

1)

.

LEMMA A.14. Under Assumptions (A1)-(A7), as   ,

(A.20)

( ) sup , (1) - E , (1) = .. log / 
1[0,1],2 1 +1

(A.21)

()

sup  (1) = sup , (1) = .. 1/2 .

1[0,1]

1[0,1],2

1 +1,

Proof. First, one computes



E , (1) =

 (1 - 1) , ()  (1,) 1

= =

(,2()-11){,,(22)(1()1 ++1,2(1,2))

1 (1 +

1

,

2)

12

+

(

 +1,2 ,2

)1/2





}  (1) ,2 (2)  (1 + 1, 2) 12 .

GENERALIZED ADDITIVE MODEL

{ 

 ,2-1

 (1)  ()  (1 + 1, ) 1

+

,  -1,





}  (1) -1 ()  (1 + 1, ) 1 .

27

The boundedness of the joint density  and the Lipschitz continuity of the kernel  imply that a constant 2 > 0 exists such that

 (1) -1 ()  (1 + 1, ) 1   2.

Therefore (A.22)

() sup E  (1) =  1/2
1[0,1]

by Lemma A.3. Similarly, E , (1)  1-1-/2, hence E , (1)2  -1. According to Lemma A.2 and similar proof of Lemma A.5 in Wang and Yang (2007), one proves (A.20). Combining with (A.22), the lemma is proved. 

LEMMA A.15. Under Assumptions (A1)-(A7), as    ()
sup ^ {~K,1 (1)} = .. -1/2 log  .
1[0,1]

Proof. Note that ~ {~K,1 (1)} = 0, thus ^ {~K,1 (1)} = ^ {~K,1 (1)} - ~ {~K,1 (1)} equals

-1


=1

[

{~K,1

(1)

+



1

(X

1)}

-



{~K,1

(1)

+

^

1

(X

] 1)}

=

-1(1=-1 1{)~K,1  (1 - 1) + 

([11/)+=11

(X {

1)} { 1 (X 1 (X 1) - ^

1) - ^ 1 (]X 1 (X 1)}2 .

1)}

Now Lemma A.13 together with (A7) imply:

1/


=1

{

1

(X

1)

-

^

1

(X

1)}2

=

..

( 

4

+

 -1 -1

log2

) 

= =



1

- (

^

122,

  )

-

^22,

.. -1/2 log  .

This yields (A.23)

() ^ {~K,1 (1)} = 1 + 1 + .. -1/2 log2  ,

28 R. LIU, L. YANG AND W. K. HA¨ RDLE

1

=

-1


=1



{~K,1

(1)

+



1

(X

1)}

2

=

{-11(X=11)-{~¯K1,1((X1 )1

)} +

 (1  1 (X

- 1) 1)}

,

{¯ 1 (X 1) - ^ 1 (X 1)}  (1 - 1) .

Applying standard kernel theory, the boundedness of  and (A.4), one ob-

tains:

(A.24)

1

 =

.. (=22

) - ..

¯

-1


=1



(1

-

1)

again by (A7) on , while 2 = 2, + 2, + 2, with

2,

=

-1 {


=1





{~K,1

(1)

+



1

(X

1)}

×

}

,0 + 1+1,2 ,,, ()  (1 - 1) ,

2,

=

{-1


=1





{~K,1

(1)

+



1

(X

1)}

×

}

,0 + 1+1,2 ,,, ()  (1 - 1) ,

2,

=

-1 {


=1





{~K,1

(1)

+



1

(X

1)}

×

}

,0 + 1+1,2 ,,, ()  (1 - 1)

where ,0, ,0, ,0, ,,, ,,, ,, are defined in (A.14), (A.15) and

(A.16). 2, is bounded by

-1


=1

{ ,0

+


1 

+1,2

,,

,

} ()



(1

-

1)





[{ 2,0

+


1 +1,2

2,,}]1/2

×

[{-1


=1



(1

-

1)}2

+


1 +1,2

{-1


=1

,

()



(1

-

1)}2]1/2

=  ×  × [.. (1) + ( + 1) × ( - 1) × .. ()] ,

so (A.25)

2,

=

..

( 

2)

according to (A.17) and (A.21). Similarly

()

(A.26)

2, = . () = .. -3/2-1 log  ()

= .. -1/2 log 

(A.27) 2, - ~2,

GENERALIZED ADDITIVE MODEL

=

..

( log

) / 

×

..

( 

-1/2-1/2

log

) 

()

= .. -1/2 log 

by making use of (A7) on  and (A6) on , where

29

~2, = ,,0,-1-1 =1=1{{(X(X)})}(, (1-)1)+(11-1)+.1,2 Applying Lemma A.2, we have ~2, equals

(A.28) =

++,0E.1.({-+1(1/X,22)-}1/2l(og,1,)-E×1){1 /2(×X)}..,(l(og)/()1 - 1) ()
~2,,1 + ~2,,2 + .. -1/2 log2  ,

in which

~2,,1

= =

,0 E{ 

E  { (X)}  { (X)}  (1


-(11-) -11)=1



(X})



0,0 + 1+1,1 ,, () ,

~2,,2

= =

{E1{+(X1,2)}, (,,)E

{ (X)} }, ()  (1 (1 - 1) 1+1,2

-

1)

=

(-11,,)=T11+1,2(X+)1,2E {{,+(X)}1,(+1,1)(1,-,,1)

  ,

} ( )

=

-1 {


=1





(X

)




1



+1,2

,,,

(1)

}

, + 1 +1,1 ,,, , ( )

where 0,0, ,, ,,, are the corresponding elements in the matrix S

defined in (A.8), the supremum of

wanhdichh,a,s,th(e1o)rd=erE( {1/(2X).)}Deno,t e()  (1

- 1)

,

30 R. LIU, L. YANG AND W. K. HA¨ RDLE

()

 = 0

2

1 +



<

0

<

2 5

, ,1 =  { > }, ,3 = E  {  },

,2 =  {  } - ,3 . Then ~2,,2 = 1 + 2 + 3 where



=


1 +1,2

,,,

(1) (1) -1


=1



(X) ,

{

}

, + 1+1,1 ,,, , ( ) ,  = 1, 2, 3.

With probability 1, 1 = 0 for large . Next

,3

=

- E  {

>

}



E 2+ 1+

() =  - (1+) ,

3



S

[
1 +1,2

2,,,

(1)


1 +1,1

{ -1


=1

  ,

( )



(X)

,3 }2]1/2





- (1+)

[
1



+1,2

2,,,

(1)


1 +1,1

{-1


=1

  ,

( )



(X)}2]1/2

=

- (1+)..

{(  

log2

/)1/2}

=

- (1+)..

{( 

log2

/)1/2}

()

(

)

= .. -1/2 = .. -1/2 log  .

Lastly,

denote

2

=

-1


=1

,

where



=

 { 1

 +1,2


,,,

(1)



(X)

,2

}

, + 1 +1,1 ,,, , ( ) .

Then E  = 0, and

{

Var () = T S 1 Var 

 (X) , , ( )  (X) ,

}+1,   ST 1 
  =1, =1

 S2VT, 1T,  =  (1) .

Ts~i2mh,e,i2nla=rp2ro=o.f.,(we-.1.c/(a2nl-osg1h/o2w)loag~c2,c)o,1rda=cincgortd.oi.n(tghe-to1o/r2Bdloeerrgnsso)tfe. iTn'1hs,eIln2eemaqmnudaaliitsy3..prTWohvieetdhn

by putting together (A.23), (A.24), (A.25), (A.26), (A.27), (A.28) and the

above bound on ~2,,2 and ~2,,1.



GENERALIZED ADDITIVE MODEL

31

LEMMA A.16. Under Assumptions (A1)-(A7), constants ,  exist such that 0 <   -^ (, 1)   <  .. for   , 1  [0, 1].

Proof. According to (4.1), one has

^

()

=

-1/


=1

[

{

+

^

1

(X

] 1)}



(1

-

1)

.

   { + sup1[,1-]

^1/1(X=1)1}

 and (1 - 1)

-



(1)

=

..

(1)

imply

the

lemma.



Proof of Theorem 4. According to (4.1) and the Mean Value Theorem,

a ¯K,1 (1) between ^SBK,1 (1) and ~K,1 (1) exists such that

^ {^SBK,1 (1)} - ^ {~K,1 (1)} = ^ (¯K,1 (1)) {^SBK,1 (1) - ~K,1 (1)} ,

Then according to ^ {^SBK,1 (1)} = 0, one has

(A.29)

^SBK,1

(1)

-

~K,1

(1)

=

-

^ ^

{~K,1 {¯K,1

(1)} (1)}

.

The theorem then follows from Lemmas A.15 and A.16.



Proof of Theorem 5. The Mean Value Theorem implies the existence

of-1¯be=t1wee {n¯^+an^d

~ such that ^ - ~ = -^ (X)} >  > 0 according

(~) to

/ (¯) ,where (A6), then

-^ (¯)

=

^

(~)

=

^

(~)

-

~

(~)

=

-1


=1

[

{~

+





(X)}

-



{~

+

^



] (X)}

=

-1 +

[1/=1={~1

+  { 

 (X)} (X) -

{ ^ 

 (X) -] (X)}2

^



(X)}

=



+

..

( 

4

+

-1

log

) 

,

by Lemma A.13, where  = 1 + 2,

1

=

-1


=1



{~ +





(X)} {



(X)

-

¯



(X)} ,

Accordingt2o=Lem-1ma A=.14,{1~=+..((X)2}),{w¯hil(eX) - ^  (X)} .

2

=

{-1 1=1+1{,1~ + (^(X,

)} -

× ¯,

)

,

} ()

32 R. LIU, L. YANG AND W. K. HA¨ RDLE

= 2, + 2, + 2,

where 2, = 2, = 2, =

-1


=1



{~

+





(X)}

{
1 +1,1

,,,

} ()

,

-1


=1



{~

+





(X)}

{
1 +1,1

,,,

} ()

,

-1


=1



{~

+





(X)}

{
1 +1,1

,,,

} ()

.

We have 2, = .. (-1/2) according to (A.17) and (A.21), see Liu et al. (2011). Similarly

( ) () 2, = .. 7/2 + -1/2-1 log  = .. -1/2 .

We

have

2,

=

~2,

+ ..

(-1/2) × ..

( 1 /2-1/2

log

)  × 

( ),

where

~2,

=

-1


=1



{

{ (X)}

1 +1,1

,,,

} ()

=

--1


=1



{

(X)}

-1


=1



(X )

 BT

(X )

S

B



(X)

where S  = S (0-1, I-1)T consists of columns 2 to  of S defined in (A.8) and B  (x) = {1,1 (1) , ..., +1, ()}T. So

~2,

=

--1

 ( =1

)(X

)



+-1




=1



(X

)



,00

(,00

,

,0,,

)

B

(X

)

+.. -1/2

by Liu et al. (2011). Putting the above together, and noticing that ,00 = E  { (X)}, one has

(A.30) (A.31)

()

^ - ~ =  + .. -1/2 ,



=

-1


=1



(X )



( ,00

-

-,010,

) ,0,,

B

(X )

.

According to (A.8) and matrix theory

()

S =

-,010 + -,020 -1T --,010 -1

--,010 -1T

 -1



=

(,0,,) , 

=

( ,,,  ,

)

-

T-,010.

According to (A.11)

0 < ,  ,00  , < +

GENERALIZED ADDITIVE MODEL

33

()

,I-1  ,,,,  ,I-1

while

the

definition

of

B (X)

implies

that



=



( 

3/2)

and

hence

 =  (). Thus a constant  > 0 exists such that for sufficiently large

,    I-1 and hence  -1  - 1I-1. Putting the above together

leads to

,00

=

-,010

+

-,020 -1T

=

-,010

+



( 

2)

,0,, = --,010 -1, ,0,,2 =  () .

Applying Cauchy-Schwartz inequality,  is bounded by

{( ,00

-

-,010)2

+

}1/2 ,0,,22

×

 1/2

{-1


=1



(X)

}2

+



{-1


=1



(X)

,

()}2

,

( ) () =  () × .  1/2-1/2 log  = . -1/2 .

This, together with (A.30) and (A.31) prove the Theorem.



REFERENCES
Bosq, D. (1998). Nonparametric Statistics for Stochastic Processes. Springer-Verlag, New York.
de Boor, C. (2001). A Practical Guide to Splines. Springer-Verlag, New York. Fan, J. Ha¨rdle, W. and Mammen, E. (1998). Direct estimation of low-dimensional
components in additive models. Ann. Statist. 26, 943­971. Ha¨rdle, W., Hoffmann, L. and Moro, R. (2011). Learning Machines Supporting
Bankruptcy prediction. Statistical Tools in Finance and Insurance (2nd ed.), Cizek, Ha¨rdle, Weron, Springer Verlag. Hastie, T. J. and Tibshirani, R. J. (1990). Generalized Additive Models. Chapman and Hall, London. Horowitz, J. and Mammen, E. (2004). Nonparametric estimation of an additive model with a link function. Ann. Statist. 32 2412­2443. Horowitz, J. Klemela¨, J. and Mammen, E. (2006). Optimal estimation in additive regression. Bernoulli 12 271­298. Huang, J. Z. and Yang, L. (2004). Identification of nonlinear additive autoregression models. J. R. Stat. Soc. Ser. B Stat. Methodol. 66 463­477. Linton, O. B. and Nielsen, J. P. (1995). A kernel method of estimating structured nonparametric regression based on marginal integration. Biometrika 82 93­100. Linton, O. B. (1997). Efficient estimation of additive nonparametric regression models. Biometrika 84 469­473. Linton, O. B. and Ha¨rdle, W. (1996). Estimation of additive regression models with known links. Biometrika 83 529­540.

34 R. LIU, L. YANG AND W. K. HA¨ RDLE
Liu, R. and Yang, L. (2010). Spline-backfitted kernel smoothing of additive coefficient model. Econometric Theory 26 29­59.
Liu, R., Yang, L. and Ha¨rdle, W. (2011) Supplement to "Oracally efficient two-step estimation of generalized addtive model". Manuscript.
Ma, S. and Yang, L. (2011). Spline-backfitted kernel smoothing of partially linear additive model. Journal of Statistical Planning and Inference 141, 204­219.
Portnoy, S. (2011). Local asymptotics for quantile smoothing splines. Ann. Statist. 25, 414­434.
Song, Q. and Yang, L. (2010). Oracally efficient spline smoothing of nonlinear additive autoregression model with simultaneous confidence band. Journal of Multivariate Analysis 101, 2008­2025.
Stone, C. J. (1985). Additive regression and other nonparametric models. Ann. Statist. 13 689­705.
Stone, C. J. (1986). The dimensionality reduction principle for generalized additive models. Ann. Statist. 14 590­606.
Stone, C. J. (1994). The use of polynomial splines and their tensor products in multivariate function estimation. Ann. Statist. 22 118­184.
Sunklodas, J. (1984). On the rate of convergence in the central limit theorem for strongly mixing random variables. Lithuanian Mathematical Journal 24 182­190.
Tjøstheim, D. and Auestad, B. (1994). Nonparametric identification of nonlinear time series: projections. J. Amer. Statist. Assoc. 89 1398­1409.
Wang, L. and Yang, L. (2007). Spline-backfitted kernel smoothing of nonlinear additive autoregression model. Ann. Statist. 35 2474­2503.
Wang, J. and Yang, L. (2009). Efficient and fast spline-backfitted kernel smoothing of additive regression model. Annals of the Institute of Statistical Mathematics 61 663­ 690.
Xue, L. and Liang, H. (2010). Polynomial spline estimation for a generalized additive coefficient model. Scandinavian Journal of Statistics 37 26­46.
Xue, L. and Yang, L. (2006a). Additive coefficient modeling via polynomial spline. Statistica Sinica 16 1423­1446.
Xue, L. and Yang, L. (2006b). Estimation of semiparametric additive coefficient model. Journal of Statistical Planning and Inference 136 2506­2534.
Yang, L., Ha¨rdle, W. and Nielsen, J. P. (1999). Nonparametric autoregression with multiplicative volatility and additive mean. J. Time Ser. Anal. 20 579­604.
Yang, L., Sperlich, S. and Ha¨rdle, W. (2003). Derivative estimation and testing in generalized additive models. Journal of Statistical Planning and Inference 115 521­542.
Yang, L., Park, B. U., Xue, L. and Ha¨rdle, W. (2006). Estimation and testing for varying coefficients in additive models with marginal integration. J. Amer. Statist. Assoc. 101 1212­1227.

 = 5  = 0,  = 0.  = 0.5,  = 0.5.

 MISE (^SBK,1) MISE (~SBK,1) EFF (^SBK,1) std {EFF (^SBK,1)}

500 0.0548

0.0600

1.1120

0.2741

500 0.1017

0.0944

1.0233

0.2796

Table 1 Example 1: the means and standard deviations of MISEs and EFFs of ^SBK,1, ~SBK,1
for  = 5,  = 500.

 = 5  = 0,  = 0.  = 0.5,  = 0.5.

 MISE (^SBK,2) MISE (~SBK,2) EFF (^SBK,2) std {EFF (^SBK,2)}

500 0.0179

0.0271

1.5032

0.8965

500 0.0365

0.4178

0.9977

0.4006

Table 2 Example 1: the means and standard deviations of MISEs and EFFs of ^SBK,2, ~SBK,2
for  = 5,  = 500.

 = 10  = 0,  = 0.
 = 0,  = 0.5.
 = 0.5,  = 0.
 = 0.5,  = 0.5.

 500 1000 1500 2000 500 1000 1500 2000 500 1000 1500 2000 500 1000 1500 2000

MISE (^SBK,1) 0.0965 0.0491 0.0298 0.0246 0.0992 0.0453 0.0285 0.0259 0.2318 0.1343 0.0756 0.0567 0.2757 0.1389 0.0776 0.0593

MISE (~K,1) 0.0701 0.0453 0.0331 0.0280 0.0735 0.0440 0.0327 0.0282 0.1373 0.0885 0.0605 0.0474 0.1386 0.0899 0.0601 0.0485

EFF (^SBK,1) 0.9868 1.0228 1.1021 1.1014 0.9515 1.0489 1.0957 1.0801 0.8732 0.9186 0.9294 0.9811 0.8509 0.8950 0.9686 0.9885

std {EFF (^SBK,1)} 0.3813 0.2324 0.3123 0.2161 0.3154 0.2741 0.2306 0.1823 0.3122 0.4027 0.2493 0.2877 0.3356 0.2731 0.2715 0.3050

Table 3
Simulated example 2: the MISEs and EFFs of ^SBK,1, ~K,1 for  = 10,  = 500, 1000, 1500, 2000.

Ratio No. 1 2 3 4

Definition Net Income/Sales Operating Income/Total Assets Ebit/Total Assets Total Liabilities/Total Assets

Ratio No. 5 6 7 8

Definition Cash/Total Assets Inventories/Sales Accounts Payable/Sales log(Total Assets)

Table 4 Real data example 3: Definitions of financial ratios.

Rong Liu Department of Mathematics, University of Toledo, Toledo, OH 43606, USA E-mail: rong.liu@utoledo.edu

Lijian Yang Center for Advanced Statistics and Econometrics Research, Soochow University, Suzhou 215006, People's Republic of China and Department of Statistics and Probability, Michigan State University, East Lansing, MI 48824, USA E-mail: yang@stt.msu.edu

Wolfgang K. Ha¨rdle Center for Applied Statistics and Economics, Humboldt-Universita¨t zu Berlin, Unter den Linden 6, 10099 Berlin, Germany E-mail: stat@wiwi.hu-berlin.de

Efficiency of the 1-st estimator, r=0, a=0

Efficiency of the 1-st estimator, r=0, a=0.5

0 0.5 1 1.5 2 2.5

0 0.5 1 1.5 2 2.5

0 0.5 1 1.5 X
(a)
Efficiency of the 1-st estimator, r=0.5, a=0

0 0.5 1 1.5 X
(b)
Efficiency of the 1-st estimator, r=0.5, a=0.5

0 0.5 1 1.5 2 2.5

0 0.5 1 1.5 2 2.5

0 0.5 1 1.5 X
(c)

0 0.5 1 1.5 X
(d)

Fig 1. Plots of empirical distribution of relative efficiency of  = 500 - dashed line,  = 1000 - dotted line,  = 1500 - thin solid line, = 2000 - thick solid line for (a)  = 0,  = 0, (b)  = 0,  = 0.5, (c)  = 0.5,  = 0, (d)  = 0.5,  = 0.5.

Confidence Level = 0.95, n = 500

Confidence Level = 0.95, n = 1000

-1.5 -1 -0.5 0 0.5 1 1.5 2

-1.5 -1 -0.5 0 0.5 1 1.5 2

0 0.5 x
(a)

1

Confidence Level = 0.95, n = 1500

0 0.5 x
(b)

1

Confidence Level = 0.95, n = 2000

-1.5 -1 -0.5 0 0.5 1 1.5 2

-1.5 -1 -0.5 0 0.5 1 1.5 2

0 0.5 x
(c)

10

0.5 x

(d)

Fig 2. Plots of 1(1) - solid line, ~K,1(1) - dashed line, confidence bands and ^SBK,1(1) - three dotted lines for  = 0,  = 0 and (a)  = 500, (b)  = 1000, (c)  = 1500, (d)  = 2000.

1

Confidence Level = 0.95, n = 500

Confidence Level = 0.95, n = 1000

-1 0 1 2

-1.5 -1 -0.5 0 0.5 1 1.5 2

0 0.5 x
(a)

1

Confidence Level = 0.95, n = 1500

0 0.5 x
(b)

1

Confidence Level = 0.95, n = 2000

-1 0 1 2

-1 0 1 2

0 0.5 x
(c)

10

0.5 x

(d)

Fig 3. Plots of 1(1) - solid line, ~K,1(1) - dashed line, confidence bands and ^SBK,1(1) - three dotted lines for  = 0.5,  = 0.5 and (a)  = 500, (b)  = 1000, (c)  = 1500, (d)  = 2000.

1

Estimation for function m

Estimation for function m

estimation -1.5 -1.0 -0.5 0.0 0.5

0.1 0.2 0.3

estimation

-0.1

-0.3

0.2 0.4 0.6 0.8 x3
(a)

0.2 0.4 0.6 0.8 x8 (b)

Fig 4. Estimations for (a) 3() and (b) 8().

SFB 649 Discussion Paper Series 2011
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Localising temperature risk" by Wolfgang Karl Härdle, Brenda López Cabrera, Ostap Okhrin and Weining Wang, January 2011.
002 "A Confidence Corridor for Sparse Longitudinal Data Curves" by Shuzhuan Zheng, Lijian Yang and Wolfgang Karl Härdle, January 2011.
003 "Mean Volatility Regressions" by Lu Lin, Feng Li, Lixing Zhu and Wolfgang Karl Härdle, January 2011.
004 "A Confidence Corridor for Expectile Functions" by Esra Akdeniz Duran, Mengmeng Guo and Wolfgang Karl Härdle, January 2011.
005 "Local Quantile Regression" by Wolfgang Karl Härdle, Vladimir Spokoiny and Weining Wang, January 2011.
006 "Sticky Information and Determinacy" by Alexander Meyer-Gohde, January 2011.
007 "Mean-Variance Cointegration and the Expectations Hypothesis" by Till Strohsal and Enzo Weber, February 2011.
008 "Monetary Policy, Trend Inflation and Inflation Persistence" by Fang Yao, February 2011.
009 "Exclusion in the All-Pay Auction: An Experimental Investigation" by Dietmar Fehr and Julia Schmid, February 2011.
010 "Unwillingness to Pay for Privacy: A Field Experiment" by Alastair R. Beresford, Dorothea Kübler and Sören Preibusch, February 2011.
011 "Human Capital Formation on Skill-Specific Labor Markets" by Runli Xie, February 2011.
012 "A strategic mediator who is biased into the same direction as the expert can improve information transmission" by Lydia Mechtenberg and Johannes Münster, March 2011.
013 "Spatial Risk Premium on Weather Derivatives and Hedging Weather Exposure in Electricity" by Wolfgang Karl Härdle and Maria Osipenko, March 2011.
014 "Difference based Ridge and Liu type Estimators in Semiparametric Regression Models" by Esra Akdeniz Duran, Wolfgang Karl Härdle and Maria Osipenko, March 2011.
015 "Short-Term Herding of Institutional Traders: New Evidence from the German Stock Market" by Stephanie Kremer and Dieter Nautz, March 2011.
016 "Oracally Efficient Two-Step Estimation of Generalized Additive Model" by Rong Liu, Lijian Yang and Wolfgang Karl Härdle, March 2011.
SFB 649, Ziegelstraße 13a, D-10117 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

