BERLIN

SFB 6 4 9 E C O N O M I C R I S K

SFB 649 Discussion Paper 2009-045
Quantifizierbarkeit von Risiken auf Finanzmärkten
Wolfgang Karl Härdle* Christian Wolfgang Friedrich Kirchner*
* Humboldt-Universität zu Berlin, Germany
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".
http://sfb649.wiwi.hu-berlin.de ISSN 1860-5664
SFB 649, Humboldt-Universität zu Berlin Spandauer Straße 1, D-10178 Berlin

Quantifizierbarkeit von Risiken auf Finanzmärkten

Wolfgang Karl Härdle*

Christian Friedrich Wolfgang Kirchner*

Abstract
Die Krise der internationalen Finanzmärkte hat die allgemeine Wahrnehmung für die in diesen Märkten inhärenten Risiken merklich verändert. Glaubten manche Anleger in den Boomphasen der Finanzmärkte, dass sich eine hohe Kapitalrendite mit geringem Risiko verbinden ließe, wenn man nur die Finanzprodukte entsprechend gestaltete, hat sich diese Wahnvorstellung zwischenzeitlich verflüchtigt. Will man vernünftig mit diesen Risiken umgehen, ist es notwendig, diese quantifizieren zu können. Hier gilt es, eine Reihe methodischer Probleme zu bewältigen, da sich einfache statistische Methodiken als nicht adäquat für die vielschichtigen Finanzmarktrisiken erweisen. Die Vielschichtigkeit dieser Risiken hat in den letzten Jahrzehnten zugenommen, insbesondere seitdem hypothekengesicherte Darlehen in verbriefter und verpackter Form auf Finanzmärkten abgesetzt wurden. Der Fokus der folgenden Ausführungen liegt bei der Quantifizierung der Risikoeinschätzungen, und zwar unter Beachtung von Wahrnehmungsproblemen, wie sie in der modernen Verhaltensökonomik erörtert werden. Daneben werden aber auch Probleme des demographischen Risikos angesprochen.
JEL-Codes: B23, C14, G32, K22 Keywords: pricing kernels, risk aversion, risk neutral density

Acknowledgement: Mit herzlichem Dank für die finanzielle Unterstützung der DFG über den SFB 649 "Ökonomisches Risiko".

* CASE - Center for Applied Statistics and Economics, Humboldt-Universität zu Berlin, Spandauer Straße 1, 10178 Berlin, Germany.

Quantifizierbarkeit von Risiken
auf Finanzmärkten?
WOLFGANG KARL HÄRDLE CHRISTIAN FRIEDRICH WOLFGANG KIRCHNER
1. Einführung
Die Krise der internationalen Finanzmärkte hat die allgemeine Wahrnehmung für die in diesen Märkten inhärenten Risiken merklich verändert. Glaubten manche Anleger in den Boomphasen der Finanzmärkte, dass sich eine hohe Kapitalrendite mit geringem Risiko verbinden ließe, wenn man nur die Finanzprodukte entsprechend gestaltete, hat sich diese Wahnvorstellung zwischenzeitlich verflüchtigt. Es ist der alte Gemeinplatz in aller Munde, dass nämlich alle Lebensbereiche Risiken unterliegen und dass all diejenigen, die Risiken tragen, diese zu verteilen oder zu transferieren trachten. Das gilt auch und insbesondere für Risiken auf Finanzmärkten. Will man vernünftig mit diesen Risiken umgehen, ist es notwendig, diese quantifizieren zu können. Hier gilt es, eine Reihe methodischer Probleme zu bewältigen, da sich einfache statistische Methodiken als nicht adäquat für die vielschichtigen Finanzmarktrisiken erweisen. Die Vielschichtigkeit dieser Risiken hat in den letzten Jahrzehnten zugenommen, insbesondere seitdem hypothekengesicherte Darlehen in verbriefter und verpackter Form auf Finanzmärkten abgesetzt wurden. Hier werden Immobilienmarktrisiken mit Finanzmarktrisiken verknüpft. Hinzu treten das Verbriefungsrisiko und das damit verbundene Ratingrisiko. Ein weiteres Risiko implizieren die spezifischen Formen der Verpackung und Verbriefung hypothekenbesicherter Kredite ­ nämlich das
1

WOLFGANG HÄRDLE / CHRISTIAN F.W. KIRCHNER
der Entbündelung: Werden einige Kredite notleidend, können sie nicht getrennt saniert werden.
Zu den genannten Finanzmarktrisiken kommt dann das so genannte demographische Risiko, wenn nämlich bei Systemen kapitalbasierter Alterssicherung wiederum die Kapitalanlage auf Finanzmärkten erfolgt. Diese Kapitalanlage kann wiederum in Verbriefungen hypothekenbesicherter Kredite bestehen. Werden bei der kapitalbasierten Alterssicherung Finanzmarktrisiken direkt mit demographischen Risiken verknüpft, ist der Konnex bei der umlagenfinanzierten Alterssicherung anders gestaltet. Hier treten Faktoren der laufenden Finanzierung der laufenden Pensionszahlungen (Renten) aus dem Arbeitseinkommen des arbeitenden Teils der Bevölkerung hinzu.
Sowohl bei Finanzmarktrisiken wie bei demographischen Risiken sind Risikoeinschätzungen erforderlich, wenn man den institutionellen Rahmen für Finanzmärkte gestalten oder Alterssicherungssysteme verbessern will. Der Fokus der folgenden Ausführungen wird bei der Quantifizierung der Risikoeinschätzungen liegen, und zwar unter Beachtung von Wahrnehmungsproblemen, wie sie in der modernen Verhaltensökonomik erörtert werden. Daneben werden aber auch Probleme des demographischen Risikos angesprochen.
2. Grundlegende methodische Annahmen
Bei den folgenden Überlegungen wird von der Annahme unvollständiger Information, beschränkter Rationalität (bounded rationality) ­ im Sinne der modernen Verhaltensökonomik ­ und von der Existenz von Transaktionskosten ausgegangen. Diese zwei Annahmen sind deshalb für die Untersuchung erheblich, weil von den tatsächlichen Handlungsbedingungen auszugehen ist, unter den Menschen Entscheidungen in Bezug auf Aktivitäten auf Finanzmärkten und in Bezug auf die Alterssicherung treffen. Zu diesen tatsächlichen Bedingungen zählen Transaktionskosten und die Unvollständigkeit der Information. Zudem ist zu berücksichtigen, wie Menschen Entscheidungen treffen. Könnte vollständige Information, das Fehlen von Transaktionskosten und vollständige Rationalität vorausgesetzt werden, wären Finanzmärkte ex definitione effizient. Es gäbe kein Problem. Fraglich ist dann aber, ob mit einem solchen Ansatz brauchbare Risikoeinschätzungen erfolgen können.
2

QUANTIFIZIERBARKEIT VON RISIKEN AUF FINANZMÄRKTEN?
3. Entwicklung der statistischen Methodik
Die mathematische Statistik geht zurück auf eine Teildisziplin der Geographie und Astronomie gegen Ende des achtzehnten Jahrhunderts. Das stochastische Denken entstand in einer Zeit, in der die Naturwissenschaften die Newtonschen Naturgesetze hochhielten. Die Newtonschen Gesetze, an denen sich auch Ökonomen wie David Riccardo orientierten, arbeiteten mit exakten Kausalzusammenhängen; wohingegen das stochastische Denken an Zustandswahrscheinlichkeiten interessiert war. Ist es nicht verwunderlich, dass sich die Naturwissenschaften von dem mechanistischen Denkansatz früher befreiten als die Ökonomen? Die Entdeckung des Nutzens der Stochastik für die Ökonomik ging einen ähnlichen Weg wie die Erkenntnis der Heisenbergschen Unschärferelation für die Physik des letzten Jahrhunderts: Sie bereitete in den 50er Jahren des letzten Jahrhunderts den Weg für die Ökonometrie. Erst damit eröffnete sich die Möglichkeit, ökonomische Hypothesen empirisch zu überprüfen.
Exkurs. Lotterie Das Lotto, wie wir es heute kennen, entstand in Genua als Verfahren
zur >fairen< Auswahl des Genueser Konzils. Aus einer großen Zahl einflussreicher Patrizierfamilien wurde eine Untermenge durch Lottoverfahren ermitteln. Ausgangspunkt war die Annahme gleicher Wahrscheinlichkeiten für jede der Ziehungen. Die Anzahl der Möglichkeiten ist gegeben durch:
Lotteriespiele wurden daraufhin von den Regierungen als Einnahmequelle entdeckt. Für das deutsche Lotto ist s=49 und r=6 zu setzen, welches insgesamt zu einer Wahrscheinlichkeit von 1/13.983.816 für >sechs Richtige< führt. Die Verdienstmöglichkeiten durch Lottospiele wurden durch Casanova popularisiert. Insbesondere in Frankreich erfreute sich die Lotterie großer Beliebtheit.
3

WOLFGANG HÄRDLE / CHRISTIAN F.W. KIRCHNER
Schaubild 1. Genua und Casanova (als Lotterieexperte)
Ein Test von ökonomischen Zusammenhängen erfordert den Einsatz statistischer Mittel zunächst einmal auf der Ebene der Datenerhebung, aber ganz besonders in der Durchführung der Modellanpassung an ökonomische Daten. Die Statistik konnte erst zur wissenschaftlichen Disziplin werden, als sie über eine ausreichende Datenbasis verfügte. Dies verdankte sie insbesondere den für Steuerzwecke notwendigen Datenerhebungen, was allerdings des Öfteren zu einer Diskrepanz zwischen den vorhandenen Datensätzen und den für die ökonomischen Fragestellungen relevanten Daten führte. Wenn beispielsweise die Ökonomen an der Bekämpfung der Arbeitslosigkeit interessiert waren, lieferte ihnen das Datenmaterial Angaben zur Einkommensverteilung, also Daten von in Arbeit stehenden Personen.
Vor den 50er Jahren des letzten Jahrhunderts arbeitete die Statistik mit einem relativ simplen methodischen Ansatz. Im Zentrum standen Häufigkeitsverteilungen vom Gaußschen Typ (komparativ statischer Ansatz) und das Paradigma der Unabhängigkeit der Beobachtungen (Produktmaß) und deren identische Verteilung. Damit wurden Heterogenitäten, beispielsweise verursacht durch verschiedene psychologische Einflussfaktoren, zwangsläufig ausgeblendet. Darüber hinaus konnten die Verknüpfungen über die Zeit nicht dargestellt werden. Die dynamischen Aspekte statistischer Daten konnten dann erst durch die Einführung von Zeitreihenanalysen erfasst werden. Der hohen Dimensionalität, Abhängigkeit und Heterogenität der ökonomischen Entwicklungen wurde mit dem Werkzeug der multivariaten statistischen Analyse Rechnung getragen.
Aber selbst in diesem hochentwickelten Instrumentarium finden sich eine Reihe versteckter Annahmen, die den heuristischen Wert auch dieses Ansatzes schmälern. Da es in erster Linie um Interpretation der Daten durch ein statistisches Modell geht, ist eine Metrik erforderlich. Die
4

QUANTIFIZIERBARKEIT VON RISIKEN AUF FINANZMÄRKTEN?
Wahl der Metrik steuert damit das Ergebnis. In der konventionellen Herangehensweise, die im Wesentlichen durch Ronald A. Fisher bestimmt wurde, wird das sogenannte Maximum-Likelihood-Prinzip verwendet, das einer Kullback-Leibler-Divergenz entspricht, das aber keineswegs selbstverständlich ist, Fisher (1936). Im Falle der Gaußschen Normalverteilung führte dieses Anpassungsprinzip zu simplen, leicht optimierbaren Algorithmen, von denen aber keineswegs sicher ist, ob sie die Zustandswahrscheinlichkeiten richtig erfassen.
Eine noch fundamentalere Annahme als die bisher erörterten betrifft die Stationarität der Verteilung der Daten. Stationarität bedeutet, dass die Verteilung über die Zeit konstant bleibt. Ist dieses aber nicht der Fall, führt die Annahme der Stationarität zu fehlerhaften Aussagen. Dies zeigt sich, wenn man an Strukturbrüche in der Stochastik der Ökonomik denkt. Eine Vielzahl von Beispielen kann dies belegen. Die durch technologische Fortschritte bedingten Lohnentwicklungen, die die Substitution von Arbeit und Kapital berühren und die unterschiedlich schnell in verschiedenen Szenarien verlaufen, sind ein solches. Andere Beispiele sind plötzliche klimatische Veränderungen oder die Verlagerung von bewaffneten Auseinandersetzungen hin zu Kleingruppenterrorismus. Näheres zu nichtstationären Phänomenen z.B. von immobilienbesicherten Finanzinstrumenten erörtern wir in Abschnitt 4.
Ein weiteres Defizit der konventionellen Ansätze besteht in einem Modellrisiko. Die Substruktur ökonomischer Modelle geht oft unbefragt von bestimmten Zusammenhängen der Variablen aus, wie z.B. in der Cobb-Douglas-Funktion der Fall ist. Die Cobb-Douglas-Funktion nimmt einen logarithmisch linearen Zusammenhang für die Substitution von Kapital und Arbeit an, ohne dass dieses durch irgendwelche empirischen Fakten belegt wäre. Sie wird typischerweise hergeleitet aus der Überlegung abnehmender Grenzproduktivitäten von Kapital und Arbeit. Dies belegt beispielhaft, wie durch aus der Luft gegriffene Modellannahmen die Analyse gesteuert wird. Ein Gegenbeispiel reflektierter ökonomischer Analyse ist die Arbeit von Ernst Engel, der durch nichtparametrische Verfahren das Engelsche Gesetz »Je ärmer eine Familie ist, einen desto größeren Anteil von den Gesamtausgaben muss sie zur Beschaffung der Nahrung aufwenden« entdeckt hat. Dies ist eine Aussage, die Phänomene in vielen Entwicklungsländern noch heute gut erklären kann.
All diese Ansätze arbeiten mit der Annahme unbeschränkter, vollständiger Rationalität. Dieses wird besonders deutlich, wenn mit der Maximierung des erwarteten Nutzens gearbeitet wird. Verhalten sich Menschen aber nur beschränkt rational, kann mit solchen Annahmen keine tragfähige Prognose erstellt werden. Eine typische Verhaltens-
5

WOLFGANG HÄRDLE / CHRISTIAN F.W. KIRCHNER
anomalie ist die unterschiedliche Gewichtung erwarteter Nutzensteigerungen und Nutzenminderungen. Ein anderes Beispiel ist die systematische Fehleinschätzung kleiner Wahrscheinlichkeiten. Letztendlich spielen Überoptimismus in der Aufschwungsphase und entsprechend Überpessimismus in der Abschwungsphase eine entscheidende Rolle für die Erklärung von Auf- und Abschwungsbewegungen auf den Finanzmärkten. Die neuere Verhaltensökonomik (behavorial economics) schlägt daher vor, die Annahme unbeschränkter Rationalität durch die beschränkte Rationalität zu ersetzen. Exkurs. Wahrscheinlichkeit
Ein bekanntes Beispiel der Fehleinschätzung von Wahrscheinlichkeiten ist das Drei-Türen-Spiel. Darin werden drei Türen gezeigt mit dem Hinweis darauf, dass sich hinter einer von diesen ein wertvoller Preis (z.B. ein PKW) verbirgt. Nach Wahl einer Tür (die aber noch nicht geöffnet wird) offeriert der Spielleiter zusätzliche Information, indem er eine der nicht gewählten Türen öffnet, hinter der sich eine Bergziege verbirgt. Nun wird der Spieler befragt, ob er die Tür wechseln möchte, oder ob er bei seiner bei seiner Erstwahl bleiben möchte. In der Mehrzahl der Fälle stellt sich heraus, dass die zusätzliche Information für das Entscheidungsproblem ignoriert wird, da davon ausgegangen wird, dass die Wahrscheinlichkeit des Preises weiterhin bei 1/3 liegt. Tatsächlich ist die Wahrscheinlichkeit bei der Strategie »Wechsele die Tür« genau 2/3, siehe Schaubild 2.
Schaubild 2. Entscheidungsproblem Bergziege/PKW
6

QUANTIFIZIERBARKEIT VON RISIKEN AUF FINANZMÄRKTEN?
Angesichts dieser Vielzahl kritischer Einwände gegen methodische Ansätze in der Statistik und Ökonomik könnte der Gedanke nahe liegen, die Hoffnung auf eine Quantifizierung von Risiko aufzugeben. Damit wäre das Fragezeichen in der Überschrift dieses Artikels zugleich das Ende der Diskussion. Ein Ausweg könnte aber darin liegen, die methodischen Einwände zu verarbeiten, um moderne tragfähigere Konzepte zu entwickeln.
Bei suffizienter Datenmenge wäre ein adäquater Ausweg, die angemessene Metrik asymptotisch zu identifizieren. Diese Lösung stößt jedoch an Transaktionskostengrenzen, da die Beschaffung solcher Datenmengen mit Kosten verbunden wären, die durch den damit erzeugten Erkenntnismehrwert nicht gerechtfertigt wären. Dieses Problem lässt sich auch nicht durch eine Kostendegression bei der Datenerhebung lösen, da gleichzeitig die Heterogenität steigt. Eine Hilfslösung, die uns nicht ans Ziel aber näher heran führt, ist die robuste Statistik. Sie versucht, bei möglichst freier Wahl der zugrunde liegenden Verteilung Effizienz- und Verlustschranken möglichst im Rahmen zu halten.
Lokale Stationarität ist die Antwort auf nicht vorhersehbare Änderungen der Stationarität. Da es nicht möglich ist, Änderungen der Stationarität vorherzusehen, kann ein pragmatischer Problemzugang nur darin bestehen, Indikatoren für Stationaritätsänderungen zu entwickeln. Dies ist ein sehr aktuelles Thema der Statistik und wird etwa von Lo, Härdle und Spokoiny behandelt.
4. Analyse
4.1 Risiken 4.1.1. Finanzmärkte
Die Käufer der durch Immobilien besicherten Finanzinstrumente versuchen das Risiko einzuschätzen. Sie müssen davon ausgehen, dass die Kreditgeber, die die Kredite verbriefen, das Kreditrisiko weitergeben können und damit einem moralischen Risiko ausgesetzt sind. Aus diesem Grunde benötigen sie eine Bonitätseinschätzung durch unabhängige Dritte (etwa durch Rating-Agenturen). Gehen sie davon aus, dass diese Rating-Agenturen ihrerseits keinem moralischen Risiko ausgesetzt sind, verlassen sie sich auf deren Risikoeinschätzungen. Der Kaufanreiz wird gesteuert durch eine exzellente Risikoeinschätzung verbunden mit der Erwartung einer besonders hohen Rendite. Die Einschätzung, dass bei den Rating-Agenturen kein moralisches Risiko vorläge, erweist sich
7

WOLFGANG HÄRDLE / CHRISTIAN F.W. KIRCHNER
dann als falsch, wenn letztere sowohl das Rating vornehmen als auch beratend bei den Verbriefungs- und Verpackungsaktivitäten beteiligt sind.
Hinzu kommt, dass die Institute, die Kredite verbriefen und verpacken, diese an Zweckgesellschaften (special investment vehicles: SIV) veräußern, die ihrerseits den Ankauf dieser Finanzinstrumente durch Ausgabe von kurzfristigen Schuldtiteln (collateralized debt obligations: CDO) finanzieren. Da dies eine kurzfristige Refinanzierung langfristiger Verbindlichkeiten bedeutet, benötigen die SIV entsprechende Liquiditätszusagen der Finanzinstitute, von denen sie diese hybriden Finanzinstrumente übernehmen. Kommt irgendwann der Absatz der CDOs ins Stocken, müssen die Finanzinstitute, die die Liquiditätszusagen gegeben haben, diese einlösen. Dies war genau der Fall als z.B. die sächsische Landesbank ihre Liquiditätszusagen gegenüber ihrem SIV einlösen und deren CDO in die eigene Bilanz übernehmen musste.
Exkurs. Stochastik der Finanzmärkte
Schaubild 3 Dax und stochastische Modellierung
Das Verhalten z.B. des DAX wird oft mit so genannten stochastischen Differentialgleichungen beschrieben. In Schaubild 3 wird ein stochastischer Prozess simuliert und seine zukünftige Wahrscheinlichkeitsverteilung (schraffierte Fläche) dargestellt Die schraffierte Fläche gibt die Zustandswahrscheinlichkeiten (Arrow-Debreu-Preise) des Aktienkurses an. Damit können Derivate und Optionen auf den Aktienkurs berechnet werden. Für diese Simulation wurde eine Gausssche Normalverteilung zugrunde gelegt und angenommen, dass sich deren Volatilität (Standardabweichung) über die Zeit nicht ändert. Trifft diese Annahme nicht zu, ändern sich die Ergebnisse.
8

QUANTIFIZIERBARKEIT VON RISIKEN AUF FINANZMÄRKTEN?
4.1.2 Demographie
Die Risiken der demographischen Entwicklung stellen sich unterschiedlich dar bei umlagenfinanzierten und kapitalbasierten Alterssicherungssystemen. Bei kapitalbasierten Alterssicherungssystemen taucht das Problem des demographischen Risikos bei der Prognose der langfristigen Rendite Erwartung der Kapitalanlage auf.
4.1.2.1 Kapitalbasierte Alterssicherungssysteme Die Erwartungen bezüglich der Rentabilität der Finanzanlagen hän-
gen auch vom demographischen Risiko ab. Der Einfluss einer sich verschlechternden demographischen Struktur kann bei unterschiedlichen Arten der Finanzanlage positiv oder negativ niederschlagen. So können sich bei Investitionen in Seniorenstifte positive Effekte ergeben, bei Investitionen in Kinderspielplätze dagegen negative. Bezogen auf immobilienbesicherte Finanzinstrumente sind offensichtlich die Arten von Immobilien entscheidend.
4.1.2.2 Umlagenfinanzierte Alterssicherungssysteme Auf den ersten Blick scheint es einen linearen Zusammenhang zwi-
schen der Entwicklung der Alterspyramide und der Belastung der jüngeren Generation durch Zahlung für die Alterssicherung der älteren Generation zu geben. Dieser Zusammenhang ist genauso simpel wie falsch. Entscheidend sind folgende Faktoren: (1) Die Entwicklung der Lohnsumme, (2) die Belastbarkeit der Zahlungsverpflichteten, (3) die Struktur der Arbeitnehmerschaft (etwa der Anteil der weiblichen Arbeitnehmerinnen), (4) die unterschiedliche Belastung verschiedener Einkommensarten. Für die verschiedenen Einschätzungen sind unterschiedliche Annahmen relevant; dazu mehr in Abschnitt 4.2.
4.2 Konventionelle Analyse
4.2.1 Finanzmarktrisiko
Die Rating-Agenturen haben das Rating für die verpackten und verbrieften Hypothekarkredite mit Blick auf die Laufzeit dieser Kredite erstellt. Die kurzfristige Finanzierung der CDO war dann aus dem Gleichgewicht gebracht, als der Marktwert der zugrunde liegenden Grundstücke der immobilienbesicherten Finanzinstrumente sank. Die konventionelle Analyse geht davon aus, dass der Wert der Finanzmarktinstrumente in stabiler Weise dadurch gesichert war, dass Portefeuilles von verschiedenen Bonitätsstufen von immobilienbesicherten Krediten gebildet wur-
9

WOLFGANG HÄRDLE / CHRISTIAN F.W. KIRCHNER
den. Die Ratingklassen umfassten den Bereich von erstklassigen Hypotheken bis hinunter zu Ramschhypotheken (subprime mortgages). Genauer nahm die konventionelle Analyse an, dass sich die Bonitäten über die Zeit nicht drastisch veränderten, so wie wir es bei Strukturbrüchen typischerweise beobachten. Ebenso blendete die konventionelle Sichtweise die Wertentwicklung des CDO in Abhängigkeit von den sich über die Zeit häufenden Insolvenzen (defaults) aus. Geht die konventionelle Analyse nach wie vor von rationalem Verhalten aus, nimmt sie das Phänomen des Herdenverhaltens und des Überpessimismus nicht in den Blick. Der konventionellen Analyse zufolge hätten die Marktpreise der CDO parallel zu den Immobilienpreisen fallen müssen. Die psychologisch bedingten Transaktionen führten bekanntermaßen genau ins Gegenteil. Zudem wurde das Zusammenspiel der Regulierung über Standards der Rechnungslegung und der Mindesteigenkapitalvorschriften für Banken (Basel II) nicht gesehen. Sinkende Wertansätze von Finanzinstrumenten auf der Aktivseite führten zur Reduktion des bankenaufsichtsrechtlich relevanten Eigenkapitals. Dies zwang Banken zu weiteren Verkäufen von CDO. Dies führte zu einem Zusammenbruch der CDO-Märkte.
4.2.2 Demographisches Risiko
4.2.2.1 Kapitalbasierte Alterssicherungssysteme Ein erstaunliches Merkmal der konventionellen Analyse des demog-
raphischen Risikos ist die Extrapolation existierender Szenarien, ohne die in 4.1.2.2. genannten Faktoren zu berücksichtigen. Das schließt nicht aus, dass moderne Versicherungspolicen diese Faktoren sehr wohl mit einbeziehen. Auch das häufig ins Spiel gebrachte Lee-Carter-Modell geht von einem stationären Modell über die Zerlegung in statische Hauptfaktoren aus. Diese statische Zerlegung bedeutet im Klartext, dass z.B. Faktoren wie Migration oder Reproduktionsindex konstant über die Zeit angenommen werden.
4.2.2.2 Umlagenfinanzierte Alterssicherungssysteme Bei umlagenfinanzierten Alterssicherungssysteme werden in der Re-
gel demographische Entwicklungen einfach extrapoliert ohne die unter 4.1.2.2. erwähnten Faktoren in Rechnung zu stellen. Dies kann von politischen Entscheidungsträgern dergestalt instrumentalisiert werden, dass sie der veränderten Wählerstruktur Rechnung tragen, um neue Mehrheiten einer alternden Bevölkerungsstruktur zu gewinnen.
10

QUANTIFIZIERBARKEIT VON RISIKEN AUF FINANZMÄRKTEN?
4.3 Moderne nichtparametrische Analyse
4.3.1 Finanzmarktrisiko
Bevor man daran geht, Entwicklungen der Finanzmärkte zu prognostizieren, ist es ratsam, zwischen zwei Handlungsebenen zu unterscheiden, nämlich der Ebene der Entscheidungen auf Märkten und der Ebene der Handlungsbedingungen (Regulierung). Auf beiden Ebenen agieren die Akteure beschränkt rational. Mit geänderten Regulierungen ändern sich auch die für den Statistiker sichtbaren stochastischen Grundlagen. Die auf der Handlungsebene tätigen Akteure versuhen ihrerseits, Einfluss auf die Regulierungsebene zu nehmen.
Trotz starker Regulierung der klassischen Bankgeschäfte gab es die Möglichkeit für Regulierungsarbitrage, insbesondere für Verbriefungsaktivitäten, für Rating-Agenturen, Investmentbanking und Hedgefonds. In der Tat wurde die Regulierung für Investmentbanken in den USA in Bezug auf die Eigenmittelunterlegung der übernommenen Risiken sogar gelockert. Dem folgte der Londoner Finanzplatz. Aus Sicht der politischen Entscheidungsträger war dieses angesichts der anscheinend sicheren Kapitalrenditen rational. In diesem Marktumfeld spielten Abweichungen von der Annahme vollständiger Rationalität eine wichtige Rolle (Überoptimismus, Überpessimismus, Herdenverhalten). Zu beantworten ist die Frage, wie die massiven Investitionen in CDO erklärt werden konnten und wer als Käufer auftrat.
Man konnte Papiere erwerben, deren Rendite weit über den Kosten der Refinanzierung lag. Dabei konnten sich Geschäftsbanken spezieller Zweckgesellschaften (SIV) bedienen, die sie allerdings über Liquiditätszusagen absichern mussten. Dieses Schneeballsystem (Ponzi scheme) lief solange, wie neue CDO geformt werden konnten. In diese Phase des Überoptimismus setzte ein Herdenverhalten ein, dem sich auch seriöse Finanzinstitute nicht vollständig entziehen konnten. In der Schlussphase wurden CDO auch geformt auf der Grundlage von Hypothekarkrediten von Immobilienerwerbern niedrigster Bonität (Ramschhypotheken, subprime mortgages). Eine Analyse, die alle Faktoren des Marktes mit einbezieht, hätte vorhersagen müssen, dass ein Einbruch bei den Immobilienpreisen diesem System die Grundlagen entziehen würde. Jetzt wurde die Korrelation zwischen den unterschiedlichen Bonitätsklassen der Hypothekarkredite sichtbar, ein Faktor der offensichtlich im Rating nicht berücksichtigt war. Die Reaktion der Rating-Agenturen war eine Herabstufung der Bonitäten der betreffenden Finanzinstrumente.
Aus statistischer Sicht sind die beobachtbaren Phänomene der Finanzmärkte nicht mit einem endlich dimensionalen Parameterraum ­
11

WOLFGANG HÄRDLE / CHRISTIAN F.W. KIRCHNER
z.B. linearen Modellen ­ abbildbar. In den letzten zwei Jahrzehnten entstanden nichtparametrische Verfahren, die eine flexible Dimensionalität aufweisen und typischerweise an Querschnittsdaten (Konsumstichproben) kalibriert werden. Unter moderner nicht parametrischer Statistik versteht man die Anwendung solcher Verfahren in einem dynamischen Kontext unter gleichzeitiger Adaption an >lokale Stationarität<. Unter lokaler Stationarität versteht man ein Zeitintervall, in dem man ohne Schwierigkeiten Stationarität annehmen kann. Moderne adaptive statistische Verfahren identifizieren solche Zeitintervalle und sind daher geeignet, auch kleinste Stabilitätsintervalle bei der Analyse der Finanzmärkte zu identifizieren. Ein konkretes Beispiel der Nichtanwendung moderner adaptiver Verfahren ist die Riskmetrics Technology, die genau ein Jahr zurückschaut und aus diesen Informationen den Value at Risk bestimmt. Die Rating-Agenturen gingen genauso vor, in dem sie feste Stationaritätsintervalle extrapolierten, ohne eine Adaption an sich ändernde Bedingungen zuzulassen. Moderne Techniken können den Value at Risk aus sich in der Zeit verändernden Intervallen bestimmen.
Jede Analyse eines Finanzmarktrisikos muss daher im Rahmen der gegebenen Stationarität analysiert werden. Damit rückt in den Vordergrund die Analyse der sich verändernden Stationaritäten. Das bedeutet für die Analyse der Finanzmärkte, dass das sich ständig ändernde regulatorische Umfeld in den Blick zu nehmen ist. Insbesondere spielten die Weiterexistenz oder Eliminierung der Regulierungsarbitrage, nationale Unterschiede im Regulierungsniveau, die Abstimmung der Bankenaufsichtsregulierung mit der Regulierung über Rechnungslegungsstandards, sowie die Einführung sogenannter weicher Verhaltensstandards (soft law) eine Rolle. Damit können wir Aussagen produzieren für das gegebene Finanzmarktrisiko auf den jetzigen Märkten. Insbesondere können Metaparameter der modernen nichtparametrischen Verfahren damit eingestellt werden. Metaparameter sind die Stellgrößen, die verwendet werden, um die Sensibilitäten der Identifikation der Stationaritätsintervallen zu steuern.
4.3.2 Demographisches Risiko
Auch beim demographischen Risiko führen Ansätze, die mit vereinfachten Parametersätzen arbeiten, zu systematisch falschen Ergebnissen. Bisherige Verfahren (Lee Carter, 1992) identifizieren zwar Faktoren wie Migration und Reproduktionsindex, sind aber nicht sensibel für sich verändernde Rahmenbedingungen (Steuersystem, Beschäftigtenanteil, Erwerbsquote etc.). Die relevanten Metaparameter können auch hier als exogene Faktoren abgelesen werden.
12

QUANTIFIZIERBARKEIT VON RISIKEN AUF FINANZMÄRKTEN?
5. Ergebnisse
Entscheidend für die lokal adaptive Wahl von Stationarität ist der folgende Algorithmus zur Bestimmung von Stationaritätsintervallen: 1. Für jeden Zeitpunkt t bestimme eine Folge von Testintervallen I_1,
I_2, I_3, ..., I_m und berechne einen kritischen Wert c_a für die multiplen Intervalle. 2. Beginnend beim Intervall I_1 teste die Homogenität (Stationarität) der Intervalle mit Hilfe eines Likelihoodquotiententestes. Dessen Signifikanzniveau ist ein steuerbarer Metaparameter. Ergebnis: ein gewähltes Intervall I*. 3. Berechne das Risiko auf diesem Intervall I*.
In einer Reihe von Anwendungen zur Berechnung des Value at Risk zeigt sich via backtesting dieses moderne adaptive Verfahren den klassischen Verfahren gegenüber überlegen. Das Risiko, gemessen beispielsweise durch ein Quantil der Gewinn- und Verlustverteilung wird wesentlich genauer bestimmt.
Bei gegebener Schranke des lokalen Bias (SMB Bedingung) gibt es ein »oracle stationäres Intervall«, in dem klassische Verfahren angewandt werden können. Das Orakel Intervall hat seinen Namen daher, dass wir es bei Kenntnis der Risikodynamik berechnen könnten. In einer nichtparametrischen Situation hat man jedoch keine Kenntnis dieser Dynamik, sondern lediglich Schranken und das Risiko:
Hier bezeichnet f(·) die uns unbekannte stochastische Risikodynamik und  die SMB Schranke. L(.,.) den Likelihoodquotienten in einen konventionellen System und (.) eine Schranke an den Likelihood in einem konventionellen System. Die obige Formel gibt also genau an, wie viel man >bezahlen muss< für eine lokale nichtparametrische Modellwahl.
Die moderne nicht parametrische Statistik hat nun mit dem oben angeführten Algorithmus bewiesen, dass dieses Orakel Intervall adaptiv aus den Daten bestimmt werden kann. Der Algorithmus ist adaptiv, da er an jedem Zeitpunkt t erneut eine Approximation an das Orakel Intervall findet. Genauer wird das Risiko in folgender Formel beschrieben:
13

WOLFGANG HÄRDLE / CHRISTIAN F.W. KIRCHNER
Der Algorithmus hat eine Folge von kritischen Werten z_k^* zu bestimmen, die unter der Nullhypothese (globale konventionelle Analyse) berechnet werden. Der kritische Wert hängt ab von (a) der SMB Schranke, (b) dem Likelihood-Verhalten in den Flanken der Profit und Verlustverteilung.
Eine Reihe von Untersuchungen haben die Insensitivität des Likelihood festgestellt: Chen, Härdle, Spokoiny (2009). Die SMB Schranke ist genau der Parameter, der exogen vorgegeben werden kann; er entspricht der Einstellung des Fehlers erster Art bei einem statistischen Test. Erwartet man aufgrund von höheren Regulierungen eine Änderung in der Stochastik wird dieser Parameter kleiner gemacht.
Literatur
Chen, Ying / Härdle, Wolfgang / Spokoiny, Vladimir: GHICA ­ Value at risk analysis with Generalized Hyperbolic Distributions and Independent Components, in: Empirical Finance, 2009 (im Erscheinen).
Engel, Ernst: Die Productions- und Consumtionsverhältnisse des Königreichs Sachsen, in: Zeitschrift des statistischen Bureaus des Königlich Sächsischen Ministerium des Inneren 8/9 (1857).
Fisher, Roland A.: Statistical Methods for Research Workers, Edinburgh: Oliver & Boyd 1925
Härdle, W. und Spokoiny, V. (2009) Modern Nonparametric Statistics. A Course.
Homann, Karl / Suchanek, Andreas: Ökonomik. Eine Einführung, Tübingen: Mohr Siebeck 22005.
Kahneman, Daniel / Tversky, Amos: Prospect theory: An analysis of decisions under risk, in: Econometrica 47, (1979), S. 263-292.
Kirchner, Christian: New Challenges to the Rationality Assumption. Comment zum gleichnamigen Artikel von Daniel Kahnemann, in: Zeitschrift für die gesamte Staatswissenschaft/Journal of Institutional and Theoretical Economics 150 (1994), S. 37-41.
Lee, Roland D. / Carter, Lawrence R.: Modelling and Forecasting U.S. Mortality, in: Journal of the American Statistical Association 87 (1992) S. 659-671.
14

QUANTIFIZIERBARKEIT VON RISIKEN AUF FINANZMÄRKTEN?
Lo, Andrew: The Adaptive Market Hypothesis, in: The Journal of Investment Consulting 7,2(2005), S. 21-44
Monatsbericht der Deutschen Bundesbank, Juli 2009
http://www.bundesbank.de/download/volkswirtschaft/monatsbericht e/2009/200907mb_bbk.pdf Stigler, Stephen M.: Statistics on the Table: The History of Statistical Concepts and Methods, Cambridge/Mass.: Harvard UP, 1999. Mit herzlichem Dank für die finanzielle Unterstützung der DFG über den SFB 649 »Ökonomisches Risiko«, Humboldt-Universität zu Berlin.
15

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
001 "Implied Market Price of Weather Risk" by Wolfgang Härdle and Brenda López Cabrera, January 2009.
002 "On the Systemic Nature of Weather Risk" by Guenther Filler, Martin Odening, Ostap Okhrin and Wei Xu, January 2009.
003 "Localized Realized Volatility Modelling" by Ying Chen, Wolfgang Karl Härdle and Uta Pigorsch, January 2009.
004 "New recipes for estimating default intensities" by Alexander Baranovski, Carsten von Lieres and André Wilch, January 2009.
005 "Panel Cointegration Testing in the Presence of a Time Trend" by Bernd Droge and Deniz Dilan Karaman Örsal, January 2009.
006 "Regulatory Risk under Optimal Incentive Regulation" by Roland Strausz, January 2009.
007 "Combination of multivariate volatility forecasts" by Alessandra Amendola and Giuseppe Storti, January 2009.
008 "Mortality modeling: Lee-Carter and the macroeconomy" by Katja Hanewald, January 2009.
009 "Stochastic Population Forecast for Germany and its Consequence for the German Pension System" by Wolfgang Härdle and Alena Mysickova, February 2009.
010 "A Microeconomic Explanation of the EPK Paradox" by Wolfgang Härdle, Volker Krätschmer and Rouslan Moro, February 2009.
011 "Defending Against Speculative Attacks" by Tijmen Daniëls, Henk Jager and Franc Klaassen, February 2009.
012 "On the Existence of the Moments of the Asymptotic Trace Statistic" by Deniz Dilan Karaman Örsal and Bernd Droge, February 2009.
013 "CDO Pricing with Copulae" by Barbara Choros, Wolfgang Härdle and Ostap Okhrin, March 2009.
014 "Properties of Hierarchical Archimedean Copulas" by Ostap Okhrin, Yarema Okhrin and Wolfgang Schmid, March 2009.
015 "Stochastic Mortality, Macroeconomic Risks, and Life Insurer Solvency" by Katja Hanewald, Thomas Post and Helmut Gründl, March 2009.
016 "Men, Women, and the Ballot Woman Suffrage in the United States" by Sebastian Braun and Michael Kvasnicka, March 2009.
017 "The Importance of Two-Sided Heterogeneity for the Cyclicality of Labour Market Dynamics" by Ronald Bachmann and Peggy David, March 2009.
018 "Transparency through Financial Claims with Fingerprints ­ A Free Market Mechanism for Preventing Mortgage Securitization Induced Financial Crises" by Helmut Gründl and Thomas Post, March 2009.
019 "A Joint Analysis of the KOSPI 200 Option and ODAX Option Markets Dynamics" by Ji Cao, Wolfgang Härdle and Julius Mungo, March 2009.
020 "Putting Up a Good Fight: The Galí-Monacelli Model versus `The Six Major Puzzles in International Macroeconomics'", by Stefan Ried, April 2009.
021 "Spectral estimation of the fractional order of a Lévy process" by Denis Belomestny, April 2009.
022 "Individual Welfare Gains from Deferred Life-Annuities under Stochastic Lee-Carter Mortality" by Thomas Post, April 2009.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
023 "Pricing Bermudan options using regression: optimal rates of convergence for lower estimates" by Denis Belomestny, April 2009.
024 "Incorporating the Dynamics of Leverage into Default Prediction" by Gunter Löffler and Alina Maurer, April 2009.
025 "Measuring the effects of geographical distance on stock market correlation" by Stefanie Eckel, Gunter Löffler, Alina Maurer and Volker Schmidt, April 2009.
026 "Regression methods for stochastic control problems and their convergence analysis" by Denis Belomestny, Anastasia Kolodko and John Schoenmakers, May 2009.
027 "Unionisation Structures, Productivity, and Firm Performance" by Sebastian Braun, May 2009.
028 "Optimal Smoothing for a Computationally and Statistically Efficient Single Index Estimator" by Yingcun Xia, Wolfgang Härdle and Oliver Linton, May 2009.
029 "Controllability and Persistence of Money Market Rates along the Yield Curve: Evidence from the Euro Area" by Ulrike Busch and Dieter Nautz, May 2009.
030 "Non-constant Hazard Function and Inflation Dynamics" by Fang Yao, May 2009.
031 "De copulis non est disputandum - Copulae: An Overview" by Wolfgang Härdle and Ostap Okhrin, May 2009.
032 "Weather-based estimation of wildfire risk" by Joanne Ho and Martin Odening, June 2009.
033 "TFP Growth in Old and New Europe" by Michael C. Burda and Battista Severgnini, June 2009.
034 "How does entry regulation influence entry into self-employment and occupational mobility?" by Susanne Prantl and Alexandra Spitz-Oener, June 2009.
035 "Trade-Off Between Consumption Growth and Inequality: Theory and Evidence for Germany" by Runli Xie, June 2009.
036 "Inflation and Growth: New Evidence From a Dynamic Panel Threshold Analysis" by Stephanie Kremer, Alexander Bick and Dieter Nautz, July 2009.
037 "The Impact of the European Monetary Union on Inflation Persistence in the Euro Area" by Barbara Meller and Dieter Nautz, July 2009.
038 "CDO and HAC" by Barbara Choro, Wolfgang Härdle and Ostap Okhrin, July 2009.
039 "Regulation and Investment in Network Industries: Evidence from European Telecoms" by Michal Grajek and Lars-Hendrik Röller, July 2009.
040 "The Political Economy of Regulatory Risk" by Roland Strausz, August 2009.
041 "Shape invariant modelling pricing kernels and risk aversion" by Maria Grith, Wolfgang Härdle and Juhyun Park, August 2009.
042 "The Cost of Tractability and the Calvo Pricing Assumption" by Fang Yao, September 2009.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

SFB 649 Discussion Paper Series 2009
For a complete list of Discussion Papers published by the SFB 649, please visit http://sfb649.wiwi.hu-berlin.de.
043 "Evidence on Unemployment, Market Work and Household Production" by Michael C. Burda and Daniel S. Hamermesh, September 2009.
044 "Modelling and Forecasting Liquidity Supply Using Semiparametric Factor Dynamics" by Wolfgang Karl Härdle, Nikolaus Hautsch and Andrija Mihoci, September 2009.
045 "Quantifizierbarkeit von Risiken auf Finanzmärkten" by Wolfgang Karl Härdle and Christian Wolfgang Friedrich Kirchner, October 2009.
SFB 649, Spandauer Straße 1, D-10178 Berlin http://sfb649.wiwi.hu-berlin.de
This research was supported by the Deutsche Forschungsgemeinschaft through the SFB 649 "Economic Risk".

